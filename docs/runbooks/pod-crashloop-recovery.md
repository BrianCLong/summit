# ðŸ”„ Pod CrashLoopBackOff Recovery Runbook

**Severity:** High
**MTTR Target:** < 15 minutes
**Escalation:** After 10 minutes

## ðŸ” Symptoms

- Pods stuck in `CrashLoopBackOff` state
- Grafana alert: `SaturationHigh` or `ThroughputDrop`
- Application metrics dropping to zero
- Kubernetes events showing repeated container restarts

## âš¡ Immediate Triage (0-5 minutes)

### 1. Identify Affected Pods

```bash
# List crashlooping pods
kubectl get pods -n intelgraph-prod | grep -E "(CrashLoopBackOff|Error|Pending)"

# Get detailed pod status
kubectl describe pod -n intelgraph-prod $(kubectl get pods -n intelgraph-prod -o name | grep intelgraph | head -1)
```

### 2. Check Recent Changes

```bash
# Recent deployments
kubectl rollout history deployment/intelgraph -n intelgraph-prod

# Recent configmap/secret changes
kubectl get events -n intelgraph-prod --sort-by='.lastTimestamp' | head -20
```

### 3. Extract Container Logs

```bash
# Current container logs
kubectl logs -n intelgraph-prod deployment/intelgraph --tail=50

# Previous container logs (if crashed)
kubectl logs -n intelgraph-prod deployment/intelgraph --previous --tail=50
```

## ðŸ”§ Diagnostic Categories

### Application Startup Issues

**Common Patterns:**

- Database connection failures
- Missing environment variables
- Configuration parsing errors
- Memory allocation failures

```bash
# Check environment variables
kubectl exec -n intelgraph-prod deployment/intelgraph -- env | grep -E "(DATABASE|REDIS|NEO4J)"

# Validate configuration
kubectl get configmap -n intelgraph-prod intelgraph-config -o yaml
kubectl get secret -n intelgraph-prod intelgraph-secrets -o yaml
```

### Resource Constraints

**OOMKilled Pattern:**

```bash
# Check resource limits
kubectl describe pod -n intelgraph-prod $(kubectl get pods -n intelgraph-prod -o name | grep intelgraph | head -1) | grep -A10 "Containers:"

# Node memory pressure
kubectl describe nodes | grep -A5 "MemoryPressure"
```

### Dependency Failures

**Database Issues:**

```bash
# PostgreSQL connectivity
kubectl exec -n intelgraph-prod deployment/postgres -- pg_isready

# Neo4j status
kubectl exec -n intelgraph-prod deployment/neo4j -- cypher-shell "RETURN 1" || echo "Neo4j unreachable"

# Redis connectivity
kubectl exec -n intelgraph-prod deployment/redis -- redis-cli ping || echo "Redis unreachable"
```

## ðŸ› ï¸ Resolution Strategies

### Quick Fixes (5-10 minutes)

#### 1. Rollback to Previous Version

```bash
# Rollback to last stable deployment
kubectl rollout undo deployment/intelgraph -n intelgraph-prod

# Monitor rollback progress
kubectl rollout status deployment/intelgraph -n intelgraph-prod --timeout=300s
```

#### 2. Resource Scaling

```bash
# Increase memory limits for OOMKilled pods
kubectl patch deployment intelgraph -n intelgraph-prod --patch='
{
  "spec": {
    "template": {
      "spec": {
        "containers": [{
          "name": "intelgraph",
          "resources": {
            "limits": {"memory": "4Gi", "cpu": "2000m"},
            "requests": {"memory": "2Gi", "cpu": "1000m"}
          }
        }]
      }
    }
  }
}'
```

#### 3. Configuration Fixes

```bash
# Reset to known-good configuration
kubectl apply -f k8s/overlays/production/configmap.yml
kubectl apply -f k8s/overlays/production/secrets.yml

# Restart deployment to pick up changes
kubectl rollout restart deployment/intelgraph -n intelgraph-prod
```

### Advanced Recovery (10-15 minutes)

#### 1. Dependency Service Recovery

```bash
# Restart database services
kubectl rollout restart deployment/postgres -n intelgraph-prod
kubectl rollout restart deployment/neo4j -n intelgraph-prod
kubectl rollout restart deployment/redis -n intelgraph-prod

# Wait for services to be ready
kubectl wait --for=condition=available --timeout=300s deployment/postgres -n intelgraph-prod
```

#### 2. Network Policy Reset

```bash
# Temporarily disable network policies for debugging
kubectl delete networkpolicy --all -n intelgraph-prod

# Test connectivity
kubectl exec -n intelgraph-prod deployment/intelgraph -- nslookup postgres.intelgraph-prod.svc.cluster.local
```

#### 3. Image Pull Issues

```bash
# Check image pull status
kubectl describe pod -n intelgraph-prod $(kubectl get pods -n intelgraph-prod -o name | grep intelgraph | head -1) | grep -A5 "Events:"

# Force re-pull latest image
kubectl patch deployment intelgraph -n intelgraph-prod -p='
{
  "spec": {
    "template": {
      "metadata": {
        "annotations": {
          "kubectl.kubernetes.io/restartedAt": "'$(date +%Y-%m-%dT%H:%M:%S%z)'"
        }
      }
    }
  }
}'
```

## ðŸ“Š Verification Steps

```bash
# Verify pods are running
kubectl get pods -n intelgraph-prod -l app=intelgraph

# Check readiness probes
kubectl get pods -n intelgraph-prod -l app=intelgraph -o wide

# Test application endpoints
curl -f https://api.intelgraph.ai/health
curl -f https://api.intelgraph.ai/metrics

# Monitor for sustained recovery
watch -n 5 'kubectl get pods -n intelgraph-prod -l app=intelgraph'
```

## ðŸš¨ Escalation Path

### 5 minutes: No pods running

- Scale down to 1 replica for faster troubleshooting
- Page on-call engineer

### 10 minutes: Still failing

- Escalate to senior SRE
- Consider activating backup environment

### 15 minutes: Critical outage

- Engage incident commander
- Activate disaster recovery procedures

## ðŸ” Common Root Causes

1. **Memory Leaks** â†’ Increase limits + schedule restart
2. **Database Migration Failures** â†’ Rollback migration
3. **Config Drift** â†’ Apply infrastructure as code
4. **Dependency Version Conflicts** â†’ Pin versions in Dockerfile
5. **Resource Exhaustion** â†’ Node scaling + resource optimization

## ðŸ“ˆ Prevention

1. **Resource Monitoring** â†’ Set up memory usage alerts at 80%
2. **Deployment Gates** â†’ Require health checks before rollout
3. **Canary Deployments** â†’ Deploy to 10% of pods first
4. **Chaos Engineering** â†’ Regular pod failure testing

**Runbook Owner:** Platform Team
**Last Updated:** September 23, 2025
