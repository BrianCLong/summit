apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-validation
  namespace: database
  labels:
    app.kubernetes.io/name: backup-validation
    app.kubernetes.io/component: backup
    app.kubernetes.io/part-of: intelgraph
spec:
  schedule: "0 4 * * 0"  # Weekly on Sunday at 4 AM UTC
  timeZone: "UTC"
  successfulJobsHistoryLimit: 4
  failedJobsHistoryLimit: 2
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 600
  jobTemplate:
    spec:
      activeDeadlineSeconds: 14400  # 4 hour timeout for full validation
      backoffLimit: 1
      template:
        metadata:
          labels:
            app.kubernetes.io/name: backup-validation
            app.kubernetes.io/component: backup
        spec:
          restartPolicy: Never
          securityContext:
            runAsNonRoot: true
            runAsUser: 65532
            runAsGroup: 65532
            fsGroup: 65532
          serviceAccountName: backup-validation
          containers:
          - name: backup-validator
            image: ghcr.io/brianlong/intelgraph/backup-tools:latest
            imagePullPolicy: IfNotPresent
            command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail
              
              VALIDATION_NAME="backup-validation-$(date +%Y%m%d-%H%M%S)"
              VALIDATION_DIR="/validations/${VALIDATION_NAME}"
              VALIDATION_LOG="${VALIDATION_DIR}/validation.log"
              
              echo "Starting backup validation: ${VALIDATION_NAME}"
              
              # Create validation directory
              mkdir -p "${VALIDATION_DIR}"
              
              # Initialize validation report
              cat > "${VALIDATION_DIR}/validation-report.json" << EOF
              {
                "validation_name": "${VALIDATION_NAME}",
                "start_time": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                "status": "in_progress",
                "tests": {}
              }
              EOF
              
              # Function to log with timestamp
              log() {
                echo "$(date -u +%Y-%m-%dT%H:%M:%SZ) $1" | tee -a "${VALIDATION_LOG}"
              }
              
              # Function to update validation report
              update_report() {
                local test_name="$1"
                local status="$2"
                local details="$3"
                
                jq --arg test "$test_name" --arg status "$status" --arg details "$details" \
                  '.tests[$test] = {"status": $status, "details": $details, "timestamp": now | strftime("%Y-%m-%dT%H:%M:%SZ")}' \
                  "${VALIDATION_DIR}/validation-report.json" > "${VALIDATION_DIR}/validation-report.tmp" && \
                  mv "${VALIDATION_DIR}/validation-report.tmp" "${VALIDATION_DIR}/validation-report.json"
              }
              
              # Test 1: Validate backup availability and integrity
              log "Testing backup availability..."
              
              # Check if recent backups exist
              RECENT_NEO4J=$(aws s3 ls "s3://${BACKUP_BUCKET}/neo4j/" --recursive | tail -1 | awk '{print $4}')
              RECENT_POSTGRES=$(aws s3 ls "s3://${BACKUP_BUCKET}/postgres/" --recursive | tail -1 | awk '{print $4}')
              RECENT_REDIS=$(aws s3 ls "s3://${BACKUP_BUCKET}/redis/" --recursive | tail -1 | awk '{print $4}')
              
              if [[ -n "$RECENT_NEO4J" && -n "$RECENT_POSTGRES" && -n "$RECENT_REDIS" ]]; then
                update_report "backup_availability" "passed" "All recent backups found: Neo4j: $RECENT_NEO4J, PostgreSQL: $RECENT_POSTGRES, Redis: $RECENT_REDIS"
                log "✅ Backup availability test passed"
              else
                update_report "backup_availability" "failed" "Missing recent backups"
                log "❌ Backup availability test failed"
              fi
              
              # Test 2: Download and verify backup integrity
              log "Testing backup integrity..."
              
              mkdir -p "${VALIDATION_DIR}/downloads"
              
              # Download latest Neo4j backup
              if [[ -n "$RECENT_NEO4J" ]]; then
                aws s3 cp "s3://${BACKUP_BUCKET}/${RECENT_NEO4J}" "${VALIDATION_DIR}/downloads/neo4j-backup.tar.gz"
                if tar -tzf "${VALIDATION_DIR}/downloads/neo4j-backup.tar.gz" >/dev/null 2>&1; then
                  update_report "neo4j_integrity" "passed" "Neo4j backup archive is valid"
                  log "✅ Neo4j backup integrity test passed"
                else
                  update_report "neo4j_integrity" "failed" "Neo4j backup archive is corrupted"
                  log "❌ Neo4j backup integrity test failed"
                fi
              fi
              
              # Download latest PostgreSQL backup
              if [[ -n "$RECENT_POSTGRES" ]]; then
                aws s3 cp "s3://${BACKUP_BUCKET}/${RECENT_POSTGRES}" "${VALIDATION_DIR}/downloads/postgres-backup.tar.gz"
                if tar -tzf "${VALIDATION_DIR}/downloads/postgres-backup.tar.gz" >/dev/null 2>&1; then
                  update_report "postgres_integrity" "passed" "PostgreSQL backup archive is valid"
                  log "✅ PostgreSQL backup integrity test passed"
                else
                  update_report "postgres_integrity" "failed" "PostgreSQL backup archive is corrupted"
                  log "❌ PostgreSQL backup integrity test failed"
                fi
              fi
              
              # Test 3: Cross-region replication verification
              log "Testing cross-region replication..."
              
              DR_NEO4J=$(aws s3 ls "s3://${DR_BACKUP_BUCKET}/neo4j/" --recursive --region ${DR_AWS_REGION} | tail -1 | awk '{print $4}' || echo "")
              DR_POSTGRES=$(aws s3 ls "s3://${DR_BACKUP_BUCKET}/postgres/" --recursive --region ${DR_AWS_REGION} | tail -1 | awk '{print $4}' || echo "")
              
              if [[ -n "$DR_NEO4J" && -n "$DR_POSTGRES" ]]; then
                update_report "cross_region_replication" "passed" "DR backups found in secondary region"
                log "✅ Cross-region replication test passed"
              else
                update_report "cross_region_replication" "failed" "Missing DR backups in secondary region"
                log "❌ Cross-region replication test failed"
              fi
              
              # Test 4: Backup metadata validation
              log "Testing backup metadata..."
              
              # Extract and validate backup manifests
              tar -xzf "${VALIDATION_DIR}/downloads/neo4j-backup.tar.gz" -C "${VALIDATION_DIR}/downloads/" || true
              tar -xzf "${VALIDATION_DIR}/downloads/postgres-backup.tar.gz" -C "${VALIDATION_DIR}/downloads/" || true
              
              # Find manifest files
              NEO4J_MANIFEST=$(find "${VALIDATION_DIR}/downloads" -name "*backup-manifest.json" -path "*/neo4j*" | head -1)
              POSTGRES_MANIFEST=$(find "${VALIDATION_DIR}/downloads" -name "*backup-manifest.json" -path "*/postgres*" | head -1)
              
              MANIFEST_VALID=true
              
              if [[ -f "$NEO4J_MANIFEST" ]]; then
                if jq empty "$NEO4J_MANIFEST" 2>/dev/null; then
                  log "✅ Neo4j manifest is valid JSON"
                else
                  MANIFEST_VALID=false
                  log "❌ Neo4j manifest is invalid JSON"
                fi
              else
                MANIFEST_VALID=false
                log "❌ Neo4j manifest not found"
              fi
              
              if [[ -f "$POSTGRES_MANIFEST" ]]; then
                if jq empty "$POSTGRES_MANIFEST" 2>/dev/null; then
                  log "✅ PostgreSQL manifest is valid JSON"
                else
                  MANIFEST_VALID=false
                  log "❌ PostgreSQL manifest is invalid JSON"
                fi
              else
                MANIFEST_VALID=false
                log "❌ PostgreSQL manifest not found"
              fi
              
              if $MANIFEST_VALID; then
                update_report "metadata_validation" "passed" "All backup manifests are valid"
              else
                update_report "metadata_validation" "failed" "Some backup manifests are missing or invalid"
              fi
              
              # Test 5: Restore simulation (limited scope for safety)
              log "Testing restore simulation..."
              
              # This is a simplified restore test - in production you'd restore to a test environment
              if [[ -f "$POSTGRES_MANIFEST" ]]; then
                BACKUP_SIZE=$(jq -r '.backup_size_bytes' "$POSTGRES_MANIFEST" 2>/dev/null || echo "0")
                BACKUP_TYPE=$(jq -r '.backup_type' "$POSTGRES_MANIFEST" 2>/dev/null || echo "unknown")
                
                if [[ "$BACKUP_SIZE" -gt "1000" && "$BACKUP_TYPE" == "full" ]]; then
                  update_report "restore_simulation" "passed" "Backup appears restorable (size: $BACKUP_SIZE bytes, type: $BACKUP_TYPE)"
                  log "✅ Restore simulation test passed"
                else
                  update_report "restore_simulation" "failed" "Backup may not be restorable"
                  log "❌ Restore simulation test failed"
                fi
              else
                update_report "restore_simulation" "failed" "Cannot simulate restore without manifest"
                log "❌ Restore simulation test failed"
              fi
              
              # Test 6: Encryption verification
              log "Testing backup encryption..."
              
              # Check S3 encryption metadata
              NEO4J_ENCRYPTION=$(aws s3api head-object --bucket ${BACKUP_BUCKET} --key "${RECENT_NEO4J#s3://${BACKUP_BUCKET}/}" --query 'ServerSideEncryption' --output text 2>/dev/null || echo "None")
              POSTGRES_ENCRYPTION=$(aws s3api head-object --bucket ${BACKUP_BUCKET} --key "${RECENT_POSTGRES#s3://${BACKUP_BUCKET}/}" --query 'ServerSideEncryption' --output text 2>/dev/null || echo "None")
              
              if [[ "$NEO4J_ENCRYPTION" == "AES256" && "$POSTGRES_ENCRYPTION" == "AES256" ]]; then
                update_report "encryption_verification" "passed" "All backups are encrypted with AES256"
                log "✅ Encryption verification test passed"
              else
                update_report "encryption_verification" "failed" "Some backups are not properly encrypted"
                log "❌ Encryption verification test failed"
              fi
              
              # Test 7: Retention policy compliance
              log "Testing retention policy compliance..."
              
              # Count backups older than retention period
              OLD_BACKUPS=$(aws s3 ls "s3://${BACKUP_BUCKET}/" --recursive | awk '{if($1 < "'$(date -d "30 days ago" +%Y-%m-%d)'") print $4}' | wc -l)
              
              if [[ "$OLD_BACKUPS" -eq "0" ]]; then
                update_report "retention_compliance" "passed" "No backups older than retention period found"
                log "✅ Retention policy compliance test passed"
              else
                update_report "retention_compliance" "warning" "Found $OLD_BACKUPS backups older than retention period"
                log "⚠️ Retention policy compliance test warning: $OLD_BACKUPS old backups"
              fi
              
              # Finalize validation report
              OVERALL_STATUS="passed"
              FAILED_TESTS=$(jq -r '.tests | to_entries[] | select(.value.status == "failed") | .key' "${VALIDATION_DIR}/validation-report.json" | wc -l)
              
              if [[ "$FAILED_TESTS" -gt "0" ]]; then
                OVERALL_STATUS="failed"
              fi
              
              jq --arg status "$OVERALL_STATUS" --arg end_time "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
                '. + {"status": $status, "end_time": $end_time, "failed_tests": '${FAILED_TESTS}'}' \
                "${VALIDATION_DIR}/validation-report.json" > "${VALIDATION_DIR}/validation-report.tmp" && \
                mv "${VALIDATION_DIR}/validation-report.tmp" "${VALIDATION_DIR}/validation-report.json"
              
              # Upload validation results
              log "Uploading validation results..."
              aws s3 cp "${VALIDATION_DIR}/validation-report.json" \
                "s3://${BACKUP_BUCKET}/validation/$(date +%Y/%m/%d)/${VALIDATION_NAME}-report.json" \
                --server-side-encryption AES256
              
              aws s3 cp "${VALIDATION_LOG}" \
                "s3://${BACKUP_BUCKET}/validation/$(date +%Y/%m/%d)/${VALIDATION_NAME}.log" \
                --server-side-encryption AES256
              
              # Send notification
              if [[ "$OVERALL_STATUS" == "passed" ]]; then
                EMOJI="✅"
                COLOR="good"
              else
                EMOJI="❌"
                COLOR="danger"
              fi
              
              curl -X POST "${SLACK_WEBHOOK_URL}" \
                -H 'Content-Type: application/json' \
                -d "{
                  \"text\": \"${EMOJI} Backup Validation Report: ${VALIDATION_NAME}\",
                  \"attachments\": [{
                    \"color\": \"${COLOR}\",
                    \"fields\": [
                      {\"title\": \"Status\", \"value\": \"${OVERALL_STATUS}\", \"short\": true},
                      {\"title\": \"Failed Tests\", \"value\": \"${FAILED_TESTS}\", \"short\": true},
                      {\"title\": \"Report\", \"value\": \"s3://${BACKUP_BUCKET}/validation/$(date +%Y/%m/%d)/${VALIDATION_NAME}-report.json\", \"short\": false}
                    ]
                  }]
                }" || true
              
              log "Backup validation completed: ${OVERALL_STATUS} (${FAILED_TESTS} failed tests)"
              
              # Exit with error code if validation failed
              if [[ "$OVERALL_STATUS" == "failed" ]]; then
                exit 1
              fi
              
            env:
            - name: BACKUP_BUCKET
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: backup-bucket
            - name: DR_BACKUP_BUCKET
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: dr-backup-bucket
            - name: AWS_REGION
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: aws-region
            - name: DR_AWS_REGION
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: dr-aws-region
            - name: SLACK_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: backup-secrets
                  key: slack-webhook-url
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: backup-secrets
                  key: aws-access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: backup-secrets
                  key: aws-secret-access-key
            resources:
              requests:
                memory: "1Gi"
                cpu: "500m"
              limits:
                memory: "4Gi"
                cpu: "2000m"
            volumeMounts:
            - name: validation-storage
              mountPath: /validations
          volumes:
          - name: validation-storage
            emptyDir:
              sizeLimit: 50Gi
          tolerations:
          - key: "backup-workload"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"
          nodeSelector:
            node-type: "backup-optimized"
---
# Restore testing job (manual trigger)
apiVersion: batch/v1
kind: Job
metadata:
  name: restore-test-template
  namespace: database
  labels:
    app.kubernetes.io/name: restore-test
    app.kubernetes.io/component: backup
spec:
  template:
    spec:
      restartPolicy: Never
      securityContext:
        runAsNonRoot: true
        runAsUser: 65532
        runAsGroup: 65532
      serviceAccountName: backup-validation
      containers:
      - name: restore-tester
        image: ghcr.io/brianlong/intelgraph/backup-tools:latest
        command:
        - /bin/bash
        - -c
        - |
          set -euo pipefail
          
          RESTORE_TEST_NAME="restore-test-$(date +%Y%m%d-%H%M%S)"
          TEST_DIR="/restore-tests/${RESTORE_TEST_NAME}"
          
          echo "Starting restore test: ${RESTORE_TEST_NAME}"
          
          mkdir -p "${TEST_DIR}"
          
          # This is a template for manual restore testing
          # In practice, this would:
          # 1. Create isolated test environments
          # 2. Restore backups to test instances
          # 3. Verify data integrity and completeness
          # 4. Run application smoke tests
          # 5. Clean up test resources
          
          echo "Restore test template created. Customize for specific restore scenarios."
          
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        volumeMounts:
        - name: restore-storage
          mountPath: /restore-tests
      volumes:
      - name: restore-storage
        emptyDir:
          sizeLimit: 100Gi