// ────────────────────────────────────────────────────────────────────────────────
// Agent Workbench Backend — Node/Apollo (MC‑aligned)
// Persisted‑only • Audit‑logged • Multi‑LLM router • Policy simulation (OPA)
// ────────────────────────────────────────────────────────────────────────────────
// Project layout (single‑file preview). Suggested real layout:
//  /src/server.ts                → Express + Apollo bootstrap
//  /src/routes/workbench.ts      → REST routes (chat, pq, policy, audit)
//  /src/lib/persisted.ts         → persisted query index loader
//  /src/lib/policy.ts            → OPA simulate client
//  /src/lib/audit.ts             → audit sink (console/file/http)
//  /src/lib/llm/router.ts        → provider router (OpenAI/Anthropic/Generic)
//  /src/lib/llm/providers/*      → concrete provider adapters
//  /src/types.ts                 → shared types
//  /config/persisted-queries.json → generated by CI (mc pq hash)
//  /README.md                    → integration notes (Zed, Claude Code CLI, GH Speckit)
// ────────────────────────────────────────────────────────────────────────────────

// package.json (snippet)
/*
{
  "name": "agent-workbench-backend",
  "version": "0.1.0",
  "type": "module",
  "scripts": {
    "dev": "tsx watch src/server.ts",
    "start": "node dist/server.js",
    "build": "tsc -p .",
    "lint": "eslint .",
    "test": "jest"
  },
  "dependencies": {
    "express": "^4.19.2",
    "cors": "^2.8.5",
    "helmet": "^7.1.0",
    "body-parser": "^1.20.2",
    "jsonwebtoken": "^9.0.2",
    "node-fetch": "^3.3.2",
    "winston": "^3.11.0",
    "zod": "^3.23.8",
    "graphql": "^16.9.0",
    "@apollo/server": "^4.11.0",
    "@apollo/server/express4": "^4.11.0"
  },
  "devDependencies": {
    "typescript": "^5.4.5",
    "tsx": "^4.16.2",
    "@types/express": "^4.17.21",
    "@types/jsonwebtoken": "^9.0.5",
    "@types/node": "^20.12.12"
  }
}
*/

// src/types.ts
export type Purpose = "investigation"|"threat_intel"|"fraud_risk"|"t_s"|"benchmarking"|"training"|"demo";
export type Residency = "US"|"EU"|"CA"|"OTHER";
export type ChatRequest = {
  tenantId: string; purpose: Purpose; residency: Residency; model: string; pqid: string|null; input: string;
};
export type ChatResponse = {
  messages: { role: "assistant"|"user"; content: string; at: string }[];
  usage: { tokensIn: number; tokensOut: number };
  persisted: boolean;
  policy: { allow: boolean; reasons: string[] };
};

// src/lib/persisted.ts
import fs from "fs";
const PQ_INDEX_PATH = process.env.PQ_INDEX_PATH || "config/persisted-queries.json";
export type PQItem = { id: string; name: string; operation: "query"|"mutation"|"subscription" };
export function loadPersistedIndex(): Record<string, PQItem> {
  try { const raw = fs.readFileSync(PQ_INDEX_PATH, "utf8"); return JSON.parse(raw); } catch { return {}; }
}
export function hasPQ(id?: string|null){ if(!id) return false; const idx=loadPersistedIndex(); return !!idx[id]; }
export function listPQByTenant(_tenant: string): PQItem[] { // simple demo (tenant‑agnostic); extend as needed
  const idx = loadPersistedIndex(); return Object.values(idx);
}

// src/lib/policy.ts
import fetch from "node-fetch";
export async function simulatePolicy(input: {tenantId:string; purpose:Purpose; residency:Residency; pqid:string|null}): Promise<{allow:boolean; reasons:string[]}> {
  const OPA_URL = process.env.OPA_URL; // e.g., http://opa:8181/v1/data/intelgraph/allow
  if (!OPA_URL) return { allow: true, reasons: ["OPA_URL unset → default allow in dev"] };
  try {
    const res = await fetch(OPA_URL, { method: "POST", headers: {"Content-Type":"application/json"}, body: JSON.stringify({ input }) });
    const data = await res.json();
    const allow = !!(data?.result?.allow ?? data?.result === true);
    const reasons = data?.result?.reasons || [];
    return { allow, reasons };
  } catch (e:any) {
    return { allow: false, reasons: ["policy simulate error", e?.message||"unknown"] };
  }
}

// src/lib/audit.ts
import fs2 from "fs"; import path from "path"; import { Request } from "express";
const AUDIT_DIR = process.env.AUDIT_DIR || "./out/audit";
export function audit(event: string, subject: string, tenantId: string, extra: any){
  try { fs2.mkdirSync(AUDIT_DIR, { recursive: true }); } catch {}
  const entry = { ts: new Date().toISOString(), event, subject, tenantId, extra };
  fs2.appendFileSync(path.join(AUDIT_DIR, `audit-${new Date().toISOString().slice(0,10)}.log`), JSON.stringify(entry)+"\n");
}
export function tenantFromJwt(req: Request): string|undefined {
  const hdr = req.headers["authorization"]; if (!hdr) return undefined;
  try { const token = (hdr as string).replace(/^Bearer\s+/i, ""); const [,payload] = token.split("."); return JSON.parse(Buffer.from(payload, "base64").toString()).tenantId; } catch { return undefined; }
}

// src/lib/llm/providers/openai.ts
import fetch2 from "node-fetch";
export async function openaiChat(prompt:string, model:string){
  const url = process.env.OPENAI_BASE_URL || "https://api.openai.com/v1/chat/completions";
  const key = process.env.OPENAI_API_KEY || "";
  const res = await fetch2(url, { method: "POST", headers: {"Content-Type":"application/json", "Authorization": `Bearer ${key}`}, body: JSON.stringify({ model, messages:[{role:"user", content: prompt}] })});
  const data = await res.json();
  const content = data?.choices?.[0]?.message?.content ?? "(no content)";
  const usage = { tokensIn: data?.usage?.prompt_tokens ?? 0, tokensOut: data?.usage?.completion_tokens ?? 0 };
  return { content, usage };
}

// src/lib/llm/providers/anthropic.ts
import fetch3 from "node-fetch";
export async function anthropicChat(prompt:string, model:string){
  const url = process.env.ANTHROPIC_BASE_URL || "https://api.anthropic.com/v1/messages";
  const key = process.env.ANTHROPIC_API_KEY || "";
  const res = await fetch3(url, { method: "POST", headers: {"Content-Type":"application/json", "x-api-key": key, "anthropic-version": "2023-06-01"}, body: JSON.stringify({ model, max_tokens: 1024, messages:[{role:"user", content: prompt}] })});
  const data = await res.json();
  const content = data?.content?.[0]?.text ?? "(no content)";
  const usage = { tokensIn: data?.usage?.input_tokens ?? 0, tokensOut: data?.usage?.output_tokens ?? 0 };
  return { content, usage };
}

// src/lib/llm/providers/generic.ts
import fetch4 from "node-fetch";
export async function genericOpenAICompatible(prompt:string, model:string){
  const url = process.env.GENERIC_BASE_URL!; // e.g., local OAI‑compatible proxy
  const key = process.env.GENERIC_API_KEY || "";
  const res = await fetch4(url, { method: "POST", headers: {"Content-Type":"application/json", ...(key?{Authorization:`Bearer ${key}`}:{}) }, body: JSON.stringify({ model, messages:[{role:"user", content: prompt}] })});
  const data = await res.json();
  const content = data?.choices?.[0]?.message?.content ?? "(no content)";
  const usage = { tokensIn: data?.usage?.prompt_tokens ?? 0, tokensOut: data?.usage?.completion_tokens ?? 0 };
  return { content, usage };
}

// src/lib/llm/router.ts
import { openaiChat } from "./providers/openai.js";
import { anthropicChat } from "./providers/anthropic.js";
import { genericOpenAICompatible } from "./providers/generic.js";
export async function llmChat(prompt:string, modelRoute:string){
  if (modelRoute.startsWith("gpt")) return openaiChat(prompt, modelRoute);
  if (modelRoute.startsWith("claude")) return anthropicChat(prompt, modelRoute);
  if (modelRoute === "mixed-router") {
    // trivial demo: route short prompts to openai, long to anthropic
    return prompt.length < 600 ? openaiChat(prompt, process.env.DEFAULT_OPENAI_MODEL || "gpt-4o-mini") : anthropicChat(prompt, process.env.DEFAULT_ANTHROPIC_MODEL || "claude-3-5-sonnet-20240620");
  }
  return genericOpenAICompatible(prompt, modelRoute);
}

// src/routes/workbench.ts
import express from "express"; import { z } from "zod";
import { listPQByTenant, hasPQ } from "../lib/persisted.js";
import { simulatePolicy } from "../lib/policy.js";
import { audit, tenantFromJwt } from "../lib/audit.js";
import { llmChat } from "../lib/llm/router.js";
export const router = express.Router();

router.get("/workbench/pq", (req, res)=>{
  const tenantId = (req.query.tenantId as string) || tenantFromJwt(req) || "unknown";
  const items = listPQByTenant(tenantId);
  res.json({ items });
});

const ChatSchema = z.object({ tenantId:z.string(), purpose:z.string(), residency:z.string(), model:z.string(), pqid:z.string().nullable(), input:z.string().min(1) });
router.post("/workbench/chat", async (req, res)=>{
  const parsed = ChatSchema.safeParse(req.body); if(!parsed.success) return res.status(400).json({ error: parsed.error.flatten() });
  const { tenantId, purpose, residency, model, pqid, input } = parsed.data;
  const persisted = hasPQ(pqid);
  if (process.env.PERSISTED_ONLY === "true" && !persisted) {
    audit("chat.blocked.persistedOnly", "agent.workbench", tenantId, { pqid });
    return res.json({ messages:[], usage:{tokensIn:0, tokensOut:0}, persisted:false, policy:{allow:false, reasons:["persisted‑only enforced"]} });
  }
  const policy = await simulatePolicy({ tenantId, purpose: purpose as any, residency: residency as any, pqid });
  if (!policy.allow) {
    audit("chat.blocked.policyDeny", "agent.workbench", tenantId, { pqid, policy });
    return res.json({ messages:[], usage:{tokensIn:0, tokensOut:0}, persisted, policy });
  }
  const { content, usage } = await llmChat(input, model);
  const reply = { role:"assistant" as const, content, at: new Date().toISOString() };
  audit("chat.send", "agent.workbench", tenantId, { pqid, model, usage, policy });
  res.json({ messages:[reply], usage, persisted, policy });
});

router.post("/policy/simulate", async (req, res)=>{
  const { tenantId, purpose, residency, pqid } = req.body || {};
  const decision = await simulatePolicy({ tenantId, purpose, residency, pqid });
  res.json(decision);
});

router.post("/audit/log", (req, res)=>{
  const { event, subject, tenantId, extra } = req.body || {};
  audit(event||"event", subject||"agent.workbench", tenantId||tenantFromJwt(req)||"unknown", extra||{});
  res.json({ ok: true });
});

// src/server.ts
import express2 from "express"; import helmet from "helmet"; import cors from "cors"; import bodyParser from "body-parser";
import { ApolloServer } from "@apollo/server"; import { expressMiddleware } from "@apollo/server/express4";
import { router as workbench } from "./routes/workbench.js";

const app = express2();
app.use(helmet());
app.use(cors({ origin: true }));
app.use(bodyParser.json({ limit: "1mb" }));
app.use("/api", workbench);

// Minimal GraphQL (optional) — prove API health & future ops
const typeDefs = `#graphql
  type Query { health: String! }
`;
const resolvers = { Query: { health: ()=>"ok" } };
const apollo = new ApolloServer({ typeDefs, resolvers });
await apollo.start();
app.use("/graphql", expressMiddleware(apollo));

const port = Number(process.env.PORT||"8080");
app.listen(port, ()=> console.log(`[agent-workbench-backend] listening on :${port}`));

// config/persisted-queries.json (example; replace with CI‑generated index)
/*
{
  "pq.getPersonById": { "id": "pq.getPersonById", "name": "Get Person By Id", "operation": "query" },
  "pq.searchEntitiesByName": { "id": "pq.searchEntitiesByName", "name": "Search Entities By Name", "operation": "query" },
  "pq.privacyContext": { "id": "pq.privacyContext", "name": "Privacy Context", "operation": "query" }
}
*/

// README.md — Integration notes (Zed, Claude Code CLI, GH Speckit)
/*
# Agent Workbench Backend — Integrations

## Any‑LLM via Router ("ter" → adapter router)
- Configure env routes:
  - OPENAI_API_KEY / OPENAI_BASE_URL
  - ANTHROPIC_API_KEY / ANTHROPIC_BASE_URL
  - GENERIC_BASE_URL / GENERIC_API_KEY (for OpenAI‑compatible endpoints)
- Client passes `model` (e.g., `gpt-4o-mini`, `claude-3-5-sonnet-20240620`, or a generic model id); router picks adapter.

## Zed (zed.dev) — preferred dev UX
- Use Zed’s live problems & LSP to watch this repo; pair it with the Workbench UI to see governed agent actions.
- Benefit: transparent agent behavior; not over‑complex like GH Speckit when retrofitting into existing codebases.

## Claude Code CLI (CC) — best CLI agent for GLM/Anthropic endpoints
- Point CC to this backend as a *governed* proxy by exporting PURPOSE/RESIDENCY/TENANT envs and using persisted queries.
- Example:
  ```bash
  export TENANT_ID=TENANT_001 PURPOSE=investigation RESIDENCY=US
  cc --model claude-3-5-sonnet-20240620 --prompt "Refactor module X with tests." \
     | curl -sS -X POST http://localhost:8080/api/workbench/chat \
         -H 'Content-Type: application/json' \
         -d @<(jq -n --arg t "$TENANT_ID" --arg p "$PURPOSE" --arg r "$RESIDENCY" --arg m "claude-3-5-sonnet-20240620" --arg pq "pq.getPersonById" --arg i "$(cat -)" '{tenantId:$t,purpose:$p,residency:$r,model:$m,pqid:$pq,input:$i}')
  ```

## GH Speckit — great for greenfield, tricky for retrofits
- Use Speckit to *start* new services; when integrating with existing complex codebases, keep it at the edge:
  - Generate initial scaffolds/specs; import selective parts under PR and run policy sim.
  - Always route through this backend → persisted‑only + policy sim + audit.

## Security & Policy
- Set `PERSISTED_ONLY=true` in prod; wire OPA at `OPA_URL`.
- JWT should include `tenantId`, `purpose`, `residency`; server will trust‑but‑verify with OPA.
- All requests are audit‑logged to `out/audit/` (swap to SIEM or HTTP sink in `lib/audit.ts`).

*/
