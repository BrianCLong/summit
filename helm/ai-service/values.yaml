image:
  repository: ghcr.io/BrianCLong/intelgraph/ai-service
  tag: ''
  pullPolicy: IfNotPresent

replicaCount: 1

service:
  type: ClusterIP
  port: 8080

env: []

resources:
  requests:
    cpu: 200m
    memory: 512Mi
  limits:
    cpu: 2000m
    memory: 4Gi

ingress:
  enabled: false

hpa:
  enabled: true
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: External
      external:
        metric:
          name: dcgm_gpu_utilization
          selector:
            matchLabels:
              component: gpu
              job: gpu-telemetry
        target:
          type: AverageValue
          averageValue: "70"
    - type: External
      external:
        metric:
          name: ml_inference_requests_per_second
          selector:
            matchLabels:
              app: intelgraph-ml
              component: ml-service
        target:
          type: AverageValue
          averageValue: "40"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      selectPolicy: Max
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
        - type: Pods
          value: 4
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 180
      selectPolicy: Max
      policies:
        - type: Percent
          value: 50
          periodSeconds: 120
  # Legacy knobs retained for backward compatibility with chart consumers that
  # have not migrated to the declarative metrics block above.
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  targetGPUUtilizationPercentage: null

networkPolicy:
  enabled: true
  allowNamespaces: []

keda:
  enabled: false
  redisAddress: redis-master.redis.svc:6379
  queueName: intelgraph
  queueLength: 50

istio:
  enabled: true

serviceMonitor:
  enabled: true
  interval: 30s
  portName: http

pdb:
  enabled: true
  minAvailable: 1

# Example GPU scheduling configuration
# nodeSelector:
#   accelerator: nvidia-tesla-k80
# tolerations:
#   - key: nvidia.com/gpu
#     operator: Exists
#     effect: NoSchedule
# affinity:
#   nodeAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       nodeSelectorTerms:
#         - matchExpressions:
#             - key: accelerator
#               operator: In
#               values:
#                 - nvidia-tesla-k80
nodeSelector: {}
tolerations: []
affinity: {}
