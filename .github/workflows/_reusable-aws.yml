name: Reusable AWS Operations

on:
  workflow_call:
    inputs:
      operation:
        description: "AWS operation to perform (upload, deploy, sync)"
        required: true
        type: string
      source_path:
        description: "Source path for files"
        required: false
        type: string
        default: "."
      s3_path:
        description: "S3 path for operations"
        required: false
        type: string
      aws_region:
        description: "AWS region"
        required: false
        type: string
        default: "us-east-1"
      has_aws:
        description: "Whether AWS credentials are available"
        required: false
        type: boolean
        default: true
      role-to-assume:
        description: "IAM Role to assume (OIDC)"
        required: false
        type: string
      role-session-name:
        description: "Role session name"
        required: false
        type: string
        default: "GitHubActions"
    secrets:
      AWS_REGION:
        description: "AWS Region"
        required: false
      S3_BUCKET:
        description: "S3 Bucket name"
        required: false

env:
  AWS_DEFAULT_REGION: ${{ secrets.AWS_REGION || inputs.aws_region }}
  S3_BUCKET: ${{ secrets.S3_BUCKET }}
  AWS_ROLE_TO_ASSUME: ${{ inputs.role-to-assume }}

jobs:
  aws-operation:
    name: AWS ${{ inputs.operation }}
    runs-on: ubuntu-latest
    if: ${{ inputs.has_aws }}
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}
          role-session-name: ${{ inputs.role-session-name }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}
          mask-aws-account-id: true

      - name: S3 Upload
        if: ${{ inputs.operation == 'upload' && env.AWS_ROLE_TO_ASSUME != '' }}
        run: |
          if [ -n "${{ inputs.s3_path }}" ] && [ -n "${{ env.S3_BUCKET }}" ]; then
            aws s3 cp "${{ inputs.source_path }}" "s3://${{ env.S3_BUCKET }}/${{ inputs.s3_path }}" --recursive
            echo "✅ Files uploaded to s3://${{ env.S3_BUCKET }}/${{ inputs.s3_path }}"
          else
            echo "❌ S3 path or bucket not specified"
            exit 1
          fi

      - name: S3 Sync
        if: ${{ inputs.operation == 'sync' && env.AWS_ROLE_TO_ASSUME != '' }}
        run: |
          if [ -n "${{ inputs.s3_path }}" ] && [ -n "${{ env.S3_BUCKET }}" ]; then
            aws s3 sync "${{ inputs.source_path }}" "s3://${{ env.S3_BUCKET }}/${{ inputs.s3_path }}" --delete
            echo "✅ Files synced to s3://${{ env.S3_BUCKET }}/${{ inputs.s3_path }}"
          else
            echo "❌ S3 path or bucket not specified"
            exit 1
          fi

      - name: Skip AWS operations
        if: ${{ env.AWS_ROLE_TO_ASSUME == '' }}
        run: |
          echo "⏭️ AWS role not available, skipping ${{ inputs.operation }}"
