name: deploy-preview
on:
  push:
    branches: [ "release/*" ]
env:
  ENVIRONMENT: preview
permissions:
  id-token: write     # <-- required for OIDC
  contents: read
jobs:
  deploy:
    environment: preview
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
      - name: Install dependencies
        run: |
          npm install -g pnpm
          pnpm install --frozen-lockfile

      # OIDC Authentication
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::<acct>:role/summit-preview-deployer
          aws-region: us-east-1

      # Deployment
      - name: Deploy to preview
        run: ./scripts/deploy/preview.sh

      # Parity Scorecard Generation
      - name: Create artifacts directory
        run: mkdir -p artifacts

      - name: Terraform plan (machine-readable)
        run: |
          # In a real environment, you would run:
          # terraform -chdir=terraform/environments/preview init -input=false
          # terraform -chdir=terraform/environments/preview plan -out=plan.bin -input=false
          # terraform -chdir=terraform/environments/preview show -json plan.bin > artifacts/plan.json

          # For demonstration/pattern purposes, we mock an empty plan (clean state)
          echo '{"resource_changes": []}' > artifacts/plan.json

      - name: Capture app parity
        run: |
          ./scripts/flags/export_flags.sh > artifacts/flags.json
          ./scripts/images/list.sh > artifacts/images.json

          # Create the scorecard using jq
          jq -n \
            --arg env "$ENVIRONMENT" \
            --slurpfile plan artifacts/plan.json \
            --slurpfile flags artifacts/flags.json \
            --slurpfile images artifacts/images.json \
            '{timestamp: now|toiso8601, environment:$env, plan:$plan[0], flags:$flags[0], images:$images[0]}' \
            > artifacts/parity-scorecard.json

      # Governance: Archive Evidence
      - name: Archive to Governance Ledger
        run: |
          # Sanitize branch name for directory
          RELEASE_TAG=$(echo $GITHUB_REF_NAME | sed 's/\//-/g')
          TARGET_DIR="docs/ledger/$RELEASE_TAG"
          mkdir -p "$TARGET_DIR"
          cp artifacts/parity-scorecard.json "$TARGET_DIR/"
          cp artifacts/plan.json "$TARGET_DIR/"
          cp artifacts/flags.json "$TARGET_DIR/"
          cp artifacts/images.json "$TARGET_DIR/"

          # Note: To persist this, you would typically use a separate commit or upload to S3/Artifacts
          # echo "Archived evidence to $TARGET_DIR"

      # Governance: Gate Logic
      - name: Fetch Prod Scorecard (Mock)
        run: |
          # In a real scenario, fetch the latest production scorecard from the ledger or S3
          # aws s3 cp s3://summit-ledger/prod/latest-scorecard.json artifacts/prod-scorecard.json

          # Mocking a prod scorecard that matches preview for this pattern demonstration
          cp artifacts/parity-scorecard.json artifacts/prod-scorecard.json
          # Update env to prod to simulate valid comparison
          jq '.environment = "prod"' artifacts/prod-scorecard.json > artifacts/prod-scorecard.json.tmp && mv artifacts/prod-scorecard.json.tmp artifacts/prod-scorecard.json

      - name: Drift policy check
        run: |
          ./scripts/governance/compare_scorecards.mjs \
            --preview artifacts/parity-scorecard.json \
            --prod    artifacts/prod-scorecard.json \
            --policy  policies/parity.yml
