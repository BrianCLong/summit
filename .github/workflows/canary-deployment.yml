name: Canary Deployment

permissions:
  contents: read
  packages: write
  deployments: write

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Version to deploy as canary'
        required: true
        type: string
      canary_percentage:
        description: 'Percentage of traffic for canary (1-50)'
        required: false
        default: '10'
        type: choice
        options:
          - '5'
          - '10'
          - '20'
          - '50'
      environment:
        description: 'Target environment'
        required: true
        type: choice
        options:
          - 'staging'
          - 'production'
      service:
        description: 'Target service'
        required: true
        type: choice
        options:
          - 'maestro'
          - 'intelgraph'
          - 'companyos'
      action:
        description: 'Canary action to perform'
        required: true
        type: choice
        options:
          - 'deploy'
          - 'promote'
          - 'abort'

env:
  REGISTRY: ghcr.io
  CANARY_CONFIG_PATH: summit_release_env_pack/k8s/canary/canary-stage-config.json

concurrency:
  group: canary-${{ inputs.environment }}
  cancel-in-progress: false

jobs:
  validate-inputs:
    runs-on: ubuntu-latest
    outputs:
      version-valid: ${{ steps.validate.outputs.version-valid }}
      namespace: ${{ steps.validate.outputs.namespace }}
      release: ${{ steps.validate.outputs.release }}
      feature-flag: ${{ steps.validate.outputs.feature-flag }}
      weights: ${{ steps.validate.outputs.weights }}
      service-name: ${{ steps.validate.outputs.service-name }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate inputs
        id: validate
        run: |
          SERVICE="${{ inputs.service }}"
          TARGET_ENV="${{ inputs.environment }}"
          export SERVICE TARGET_ENV

          # Validate version format
          if [[ "${{ inputs.version }}" =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "version-valid=true" >> $GITHUB_OUTPUT
          else
            echo "version-valid=false" >> $GITHUB_OUTPUT
            echo "‚ùå Invalid version format. Expected: vX.Y.Z"
            exit 1
          fi

          python - <<'PY'
import json, os, pathlib, sys

config_path = pathlib.Path(os.environ["CANARY_CONFIG_PATH"])
if not config_path.exists():
    sys.exit(f"Missing canary config at {config_path}")

service = os.environ.get("SERVICE")
target_env = os.environ.get("TARGET_ENV")
data = json.loads(config_path.read_text())
service_cfg = data.get("services", {}).get(service)
if not service_cfg:
    sys.exit(f"Service '{service}' not found in {config_path}")

namespaces = service_cfg.get("namespaces", {})
namespace = namespaces.get("production" if target_env == "production" else "staging")
feature_flag = service_cfg.get("featureFlag", f"{service}_canary_enabled")
release = service_cfg.get("release", service)
weights = service_cfg.get("trafficWeights", [])
service_name = service_cfg.get("serviceName", f"{service}-service")

if not namespace:
    sys.exit(f"No namespace configured for env '{target_env}' in {config_path}")

with open(os.environ["GITHUB_OUTPUT"], "a") as fh:
    fh.write(f"namespace={namespace}\n")
    fh.write(f"release={release}\n")
    fh.write(f"feature-flag={feature_flag}\n")
    fh.write(f"weights={','.join(map(str, weights))}\n")
    fh.write(f"service-name={service_name}\n")
PY

          # Validate canary percentage
          if [ "${{ inputs.canary_percentage }}" -gt 50 ]; then
            echo "‚ùå Canary percentage cannot exceed 50%"
            exit 1
          fi

  pre-flight-checks:
    if: inputs.action == 'deploy'
    needs: validate-inputs
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: v1.29.0

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > ~/.kube/config
          chmod 600 ~/.kube/config

      - name: Verify cluster connectivity
        run: |
          kubectl cluster-info
          kubectl get nodes

      - name: Check current deployment health
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          RELEASE="${{ needs.validate-inputs.outputs.release }}"

          # Check if main deployment is healthy
          kubectl get deployment "${RELEASE}" -n ${NAMESPACE}
          kubectl rollout status deployment/"${RELEASE}" -n ${NAMESPACE} --timeout=60s

          # Check if canary already exists
          if kubectl get deployment "${RELEASE}"-canary -n ${NAMESPACE} 2>/dev/null; then
            echo "‚ö†Ô∏è Canary deployment already exists!"
            echo "Please promote or abort existing canary before deploying new one"
            exit 1
          fi

      - name: Verify image exists
        run: |
          # Check if the specified version exists in registry
          docker manifest inspect ${{ env.REGISTRY }}/${{ github.repository }}/server:${{ inputs.version }}
          docker manifest inspect ${{ env.REGISTRY }}/${{ github.repository }}/client:${{ inputs.version }}

  deploy-canary:
    if: inputs.action == 'deploy'
    needs: [validate-inputs, pre-flight-checks]
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.14.0

      - name: Install cosign and conftest
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq
          COSIGN_VERSION="v2.4.0"
          curl -sSL -o cosign.tgz "https://github.com/sigstore/cosign/releases/download/${COSIGN_VERSION}/cosign-linux-amd64.tar.gz"
          tar -xzf cosign.tgz cosign && sudo mv cosign /usr/local/bin/
          curl -sSfL https://github.com/open-policy-agent/conftest/releases/download/v0.56.0/conftest_0.56.0_Linux_x86_64.tar.gz | sudo tar -xz -C /usr/local/bin conftest

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: v1.29.0

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > ~/.kube/config
          chmod 600 ~/.kube/config

      - name: Verify signatures, digests, and attestations
        id: verify-artifacts
        env:
          IMAGE_SERVER: ${{ env.REGISTRY }}/${{ github.repository }}/server:${{ inputs.version }}
          IMAGE_CLIENT: ${{ env.REGISTRY }}/${{ github.repository }}/client:${{ inputs.version }}
        run: |
          set -euo pipefail

          verify_and_pin() {
            local image="$1"
            local name="$2"

            verification=$(cosign verify --output json "$image")
            digest=$(echo "$verification" | jq -r '.[0].critical.image["docker-manifest-digest"]')

            if [ -z "$digest" ] || [ "$digest" = "null" ]; then
              echo "‚ùå Unable to resolve digest after verification for $image" >&2
              exit 1
            fi

            pinned="${image%@*}@${digest}"
            echo "${name}-pinned=${pinned}" >> "$GITHUB_OUTPUT"

            cosign verify-attestation --type slsaprovenance "$pinned"
            cosign verify-attestation --type spdx "$pinned"
          }

          verify_and_pin "$IMAGE_SERVER" server
          verify_and_pin "$IMAGE_CLIENT" client

      - name: Download SBOM attestations
        env:
          IMAGE_SERVER: ${{ steps.verify-artifacts.outputs.server-pinned }}
          IMAGE_CLIENT: ${{ steps.verify-artifacts.outputs.client-pinned }}
        run: |
          set -euo pipefail
          for image in "$IMAGE_SERVER" "$IMAGE_CLIENT"; do
            target=$(echo "$image" | sed 's/[:\/]/-/g')
            cosign download attestation --predicate-type spdx "$image" > "${target}-att.json"
            cat "${target}-att.json" | jq -r '.attestations[0].payload' | base64 -d | jq -r '.predicate' > "${target}-sbom.json"
          done

      - name: Enforce SBOM policy with Conftest
        run: |
          for sbom in *sbom.json; do
            conftest test "$sbom" -p policy/sbom_policy.rego
          done

      - name: Abort canary on guard failure
        if: failure()
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          ./scripts/rollback-deployment.sh canary-abort --namespace ${NAMESPACE} || true

      - name: Deploy canary
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          RELEASE="${{ needs.validate-inputs.outputs.release }}"

          RELEASE_NAME="${RELEASE}" NAMESPACE="${NAMESPACE}" \
            ./scripts/rollback-deployment.sh canary-deploy ${{ inputs.version }} \
            --namespace ${NAMESPACE} \
            --release ${RELEASE} \
            --canary-percent ${{ inputs.canary_percentage }}

      - name: Run canary health checks (SLO Gate)
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          RELEASE="${{ needs.validate-inputs.outputs.release }}"

          # Wait for canary to be ready
          kubectl rollout status deployment/"${RELEASE}"-canary -n ${NAMESPACE} --timeout=300s

          # Run comprehensive health checks
          ./scripts/rollback-deployment.sh health-check --namespace ${NAMESPACE}

          # CHECK SLOs (Simulated - TEMPLATE)
          # TODO: Implement real SLO check using Prometheus or similar.
          # Example: curl -s "http://prometheus/api/v1/query?query=..."
          # For this template, we simulate success.
          echo "Checking SLOs for canary..."
          echo "‚úÖ [TEMPLATE] SLO Check Passed: Error Rate < 1%, p95 Latency < 500ms"

      - name: Update deployment status
        run: |
          echo "üöÄ Canary deployment successful!"
          echo "Service: ${{ inputs.service }}"
          echo "Version: ${{ inputs.version }}"
          echo "Traffic: ${{ inputs.canary_percentage }}%"
          echo "Environment: ${{ inputs.environment }}"

  promote-canary:
    if: inputs.action == 'promote'
    needs: validate-inputs
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup tools
        uses: azure/setup-kubectl@v3
        with:
          version: v1.29.0

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.14.0

      - name: Install cosign and conftest
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq
          COSIGN_VERSION="v2.4.0"
          curl -sSL -o cosign.tgz "https://github.com/sigstore/cosign/releases/download/${COSIGN_VERSION}/cosign-linux-amd64.tar.gz"
          tar -xzf cosign.tgz cosign && sudo mv cosign /usr/local/bin/
          curl -sSfL https://github.com/open-policy-agent/conftest/releases/download/v0.56.0/conftest_0.56.0_Linux_x86_64.tar.gz | sudo tar -xz -C /usr/local/bin conftest

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > ~/.kube/config
          chmod 600 ~/.kube/config

        - name: Verify signatures, digests, and attestations
          id: verify-promote-artifacts
          env:
            IMAGE_SERVER: ${{ env.REGISTRY }}/${{ github.repository }}/server:${{ inputs.version }}
            IMAGE_CLIENT: ${{ env.REGISTRY }}/${{ github.repository }}/client:${{ inputs.version }}
          run: |
            set -euo pipefail

            verify_and_pin() {
              local image="$1"
              local name="$2"

              verification=$(cosign verify --output json "$image")
              digest=$(echo "$verification" | jq -r '.[0].critical.image["docker-manifest-digest"]')

              if [ -z "$digest" ] || [ "$digest" = "null" ]; then
                echo "‚ùå Unable to resolve digest after verification for $image" >&2
                exit 1
              fi

              pinned="${image%@*}@${digest}"
              echo "${name}-pinned=${pinned}" >> "$GITHUB_OUTPUT"

              cosign verify-attestation --type slsaprovenance "$pinned"
              cosign verify-attestation --type spdx "$pinned"
            }

            verify_and_pin "$IMAGE_SERVER" server
            verify_and_pin "$IMAGE_CLIENT" client

        - name: Download SBOM attestations
          env:
            IMAGE_SERVER: ${{ steps.verify-promote-artifacts.outputs.server-pinned }}
            IMAGE_CLIENT: ${{ steps.verify-promote-artifacts.outputs.client-pinned }}
          run: |
            set -euo pipefail
            for image in "$IMAGE_SERVER" "$IMAGE_CLIENT"; do
              target=$(echo "$image" | sed 's/[:\/]/-/g')
              cosign download attestation --predicate-type spdx "$image" > "${target}-att.json"
              cat "${target}-att.json" | jq -r '.attestations[0].payload' | base64 -d | jq -r '.predicate' > "${target}-sbom.json"
            done

      - name: Enforce SBOM policy with Conftest
        run: |
          for sbom in *sbom.json; do
            conftest test "$sbom" -p policy/sbom_policy.rego
          done

      - name: Pre-promote health check (SLO Gate)
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          RELEASE="${{ needs.validate-inputs.outputs.release }}"

          # Verify canary exists and is healthy
          kubectl get deployment "${RELEASE}"-canary -n ${NAMESPACE}
          kubectl rollout status deployment/"${RELEASE}"-canary -n ${NAMESPACE} --timeout=60s

          # Run health checks
          ./scripts/rollback-deployment.sh health-check --namespace ${NAMESPACE}

          # BURN RATE CHECK (Simulated - TEMPLATE)
          # TODO: Implement real burn rate check.
          echo "Checking Burn Rate..."
          echo "‚úÖ [TEMPLATE] Burn Rate within limits."

      - name: Promote canary to production
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          RELEASE="${{ needs.validate-inputs.outputs.release }}"

          RELEASE_NAME="${RELEASE}" ./scripts/rollback-deployment.sh canary-promote --namespace ${NAMESPACE}

      - name: Post-promotion verification
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          RELEASE="${{ needs.validate-inputs.outputs.release }}"

          # Verify promotion
          kubectl rollout status deployment/"${RELEASE}" -n ${NAMESPACE} --timeout=300s

          # Final health check
          ./scripts/rollback-deployment.sh health-check --namespace ${NAMESPACE}

      - name: Update deployment status
        run: |
          echo "‚úÖ Canary promoted to production!"
          echo "Version: ${{ inputs.version }}"
          echo "Traffic: 100%"
          echo "Service: ${{ inputs.service }}"
          echo "Environment: ${{ inputs.environment }}"

  abort-canary:
    if: inputs.action == 'abort'
    needs: validate-inputs
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup tools
        uses: azure/setup-kubectl@v3
        with:
          version: v1.29.0

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.14.0

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > ~/.kube/config
          chmod 600 ~/.kube/config

      - name: Abort canary deployment
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          RELEASE="${{ needs.validate-inputs.outputs.release }}"

          RELEASE_NAME="${RELEASE}" ./scripts/rollback-deployment.sh canary-abort --namespace ${NAMESPACE}

      - name: Verify abort and rollback
        run: |
          NAMESPACE="${{ needs.validate-inputs.outputs.namespace }}"
          RELEASE="${{ needs.validate-inputs.outputs.release }}"

          # Verify canary is removed
          if kubectl get deployment "${RELEASE}"-canary -n ${NAMESPACE} 2>/dev/null; then
            echo "‚ùå Canary deployment still exists!"
            exit 1
          fi

          # Verify stable deployment is healthy
          kubectl rollout status deployment/"${RELEASE}" -n ${NAMESPACE} --timeout=300s

          # Health check
          ./scripts/rollback-deployment.sh health-check --namespace ${NAMESPACE}

      - name: Update deployment status
        run: |
          echo "üîÑ Canary aborted and rolled back!"
          echo "Service: ${{ inputs.service }}"
          echo "Environment: ${{ inputs.environment }}"
          echo "Status: Stable deployment restored"

  notify-teams:
    if: always()
    needs: [deploy-canary, promote-canary, abort-canary]
    runs-on: ubuntu-latest
    steps:
      - name: Determine status
        id: status
        run: |
          if [ "${{ inputs.action }}" = "deploy" ]; then
            if [ "${{ needs.deploy-canary.result }}" = "success" ]; then
              echo "status=üöÄ Canary Deployed" >> $GITHUB_OUTPUT
              echo "color=good" >> $GITHUB_OUTPUT
            else
              echo "status=‚ùå Canary Deploy Failed" >> $GITHUB_OUTPUT
              echo "color=danger" >> $GITHUB_OUTPUT
            fi
          elif [ "${{ inputs.action }}" = "promote" ]; then
            if [ "${{ needs.promote-canary.result }}" = "success" ]; then
              echo "status=‚úÖ Canary Promoted" >> $GITHUB_OUTPUT
              echo "color=good" >> $GITHUB_OUTPUT
            else
              echo "status=‚ùå Canary Promote Failed" >> $GITHUB_OUTPUT
              echo "color=danger" >> $GITHUB_OUTPUT
            fi
          elif [ "${{ inputs.action }}" = "abort" ]; then
            if [ "${{ needs.abort-canary.result }}" = "success" ]; then
              echo "status=üîÑ Canary Aborted" >> $GITHUB_OUTPUT
              echo "color=warning" >> $GITHUB_OUTPUT
            else
              echo "status=‚ùå Canary Abort Failed" >> $GITHUB_OUTPUT
              echo "color=danger" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Notify Slack
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              attachments: [{
                color: '${{ steps.status.outputs.color }}',
                title: 'IntelGraph Canary Deployment',
                fields: [{
                  title: 'Status',
                  value: '${{ steps.status.outputs.status }}',
                  short: true
                }, {
                  title: 'Environment',
                  value: '${{ inputs.environment }}',
                  short: true
                }, {
                  title: 'Service',
                  value: '${{ inputs.service }}',
                  short: true
                }, {
                  title: 'Version',
                  value: '${{ inputs.version }}',
                  short: true
                }, {
                  title: 'Action',
                  value: '${{ inputs.action }}',
                  short: true
                }, {
                  title: 'Workflow',
                  value: '<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>',
                  short: false
                }]
              }]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
