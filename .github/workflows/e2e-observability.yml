name: E2E Observability Pipeline

on:
  workflow_dispatch:
    inputs:
      collect_traces:
        description: 'Collect OpenTelemetry traces'
        type: boolean
        default: true
      benchmark_mode:
        description: 'Run in benchmark mode (multiple iterations)'
        type: boolean
        default: false
  schedule:
    # Run observability metrics collection every 6 hours
    - cron: '0 */6 * * *'
  pull_request:
    paths:
      - 'e2e/**'
      - 'client/tests/**'
      - 'observability/**'
      - '.github/workflows/e2e*.yml'

env:
  OTEL_EXPORTER_OTLP_ENDPOINT: http://localhost:4317
  OTEL_SERVICE_NAME: e2e-test-runner
  METRICS_PUSH_GATEWAY: http://localhost:9091

jobs:
  # ============================================================
  # Stage 1: Initialize Observability Infrastructure
  # ============================================================
  setup-observability:
    name: ðŸ”­ Setup Observability Stack
    runs-on: ubuntu-latest
    outputs:
      run_id: ${{ steps.meta.outputs.run_id }}
      timestamp: ${{ steps.meta.outputs.timestamp }}
    steps:
      - uses: actions/checkout@v4

      - name: Generate run metadata
        id: meta
        run: |
          echo "run_id=${{ github.run_id }}-${{ github.run_attempt }}" >> $GITHUB_OUTPUT
          echo "timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT

      - name: Cache observability configs
        uses: actions/cache@v4
        with:
          path: |
            observability/dashboards
            observability/alerts
          key: observability-${{ hashFiles('observability/**') }}

  # ============================================================
  # Stage 2: E2E Test Execution with Metrics Collection
  # ============================================================
  e2e-with-metrics:
    name: ðŸ“Š E2E Tests with Metrics
    runs-on: ubuntu-latest
    needs: setup-observability
    strategy:
      fail-fast: false
      matrix:
        test_suite:
          - golden-path
          - analytics-bridge
          - graph-visualization
          - real-time-updates
    outputs:
      metrics_artifact: ${{ steps.metrics.outputs.artifact_name }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: npm
          cache-dependency-path: '**/package-lock.json'

      - name: Install dependencies
        run: |
          npm ci --prefix client || true
          npm ci --prefix server || true

      - name: Install Playwright with browsers
        run: |
          cd client
          npx playwright install --with-deps chromium

      - name: Start observability stack
        run: |
          docker compose -f docker-compose.observability.yml up -d prometheus grafana pushgateway otel-collector || true
          sleep 5

      - name: Start application stack
        run: |
          docker compose -f docker-compose.yml up -d postgres neo4j redis server client || true
          # Wait for services
          for i in {1..60}; do
            curl -fsS http://localhost:4000/health && break || sleep 2
          done

      - name: Run E2E tests with metrics collection
        id: e2e_run
        run: |
          START_TIME=$(date +%s)

          # Run tests and capture output
          cd client
          set +e
          npx playwright test --project=chromium --reporter=json,html,line 2>&1 | tee test-output.log
          TEST_EXIT_CODE=${PIPESTATUS[0]}
          set -e

          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))

          # Parse test results
          if [ -f "test-results.json" ]; then
            PASSED=$(jq '.stats.expected // 0' test-results.json 2>/dev/null || echo "0")
            FAILED=$(jq '.stats.unexpected // 0' test-results.json 2>/dev/null || echo "0")
            SKIPPED=$(jq '.stats.skipped // 0' test-results.json 2>/dev/null || echo "0")
            FLAKY=$(jq '.stats.flaky // 0' test-results.json 2>/dev/null || echo "0")
          else
            PASSED=0
            FAILED=0
            SKIPPED=0
            FLAKY=0
          fi

          # Output metrics
          echo "test_suite=${{ matrix.test_suite }}" >> $GITHUB_OUTPUT
          echo "duration=${DURATION}" >> $GITHUB_OUTPUT
          echo "passed=${PASSED}" >> $GITHUB_OUTPUT
          echo "failed=${FAILED}" >> $GITHUB_OUTPUT
          echo "skipped=${SKIPPED}" >> $GITHUB_OUTPUT
          echo "flaky=${FLAKY}" >> $GITHUB_OUTPUT
          echo "exit_code=${TEST_EXIT_CODE}" >> $GITHUB_OUTPUT

          # Push metrics to Prometheus Pushgateway
          cat <<METRICS | curl --data-binary @- http://localhost:9091/metrics/job/e2e_tests/suite/${{ matrix.test_suite }} 2>/dev/null || true
          # HELP e2e_test_duration_seconds Total duration of E2E test suite
          # TYPE e2e_test_duration_seconds gauge
          e2e_test_duration_seconds{suite="${{ matrix.test_suite }}",run_id="${{ needs.setup-observability.outputs.run_id }}"} ${DURATION}
          # HELP e2e_tests_passed_total Number of passed tests
          # TYPE e2e_tests_passed_total gauge
          e2e_tests_passed_total{suite="${{ matrix.test_suite }}"} ${PASSED}
          # HELP e2e_tests_failed_total Number of failed tests
          # TYPE e2e_tests_failed_total gauge
          e2e_tests_failed_total{suite="${{ matrix.test_suite }}"} ${FAILED}
          # HELP e2e_tests_flaky_total Number of flaky tests
          # TYPE e2e_tests_flaky_total gauge
          e2e_tests_flaky_total{suite="${{ matrix.test_suite }}"} ${FLAKY}
          METRICS

          exit $TEST_EXIT_CODE
        env:
          OTEL_TRACES_EXPORTER: otlp
          OTEL_METRICS_EXPORTER: otlp
          PLAYWRIGHT_JSON_OUTPUT_NAME: test-results.json

      - name: Collect browser performance metrics
        if: always()
        run: |
          # Extract performance metrics from Playwright traces
          cd client
          for trace in test-results/**/trace.zip; do
            if [ -f "$trace" ]; then
              unzip -q "$trace" -d trace-extract 2>/dev/null || true
              if [ -f "trace-extract/resources.json" ]; then
                # Parse resource timing
                jq -r '.[] | select(.responseEnd) | "\(.name): \(.responseEnd - .startTime)ms"' trace-extract/resources.json 2>/dev/null | head -20 || true
              fi
              rm -rf trace-extract
            fi
          done

      - name: Generate metrics artifact
        id: metrics
        if: always()
        run: |
          mkdir -p metrics-output

          # Create metrics JSON
          cat > metrics-output/e2e-metrics.json <<EOF
          {
            "run_id": "${{ needs.setup-observability.outputs.run_id }}",
            "timestamp": "${{ needs.setup-observability.outputs.timestamp }}",
            "suite": "${{ matrix.test_suite }}",
            "duration_seconds": ${{ steps.e2e_run.outputs.duration || 0 }},
            "tests": {
              "passed": ${{ steps.e2e_run.outputs.passed || 0 }},
              "failed": ${{ steps.e2e_run.outputs.failed || 0 }},
              "skipped": ${{ steps.e2e_run.outputs.skipped || 0 }},
              "flaky": ${{ steps.e2e_run.outputs.flaky || 0 }}
            },
            "exit_code": ${{ steps.e2e_run.outputs.exit_code || 1 }},
            "git": {
              "sha": "${{ github.sha }}",
              "ref": "${{ github.ref }}",
              "actor": "${{ github.actor }}"
            }
          }
          EOF

          echo "artifact_name=e2e-metrics-${{ matrix.test_suite }}" >> $GITHUB_OUTPUT

      - name: Upload metrics artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-metrics-${{ matrix.test_suite }}
          path: metrics-output/
          retention-days: 30

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.test_suite }}
          path: |
            client/test-results/
            client/playwright-report/
          retention-days: 14

      - name: Cleanup
        if: always()
        run: |
          docker compose -f docker-compose.yml down -v || true
          docker compose -f docker-compose.observability.yml down -v || true

  # ============================================================
  # Stage 3: Aggregate Metrics and Generate Report
  # ============================================================
  aggregate-metrics:
    name: ðŸ“ˆ Aggregate Metrics
    runs-on: ubuntu-latest
    needs: [setup-observability, e2e-with-metrics]
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Download all metrics artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: e2e-metrics-*
          path: all-metrics/

      - name: Aggregate metrics
        run: |
          mkdir -p aggregated

          # Combine all metrics files
          echo '{"suites": [' > aggregated/combined-metrics.json
          first=true
          for file in all-metrics/*/e2e-metrics.json; do
            if [ -f "$file" ]; then
              if [ "$first" = true ]; then
                first=false
              else
                echo ',' >> aggregated/combined-metrics.json
              fi
              cat "$file" >> aggregated/combined-metrics.json
            fi
          done
          echo ']}' >> aggregated/combined-metrics.json

          # Calculate totals
          cat aggregated/combined-metrics.json | jq '{
            run_id: .suites[0].run_id,
            timestamp: .suites[0].timestamp,
            total_duration: ([.suites[].duration_seconds] | add),
            total_passed: ([.suites[].tests.passed] | add),
            total_failed: ([.suites[].tests.failed] | add),
            total_skipped: ([.suites[].tests.skipped] | add),
            total_flaky: ([.suites[].tests.flaky] | add),
            suites: .suites
          }' > aggregated/summary.json

      - name: Generate Prometheus metrics file
        run: |
          cat aggregated/summary.json | jq -r '
            "# HELP e2e_total_duration_seconds Total E2E test duration",
            "# TYPE e2e_total_duration_seconds gauge",
            "e2e_total_duration_seconds \(.total_duration // 0)",
            "",
            "# HELP e2e_total_tests_passed Total passed tests across all suites",
            "# TYPE e2e_total_tests_passed gauge",
            "e2e_total_tests_passed \(.total_passed // 0)",
            "",
            "# HELP e2e_total_tests_failed Total failed tests across all suites",
            "# TYPE e2e_total_tests_failed gauge",
            "e2e_total_tests_failed \(.total_failed // 0)",
            "",
            "# HELP e2e_test_success_rate E2E test success rate",
            "# TYPE e2e_test_success_rate gauge",
            "e2e_test_success_rate \(if (.total_passed + .total_failed) > 0 then (.total_passed / (.total_passed + .total_failed)) else 0 end)"
          ' > aggregated/prometheus-metrics.txt

      - name: Generate GitHub Summary
        run: |
          SUMMARY=$(cat aggregated/summary.json)

          cat >> $GITHUB_STEP_SUMMARY <<EOF
          # ðŸ“Š E2E Observability Report

          **Run ID:** $(echo $SUMMARY | jq -r '.run_id')
          **Timestamp:** $(echo $SUMMARY | jq -r '.timestamp')

          ## ðŸ“ˆ Overall Metrics

          | Metric | Value |
          |--------|-------|
          | Total Duration | $(echo $SUMMARY | jq -r '.total_duration // 0')s |
          | Tests Passed | $(echo $SUMMARY | jq -r '.total_passed // 0') âœ… |
          | Tests Failed | $(echo $SUMMARY | jq -r '.total_failed // 0') âŒ |
          | Tests Skipped | $(echo $SUMMARY | jq -r '.total_skipped // 0') â­ï¸ |
          | Tests Flaky | $(echo $SUMMARY | jq -r '.total_flaky // 0') ðŸ”„ |
          | Success Rate | $(echo $SUMMARY | jq -r 'if (.total_passed + .total_failed) > 0 then ((.total_passed / (.total_passed + .total_failed)) * 100 | floor | tostring) + "%" else "N/A" end') |

          ## ðŸ“‹ Suite Breakdown

          | Suite | Duration | Passed | Failed | Status |
          |-------|----------|--------|--------|--------|
          $(echo $SUMMARY | jq -r '.suites[] | "| \(.suite) | \(.duration_seconds)s | \(.tests.passed) | \(.tests.failed) | \(if .exit_code == 0 then "âœ…" else "âŒ" end) |"')

          ---

          *Metrics collected by E2E Observability Pipeline*
          EOF

      - name: Upload aggregated metrics
        uses: actions/upload-artifact@v4
        with:
          name: e2e-aggregated-metrics
          path: aggregated/
          retention-days: 90

  # ============================================================
  # Stage 4: Update Observability Dashboards
  # ============================================================
  update-dashboards:
    name: ðŸ“Š Update Dashboards
    runs-on: ubuntu-latest
    needs: aggregate-metrics
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v4

      - name: Download aggregated metrics
        uses: actions/download-artifact@v4
        with:
          name: e2e-aggregated-metrics
          path: metrics/

      - name: Update historical metrics
        run: |
          HISTORY_FILE="observability/e2e-metrics-history.json"

          # Initialize or load history
          if [ -f "$HISTORY_FILE" ]; then
            HISTORY=$(cat "$HISTORY_FILE")
          else
            HISTORY='{"runs": []}'
          fi

          # Append new run (keep last 100 runs)
          NEW_RUN=$(cat metrics/summary.json)
          echo "$HISTORY" | jq --argjson new "$NEW_RUN" '.runs = ([$new] + .runs)[:100]' > "$HISTORY_FILE"

      - name: Generate trend data
        run: |
          HISTORY_FILE="observability/e2e-metrics-history.json"

          # Generate trend analysis
          cat "$HISTORY_FILE" | jq '{
            latest: .runs[0],
            trends: {
              avg_duration: ([.runs[].total_duration] | add / length),
              avg_pass_rate: ([.runs[] | if (.total_passed + .total_failed) > 0 then (.total_passed / (.total_passed + .total_failed)) else 0 end] | add / length),
              total_runs: (.runs | length),
              flaky_trend: ([.runs[:10][].total_flaky] | add / ([.runs[:10][].total_flaky] | length))
            }
          }' > observability/e2e-trends.json

      - name: Commit metrics history
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add observability/e2e-metrics-history.json observability/e2e-trends.json || true
          git diff --staged --quiet || git commit -m "chore(observability): update E2E metrics history [skip ci]"
          git push || true

  # ============================================================
  # Stage 5: Alert on Failures
  # ============================================================
  alert-on-failures:
    name: ðŸš¨ Alert on Failures
    runs-on: ubuntu-latest
    needs: [aggregate-metrics]
    if: failure()
    steps:
      - name: Download metrics
        uses: actions/download-artifact@v4
        with:
          name: e2e-aggregated-metrics
          path: metrics/
        continue-on-error: true

      - name: Generate failure alert
        run: |
          cat >> $GITHUB_STEP_SUMMARY <<EOF
          # ðŸš¨ E2E Test Failure Alert

          **Severity:** HIGH
          **Pipeline:** E2E Observability
          **Run:** ${{ github.run_id }}
          **Branch:** ${{ github.ref }}
          **Actor:** ${{ github.actor }}

          ## Action Required

          E2E tests have failed. Please investigate:

          1. Check the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          2. Review test artifacts for detailed failure information
          3. Check for infrastructure issues in observability stack

          ## Quick Links

          - [Test Results Artifacts](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}#artifacts)
          - [E2E Dashboard](http://localhost:3001/d/e2e-tests)

          EOF

      - name: Create issue for persistent failures
        if: github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ðŸš¨ E2E Test Failure - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## E2E Test Failure Alert

            The scheduled E2E observability pipeline has failed.

            **Run:** [#${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            **Branch:** ${{ github.ref }}
            **Time:** ${new Date().toISOString()}

            Please investigate and resolve the failing tests.

            /cc @intelgraph/platform-team
            `;

            // Check for existing open issue
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'e2e-failure,automated'
            });

            if (issues.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['e2e-failure', 'automated', 'priority:high']
              });
            }
