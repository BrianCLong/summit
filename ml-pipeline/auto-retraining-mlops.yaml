# MLOps Auto-Retraining Pipeline
# Kubeflow Pipeline for automated model retraining with concept drift detection

apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: threat-detection-retraining
  namespace: intelgraph-ml
  labels:
    component: mlops
    version: v2.0.0
spec:
  entrypoint: retraining-pipeline
  serviceAccountName: mlops-pipeline
  onExit: resource-usage-report

  arguments:
    parameters:
      - name: training-parallelism
        value: '4'
      - name: prometheus-url
        value: 'http://prometheus.monitoring.svc.cluster.local:9090'
      - name: resource-metrics-range
        value: '30m'

  metrics:
    prometheus:
      - name: workflow_cpu_seconds_total
        help: "Total CPU core-seconds consumed by the workflow"
        when: "{{workflow.status}} == Succeeded || {{workflow.status}} == Failed"
        gauge:
          value: "{{workflow.resourcesDuration.cpu | default 0}}"
      - name: workflow_memory_mebibytes
        help: "Aggregate memory usage reported in mebibyte-seconds"
        when: "{{workflow.status}} == Succeeded || {{workflow.status}} == Failed"
        gauge:
          value: "{{=sprig.divf (sprig.toFloat64 (workflow.resourcesDuration.memory | default 0)) 1048576}}"

  podDisruptionBudget:
    minAvailable: 1
    selector:
      matchLabels:
        workflows.argoproj.io/workflow: "{{workflow.name}}"

  # Volume claims for model artifacts
  volumeClaimTemplates:
    - metadata:
        name: model-artifacts
      spec:
        accessModes: ['ReadWriteOnce']
        resources:
          requests:
            storage: 20Gi

  templates:
    # Main pipeline orchestration
    - name: retraining-pipeline
      dag:
        tasks:
          - name: drift-detection
            template: concept-drift-detection

          - name: data-validation
            template: validate-training-data
            depends: 'drift-detection.Succeeded'

          - name: feature-engineering
            template: feature-engineering
            depends: 'data-validation.Succeeded'

          - name: parallel-model-training
            template: parallel-model-training
            depends: 'feature-engineering.Succeeded'

          - name: model-evaluation
            template: evaluate-models
            depends: 'parallel-model-training.Succeeded'

          - name: champion-challenger
            template: champion-challenger-test
            depends: 'model-evaluation.Succeeded'

          - name: model-deployment
            template: deploy-best-model
            depends: 'champion-challenger.Succeeded'

          - name: performance-monitoring
            template: setup-monitoring
            depends: 'model-deployment.Succeeded'

    # Concept drift detection
    - name: concept-drift-detection
      container:
        image: intelgraph/ml-drift-detector:v2.0.0
        command: [python]
        args:
          - /app/drift_detector.py
          - --window-days=7
          - --threshold=0.1
          - --metrics-endpoint=prometheus:9090
        env:
          - name: MLFLOW_TRACKING_URI
            value: 'http://mlflow-server:5000'
          - name: FEATURE_STORE_URI
            value: 'redis://redis-cluster:6379'
        resources:
          requests:
            memory: '2Gi'
            cpu: '1'
          limits:
            memory: '4Gi'
            cpu: '2'
      outputs:
        parameters:
          - name: drift-detected
            valueFrom:
              path: /tmp/drift_detected.txt
          - name: drift-score
            valueFrom:
              path: /tmp/drift_score.txt

    # Data validation and preparation
    - name: validate-training-data
      container:
        image: intelgraph/data-validator:v2.0.0
        command: [python]
        args:
          - /app/validate_data.py
          - --data-source=timescaledb
          - --validation-suite=threat_detection
          - --lookback-days=30
        env:
          - name: POSTGRES_URL
            valueFrom:
              secretKeyRef:
                name: database-secrets
                key: timescaledb-url
          - name: GREAT_EXPECTATIONS_CONFIG
            value: '/config/data_validation.yaml'
        volumeMounts:
          - name: model-artifacts
            mountPath: /artifacts
        resources:
          requests:
            memory: '2Gi'
            cpu: '1'
          limits:
            memory: '4Gi'
            cpu: '2'
      outputs:
        parameters:
          - name: data-quality-score
            valueFrom:
              path: /artifacts/data_quality.json
        artifacts:
          - name: validated-dataset
            path: /artifacts/validated_dataset.parquet

    # Feature engineering pipeline
    - name: feature-engineering
      container:
        image: intelgraph/feature-engineer:v2.0.0
        command: [python]
        args:
          - /app/feature_pipeline.py
          - --input=/artifacts/validated_dataset.parquet
          - --output=/artifacts/feature_store
          - --feature-config=/config/features.yaml
        env:
          - name: FEAST_REPO_PATH
            value: '/config/feast'
        volumeMounts:
          - name: model-artifacts
            mountPath: /artifacts
        resources:
          requests:
            memory: '4Gi'
            cpu: '2'
          limits:
            memory: '8Gi'
            cpu: '4'
      inputs:
        artifacts:
          - name: dataset
            path: /artifacts/validated_dataset.parquet
            from: '{{tasks.data-validation.outputs.artifacts.validated-dataset}}'
      outputs:
        artifacts:
          - name: feature-store
            path: /artifacts/feature_store

    # Parallel model training (ensemble approach)
    - name: parallel-model-training
      parallelism: "{{workflow.parameters.training-parallelism}}"
      container:
        image: intelgraph/model-trainer:v2.0.0
        command: [python]
        args:
          - /app/train_model.py
          - --model-type={{item}}
          - --features=/artifacts/feature_store
          - --output=/artifacts/models/{{item}}
          - --config=/config/training_config.yaml
        env:
          - name: CUDA_VISIBLE_DEVICES
            value: '{{workflow.parameters.gpu-id}}'
          - name: MLFLOW_EXPERIMENT_NAME
            value: 'threat-detection-retraining-{{workflow.creationTimestamp}}'
        volumeMounts:
          - name: model-artifacts
            mountPath: /artifacts
        resources:
          requests:
            memory: '8Gi'
            cpu: '4'
            nvidia.com/gpu: 1
          limits:
            memory: '16Gi'
            cpu: '8'
            nvidia.com/gpu: 1
      withItems:
        - 'network-behavior'
        - 'process-behavior'
        - 'temporal-anomaly'
        - 'graph-anomaly'
      inputs:
        artifacts:
          - name: features
            path: /artifacts/feature_store
            from: '{{tasks.feature-engineering.outputs.artifacts.feature-store}}'
      outputs:
        artifacts:
          - name: trained-model
            path: /artifacts/models/{{item}}

    # Model evaluation and comparison
    - name: evaluate-models
      container:
        image: intelgraph/model-evaluator:v2.0.0
        command: [python]
        args:
          - /app/evaluate_models.py
          - --models-dir=/artifacts/models
          - --test-data=/artifacts/test_set.parquet
          - --metrics-output=/artifacts/evaluation_results.json
        env:
          - name: MLFLOW_TRACKING_URI
            value: 'http://mlflow-server:5000'
        volumeMounts:
          - name: model-artifacts
            mountPath: /artifacts
        resources:
          requests:
            memory: '4Gi'
            cpu: '2'
          limits:
            memory: '8Gi'
            cpu: '4'
      outputs:
        parameters:
          - name: best-model
            valueFrom:
              path: /artifacts/best_model.txt
          - name: performance-metrics
            valueFrom:
              path: /artifacts/evaluation_results.json
        artifacts:
          - name: model-comparison
            path: /artifacts/model_comparison_report.html

    # Champion vs Challenger testing
    - name: champion-challenger-test
      container:
        image: intelgraph/champion-challenger:v2.0.0
        command: [python]
        args:
          - /app/champion_challenger.py
          - --champion-model={{workflow.parameters.current-model}}
          - --challenger-model={{tasks.evaluate-models.outputs.parameters.best-model}}
          - --test-traffic-percentage=10
          - --duration-minutes=60
          - --success-threshold=0.05
        env:
          - name: KUBERNETES_NAMESPACE
            value: 'intelgraph-ml'
          - name: ISTIO_GATEWAY
            value: 'intelgraph-gateway'
        volumeMounts:
          - name: model-artifacts
            mountPath: /artifacts
        resources:
          requests:
            memory: '2Gi'
            cpu: '1'
          limits:
            memory: '4Gi'
            cpu: '2'
      outputs:
        parameters:
          - name: challenger-wins
            valueFrom:
              path: /artifacts/challenger_result.txt
          - name: performance-improvement
            valueFrom:
              path: /artifacts/improvement.txt

    # Deploy winning model
    - name: deploy-best-model
      container:
        image: intelgraph/model-deployer:v2.0.0
        command: [python]
        args:
          - /app/deploy_model.py
          - --model-path=/artifacts/models/{{tasks.evaluate-models.outputs.parameters.best-model}}
          - --deployment-target=production
          - --rollout-strategy=canary
          - --traffic-split=5
        env:
          - name: KUBERNETES_NAMESPACE
            value: 'intelgraph'
          - name: MODEL_REGISTRY_URL
            value: 'http://mlflow-server:5000'
          - name: SELDON_CORE_ENABLED
            value: 'true'
        volumeMounts:
          - name: model-artifacts
            mountPath: /artifacts
        resources:
          requests:
            memory: '1Gi'
            cpu: '0.5'
          limits:
            memory: '2Gi'
            cpu: '1'
      outputs:
        parameters:
          - name: deployment-id
            valueFrom:
              path: /artifacts/deployment_id.txt
          - name: model-endpoint
            valueFrom:
              path: /artifacts/model_endpoint.txt

    # Set up monitoring for deployed model
    - name: setup-monitoring
      container:
        image: intelgraph/monitoring-setup:v2.0.0
        command: [python]
        args:
          - /app/setup_monitoring.py
          - --deployment-id={{tasks.deploy-best-model.outputs.parameters.deployment-id}}
          - --model-endpoint={{tasks.deploy-best-model.outputs.parameters.model-endpoint}}
          - --monitoring-config=/config/monitoring.yaml
        env:
          - name: PROMETHEUS_URL
            value: 'http://prometheus:9090'
          - name: GRAFANA_URL
            value: 'http://grafana:3000'
          - name: ALERT_MANAGER_URL
            value: 'http://alertmanager:9093'
        resources:
          requests:
            memory: '512Mi'
            cpu: '0.25'
          limits:
            memory: '1Gi'
            cpu: '0.5'

    # Post-workflow resource usage aggregation
    - name: resource-usage-report
      inputs:
        parameters:
          - name: prometheus-url
            value: '{{workflow.parameters.prometheus-url}}'
          - name: metrics-range
            value: '{{workflow.parameters.resource-metrics-range}}'
      container:
        image: python:3.11-slim
        command: [python, -c]
        args:
          - |
            import datetime
            import json
            import os
            import sys
            import urllib.parse
            import urllib.request

            prom_url = os.environ.get('PROM_URL', '').rstrip('/')
            workflow = os.environ.get('WORKFLOW_NAME')
            namespace = os.environ.get('WORKFLOW_NAMESPACE')
            window = os.environ.get('METRICS_RANGE', '30m')

            if not prom_url:
                raise SystemExit('PROM_URL environment variable must be set')

            def query(name, prom_query):
                encoded = urllib.parse.urlencode({'query': prom_query})
                url = f"{prom_url}/api/v1/query?{encoded}"
                with urllib.request.urlopen(url, timeout=30) as response:
                    payload = json.load(response)
                if payload.get('status') != 'success':
                    raise RuntimeError(f"Prometheus query for {name} failed: {payload}")
                return payload['data']['result']

            pod_selector = f"{workflow}-.*"
            cpu_query = (
                "sum(rate(container_cpu_usage_seconds_total{{namespace=\"{ns}\",pod=~\"{pod}\"}}[{window}]))"
            ).format(ns=namespace, pod=pod_selector, window=window)
            avg_mem_query = (
                "avg_over_time(container_memory_usage_bytes{{namespace=\"{ns}\",pod=~\"{pod}\"}}[{window}])"
            ).format(ns=namespace, pod=pod_selector, window=window)
            max_mem_query = (
                "max_over_time(container_memory_usage_bytes{{namespace=\"{ns}\",pod=~\"{pod}\"}}[{window}])"
            ).format(ns=namespace, pod=pod_selector, window=window)

            results = {
                'cpuSecondsRate': query('cpuSecondsRate', cpu_query),
                'avgMemoryBytes': query('avgMemoryBytes', avg_mem_query),
                'maxMemoryBytes': query('maxMemoryBytes', max_mem_query),
            }

            def extract_scalar(series):
                if not series:
                    return 0.0
                value = series[0].get('value') if isinstance(series[0], dict) else None
                if not value:
                    return 0.0
                return float(value[1])

            summary = {
                'cpuCores': extract_scalar(results['cpuSecondsRate']),
                'avgMemoryMiB': extract_scalar(results['avgMemoryBytes']) / 1048576,
                'maxMemoryMiB': extract_scalar(results['maxMemoryBytes']) / 1048576,
            }

            report = {
                'workflow': workflow,
                'namespace': namespace,
                'range': window,
                'generatedAt': datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).isoformat(),
                'summary': summary,
                'rawResults': results,
            }

            with open('/tmp/resource-report.json', 'w', encoding='utf-8') as handle:
                json.dump(report, handle, indent=2)

            json.dump(report, sys.stdout, indent=2)
        env:
          - name: PROM_URL
            value: '{{inputs.parameters.prometheus-url}}'
          - name: WORKFLOW_NAME
            value: '{{workflow.name}}'
          - name: WORKFLOW_NAMESPACE
            value: '{{workflow.namespace}}'
          - name: METRICS_RANGE
            value: '{{inputs.parameters.metrics-range}}'
        resources:
          requests:
            memory: '128Mi'
            cpu: '50m'
          limits:
            memory: '256Mi'
            cpu: '100m'
      outputs:
        artifacts:
          - name: workflow-resource-report
            path: /tmp/resource-report.json

---
# Scheduled workflow for regular retraining
apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: scheduled-threat-model-retraining
  namespace: intelgraph-ml
spec:
  schedule: '0 2 * * 0' # Weekly on Sunday at 2 AM
  timezone: 'America/New_York'
  workflowSpec:
    entrypoint: retraining-pipeline
    workflowTemplateRef:
      name: threat-detection-retraining
    arguments:
      parameters:
        - name: current-model
          value: 'threat-detection-v1.0.0'
        - name: gpu-id
          value: '0'
        - name: training-parallelism
          value: '8'
        - name: prometheus-url
          value: 'http://prometheus.monitoring.svc.cluster.local:9090'
        - name: resource-metrics-range
          value: '1h'

---
# Feature store configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: threat-detection-features
  namespace: intelgraph-ml
data:
  features.yaml: |
    feature_views:
    - name: network_features
      entities:
      - indicator_id
      features:
      - src_ip_reputation
      - dst_ip_reputation
      - port_number
      - protocol_type
      - packet_count
      - byte_count
      - connection_duration
      ttl: 7d
      
    - name: process_features
      entities:
      - indicator_id
      features:
      - process_name
      - command_line_entropy
      - parent_process
      - execution_count
      - is_system_process
      - has_base64_encoding
      ttl: 7d
      
    - name: temporal_features
      entities:
      - indicator_id
      features:
      - hour_of_day
      - day_of_week
      - is_weekend
      - is_business_hours
      - execution_frequency
      ttl: 7d
      
    - name: graph_features
      entities:
      - indicator_id
      features:
      - node_centrality
      - community_size
      - shortest_path_to_bad
      - clustering_coefficient
      ttl: 7d

---
# Model training configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: training-config
  namespace: intelgraph-ml
data:
  training_config.yaml: |
    models:
      network-behavior:
        type: "isolation_forest"
        hyperparameters:
          contamination: 0.1
          n_estimators: 100
          random_state: 42
        features:
        - network_features.*
        
      process-behavior:
        type: "transformer_classifier"
        hyperparameters:
          model_name: "distilbert-base-uncased"
          num_labels: 2
          learning_rate: 2e-5
          num_train_epochs: 3
        features:
        - process_features.*
        
      temporal-anomaly:
        type: "lstm_autoencoder"
        hyperparameters:
          sequence_length: 24
          hidden_size: 128
          num_layers: 2
          dropout: 0.1
        features:
        - temporal_features.*
        
      graph-anomaly:
        type: "graph_neural_network"
        hyperparameters:
          num_layers: 3
          hidden_channels: 64
          dropout: 0.2
          learning_rate: 0.01
        features:
        - graph_features.*
        
    training:
      validation_split: 0.2
      test_split: 0.1
      cross_validation_folds: 5
      early_stopping_patience: 10
      metrics:
      - accuracy
      - precision
      - recall
      - f1_score
      - roc_auc
      
    drift_detection:
      window_size: 1000
      reference_window_size: 5000
      drift_threshold: 0.1
      statistical_tests:
      - kolmogorov_smirnov
      - population_stability_index
      - jensen_shannon_divergence

---
# Monitoring and alerting configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-monitoring-config
  namespace: intelgraph-ml
data:
  monitoring.yaml: |
    metrics:
      model_performance:
      - name: "threat_detection_accuracy"
        description: "Model accuracy on validation set"
        type: "gauge"
        threshold: 0.85
        
      - name: "threat_detection_latency_p95"
        description: "95th percentile prediction latency"
        type: "histogram"
        threshold: 100  # milliseconds
        
      - name: "concept_drift_score"
        description: "Statistical drift score"
        type: "gauge"
        threshold: 0.1
        
      - name: "false_positive_rate"
        description: "False positive rate"
        type: "gauge"
        threshold: 0.05
        
      - name: "model_feature_importance_drift"
        description: "Change in feature importance"
        type: "gauge"
        threshold: 0.2
        
    alerts:
    - alert: ModelAccuracyDegraded
      expr: threat_detection_accuracy < 0.85
      for: 5m
      labels:
        severity: warning
        component: ml-model
      annotations:
        summary: "Threat detection model accuracy below threshold"
        description: "Model accuracy {{ $value }} is below 85% threshold"
        
    - alert: ModelLatencyHigh
      expr: histogram_quantile(0.95, threat_detection_latency_p95) > 100
      for: 2m
      labels:
        severity: critical
        component: ml-model
      annotations:
        summary: "High model prediction latency"
        description: "95th percentile latency {{ $value }}ms exceeds 100ms"
        
    - alert: ConceptDriftDetected
      expr: concept_drift_score > 0.1
      for: 1m
      labels:
        severity: warning
        component: ml-model
      annotations:
        summary: "Concept drift detected in threat model"
        description: "Drift score {{ $value }} indicates model retraining needed"
        
    dashboards:
    - name: "ML Model Performance"
      panels:
      - title: "Model Accuracy Over Time"
        type: "graph"
        query: "threat_detection_accuracy"
        
      - title: "Prediction Latency Distribution"
        type: "heatmap"
        query: "threat_detection_latency_p95"
        
      - title: "Concept Drift Score"
        type: "stat"
        query: "concept_drift_score"
        
      - title: "Feature Importance Heatmap"
        type: "heatmap"
        query: "model_feature_importance"
        
      - title: "Confusion Matrix"
        type: "table"
        query: "threat_detection_confusion_matrix"
