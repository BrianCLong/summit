# GA-Core Hypercare SLOs & Alerts
# Production monitoring configuration for Day-0+ operations

# Core SLOs - Production Targets
slos:
  graph_query_latency:
    target: "p95 â‰¤ 1.5s"
    description: "Graph query response time for intelligence analysis"
    
  ingest_e2e_delay:
    target: "p95 â‰¤ 5s"
    description: "End-to-end ingest processing including PII redaction"
    
  trace_coverage:
    target: "â‰¥ 90%"
    description: "OTEL trace coverage across gatewayâ†’services"
    
  provenance_verify:
    target: "0 failures / 24h"
    description: "Provenance bundle verification success rate"
    
  policy_denial_rate:
    target: "â‰¤ 15% of GraphQL requests"
    description: "Authority-based policy denial rate (outside test tenants)"

# PromQL Alert Rules
alerts:
  - alert: GraphQueryLatencyHigh
    expr: histogram_quantile(0.95, sum by (le) (rate(graph_query_latency_seconds_bucket[5m]))) > 1.5
    for: 10m
    labels:
      severity: warning
      component: graph
    annotations:
      summary: "Graph query p95 latency exceeding 1.5s SLO"
      description: "p95 graph query latency is {{ $value }}s, above 1.5s threshold"
      
  - alert: IngestBacklogGrowing
    expr: rate(ingest_queue_depth[5m]) > 0 and avg_over_time(ingest_queue_depth[15m]) > 1000
    for: 15m
    labels:
      severity: critical
      component: ingest
    annotations:
      summary: "Ingest queue backlog growing consistently"
      description: "Ingest backlog at {{ $value }} and growing for 15m"
      
  - alert: ProvenanceVerifyFailures
    expr: increase(prov_ledger_bundle_verify_fail_total[15m]) > 0
    for: 5m
    labels:
      severity: critical
      component: provenance
    annotations:
      summary: "Provenance verification failures detected"
      description: "{{ $value }} provenance verification failures in 15m"
      
  - alert: PolicyDenialAnomaly
    expr: increase(policy_denial_total[15m]) / increase(graphql_requests_total[15m]) > 0.15
    for: 10m
    labels:
      severity: warning
      component: policy
    annotations:
      summary: "Policy denial rate anomaly detected"
      description: "Policy denial rate {{ $value | humanizePercentage }} exceeds 15% threshold"
      
  - alert: TraceCoverageLow
    expr: (1 - (sum(rate(otel_spans_untraced_total[5m])) / sum(rate(otel_spans_total[5m])))) < 0.90
    for: 15m
    labels:
      severity: warning
      component: observability
    annotations:
      summary: "OTEL trace coverage below 90% threshold"
      description: "Trace coverage at {{ $value | humanizePercentage }}, below 90% SLO"

# Dashboard Configuration
dashboards:
  - name: "GA-Core Operations Overview"
    panels:
      - title: "Query Latency Heatmap"
        type: "heatmap"
        query: "histogram_quantile([0.50,0.90,0.95,0.99], sum by (le) (rate(graph_query_latency_seconds_bucket[5m])))"
        
      - title: "Ingest Throughput"
        type: "graph"
        query: "rate(ingest_messages_processed_total[5m])"
        
      - title: "OTEL Coverage"
        type: "stat"
        query: "(1 - (sum(rate(otel_spans_untraced_total[5m])) / sum(rate(otel_spans_total[5m])))) * 100"
        
      - title: "Detector ROC (live)"
        type: "graph" 
        query: "detector_true_positive_rate / (detector_true_positive_rate + detector_false_negative_rate)"
        
      - title: "Policy Decisions"
        type: "table"
        query: "topk(10, increase(policy_decisions_total[1h]))"
        
      - title: "Provenance Coverage"
        type: "stat"
        query: "provenance_bundles_verified_total / provenance_bundles_created_total * 100"
        
      - title: "Error Budget Burn-down"
        type: "graph"
        query: "1 - (slo_target - slo_actual) / slo_target"

# Notification Configuration
notifications:
  channels:
    - name: "ops-chat"
      type: "slack"
      url: "${SLACK_WEBHOOK_URL}"
      title_template: "ðŸš¨ GA-Core Alert: {{ .CommonLabels.alertname }}"
      
    - name: "on-call"
      type: "pagerduty" 
      integration_key: "${PAGERDUTY_INTEGRATION_KEY}"
      severity_map:
        critical: "critical"
        warning: "warning"
        
  quiet_hours:
    timezone: "America/New_York"
    start: "22:00"
    end: "06:00"
    weekends: true
    
# Canary Configuration
canary:
  enabled: true
  tenant_slice: 10  # percent
  rollback_triggers:
    - alert: "GraphQueryLatencyHigh"
      duration: "5m"
    - alert: "IngestBacklogGrowing" 
      duration: "2m"
    - alert: "ProvenanceVerifyFailures"
      duration: "1m"