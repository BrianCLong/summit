# Alertmanager configuration for IntelGraph
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@intelgraph.com'
  smtp_auth_username: ''
  smtp_auth_password: ''

# Notification templates
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Routing tree for alerts
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
    # Critical alerts go to on-call immediately
    - match:
        severity: critical
      receiver: critical-alerts
      group_wait: 0s
      repeat_interval: 5m
    
    # Warning alerts are grouped and sent less frequently
    - match:
        severity: warning
      receiver: warning-alerts
      group_wait: 30s
      repeat_interval: 30m
    
    # Info alerts are sent to monitoring channel
    - match:
        severity: info
      receiver: info-alerts
      repeat_interval: 4h

# Alert receivers
receivers:
  # Default webhook receiver
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://server:4000/api/monitoring/alerts'
        send_resolved: true
        http_config:
          bearer_token: '${WEBHOOK_TOKEN}'

  # Critical alerts - immediate notification
  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@intelgraph.com'
        subject: 'üö® CRITICAL Alert: {{ .GroupLabels.alertname }}'
        body: |
          Alert: {{ .GroupLabels.alertname }}
          Severity: {{ .CommonLabels.severity }}
          
          {{ range .Alerts }}
          - {{ .Annotations.summary }}
            {{ .Annotations.description }}
            Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          {{ end }}
          
          Dashboard: http://grafana:3000/dashboards
          Prometheus: http://prometheus:9090
    
    # Slack integration for critical alerts
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts-critical'
        title: 'üö® Critical Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          
          *Labels:* {{ range .Labels.SortedPairs }}`{{ .Name }}={{ .Value }}` {{ end }}
          {{ end }}
        send_resolved: true

  # Warning alerts - batched notification
  - name: 'warning-alerts'
    email_configs:
      - to: 'monitoring@intelgraph.com'
        subject: '‚ö†Ô∏è Warning Alerts ({{ .Alerts | len }})'
        body: |
          {{ .Alerts | len }} warning alerts triggered:
          
          {{ range .Alerts }}
          - {{ .Annotations.summary }}
            {{ .Annotations.description }}
            Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts-warning'
        title: '‚ö†Ô∏è Warning Alerts ({{ .Alerts | len }})'
        text: |
          {{ range .Alerts }}
          *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          {{ end }}

  # Info alerts - low priority
  - name: 'info-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#monitoring'
        title: '‚ÑπÔ∏è Info Alerts ({{ .Alerts | len }})'
        text: |
          {{ range .Alerts }}
          {{ .Annotations.summary }}
          {{ end }}

# Inhibition rules to prevent alert spam
inhibit_rules:
  # If service is down, don't alert on high error rates
  - source_match:
      alertname: ServiceDown
    target_match:
      alertname: HighErrorRate
    equal: ['instance']
  
  # If there's a critical alert, suppress related warnings
  - source_match:
      severity: critical
    target_match:
      severity: warning
    equal: ['service', 'instance']