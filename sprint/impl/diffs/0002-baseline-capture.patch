diff --git a/.github/workflows/baseline-refresh.yml b/.github/workflows/baseline-refresh.yml
new file mode 100644
index 000000000..cb60b5f56
--- /dev/null
+++ b/.github/workflows/baseline-refresh.yml
@@ -0,0 +1,30 @@
+name: Baseline Refresh
+on:
+  push:
+    branches: [main]
+jobs:
+  baseline:
+    runs-on: ubuntu-latest
+    env:
+      BENCH_URL: http://localhost:4000/health
+      NEO4J_URL: bolt://localhost:7687
+      NEO4J_USER: neo4j
+      NEO4J_PASS: password
+    steps:
+      - uses: actions/checkout@v4
+      - uses: actions/setup-python@v4
+        with:
+          python-version: '3.11'
+      - uses: actions/setup-node@v4
+        with:
+          node-version: '20'
+      - name: Capture baseline
+        run: |
+          chmod +x sprint/experiments/capture_baseline.sh
+          ./sprint/experiments/capture_baseline.sh
+      - name: Commit baseline (no-op if unchanged)
+        run: |
+          git config user.name "github-actions[bot]"
+          git config user.email "github-actions[bot]@users.noreply.github.com"
+          git add sprint/benchmark/baseline.json || true
+          git diff --cached --quiet || (git commit -m "chore(bench): refresh baseline" && git push)
diff --git a/scripts/bench_graph_profile.js b/scripts/bench_graph_profile.js
new file mode 100644
index 000000000..d3217b850
--- /dev/null
+++ b/scripts/bench_graph_profile.js
@@ -0,0 +1,45 @@
+#!/usr/bin/env node
+// Dedicated PROFILE runner to avoid touching existing bench_graph.js
+const neo4j = require("neo4j-driver");
+const url = process.env.NEO4J_URL || "bolt://localhost:7687";
+const user = process.env.NEO4J_USER || "neo4j";
+const pass = process.env.NEO4J_PASS || "password";
+const query = process.argv.includes("--query")
+  ? process.argv[process.argv.indexOf("--query")+1]
+  : "MATCH (n)-[r]->(m) RETURN count(r) LIMIT 1000";
+const WARMUP = (parseInt(process.env.WARMUP_S||"5")*1000);
+const DURATION = (parseInt(process.env.DURATION_S||"30")*1000);
+
+const p = q => q[Math.floor(0.95*(q.length-1))] || 0;
+const avg = a => a.reduce((x,y)=>x+y,0)/(a.length||1);
+
+(async () => {
+  const driver = neo4j.driver(url, neo4j.auth.basic(user, pass));
+  const session = driver.session();
+  const stats = [];
+  const lat = [];
+  const start = Date.now();
+  const runOnce = async () => {
+    const t0 = Date.now();
+    const res = await session.run("PROFILE " + query);
+    const t1 = Date.now();
+    const summ = res.summary;
+    const prof = summ && summ.profile ? summ.profile : null;
+    const dbHits = prof?.arguments?.DbHits ?? prof?.arguments?.["DbHits"] ?? null;
+    return { ms: (t1 - t0), dbHits, rows: res.records.length };
+  };
+  while (Date.now()-start < WARMUP) await runOnce();
+  const t1 = Date.now(); let iters=0;
+  while (Date.now()-t1 < DURATION) {
+    const s = await runOnce(); iters++;
+    lat.push(s.ms); stats.push(s);
+  }
+  await session.close(); await driver.close();
+  lat.sort((a,b)=>a-b);
+  console.log(JSON.stringify({
+    name: "graph-query-neo4j-profile",
+    target: url, query, iters,
+    latency_ms_p95: p(lat),
+    profile: { avgDbHits: Math.round(avg(stats.map(s=>s.dbHits||0))), avgRows: Math.round(avg(stats.map(s=>s.rows||0))) }
+  }));
+})();
\ No newline at end of file
diff --git a/sprint/experiments/capture_baseline.sh b/sprint/experiments/capture_baseline.sh
index af4ec3c33..a0fab2589 100755
--- a/sprint/experiments/capture_baseline.sh
+++ b/sprint/experiments/capture_baseline.sh
@@ -1,24 +1,27 @@
-#!/bin/bash
+#!/usr/bin/env bash
+set -euo pipefail
 
-# Capture real baselines for SLO enforcement
-echo "üîç Capturing real baselines for SLO enforcement..."
+ROOT="$(git rev-parse --show-toplevel)"
+cd "$ROOT"
 
-# Run the harness to generate metrics
-echo "üèÉ Running harness to generate baseline metrics..."
+echo "‚ñ∂ IntelGraph Baseline Capture"
+echo "   repo: $ROOT"
+echo "   date: $(date -Is)"
+
+# 1) Bring up minimal stack for smoke (best-effort)
 make -f sprint/impl/Makefile run || true
-python3 sprint/experiments/harness/run.py --config sprint/experiments/configs.yaml || true
 
-# Write the baseline
-echo "üíæ Writing baseline to sprint/benchmark/baseline.json..."
-WRITE_BASELINE=1 python3 sprint/experiments/evaluate.py
+# 2) Run harness once to produce metrics/*.jsonl and metrics.md
+python3 sprint/experiments/harness/run.py --config sprint/experiments/configs.yaml || true
 
-# Commit the baseline
-echo "‚ûï Committing baseline..."
-git add sprint/benchmark/baseline.json
-git commit -m "chore(bench): establish baseline" -m "Capture real baseline metrics from current environment for SLO enforcement."
+# 3) Write/refresh baseline.json using evaluator
+export WRITE_BASELINE=1
+python3 -m pip install pyyaml >/dev/null 2>&1 || true
+python3 sprint/experiments/evaluate.py
 
-echo "‚úÖ Baseline captured and committed successfully!"
-echo "üìå Next steps:"
-echo "   1. Push to main: git push origin main"
-echo "   2. Protect main branch to require 'Aurelius Sprint Pack' + 'Enforce SLOs' jobs to pass"
-echo "   3. Open PR-6 (comment bot), PR-7 (OTel stubs), and PR-8 (PROFILE stats)"
\ No newline at end of file
+if [[ -f sprint/benchmark/baseline.json ]]; then
+  echo "‚úÖ Baseline written: sprint/benchmark/baseline.json"
+  jq . sprint/benchmark/baseline.json || true
+else
+  echo "‚ùå Baseline not produced"; exit 1
+fi
\ No newline at end of file
diff --git a/sprint/experiments/configs.yaml b/sprint/experiments/configs.yaml
index dc077a83a..1f1a3044a 100644
--- a/sprint/experiments/configs.yaml
+++ b/sprint/experiments/configs.yaml
@@ -11,6 +11,11 @@ targets:
     metrics: ["latency_ms_p95","latency_ms_p99","iters"]
     warmup_s: 5
     duration_s: 30
+  - name: graph-query-neo4j-profile
+    cmd: ["node", "scripts/bench_graph_profile.js", "--query", "MATCH (n)-[r]->(m) RETURN count(r) LIMIT 1000"]
+    metrics: ["latency_ms_p95","profile.avgDbHits","profile.avgRows"]
+    warmup_s: 5
+    duration_s: 30
 ablations:
   - name: "cache_on_vs_off"
     vary: { CACHE_ENABLED: [false, true] }
diff --git a/sprint/experiments/render_comment.py b/sprint/experiments/render_comment.py
new file mode 100755
index 000000000..49d2647ea
--- /dev/null
+++ b/sprint/experiments/render_comment.py
@@ -0,0 +1,22 @@
+#!/usr/bin/env python3
+import json, sys
+from pathlib import Path
+data = json.load(open(sys.argv[1])) if len(sys.argv)&gt;1 else json.load(sys.stdin)
+lines = []
+lines.append("### Bench (smoke)\n")
+lines.append("| target | p95 (ms) | baseline p95 | Œî (%) | err | SLO p95 | result |")
+lines.append("|---|---:|---:|---:|---:|---:|:--:|")
+for t, r in data.items():
+    cur = r["current"]; base = r.get("baseline", {}); rules = r["rules"]
+    p95 = cur.get("p95"); b95 = base.get("p95")
+    delta = "n/a"
+    if p95 is not None and b95:
+        delta = f"{((p95-b95)/b95*100):+.1f}"
+    er = cur.get("error_rate", 0.0)
+    ok = True
+    if p95 is not None and p95 &gt; rules["p95_ms_max"]: ok = False
+    if er is not None and er &gt; rules.get("error_rate_max", 1.0): ok = False
+    icon = "‚úÖ" if ok else "‚ùå"
+    lines.append(f"| {t} | {p95 or 'n/a'} | {b95 or 'n/a'} | {delta} | {er:.3f} | {rules['p95_ms_max']} | {icon} |")
+lines.append("\n_Artifacts: `sprint/benchmark/metrics/*`_")
+print("\n".join(lines))
\ No newline at end of file
