model_list:
  # ===== LOCAL-FIRST (PRIMARY ROUTING) =====
  # default "general"/coder path -> Qwen coder
  - model_name: local/llama
    litellm_params:
      custom_llm_provider: ollama
      api_base: 'http://127.0.0.1:11434'
      model: 'qwen2.5-coder:7b'
      num_ctx: 2048
      temperature: 0.2

  # 8B CPU-safe llama
  - model_name: local/llama-cpu
    litellm_params:
      custom_llm_provider: ollama
      api_base: 'http://127.0.0.1:11434'
      model: 'llama3.1-8b-cpu'
      num_ctx: 1024
      temperature: 0.2

  # small/fast llama
  - model_name: local/llama-small
    litellm_params:
      custom_llm_provider: ollama
      api_base: 'http://127.0.0.1:11434'
      model: 'llama3.2:3b'
      num_ctx: 1024
      temperature: 0.2

  # LM Studio endpoint (if running)
  - model_name: local/lmstudio
    litellm_params:
      custom_llm_provider: openai
      api_base: 'http://127.0.0.1:1234/v1'
      api_key: 'sk-local-lms'
      model: 'llama-3.1-8b-instruct'

  # ===== OPTIONAL HOSTED (POWER BURSTS: DISABLED BY DEFAULT) =====
  # Uncomment and set env vars to enable hosted API bursts
  # - model_name: gemini/1.5-pro
  #   litellm_params:
  #     custom_llm_provider: gemini
  #     model: "gemini-1.5-pro-latest"
  #     api_key: "${GOOGLE_API_KEY}"

  # - model_name: xai/grok-code-fast-1
  #   litellm_params:
  #     custom_llm_provider: openai
  #     api_base: "https://api.x.ai/v1"
  #     api_key: "${XAI_API_KEY}"
  #     model: "grok-code-fast-1"

  - model_name: cloud/deepseek-v3
    litellm_params:
      model: 'openrouter/deepseek/deepseek-chat'
      api_base: 'https://openrouter.ai/api/v1'
      api_key: '${OPENROUTER_API_KEY}'
      custom_llm_provider: 'openrouter'

  - model_name: cloud/deepseek-coder-v2
    litellm_params:
      model: 'openrouter/deepseek/deepseek-coder'
      api_base: 'https://openrouter.ai/api/v1'
      api_key: '${OPENROUTER_API_KEY}'
      custom_llm_provider: 'openrouter'

  - model_name: cloud/qwen2.5-72b
    litellm_params:
      model: 'openrouter/qwen/qwen-2.5-72b-instruct'
      api_base: 'https://openrouter.ai/api/v1'
      api_key: '${OPENROUTER_API_KEY}'
      custom_llm_provider: 'openrouter'

router_settings:
  timeout: 60
  num_retries: 2
  fallback_models:
    - 'local/llama-small'
    - 'local/llama-cpu'
  routing_strategy: 'simple-shuffle'

# ===== BUDGETS: POWER BURSTS (DISABLED BY DEFAULT) =====
# Uncomment to enable hosted API usage with strict limits
# budget:
#   model:
#     gemini/1.5-pro: 0.20 # $0.20/day cap
#     xai/grok-code-fast-1: 0.20 # $0.20/day cap

litellm_settings:
  add_function_to_prompt: true
  drop_params: true
  telemetry: false
