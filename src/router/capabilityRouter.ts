import { MaestroMemory } from '../memory';

interface ModelCapability {
  model: string;
  provider: 'openai' | 'anthropic' | 'azure' | 'local';
  capabilities: string[];
  cost: {
    inputTokens: number; // Cost per 1K input tokens
    outputTokens: number; // Cost per 1K output tokens
    minCost: number; // Minimum cost per request
  };
  performance: {
    maxTokens: number;
    avgLatency: number; // milliseconds
    reliability: number; // 0-1 score
    qualityScore: number; // 0-1 score based on evaluations
  };
  limits: {
    requestsPerMinute: number;
    requestsPerDay: number;
    tokensPerMinute: number;
  };
  context: {
    maxContextLength: number;
    supportsCaching: boolean;
    supportsStreaming: boolean;
  };
}

interface RequestContext {
  type: 'code-generation' | 'analysis' | 'review' | 'debugging' | 'testing' | 'documentation';
  priority: 'low' | 'medium' | 'high' | 'critical';
  complexity: 'simple' | 'moderate' | 'complex' | 'advanced';
  budget: {
    maxCost: number;
    timeLimit: number; // milliseconds
  };
  qualityRequirements: {
    minAccuracy: number; // 0-1
    allowExperimental: boolean;
  };
  context: {
    estimatedTokens: number;
    needsReasoning: boolean;
    requiresCreativity: boolean;
  };
}

interface RoutingResult {
  selectedModel: string;
  reason: string;
  estimatedCost: number;
  estimatedLatency: number;
  fallbackModels: string[];
  cacheHit: boolean;
  confidence: number;
}

export class CapabilityRouter {
  private models: Map<string, ModelCapability>;
  private memory: MaestroMemory;
  private budgetLimits: {
    daily: number;
    perRequest: number;
    currentSpent: number;
  };

  constructor(projectRoot: string = process.cwd()) {
    this.memory = MaestroMemory.getInstance(projectRoot);
    this.models = new Map();
    this.budgetLimits = {
      daily: 50.0, // $50 per day default
      perRequest: 5.0, // $5 per request default
      currentSpent: 0
    };
    
    this.initializeModels();
  }

  private initializeModels(): void {
    // GPT-4 Turbo - Premium option
    this.models.set('gpt-4-turbo', {
      model: 'gpt-4-turbo',
      provider: 'openai',
      capabilities: [
        'code-generation', 'analysis', 'review', 'debugging', 
        'testing', 'documentation', 'reasoning', 'creativity'
      ],
      cost: {
        inputTokens: 0.01, // $0.01 per 1K tokens
        outputTokens: 0.03, // $0.03 per 1K tokens
        minCost: 0.001
      },
      performance: {
        maxTokens: 4096,
        avgLatency: 2000,
        reliability: 0.95,
        qualityScore: 0.92
      },
      limits: {
        requestsPerMinute: 500,
        requestsPerDay: 10000,
        tokensPerMinute: 150000
      },
      context: {
        maxContextLength: 128000,
        supportsCaching: true,
        supportsStreaming: true
      }
    });

    // GPT-3.5 Turbo - Balanced option
    this.models.set('gpt-3.5-turbo', {
      model: 'gpt-3.5-turbo',
      provider: 'openai',
      capabilities: [
        'code-generation', 'analysis', 'documentation', 'testing'
      ],
      cost: {
        inputTokens: 0.0005,
        outputTokens: 0.0015,
        minCost: 0.0001
      },
      performance: {
        maxTokens: 4096,
        avgLatency: 1200,
        reliability: 0.92,
        qualityScore: 0.85
      },
      limits: {
        requestsPerMinute: 3500,
        requestsPerDay: 200000,
        tokensPerMinute: 90000
      },
      context: {
        maxContextLength: 16385,
        supportsCaching: false,
        supportsStreaming: true
      }
    });

    // Claude 3.5 Sonnet - High quality reasoning
    this.models.set('claude-3-5-sonnet', {
      model: 'claude-3-5-sonnet-20241022',
      provider: 'anthropic',
      capabilities: [
        'code-generation', 'analysis', 'review', 'debugging', 
        'reasoning', 'creativity', 'safety'
      ],
      cost: {
        inputTokens: 0.003,
        outputTokens: 0.015,
        minCost: 0.0005
      },
      performance: {
        maxTokens: 8192,
        avgLatency: 1800,
        reliability: 0.96,
        qualityScore: 0.94
      },
      limits: {
        requestsPerMinute: 1000,
        requestsPerDay: 50000,
        tokensPerMinute: 40000
      },
      context: {
        maxContextLength: 200000,
        supportsCaching: true,
        supportsStreaming: true
      }
    });

    // Claude 3 Haiku - Fast and cheap
    this.models.set('claude-3-haiku', {
      model: 'claude-3-haiku-20240307',
      provider: 'anthropic',
      capabilities: [
        'code-generation', 'analysis', 'documentation'
      ],
      cost: {
        inputTokens: 0.00025,
        outputTokens: 0.00125,
        minCost: 0.0001
      },
      performance: {
        maxTokens: 4096,
        avgLatency: 800,
        reliability: 0.90,
        qualityScore: 0.78
      },
      limits: {
        requestsPerMinute: 2000,
        requestsPerDay: 100000,
        tokensPerMinute: 100000
      },
      context: {
        maxContextLength: 200000,
        supportsCaching: true,
        supportsStreaming: true
      }
    });

    // Local model - Free but limited
    this.models.set('local-7b', {
      model: 'codellama-7b-instruct',
      provider: 'local',
      capabilities: [
        'code-generation', 'documentation'
      ],
      cost: {
        inputTokens: 0,
        outputTokens: 0,
        minCost: 0
      },
      performance: {
        maxTokens: 2048,
        avgLatency: 5000,
        reliability: 0.75,
        qualityScore: 0.65
      },
      limits: {
        requestsPerMinute: 10,
        requestsPerDay: 1000,
        tokensPerMinute: 2000
      },
      context: {
        maxContextLength: 4096,
        supportsCaching: false,
        supportsStreaming: false
      }
    });
  }

  async route(request: RequestContext): Promise<RoutingResult> {
    console.log(`ðŸ§­ Routing request: ${request.type} (${request.complexity})`);

    // Check cache first
    const cacheKey = this.generateCacheKey(request);
    const cached = await this.memory.cache.get(cacheKey);
    
    if (cached) {
      return {
        selectedModel: cached.model,
        reason: 'Cache hit - using previously successful model',
        estimatedCost: 0,
        estimatedLatency: 0,
        fallbackModels: [],
        cacheHit: true,
        confidence: 0.95
      };
    }

    // Get candidate models
    const candidates = await this.getCandidateModels(request);
    
    if (candidates.length === 0) {
      throw new Error('No models available for this request type');
    }

    // Score and rank candidates
    const scored = candidates.map(model => ({
      model,
      score: this.scoreModel(model, request),
      estimatedCost: this.estimateCost(model, request),
      estimatedLatency: this.estimateLatency(model, request)
    }));

    // Sort by score (higher is better)
    scored.sort((a, b) => b.score.total - a.score.total);

    const selected = scored[0];
    const fallbacks = scored.slice(1, 4).map(s => s.model.model);

    const result: RoutingResult = {
      selectedModel: selected.model.model,
      reason: this.generateReason(selected.model, selected.score, request),
      estimatedCost: selected.estimatedCost,
      estimatedLatency: selected.estimatedLatency,
      fallbackModels: fallbacks,
      cacheHit: false,
      confidence: selected.score.total
    };

    // Store routing decision for learning
    await this.storeRoutingDecision(request, result);

    return result;
  }

  private async getCandidateModels(request: RequestContext): Promise<ModelCapability[]> {
    const candidates: ModelCapability[] = [];

    for (const model of this.models.values()) {
      // Check if model supports required capabilities
      if (!model.capabilities.includes(request.type)) continue;

      // Check budget constraints
      const estimatedCost = this.estimateCost(model, request);
      if (estimatedCost > request.budget.maxCost) continue;
      if (estimatedCost > this.budgetLimits.perRequest) continue;
      
      // Check if we have budget remaining
      if (this.budgetLimits.currentSpent + estimatedCost > this.budgetLimits.daily) continue;

      // Check context length requirements
      if (request.context.estimatedTokens > model.context.maxContextLength) continue;

      // Check quality requirements
      if (model.performance.qualityScore < request.qualityRequirements.minAccuracy) continue;

      // Check if experimental models are allowed
      if (!request.qualityRequirements.allowExperimental && model.performance.reliability < 0.9) continue;

      candidates.push(model);
    }

    return candidates;
  }

  private scoreModel(model: ModelCapability, request: RequestContext): {
    cost: number;
    performance: number;
    reliability: number;
    capability: number;
    total: number;
  } {
    const weights = this.getWeights(request);
    
    // Cost score (lower cost = higher score)
    const estimatedCost = this.estimateCost(model, request);
    const maxBudget = Math.min(request.budget.maxCost, this.budgetLimits.perRequest);
    const costScore = Math.max(0, 1 - (estimatedCost / maxBudget));

    // Performance score
    const estimatedLatency = this.estimateLatency(model, request);
    const timeScore = Math.max(0, 1 - (estimatedLatency / request.budget.timeLimit));
    const performanceScore = (timeScore + model.performance.qualityScore) / 2;

    // Reliability score
    const reliabilityScore = model.performance.reliability;

    // Capability match score
    const requiredCapabilities = this.getRequiredCapabilities(request);
    const matchedCapabilities = requiredCapabilities.filter(cap => 
      model.capabilities.includes(cap)
    ).length;
    const capabilityScore = matchedCapabilities / requiredCapabilities.length;

    const scores = {
      cost: costScore,
      performance: performanceScore,
      reliability: reliabilityScore,
      capability: capabilityScore,
      total: 0
    };

    // Weighted total
    scores.total = 
      scores.cost * weights.cost +
      scores.performance * weights.performance +
      scores.reliability * weights.reliability +
      scores.capability * weights.capability;

    return scores;
  }

  private getWeights(request: RequestContext): {
    cost: number;
    performance: number;
    reliability: number;
    capability: number;
  } {
    // Adjust weights based on request priority and type
    const baseWeights = {
      cost: 0.3,
      performance: 0.3,
      reliability: 0.2,
      capability: 0.2
    };

    switch (request.priority) {
      case 'critical':
        return { cost: 0.1, performance: 0.3, reliability: 0.4, capability: 0.2 };
      case 'high':
        return { cost: 0.2, performance: 0.3, reliability: 0.3, capability: 0.2 };
      case 'low':
        return { cost: 0.5, performance: 0.2, reliability: 0.1, capability: 0.2 };
      default:
        return baseWeights;
    }
  }

  private getRequiredCapabilities(request: RequestContext): string[] {
    const capabilities = [request.type];
    
    if (request.context.needsReasoning) {
      capabilities.push('reasoning');
    }
    
    if (request.context.requiresCreativity) {
      capabilities.push('creativity');
    }
    
    return capabilities;
  }

  private estimateCost(model: ModelCapability, request: RequestContext): number {
    const inputTokens = request.context.estimatedTokens * 0.7; // 70% input
    const outputTokens = request.context.estimatedTokens * 0.3; // 30% output

    const inputCost = (inputTokens / 1000) * model.cost.inputTokens;
    const outputCost = (outputTokens / 1000) * model.cost.outputTokens;

    return Math.max(model.cost.minCost, inputCost + outputCost);
  }

  private estimateLatency(model: ModelCapability, request: RequestContext): number {
    const baseLatency = model.performance.avgLatency;
    
    // Adjust for token count (more tokens = more latency)
    const tokenMultiplier = 1 + (request.context.estimatedTokens / 10000) * 0.5;
    
    // Adjust for complexity
    const complexityMultiplier = {
      'simple': 1.0,
      'moderate': 1.3,
      'complex': 1.6,
      'advanced': 2.0
    }[request.complexity];

    return baseLatency * tokenMultiplier * complexityMultiplier;
  }

  private generateReason(
    model: ModelCapability, 
    score: any, 
    request: RequestContext
  ): string {
    const reasons: string[] = [];

    if (score.cost > 0.8) {
      reasons.push('cost-effective');
    }
    
    if (score.performance > 0.8) {
      reasons.push('high performance');
    }
    
    if (score.reliability > 0.9) {
      reasons.push('reliable');
    }
    
    if (score.capability === 1.0) {
      reasons.push('perfect capability match');
    }

    const primaryReason = reasons.length > 0 ? reasons.join(', ') : 'best available option';
    
    return `Selected ${model.model} for ${primaryReason} (score: ${score.total.toFixed(2)})`;
  }

  private generateCacheKey(request: RequestContext): string {
    return `route-${request.type}-${request.complexity}-${request.priority}-${request.context.estimatedTokens}`;
  }

  private async storeRoutingDecision(request: RequestContext, result: RoutingResult): Promise<void> {
    await this.memory.semantic.store(
      JSON.stringify({
        request: {
          type: request.type,
          complexity: request.complexity,
          priority: request.priority
        },
        result: {
          selectedModel: result.selectedModel,
          estimatedCost: result.estimatedCost,
          reason: result.reason
        }
      }),
      'pattern',
      {
        tags: ['routing', request.type, request.complexity, result.selectedModel],
        success: true
      }
    );
  }

  async recordModelUsage(
    model: string,
    actualCost: number,
    actualLatency: number,
    success: boolean,
    request?: RequestContext
  ): Promise<void> {
    // Update budget tracking
    this.budgetLimits.currentSpent += actualCost;

    // Record performance for future routing decisions
    const modelInfo = this.models.get(model);
    if (modelInfo && request) {
      // Update model performance metrics (exponential moving average)
      const alpha = 0.1;
      modelInfo.performance.avgLatency = 
        (1 - alpha) * modelInfo.performance.avgLatency + alpha * actualLatency;
      
      if (success) {
        modelInfo.performance.reliability = 
          Math.min(1.0, (1 - alpha) * modelInfo.performance.reliability + alpha * 1.0);
      } else {
        modelInfo.performance.reliability = 
          Math.max(0.0, (1 - alpha) * modelInfo.performance.reliability + alpha * 0.0);
      }

      // Store experience in memory
      await this.memory.storeExperience(
        `Used ${model} for ${request.type} task`,
        success ? 'solution' : 'error',
        success,
        {
          model,
          cost: actualCost,
          latency: actualLatency,
          requestType: request.type,
          complexity: request.complexity
        }
      );
    }
  }

  async getBudgetStatus(): Promise<{
    daily: { limit: number; used: number; remaining: number };
    perRequest: { limit: number };
    recommendations: string[];
  }> {
    const recommendations: string[] = [];
    const remaining = this.budgetLimits.daily - this.budgetLimits.currentSpent;
    const usagePercent = (this.budgetLimits.currentSpent / this.budgetLimits.daily) * 100;

    if (usagePercent > 80) {
      recommendations.push('High budget usage - consider using cheaper models');
    }
    
    if (usagePercent > 95) {
      recommendations.push('Budget nearly exhausted - switch to local models');
    }

    if (remaining < this.budgetLimits.perRequest) {
      recommendations.push('Insufficient budget for premium requests');
    }

    return {
      daily: {
        limit: this.budgetLimits.daily,
        used: this.budgetLimits.currentSpent,
        remaining
      },
      perRequest: {
        limit: this.budgetLimits.perRequest
      },
      recommendations
    };
  }

  async updateBudgetLimits(daily?: number, perRequest?: number): Promise<void> {
    if (daily !== undefined) {
      this.budgetLimits.daily = daily;
    }
    
    if (perRequest !== undefined) {
      this.budgetLimits.perRequest = perRequest;
    }
  }

  async getModelRecommendations(request: RequestContext): Promise<{
    recommended: string[];
    avoided: string[];
    reasons: Record<string, string>;
  }> {
    const candidates = await this.getCandidateModels(request);
    const scored = candidates.map(model => ({
      model: model.model,
      score: this.scoreModel(model, request)
    }));

    scored.sort((a, b) => b.score.total - a.score.total);

    return {
      recommended: scored.slice(0, 3).map(s => s.model),
      avoided: Array.from(this.models.keys()).filter(m => 
        !candidates.find(c => c.model === m)
      ),
      reasons: scored.reduce((acc, s) => {
        acc[s.model] = `Score: ${s.score.total.toFixed(2)} (cost: ${s.score.cost.toFixed(2)}, perf: ${s.score.performance.toFixed(2)})`;
        return acc;
      }, {} as Record<string, string>)
    };
  }
}