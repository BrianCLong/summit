# Model Orchestration Budget Configuration
# Defines cost caps, routing rules, and safety guardrails per environment

apiVersion: config.maestro.dev/v1
kind: ModelBudgets
metadata:
  name: intelgraph-model-budgets
  version: "27.0.0"

spec:
  # Global configuration
  global:
    currency: "USD"
    tracking_window: "1h"  # Budget window: 1 hour
    daily_reset: "00:00:00Z"
    cost_calculator: "token-based"  # token-based | time-based

  # Environment-specific budgets
  environments:
    dev:
      hourly_budget: 5.00    # $5/hour for development
      daily_budget: 50.00    # $50/day max
      model_limits:
        gpt-4: 2.00          # $2/hour for GPT-4
        gpt-3.5-turbo: 3.00  # $3/hour for GPT-3.5
        claude-3: 2.00       # $2/hour for Claude
      breach_action: "throttle"  # throttle | block | alert

    staging:
      hourly_budget: 15.00   # $15/hour for staging
      daily_budget: 200.00   # $200/day max
      model_limits:
        gpt-4: 8.00
        gpt-3.5-turbo: 10.00
        claude-3: 8.00
      breach_action: "alert"

    prod:
      hourly_budget: 100.00  # $100/hour for production
      daily_budget: 1000.00  # $1000/day max
      model_limits:
        gpt-4: 40.00
        gpt-3.5-turbo: 60.00
        claude-3: 40.00
      breach_action: "alert"

  # Model provider configuration
  providers:
    openai:
      api_key_ref: "openai-api-key"
      base_url: "https://api.openai.com/v1"
      timeout: "30s"
      retry_policy:
        max_retries: 3
        backoff: "exponential"
      models:
        gpt-4:
          cost_per_1k_input_tokens: 0.03
          cost_per_1k_output_tokens: 0.06
          max_tokens: 8192
          temperature_range: [0.0, 1.0]
        gpt-3.5-turbo:
          cost_per_1k_input_tokens: 0.0015
          cost_per_1k_output_tokens: 0.002
          max_tokens: 4096
          temperature_range: [0.0, 1.0]

    anthropic:
      api_key_ref: "anthropic-api-key"
      base_url: "https://api.anthropic.com"
      timeout: "45s"
      retry_policy:
        max_retries: 3
        backoff: "exponential"
      models:
        claude-3-sonnet:
          cost_per_1k_input_tokens: 0.003
          cost_per_1k_output_tokens: 0.015
          max_tokens: 4096
          temperature_range: [0.0, 1.0]

  # Routing rules
  routing:
    default_provider: "openai"
    default_model: "gpt-3.5-turbo"

    # Route by request type
    routes:
      - name: "nl2cypher"
        match:
          request_type: "nl2cypher"
        destination:
          provider: "openai"
          model: "gpt-4"
        priority: 1

      - name: "entity-extraction"
        match:
          request_type: "entity_extraction"
        destination:
          provider: "openai"
          model: "gpt-3.5-turbo"
        priority: 2

      - name: "analysis"
        match:
          request_type: "analysis"
        destination:
          provider: "anthropic"
          model: "claude-3-sonnet"
        priority: 3

      - name: "fallback"
        match:
          request_type: "*"
        destination:
          provider: "openai"
          model: "gpt-3.5-turbo"
        priority: 999

  # Safety guardrails
  guardrails:
    # Cost monitoring
    cost_monitoring:
      enabled: true
      alert_thresholds:
        - percentage: 75
          action: "alert"
          notification: "slack"
        - percentage: 90
          action: "throttle"
          notification: "pagerduty"
        - percentage: 100
          action: "block"
          notification: "emergency"

    # Request validation
    request_validation:
      max_prompt_length: 10000
      min_prompt_length: 10
      blocked_patterns:
        - "password"
        - "api_key"
        - "secret"
        - "token"
      required_headers:
        - "x-request-id"
        - "x-user-id"

    # Response filtering
    response_filtering:
      enabled: true
      max_response_length: 50000
      filter_patterns:
        - "(?i)password"
        - "(?i)api[_-]?key"
        - "(?i)secret"

  # Deterministic settings for testing
  testing:
    deterministic_mode: true
    default_temperature: 0.0
    default_seed: 42
    golden_transcripts_path: "tests/golden/model_interactions"

  # Sampling and logging
  observability:
    request_sampling: 0.1  # Sample 10% of requests
    log_level: "info"
    metrics_enabled: true
    tracing_enabled: true

    # Export configuration
    exporters:
      prometheus:
        enabled: true
        endpoint: "http://prometheus:9090"
      jaeger:
        enabled: true
        endpoint: "http://jaeger:14268"

  # Circuit breaker configuration
  circuit_breaker:
    enabled: true
    failure_threshold: 5      # Fail after 5 consecutive errors
    recovery_timeout: "30s"   # Try again after 30 seconds
    half_open_requests: 3     # Allow 3 requests when half-open

  # Model health checks
  health_checks:
    enabled: true
    interval: "5m"
    timeout: "10s"
    endpoints:
      - provider: "openai"
        model: "gpt-3.5-turbo"
        test_prompt: "Hello"
        expected_response_pattern: "(?i)hello|hi|greeting"
      - provider: "anthropic"
        model: "claude-3-sonnet"
        test_prompt: "Test"
        expected_response_pattern: "(?i)test|hello"

  # Emergency overrides
  emergency_overrides:
    enabled: true
    operator_approval_required: true
    auto_expire_after: "1h"
    audit_trail: true

    # Break-glass scenarios
    scenarios:
      - name: "production-incident"
        trigger: "manual"
        actions:
          - "remove_budget_limits"
          - "enable_all_providers"
          - "bypass_routing_rules"
        approval_roles: ["admin", "oncall-engineer"]

      - name: "provider-outage"
        trigger: "health_check_failure"
        actions:
          - "failover_to_backup_provider"
          - "increase_retry_limits"
        approval_roles: ["admin"]