# GPU-enabled Helm values for IntelGraph
ml:
  image:
    repository: intelgraph/ml
    tag: "latest-gpu"
    pullPolicy: IfNotPresent
  
  replicaCount: 1
  
  resources:
    requests:
      memory: "2Gi"
      cpu: "1"
      nvidia.com/gpu: 1
    limits:
      memory: "8Gi"
      cpu: "4"
      nvidia.com/gpu: 1
  
  nodeSelector:
    accelerator: nvidia-tesla-k80
  
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
  
  env:
    USE_SPACY: "true"
    UVICORN_HOST: "0.0.0.0"
    UVICORN_PORT: "8081"

mlWorker:
  image:
    repository: intelgraph/ml
    tag: "latest-gpu"
    pullPolicy: IfNotPresent
  
  replicaCount: 2
  
  resources:
    requests:
      memory: "1Gi"
      cpu: "0.5"
      nvidia.com/gpu: 1
    limits:
      memory: "4Gi"
      cpu: "2"
      nvidia.com/gpu: 1
  
  nodeSelector:
    accelerator: nvidia-tesla-k80
  
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

# Enhanced infrastructure for GPU workloads
redis:
  resources:
    requests:
      memory: "512Mi"
      cpu: "0.5"
    limits:
      memory: "2Gi"
      cpu: "1"

neo4j:
  resources:
    requests:
      memory: "2Gi"
      cpu: "1"
    limits:
      memory: "8Gi"
      cpu: "4"
  
  config:
    dbms.memory.heap.initial_size: "2G"
    dbms.memory.heap.max_size: "6G"
    dbms.memory.pagecache.size: "2G"

postgres:
  resources:
    requests:
      memory: "1Gi"
      cpu: "0.5"
    limits:
      memory: "4Gi"
      cpu: "2"

# Autoscaling for ML services
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Monitoring and observability
monitoring:
  enabled: true
  prometheus:
    enabled: true
  grafana:
    enabled: true
    dashboards:
      gpu: true
      ml: true