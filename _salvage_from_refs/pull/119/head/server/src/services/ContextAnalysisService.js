const { NlpManager } = require('node-nlp');
const natural = require('natural');
const logger = require('../utils/logger');

class ContextAnalysisService {
  constructor() {
    this.nlpManager = new NlpManager({ languages: ['en'], forceNER: true });
    this.tokenizer = new natural.WordTokenizer();
    this.stemmer = natural.PorterStemmer;
    this.tfidf = new natural.TfIdf();
    this.logger = logger;

    // Initialize entity patterns for intelligence analysis
    this.initializeEntityPatterns();
  }

  /**
   * Initialize NLP with intelligence-specific entity patterns
   */
  async initializeEntityPatterns() {
    try {
      // Add entity patterns for common intelligence entities
      this.nlpManager.addNamedEntityText('person', 'John Doe', ['en'], ['John Doe']);
      this.nlpManager.addNamedEntityText('organization', 'CIA', ['en'], ['CIA', 'Central Intelligence Agency']);
      this.nlpManager.addNamedEntityText('location', 'Washington DC', ['en'], ['Washington DC', 'DC']);
      this.nlpManager.addNamedEntityText('phone', 'phone', ['en'], ['+1-555-123-4567', '555-123-4567']);
      this.nlpManager.addNamedEntityText('email', 'email', ['en'], ['john@example.com']);
      this.nlpManager.addNamedEntityText('date', 'date', ['en'], ['2025-01-01', 'January 1st']);
      
      // Train the NLP manager
      await this.nlpManager.train();
      
      this.logger.info('NLP Manager initialized and trained');
    } catch (error) {
      this.logger.error('Error initializing NLP patterns:', error);
    }
  }

  /**
   * Extract entities from text using NLP
   */
  async extractEntities(text) {
    try {
      const result = await this.nlpManager.process('en', text);
      
      // Enhanced entity extraction with custom patterns
      const entities = result.entities || [];
      const customEntities = this.extractCustomEntities(text);
      
      // Merge and deduplicate entities
      const allEntities = [...entities, ...customEntities];
      const uniqueEntities = this.deduplicateEntities(allEntities);

      return {
        entities: uniqueEntities,
        intent: result.intent,
        score: result.score,
        rawResult: result
      };
    } catch (error) {
      this.logger.error('Error extracting entities:', error);
      throw error;
    }
  }

  /**
   * Extract custom entities using regex patterns
   */
  extractCustomEntities(text) {
    const entities = [];
    
    const patterns = {
      ip_address: {
        regex: /\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b/g,
        type: 'ip_address'
      },
      phone_number: {
        regex: /(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})/g,
        type: 'phone_number'
      },
      email: {
        regex: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g,
        type: 'email'
      },
      url: {
        regex: /https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)/g,
        type: 'url'
      },
      ssn: {
        regex: /\b\d{3}-\d{2}-\d{4}\b/g,
        type: 'ssn'
      },
      credit_card: {
        regex: /\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b/g,
        type: 'credit_card'
      },
      coordinates: {
        regex: /[-+]?([1-8]?\d(\.\d+)?|90(\.0+)?),\s*[-+]?(180(\.0+)?|((1[0-7]\d)|([1-9]?\d))(\.\d+)?)/g,
        type: 'coordinates'
      },
      date: {
        regex: /\b\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4}\b|\b\d{4}[\/\-]\d{1,2}[\/\-]\d{1,2}\b/g,
        type: 'date'
      },
      time: {
        regex: /\b\d{1,2}:\d{2}(?::\d{2})?(?:\s?[AaPp][Mm])?\b/g,
        type: 'time'
      }
    };

    Object.keys(patterns).forEach(patternName => {
      const pattern = patterns[patternName];
      let match;
      
      while ((match = pattern.regex.exec(text)) !== null) {
        entities.push({
          entity: pattern.type,
          start: match.index,
          end: match.index + match[0].length,
          accuracy: 0.9,
          sourceText: match[0],
          utteranceText: text,
          len: match[0].length
        });
      }
    });

    return entities;
  }

  /**
   * Deduplicate extracted entities
   */
  deduplicateEntities(entities) {
    const seen = new Set();
    return entities.filter(entity => {
      const key = `${entity.entity}-${entity.sourceText}-${entity.start}`;
      if (seen.has(key)) {
        return false;
      }
      seen.add(key);
      return true;
    });
  }

  /**
   * Analyze text topics using TF-IDF
   */
  analyzeTopics(documents) {
    try {
      // Clear previous documents
      this.tfidf = new natural.TfIdf();
      
      // Add documents to TF-IDF
      documents.forEach(doc => {
        this.tfidf.addDocument(doc);
      });

      const topics = [];
      
      documents.forEach((doc, index) => {
        const terms = [];
        this.tfidf.listTerms(index).slice(0, 10).forEach(item => {
          terms.push({
            term: item.term,
            tfidf: item.tfidf
          });
        });
        
        topics.push({
          documentIndex: index,
          topTerms: terms,
          document: doc.substring(0, 100) + (doc.length > 100 ? '...' : '')
        });
      });

      return {
        topics,
        documentCount: documents.length
      };
    } catch (error) {
      this.logger.error('Error analyzing topics:', error);
      throw error;
    }
  }

  /**
   * Extract key phrases from text
   */
  extractKeyPhrases(text, maxPhrases = 10) {
    try {
      const tokens = this.tokenizer.tokenize(text.toLowerCase());
      const stopWords = new Set([
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
        'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 
        'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'
      ]);
      
      // Filter out stop words and short words
      const meaningfulTokens = tokens.filter(token => 
        !stopWords.has(token) && token.length > 2
      );

      // Create n-grams (phrases)
      const nGrams = [];
      
      // Unigrams
      meaningfulTokens.forEach(token => {
        nGrams.push(token);
      });
      
      // Bigrams
      for (let i = 0; i < meaningfulTokens.length - 1; i++) {
        nGrams.push(`${meaningfulTokens[i]} ${meaningfulTokens[i + 1]}`);
      }
      
      // Trigrams
      for (let i = 0; i < meaningfulTokens.length - 2; i++) {
        nGrams.push(`${meaningfulTokens[i]} ${meaningfulTokens[i + 1]} ${meaningfulTokens[i + 2]}`);
      }

      // Count frequency
      const phraseFreq = {};
      nGrams.forEach(phrase => {
        phraseFreq[phrase] = (phraseFreq[phrase] || 0) + 1;
      });

      // Sort by frequency and return top phrases
      const keyPhrases = Object.entries(phraseFreq)
        .sort(([,a], [,b]) => b - a)
        .slice(0, maxPhrases)
        .map(([phrase, frequency]) => ({
          phrase,
          frequency,
          score: frequency / meaningfulTokens.length
        }));

      return {
        keyPhrases,
        totalTokens: tokens.length,
        meaningfulTokens: meaningfulTokens.length
      };
    } catch (error) {
      this.logger.error('Error extracting key phrases:', error);
      throw error;
    }
  }

  /**
   * Analyze text similarity
   */
  analyzeSimilarity(text1, text2) {
    try {
      const tokens1 = this.tokenizer.tokenize(text1.toLowerCase());
      const tokens2 = this.tokenizer.tokenize(text2.toLowerCase());
      
      // Jaccard similarity
      const set1 = new Set(tokens1);
      const set2 = new Set(tokens2);
      const intersection = new Set([...set1].filter(x => set2.has(x)));
      const union = new Set([...set1, ...set2]);
      const jaccardSimilarity = intersection.size / union.size;

      // Cosine similarity using TF-IDF
      const documents = [text1, text2];
      const tfidf = new natural.TfIdf();
      documents.forEach(doc => tfidf.addDocument(doc));
      
      const vector1 = [];
      const vector2 = [];
      const terms = new Set();
      
      tfidf.listTerms(0).forEach(item => terms.add(item.term));
      tfidf.listTerms(1).forEach(item => terms.add(item.term));
      
      terms.forEach(term => {
        vector1.push(tfidf.tfidf(term, 0));
        vector2.push(tfidf.tfidf(term, 1));
      });

      const cosineSimilarity = this.calculateCosineSimilarity(vector1, vector2);

      return {
        jaccardSimilarity,
        cosineSimilarity,
        averageSimilarity: (jaccardSimilarity + cosineSimilarity) / 2,
        sharedTerms: intersection.size,
        uniqueTerms1: set1.size - intersection.size,
        uniqueTerms2: set2.size - intersection.size
      };
    } catch (error) {
      this.logger.error('Error analyzing similarity:', error);
      throw error;
    }
  }

  /**
   * Calculate cosine similarity between two vectors
   */
  calculateCosineSimilarity(vector1, vector2) {
    const dotProduct = vector1.reduce((sum, a, i) => sum + a * vector2[i], 0);
    const magnitude1 = Math.sqrt(vector1.reduce((sum, a) => sum + a * a, 0));
    const magnitude2 = Math.sqrt(vector2.reduce((sum, a) => sum + a * a, 0));
    
    if (magnitude1 === 0 || magnitude2 === 0) return 0;
    return dotProduct / (magnitude1 * magnitude2);
  }

  /**
   * Perform comprehensive text analysis
   */
  async performComprehensiveAnalysis(text) {
    try {
      const [entities, keyPhrases] = await Promise.all([
        this.extractEntities(text),
        Promise.resolve(this.extractKeyPhrases(text))
      ]);

      // Language detection
      const language = 'en'; // Placeholder for language detection

      // Text statistics
      const stats = this.calculateTextStatistics(text);

      return {
        entities,
        keyPhrases,
        language,
        statistics: stats,
        analysis: {
          complexity: this.calculateTextComplexity(text),
          readability: this.calculateReadabilityScore(text),
          topics: this.identifyTopics(text)
        }
      };
    } catch (error) {
      this.logger.error('Error performing comprehensive analysis:', error);
      throw error;
    }
  }

  /**
   * Calculate text statistics
   */
  calculateTextStatistics(text) {
    const tokens = this.tokenizer.tokenize(text);
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
    const words = tokens.filter(token => /^[a-zA-Z]+$/.test(token));
    
    return {
      characterCount: text.length,
      wordCount: words.length,
      sentenceCount: sentences.length,
      averageWordsPerSentence: words.length / sentences.length || 0,
      averageCharactersPerWord: text.replace(/\s/g, '').length / words.length || 0,
      uniqueWords: new Set(words.map(w => w.toLowerCase())).size,
      lexicalDiversity: (new Set(words.map(w => w.toLowerCase())).size) / words.length || 0
    };
  }

  /**
   * Calculate text complexity score
   */
  calculateTextComplexity(text) {
    const tokens = this.tokenizer.tokenize(text);
    const words = tokens.filter(token => /^[a-zA-Z]+$/.test(token));
    const avgWordLength = words.reduce((sum, word) => sum + word.length, 0) / words.length || 0;
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
    const avgSentenceLength = words.length / sentences.length || 0;
    
    // Simple complexity score based on word and sentence length
    const complexityScore = (avgWordLength * 0.3) + (avgSentenceLength * 0.7);
    
    return {
      score: Math.min(10, complexityScore / 2), // Normalize to 0-10 scale
      averageWordLength: avgWordLength,
      averageSentenceLength: avgSentenceLength,
      level: complexityScore < 8 ? 'Simple' : complexityScore < 15 ? 'Moderate' : 'Complex'
    };
  }

  /**
   * Calculate readability score (simplified Flesch-Kincaid)
   */
  calculateReadabilityScore(text) {
    const tokens = this.tokenizer.tokenize(text);
    const words = tokens.filter(token => /^[a-zA-Z]+$/.test(token));
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
    
    if (sentences.length === 0 || words.length === 0) return 0;
    
    const avgSentenceLength = words.length / sentences.length;
    const avgSyllables = words.reduce((sum, word) => sum + this.countSyllables(word), 0) / words.length;
    
    // Simplified Flesch Reading Ease
    const score = 206.835 - (1.015 * avgSentenceLength) - (84.6 * avgSyllables);
    
    return {
      score: Math.max(0, Math.min(100, score)),
      level: score >= 90 ? 'Very Easy' : 
             score >= 80 ? 'Easy' : 
             score >= 70 ? 'Fairly Easy' : 
             score >= 60 ? 'Standard' : 
             score >= 50 ? 'Fairly Difficult' : 
             score >= 30 ? 'Difficult' : 'Very Difficult'
    };
  }

  /**
   * Count syllables in a word (approximation)
   */
  countSyllables(word) {
    word = word.toLowerCase();
    if (word.length <= 3) return 1;
    word = word.replace(/(?:[^laeiouy]es|ed|[^laeiouy]e)$/, '');
    word = word.replace(/^y/, '');
    const matches = word.match(/[aeiouy]{1,2}/g);
    return matches ? matches.length : 1;
  }

  /**
   * Identify topics in text
   */
  identifyTopics(text) {
    const keyPhrases = this.extractKeyPhrases(text, 5);
    
    // Intelligence-specific topic categories
    const topicCategories = {
      security: ['security', 'threat', 'risk', 'vulnerability', 'attack', 'breach', 'defense'],
      military: ['military', 'army', 'navy', 'air force', 'troops', 'deployment', 'operation'],
      political: ['government', 'policy', 'election', 'political', 'parliament', 'congress'],
      economic: ['economy', 'trade', 'market', 'financial', 'investment', 'economic'],
      social: ['social', 'community', 'people', 'society', 'cultural', 'demographic'],
      technology: ['technology', 'cyber', 'digital', 'computer', 'internet', 'software']
    };

    const detectedTopics = {};
    const textLower = text.toLowerCase();

    Object.keys(topicCategories).forEach(category => {
      const keywords = topicCategories[category];
      const matches = keywords.filter(keyword => textLower.includes(keyword));
      if (matches.length > 0) {
        detectedTopics[category] = matches.length;
      }
    });

    return {
      categories: detectedTopics,
      dominantTopic: Object.keys(detectedTopics).reduce((a, b) => 
        detectedTopics[a] > detectedTopics[b] ? a : b, 
        Object.keys(detectedTopics)[0]
      ),
      topKeyPhrases: keyPhrases.keyPhrases.slice(0, 3)
    };
  }
}

module.exports = ContextAnalysisService;