# IntelGraph Alert Rules for Prometheus

groups:
  - name: intelgraph.rules
    rules:
      # Application Health Alerts
      - alert: IntelGraphDown
        expr: up{job="intelgraph-app"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'IntelGraph application is down'
          description: 'IntelGraph application has been down for more than 1 minute.'

      - alert: IntelGraphHighErrorRate
        expr: rate(http_requests_total{job="intelgraph-app",status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High error rate in IntelGraph'
          description: 'IntelGraph is experiencing a high error rate ({{ $value }} errors/sec).'

      - alert: IntelGraphHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="intelgraph-app"}[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High latency in IntelGraph'
          description: '95th percentile latency is {{ $value }}s, which is above the 2s threshold.'

  - name: database.rules
    rules:
      # PostgreSQL Alerts
      - alert: PostgreSQLDown
        expr: up{job="postgres-exporter"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'PostgreSQL is down'
          description: 'PostgreSQL database has been down for more than 1 minute.'

      - alert: PostgreSQLHighConnections
        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'PostgreSQL high connection usage'
          description: 'PostgreSQL connection usage is {{ $value | humanizePercentage }}.'

      - alert: PostgreSQLSlowQueries
        expr: rate(pg_stat_database_tup_returned[5m]) / rate(pg_stat_database_tup_fetched[5m]) < 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: 'PostgreSQL slow queries detected'
          description: 'PostgreSQL query efficiency is low ({{ $value | humanizePercentage }}).'

      # Redis Alerts
      - alert: RedisDown
        expr: up{job="redis-exporter"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'Redis is down'
          description: 'Redis server has been down for more than 1 minute.'

      - alert: RedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'Redis high memory usage'
          description: 'Redis memory usage is {{ $value | humanizePercentage }}.'

      - alert: RedisHighConnections
        expr: redis_connected_clients > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'Redis high client connections'
          description: 'Redis has {{ $value }} client connections.'

      # Neo4j Alerts
      - alert: Neo4jDown
        expr: up{job="neo4j"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'Neo4j is down'
          description: 'Neo4j database has been down for more than 1 minute.'

  - name: system.rules
    rules:
      # System Resource Alerts
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High CPU usage'
          description: 'CPU usage is {{ $value }}% on instance {{ $labels.instance }}.'

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'High memory usage'
          description: 'Memory usage is {{ $value }}% on instance {{ $labels.instance }}.'

      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: 'High disk usage'
          description: 'Disk usage is {{ $value }}% on instance {{ $labels.instance }}, mount {{ $labels.mountpoint }}.'

      - alert: DiskWillFillIn24Hours
        expr: predict_linear(node_filesystem_avail_bytes[1h], 24*3600) < 0
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: 'Disk will fill in 24 hours'
          description: 'Disk on {{ $labels.instance }}, mount {{ $labels.mountpoint }} will likely fill within 24 hours.'

  - name: war_room.rules
    rules:
      # War Room Sync Performance
      - alert: WarRoomHighLatency
        expr: histogram_quantile(0.95, rate(war_room_sync_duration_seconds_bucket[5m])) > 0.3
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'War Room sync latency high'
          description: 'War Room sync 95th percentile latency is {{ $value }}s, exceeding 300ms target.'

      - alert: WarRoomSyncFailures
        expr: rate(war_room_sync_failures_total[5m]) > 0.01
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'War Room sync failures detected'
          description: 'War Room sync failure rate: {{ $value }} failures/sec.'

  - name: ml_models.rules
    rules:
      # ML Model Performance
      - alert: MLModelPredictionLatency
        expr: histogram_quantile(0.95, rate(ml_prediction_duration_seconds_bucket[5m])) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'ML model prediction latency high'
          description: 'ML model prediction latency is {{ $value }}s, which may impact user experience.'

      - alert: MLModelAccuracyDegraded
        expr: ml_model_accuracy < 0.85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: 'ML model accuracy degraded'
          description: 'ML model accuracy has dropped to {{ $value }}, below 85% threshold.'

  - name: security.rules
    rules:
      # Security Alerts
      - alert: HighFailedLoginRate
        expr: rate(auth_failed_attempts_total[5m]) > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High failed login rate'
          description: 'Failed login rate is {{ $value }} attempts/sec, possible brute force attack.'

      - alert: UnauthorizedAPIAccess
        expr: rate(http_requests_total{status="401"}[5m]) > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'High unauthorized access attempts'
          description: 'Unauthorized API access rate: {{ $value }} requests/sec.'

      - alert: SuspiciousActivity
        expr: rate(security_events_total{severity="high"}[10m]) > 1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'Suspicious activity detected'
          description: 'High severity security events detected: {{ $value }} events in 10 minutes.'
