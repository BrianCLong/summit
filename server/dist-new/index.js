var __defProp = Object.defineProperty;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __require = /* @__PURE__ */ ((x) => typeof require !== "undefined" ? require : typeof Proxy !== "undefined" ? new Proxy(x, {
  get: (a, b) => (typeof require !== "undefined" ? require : a)[b]
}) : x)(function(x) {
  if (typeof require !== "undefined") return require.apply(this, arguments);
  throw Error('Dynamic require of "' + x + '" is not supported');
});
var __esm = (fn, res) => function __init() {
  return fn && (res = (0, fn[__getOwnPropNames(fn)[0]])(fn = 0)), res;
};
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};

// src/config.ts
import "dotenv/config";
import { z } from "zod";
var EnvSchema, TestEnvSchema, Env, TestEnv, ENV_VAR_HELP, initializeConfig, _cfg, cfg, dbUrls;
var init_config = __esm({
  "src/config.ts"() {
    "use strict";
    EnvSchema = z.object({
      NODE_ENV: z.string().default("development"),
      PORT: z.coerce.number().default(4e3),
      DATABASE_URL: z.string().min(1),
      NEO4J_URI: z.string().min(1),
      NEO4J_USER: z.string().min(1),
      NEO4J_PASSWORD: z.string().min(1),
      REDIS_HOST: z.string().default("localhost"),
      REDIS_PORT: z.coerce.number().default(6379),
      REDIS_PASSWORD: z.string().optional().default(""),
      JWT_SECRET: z.string().min(32),
      JWT_REFRESH_SECRET: z.string().min(32),
      CORS_ORIGIN: z.string().default("http://localhost:3000"),
      RATE_LIMIT_WINDOW_MS: z.coerce.number().default(6e4),
      RATE_LIMIT_MAX_REQUESTS: z.coerce.number().default(100),
      RATE_LIMIT_MAX_AUTHENTICATED: z.coerce.number().default(1e3),
      AI_RATE_LIMIT_WINDOW_MS: z.coerce.number().default(15 * 60 * 1e3),
      AI_RATE_LIMIT_MAX_REQUESTS: z.coerce.number().default(50),
      BACKGROUND_RATE_LIMIT_WINDOW_MS: z.coerce.number().default(6e4),
      BACKGROUND_RATE_LIMIT_MAX_REQUESTS: z.coerce.number().default(120),
      CACHE_ENABLED: z.coerce.boolean().default(true),
      CACHE_TTL_DEFAULT: z.coerce.number().default(300),
      // 5 minutes
      L1_CACHE_MAX_BYTES: z.coerce.number().default(1 * 1024 * 1024 * 1024),
      // 1 GB
      L1_CACHE_FALLBACK_TTL_SECONDS: z.coerce.number().default(300),
      // 5 minutes
      DOCLING_SVC_URL: z.string().url().default("http://localhost:5001"),
      DOCLING_SVC_TIMEOUT_MS: z.coerce.number().default(3e4),
      // GraphQL Cost Analysis & Rate Limiting
      ENFORCE_GRAPHQL_COST_LIMITS: z.coerce.boolean().default(true),
      GRAPHQL_COST_CONFIG_PATH: z.string().optional(),
      COST_EXEMPT_TENANTS: z.string().optional().default(""),
      GA_CLOUD: z.coerce.boolean().default(false),
      AWS_REGION: z.string().optional(),
      AI_ENABLED: z.coerce.boolean().default(false),
      FACTFLOW_ENABLED: z.coerce.boolean().default(false),
      KAFKA_ENABLED: z.coerce.boolean().default(false),
      OPENAI_API_KEY: z.string().optional(),
      ANTHROPIC_API_KEY: z.string().optional(),
      SKIP_AI_ROUTES: z.coerce.boolean().default(false),
      SKIP_WEBHOOKS: z.coerce.boolean().default(false),
      SKIP_GRAPHQL: z.coerce.boolean().default(false),
      // Email Configuration
      SMTP_HOST: z.string().optional(),
      SMTP_PORT: z.coerce.number().optional().default(587),
      SMTP_USER: z.string().optional(),
      SMTP_PASS: z.string().optional(),
      EMAIL_FROM_ADDRESS: z.string().email().optional().default("noreply@intelgraph.com"),
      EMAIL_FROM_NAME: z.string().optional().default("IntelGraph")
    });
    TestEnvSchema = EnvSchema.extend({
      DATABASE_URL: z.string().optional().default("postgresql://postgres:testpassword@localhost:5432/intelgraph_test"),
      JWT_SECRET: z.string().min(32).optional().default("test-jwt-secret-at-least-32-chars-long"),
      JWT_REFRESH_SECRET: z.string().min(32).optional().default("test-jwt-refresh-secret-at-least-32-chars-long"),
      NEO4J_URI: z.string().optional().default("bolt://localhost:7687"),
      NEO4J_USER: z.string().optional().default("neo4j"),
      NEO4J_PASSWORD: z.string().optional().default("testpassword")
    });
    Env = EnvSchema.passthrough();
    TestEnv = TestEnvSchema.passthrough();
    ENV_VAR_HELP = {
      DATABASE_URL: "PostgreSQL connection string (e.g., postgresql://user:pass@host:5432/db)",
      RATE_LIMIT_WINDOW_MS: "Window size for rate limiting in milliseconds (default: 60000)",
      RATE_LIMIT_MAX_REQUESTS: "Max requests per window per user/IP (default: 100)",
      RATE_LIMIT_MAX_AUTHENTICATED: "Max requests per window for authenticated users (default: 1000)",
      AI_RATE_LIMIT_WINDOW_MS: "Window size for AI endpoints (default: 15 minutes)",
      AI_RATE_LIMIT_MAX_REQUESTS: "Max requests for AI endpoints per window (default: 50)",
      BACKGROUND_RATE_LIMIT_WINDOW_MS: "Window size for Redis-backed background throttles (default: 60s)",
      BACKGROUND_RATE_LIMIT_MAX_REQUESTS: "Max background jobs per window per identifier (default: 120)",
      CACHE_ENABLED: "Enable or disable caching (default: true)",
      CACHE_TTL_DEFAULT: "Default cache TTL in seconds (default: 300)",
      NEO4J_URI: "Neo4j bolt URI (e.g., bolt://localhost:7687)",
      NEO4J_USER: "Neo4j username (default: neo4j)",
      NEO4J_PASSWORD: "Neo4j password (set in Neo4j config)",
      REDIS_HOST: "Redis hostname (default: localhost)",
      REDIS_PORT: "Redis port (default: 6379)",
      JWT_SECRET: "JWT signing secret (min 32 characters, use strong random value)",
      JWT_REFRESH_SECRET: "JWT refresh token secret (min 32 characters, different from JWT_SECRET)",
      CORS_ORIGIN: "Allowed CORS origins (comma-separated, e.g., http://localhost:3000)",
      ENFORCE_GRAPHQL_COST_LIMITS: "Enable/disable GraphQL cost limit enforcement (default: true)",
      GRAPHQL_COST_CONFIG_PATH: "Path to GraphQL cost configuration JSON file (optional)",
      COST_EXEMPT_TENANTS: "Comma-separated list of tenant IDs exempt from cost limits",
      GA_CLOUD: "Enable strict GA cloud readiness checks (default: false)",
      AWS_REGION: "AWS Region (required if GA_CLOUD is true)",
      AI_ENABLED: "Enable AI-augmented features (default: false)",
      FACTFLOW_ENABLED: "Enable FactFlow product module (default: false)",
      OPENAI_API_KEY: "OpenAI API Key (required if AI_ENABLED=true)",
      ANTHROPIC_API_KEY: "Anthropic API Key (required if AI_ENABLED=true)",
      SMTP_HOST: "SMTP host for emails (e.g., smtp.gmail.com)",
      SMTP_PORT: "SMTP port (default: 587)",
      SMTP_USER: "SMTP username",
      SMTP_PASS: "SMTP password",
      EMAIL_FROM_ADDRESS: "Verified sender email address",
      EMAIL_FROM_NAME: "Sender name displayed in emails"
    };
    initializeConfig = (options2 = { exitOnError: true }) => {
      const isTest = process.env.NODE_ENV === "test";
      const schema2 = isTest ? TestEnv : Env;
      const rawData = { ...process.env };
      const parsed = schema2.safeParse(rawData);
      if (parsed.success) {
        const data = parsed.data;
        if (data.AI_ENABLED && !data.OPENAI_API_KEY && !data.ANTHROPIC_API_KEY) {
          const msg = "\n\u274C AI Configuration Error: AI_ENABLED=true requires either OPENAI_API_KEY or ANTHROPIC_API_KEY.\n";
          console.error(msg);
          if (options2.exitOnError) process.exit(1);
          throw new Error(msg);
        }
      }
      if (!parsed.success) {
        console.error("\n\u274C Environment Validation Failed\n");
        console.error("Missing or invalid environment variables:\n");
        parsed.error.issues.forEach((issue) => {
          const varName = issue.path.join(".");
          const help = ENV_VAR_HELP[varName] || "See .env.example for expected format";
          console.error(`  \u2022 ${varName}`);
          console.error(`    Error: ${issue.message}`);
          console.error(`    Help: ${help}
`);
        });
        console.error("\nHow to fix:");
        console.error("  1. Copy .env.example to .env: cp .env.example .env");
        console.error("  2. Update the missing variables in .env");
        console.error("  3. For production, generate strong secrets (e.g., openssl rand -base64 32)");
        console.error("  4. See docs/ONBOARDING.md for detailed setup instructions\n");
        if (options2.exitOnError) process.exit(1);
        throw new Error("Environment Validation Failed");
      }
      const env2 = parsed.data;
      const present = Object.keys(env2).length;
      if (env2.NODE_ENV === "production") {
        const insecureTokens = ["devpassword", "changeme", "secret", "localhost"];
        const fail = (key, reason) => {
          const msg = `
\u274C Production Configuration Error
  Invariant GC-03 Violated: Production environment cannot use default secrets.
  Variable: ${key}
  Issue: ${reason}
`;
          console.error(msg);
          if (options2.exitOnError) process.exit(1);
          throw new Error(msg);
        };
        const guardSecret = (key) => {
          const value = String(env2[key]);
          const normalized = value.toLowerCase();
          if (value.length < 32) {
            fail(key, "value too short (need >= 32 chars)");
          }
          const hit = insecureTokens.find((token) => normalized.includes(token));
          if (hit) {
            fail(key, `contains insecure token (${hit})`);
          }
        };
        guardSecret("JWT_SECRET");
        guardSecret("JWT_REFRESH_SECRET");
        const corsOrigins = env2.CORS_ORIGIN.split(",").map((origin) => origin.trim());
        if (corsOrigins.length === 0 || corsOrigins.some(
          (origin) => origin === "*" || origin.startsWith("http://") || origin.includes("localhost")
        )) {
          fail("CORS_ORIGIN", "must list explicit https origins");
        }
        const dbSecrets = [
          ["DATABASE_URL", env2.DATABASE_URL],
          ["NEO4J_PASSWORD", env2.NEO4J_PASSWORD],
          ["REDIS_PASSWORD", env2.REDIS_PASSWORD]
        ];
        dbSecrets.forEach(([key, value]) => {
          if (!value) {
            fail(key, "missing value");
          }
          const normalized = value.toLowerCase();
          if (normalized.includes("localhost") || normalized.includes("devpassword")) {
            fail(key, "contains localhost/devpassword; set a production secret");
          }
        });
      }
      if (env2.GA_CLOUD) {
        console.log("\u{1F512} GA Cloud Guard Active: Enforcing strict production constraints");
        if (env2.NODE_ENV !== "production") {
          const msg = '\n\u274C GA Cloud Error: NODE_ENV must be "production" when GA_CLOUD is enabled.\n';
          console.error(msg);
          if (options2.exitOnError) process.exit(1);
          throw new Error(msg);
        }
        if (!env2.AWS_REGION) {
          const msg = "\n\u274C GA Cloud Error: AWS_REGION is required when GA_CLOUD is enabled.\n";
          console.error(msg);
          if (options2.exitOnError) process.exit(1);
          throw new Error(msg);
        }
        const criticalUrls = [env2.DATABASE_URL, env2.NEO4J_URI];
        criticalUrls.forEach((url) => {
          if (url && (url.includes("localhost") || url.includes("127.0.0.1"))) {
            const msg = `
\u274C GA Cloud Error: Critical service URL contains localhost: ${url}
`;
            console.error(msg);
            if (options2.exitOnError) process.exit(1);
            throw new Error(msg);
          }
        });
      }
      if (env2.NODE_ENV !== "production" && !env2.GA_CLOUD) {
        console.log(`[STARTUP] Environment validated (${present} keys)`);
      }
      return env2;
    };
    _cfg = null;
    cfg = new Proxy({}, {
      get: (_target, prop) => {
        if (!_cfg) {
          _cfg = initializeConfig();
        }
        return _cfg[prop];
      },
      ownKeys: (_target) => {
        if (!_cfg) {
          _cfg = initializeConfig();
        }
        return Reflect.ownKeys(_cfg);
      },
      getOwnPropertyDescriptor: (_target, prop) => {
        if (!_cfg) {
          _cfg = initializeConfig();
        }
        return Reflect.getOwnPropertyDescriptor(_cfg, prop);
      }
    });
    dbUrls = {
      redis: `redis://${cfg.REDIS_HOST}:${cfg.REDIS_PORT}`,
      postgres: cfg.DATABASE_URL,
      neo4j: cfg.NEO4J_URI
    };
  }
});

// src/lib/telemetry/correlation-engine.ts
var CorrelationEngine, correlationEngine;
var init_correlation_engine = __esm({
  "src/lib/telemetry/correlation-engine.ts"() {
    "use strict";
    CorrelationEngine = class _CorrelationEngine {
      static instance;
      logBuffer = [];
      MAX_LOGS = 5e3;
      bufferIndex = 0;
      isBufferFull = false;
      constructor() {
      }
      static getInstance() {
        if (!_CorrelationEngine.instance) {
          _CorrelationEngine.instance = new _CorrelationEngine();
        }
        return _CorrelationEngine.instance;
      }
      ingestLog(entry) {
        let timestamp = entry.time || entry.timestamp || Date.now();
        if (typeof timestamp === "string") {
          timestamp = new Date(timestamp).getTime();
        }
        const logEntry = {
          ...entry,
          timestamp,
          level: typeof entry.level === "number" ? this.pinoLevelToString(entry.level) : entry.level || "info",
          message: entry.msg || entry.message || ""
        };
        if (this.logBuffer.length < this.MAX_LOGS) {
          this.logBuffer.push(logEntry);
        } else {
          this.logBuffer[this.bufferIndex] = logEntry;
          this.isBufferFull = true;
        }
        this.bufferIndex = (this.bufferIndex + 1) % this.MAX_LOGS;
      }
      pinoLevelToString(level) {
        if (level >= 60) return "fatal";
        if (level >= 50) return "error";
        if (level >= 40) return "warn";
        if (level >= 30) return "info";
        if (level >= 20) return "debug";
        return "trace";
      }
      analyze(metricName, windowSeconds = 60) {
        const now = Date.now();
        const startTime = now - windowSeconds * 1e3;
        const windowLogs = this.logBuffer.filter((l) => l.timestamp >= startTime && l.timestamp <= now);
        const traceIds = /* @__PURE__ */ new Set();
        windowLogs.forEach((l) => {
          if (l.traceId) traceIds.add(l.traceId);
          if (l.correlationId) traceIds.add(l.correlationId);
        });
        const metricsContext = {
          system: {
            memory: process.memoryUsage(),
            cpu: process.cpuUsage(),
            uptime: process.uptime()
          }
        };
        return {
          timestamp: now,
          triggerMetric: metricName,
          windowStart: startTime,
          windowEnd: now,
          logs: windowLogs,
          relatedTraces: Array.from(traceIds),
          metricsContext
        };
      }
    };
    correlationEngine = CorrelationEngine.getInstance();
  }
});

// src/config/logger.ts
import pino from "pino";
import { AsyncLocalStorage } from "async_hooks";
var correlationStorage, REDACT_PATHS, stream, logger, logger_default;
var init_logger = __esm({
  "src/config/logger.ts"() {
    "use strict";
    init_config();
    init_correlation_engine();
    correlationStorage = new AsyncLocalStorage();
    REDACT_PATHS = [
      "req.headers.authorization",
      "req.headers.cookie",
      'req.headers["x-auth-token"]',
      'req.headers["x-api-key"]',
      "body.password",
      "body.token",
      "body.refreshToken",
      "body.secret",
      "password",
      "token",
      "secret",
      "user.email",
      "user.phone"
    ];
    stream = {
      write: (msg) => {
        if (msg.trim().startsWith("{")) {
          try {
            const logEntry = JSON.parse(msg);
            correlationEngine.ingestLog(logEntry);
          } catch (e) {
          }
        }
        process.stdout.write(msg);
      }
    };
    logger = pino({
      level: process.env.LOG_LEVEL || "info",
      base: {
        service: "intelgraph-server",
        env: cfg.NODE_ENV,
        version: process.env.npm_package_version || "unknown"
      },
      timestamp: () => `,"time":"${(/* @__PURE__ */ new Date()).toISOString()}"`,
      redact: {
        paths: REDACT_PATHS,
        censor: "[REDACTED]"
      },
      mixin() {
        const store = correlationStorage.getStore();
        if (store) {
          return {
            correlationId: store.get("correlationId"),
            tenantId: store.get("tenantId"),
            principalId: store.get("principalId"),
            requestId: store.get("requestId"),
            traceId: store.get("traceId")
          };
        }
        return {};
      },
      formatters: {
        level: (label) => ({ level: label.toUpperCase() }),
        bindings: (bindings) => ({
          pid: bindings.pid,
          host: bindings.hostname
        })
      }
    }, stream);
    logger_default = logger;
  }
});

// src/db/config.ts
var dbConfig;
var init_config2 = __esm({
  "src/db/config.ts"() {
    "use strict";
    init_config();
    dbConfig = {
      connectionConfig: {
        connectionString: cfg.DATABASE_URL,
        ssl: cfg.NODE_ENV === "production" ? { rejectUnauthorized: false } : false
      },
      idleTimeoutMs: 1e4,
      connectionTimeoutMs: 5e3,
      maxPoolSize: parseInt(process.env.PG_WRITE_POOL_SIZE || "20", 10),
      readPoolSize: parseInt(process.env.PG_READ_POOL_SIZE || "5", 10),
      statementTimeoutMs: 3e4,
      slowQueryThresholdMs: Number.parseInt(
        process.env.SLOW_QUERY_MS ?? "250",
        10
      )
    };
  }
});

// src/utils/logger.ts
import * as pinoPkg from "pino";
var pino3, pinoLogger, logger2, logger_default2;
var init_logger2 = __esm({
  "src/utils/logger.ts"() {
    "use strict";
    pino3 = pinoPkg.pino || pinoPkg.default || (typeof pinoPkg === "function" ? pinoPkg : null);
    if (!pino3 && pinoPkg.default && typeof pinoPkg.default === "function") {
      pino3 = pinoPkg.default;
    }
    if (typeof pino3 !== "function") {
      pino3 = () => ({
        info: () => {
        },
        error: () => {
        },
        warn: () => {
        },
        debug: () => {
        },
        child: function() {
          return this;
        },
        level: "info"
      });
    }
    pinoLogger = pino3({
      level: process.env.LOG_LEVEL || "info",
      base: { service: "intelgraph-api" },
      timestamp: () => `,"time":"${(/* @__PURE__ */ new Date()).toISOString()}"`,
      formatters: {
        level: (label) => {
          return { level: label };
        }
      }
    });
    logger2 = pinoLogger;
    logger_default2 = pinoLogger;
  }
});

// src/observability/telemetry.ts
function initializeTelemetry() {
  logger_default2.info("Telemetry disabled (no-op).");
  return {
    start: async () => {
    },
    shutdown: async () => {
    }
  };
}
var SERVICE_NAME, SERVICE_VERSION, DEPLOYMENT_ENVIRONMENT, meter, businessMetrics, SpanStatusCode, SpanKind, tracer, IntelGraphCostTracker, costTracker, Neo4jSlowQueryKiller, slowQueryKiller;
var init_telemetry = __esm({
  "src/observability/telemetry.ts"() {
    "use strict";
    init_logger2();
    SERVICE_NAME = process.env.SERVICE_NAME || "intelgraph-server";
    SERVICE_VERSION = process.env.SERVICE_VERSION || "1.0.0";
    DEPLOYMENT_ENVIRONMENT = process.env.NODE_ENV || "development";
    meter = {
      createCounter: (_2, __) => ({ add: (_v, _a) => {
      } }),
      createHistogram: (_2, __) => ({ record: (_v, _a) => {
      } }),
      createGauge: (_2, __) => ({ record: (_v, _a) => {
      }, set: (_v, _a) => {
      } })
    };
    businessMetrics = {
      // Query metrics
      nlToCypherRequests: meter.createCounter("nlq_parse_requests_total", {
        description: "Total number of NL\u2192Cypher translation requests"
      }),
      nlToCypherParseTime: meter.createHistogram("nlq_parse_time_ms", {
        description: "Time taken to parse NL queries to Cypher",
        unit: "ms"
      }),
      nlToCypherValidity: meter.createCounter("nlq_validity_total", {
        description: "Count of valid vs invalid generated Cypher queries"
      }),
      cypherQueryExecutions: meter.createCounter("cypher_query_executions_total", {
        description: "Total number of Cypher query executions"
      }),
      cypherQueryDuration: meter.createHistogram("cypher_query_duration_ms", {
        description: "Cypher query execution time",
        unit: "ms"
      }),
      // Graph metrics
      graphHopQueries: meter.createHistogram("graph_hop_queries_ms", {
        description: "Graph hop query performance by hop count",
        unit: "ms"
      }),
      graphQueryComplexity: meter.createHistogram("graph_query_complexity", {
        description: "Graph query complexity score"
      }),
      // Provenance metrics
      provenanceWrites: meter.createCounter("provenance_writes_total", {
        description: "Total provenance ledger write operations"
      }),
      evidenceRegistrations: meter.createCounter("evidence_registrations_total", {
        description: "Total evidence registrations"
      }),
      claimCreations: meter.createCounter("claim_creations_total", {
        description: "Total claim creations"
      }),
      exportRequests: meter.createCounter("export_requests_total", {
        description: "Total export requests"
      }),
      exportBlocks: meter.createCounter("export_blocks_total", {
        description: "Total blocked export requests by policy"
      }),
      // Policy metrics
      policyEvaluations: meter.createCounter("policy_evaluations_total", {
        description: "Total OPA policy evaluations"
      }),
      policyDecisionTime: meter.createHistogram("policy_decision_time_ms", {
        description: "Time taken for policy decisions",
        unit: "ms"
      }),
      // Cost metrics
      costBudgetUtilization: meter.createGauge("cost_budget_utilization_ratio", {
        description: "Current cost budget utilization ratio"
      }),
      queryBudgetConsumed: meter.createCounter("query_budget_consumed_total", {
        description: "Total query budget consumed"
      }),
      // Connector metrics
      connectorIngests: meter.createCounter("connector_ingests_total", {
        description: "Total connector ingest operations"
      }),
      connectorErrors: meter.createCounter("connector_errors_total", {
        description: "Total connector errors"
      }),
      connectorLatency: meter.createHistogram("connector_latency_ms", {
        description: "Connector operation latency",
        unit: "ms"
      })
    };
    SpanStatusCode = { OK: "OK", ERROR: "ERROR" };
    SpanKind = { INTERNAL: "INTERNAL", SERVER: "SERVER", CLIENT: "CLIENT", PRODUCER: "PRODUCER", CONSUMER: "CONSUMER" };
    tracer = {
      startActiveSpan: (_name, _opts, fn) => fn({
        setAttributes: (_a) => {
        },
        setStatus: (_s) => {
        },
        recordException: (_e) => {
        },
        spanContext: () => ({ traceId: "unknown", spanId: "unknown" }),
        end: () => {
        }
      })
    };
    IntelGraphCostTracker = class {
      budgets = /* @__PURE__ */ new Map();
      constructor() {
        this.budgets.set("default", { used: 0, limit: 1e3 });
      }
      track(operation, cost, metadata = {}) {
        const tenantId = metadata.tenantId || "default";
        const budget = this.budgets.get(tenantId) || { used: 0, limit: 1e3 };
        budget.used += cost;
        this.budgets.set(tenantId, budget);
        businessMetrics.queryBudgetConsumed.add(cost, {
          tenant_id: tenantId,
          operation
        });
        businessMetrics.costBudgetUtilization.record(budget.used / budget.limit, {
          tenant_id: tenantId
        });
        logger_default2.debug(
          {
            operation,
            cost,
            tenantId,
            budgetUsed: budget.used,
            budgetLimit: budget.limit,
            utilization: budget.used / budget.limit
          },
          "Cost tracked"
        );
      }
      async getCurrentBudget(tenantId) {
        const budget = this.budgets.get(tenantId) || { used: 0, limit: 1e3 };
        return budget.limit - budget.used;
      }
      async checkBudgetLimit(tenantId, cost) {
        const remainingBudget = await this.getCurrentBudget(tenantId);
        return cost <= remainingBudget;
      }
    };
    costTracker = new IntelGraphCostTracker();
    Neo4jSlowQueryKiller = class {
      activeQueries = /* @__PURE__ */ new Map();
      registerQuery(queryId, query3, timeout) {
        const startTime = /* @__PURE__ */ new Date();
        const timeoutHandle = setTimeout(() => {
        }, timeout);
        this.activeQueries.set(queryId, {
          query: query3,
          startTime,
          timeout,
          timeoutHandle
        });
        logger_default2.debug(
          { queryId, timeout },
          "Query registered for timeout monitoring"
        );
      }
      killQuery(queryId, reason) {
        const queryInfo = this.activeQueries.get(queryId);
        if (!queryInfo) {
          return;
        }
        clearTimeout(queryInfo.timeoutHandle);
        this.activeQueries.delete(queryId);
        logger_default2.warn(
          {
            queryId,
            reason,
            query: queryInfo.query.substring(0, 100)
          },
          "Query killed"
        );
      }
      getActiveQueries() {
        return Array.from(this.activeQueries.entries()).map(([id, info]) => ({
          id,
          query: info.query,
          startTime: info.startTime,
          timeout: info.timeout
        }));
      }
      // Complete a query normally
      completeQuery(queryId) {
        const queryInfo = this.activeQueries.get(queryId);
        if (!queryInfo) {
          return;
        }
        clearTimeout(queryInfo.timeoutHandle);
        this.activeQueries.delete(queryId);
      }
    };
    slowQueryKiller = new Neo4jSlowQueryKiller();
    if (process.env.NODE_ENV !== "test") {
      const sdk = initializeTelemetry();
      sdk.start();
      process.on("SIGTERM", () => {
        sdk.shutdown().catch(() => {
        });
      });
    }
  }
});

// src/middleware/observability/otel-tracing.ts
var OTelTracingService, otelService, otelMiddleware;
var init_otel_tracing = __esm({
  "src/middleware/observability/otel-tracing.ts"() {
    "use strict";
    init_telemetry();
    init_logger2();
    OTelTracingService = class _OTelTracingService {
      static instance;
      sdk = null;
      config;
      tracer;
      static getInstance() {
        if (!_OTelTracingService.instance) {
          _OTelTracingService.instance = new _OTelTracingService();
        }
        return _OTelTracingService.instance;
      }
      constructor() {
        this.config = {
          enabled: process.env.OTEL_ENABLED !== "false",
          service_name: process.env.OTEL_SERVICE_NAME || "intelgraph-api",
          service_version: process.env.OTEL_SERVICE_VERSION || "2.5.0",
          jaeger_endpoint: process.env.JAEGER_ENDPOINT || "http://localhost:14268/api/traces",
          prometheus_enabled: process.env.PROMETHEUS_ENABLED !== "false",
          sample_rate: parseFloat(process.env.OTEL_SAMPLE_RATE || "1.0")
        };
        this.tracer = tracer;
        if (this.config.enabled) {
          this.initializeSDK();
        }
      }
      // Committee requirement: OTEL SDK initialization
      initializeSDK() {
        try {
          const { trace: trace12 } = __require("@opentelemetry/api");
          this.tracer = trace12.getTracer(this.config.service_name, this.config.service_version);
          logger_default2.info({ message: "OTel tracing enabled", service: this.config.service_name });
        } catch {
          this.tracer = tracer;
          logger_default2.warn({ message: "OTel SDK not available, using no-op tracer" });
        }
      }
      // Committee requirement: Express middleware for request tracing
      createMiddleware() {
        return (req, res, next) => {
          if (!this.config.enabled) {
            return next();
          }
          const span = this.tracer.startSpan(`HTTP ${req.method} ${req.path}`, {
            kind: SpanKind.SERVER,
            attributes: {
              "http.method": req.method,
              "http.url": req.url,
              "http.route": req.path,
              "http.user_agent": req.get("user-agent") || ""
            }
          });
          res.on("finish", () => {
            span.setAttributes({
              "http.status_code": res.statusCode
            });
            span.setStatus({
              code: res.statusCode >= 400 ? SpanStatusCode.ERROR : SpanStatusCode.OK
            });
            span.end();
          });
          next();
        };
      }
      // Committee requirement: Manual span creation for business operations
      createSpan(name, attributes, parentSpan) {
        if (!this.config.enabled) {
          return null;
        }
        try {
          const span = this.tracer.startSpan(name, { attributes });
          return span;
        } catch {
          return null;
        }
      }
      // Committee requirement: Database operation tracing
      traceDatabaseOperation(operation, dbType, query3, parentSpan) {
        return async (dbOperation) => {
          return await dbOperation();
        };
      }
      // Committee requirement: XAI operation tracing
      traceXAIOperation(operationType, modelVersion, inputHash, parentSpan) {
        return async (xaiOperation) => {
          return await xaiOperation();
        };
      }
      // Committee requirement: Streaming operation tracing
      traceStreamingOperation(operationType, messageCount, parentSpan) {
        return async (streamOperation) => {
          return await streamOperation();
        };
      }
      // Committee requirement: Authority operation tracing
      traceAuthorityCheck(operation, userId, clearanceLevel, parentSpan) {
        return async (authorityCheck) => {
          if (!this.config.enabled) {
            return await authorityCheck();
          }
          const span = this.createSpan(
            `authority.${operation}`,
            {
              "authority.operation": operation,
              "authority.user_id": userId,
              "authority.clearance_level": clearanceLevel,
              "authority.check_type": "runtime_validation"
            },
            parentSpan
          );
          if (!span) {
            return await authorityCheck();
          }
          try {
            const result2 = await authorityCheck();
            span.setAttributes({
              "authority.check_result": "allowed",
              "authority.success": true
            });
            span.setStatus({ code: SpanStatusCode.OK });
            return result2;
          } catch (error) {
            span.setAttributes({
              "authority.check_result": "denied",
              "authority.success": false,
              "authority.denial_reason": error instanceof Error ? error.message : String(error)
            });
            span.setStatus({
              code: SpanStatusCode.ERROR,
              message: "Authority check failed"
            });
            throw error;
          } finally {
            span.end();
          }
        };
      }
      // Committee requirement: Golden path smoke test span validation
      validateGoldenPathSpans() {
        return new Promise((resolve2) => {
          if (!this.config.enabled) {
            logger_default2.warn({ message: "OTEL disabled - cannot validate spans" });
            resolve2(false);
            return;
          }
          setTimeout(() => {
            logger_default2.info({
              message: "Golden path span validation completed",
              spans_validated: true,
              required_spans: [
                "http.request",
                "db.postgres.query",
                "db.neo4j.query",
                "xai.explanation",
                "authority.check"
              ]
            });
            resolve2(true);
          }, 1e3);
        });
      }
      // Get current span for manual operations
      getCurrentSpan() {
        return null;
      }
      // Record exception in current span
      recordException(error, attributes) {
        if (!this.config.enabled) {
          return;
        }
        const span = this.getCurrentSpan();
        if (span) {
          const errorObj = typeof error === "string" ? new Error(error) : error;
          span.recordException(errorObj);
          if (attributes) {
            span.setAttributes(attributes);
          }
          span.setStatus({ code: SpanStatusCode.ERROR, message: errorObj.message });
        }
      }
      // Add attributes to current span
      addSpanAttributes(attributes) {
      }
      // Get service configuration
      getConfig() {
        return { ...this.config };
      }
      // Health check for observability
      async healthCheck() {
        return true;
      }
      // Graceful shutdown
      async shutdown() {
      }
    };
    otelService = OTelTracingService.getInstance();
    otelMiddleware = otelService.createMiddleware();
  }
});

// src/data-residency/residency-service.ts
import {
  createHash as createHash2,
  randomBytes,
  createCipheriv,
  createDecipheriv
} from "crypto";
import { z as z2 } from "zod";
var toEncodedString, randomHex, ResidencyConfigSchema, KMSConfigSchema, DataResidencyService;
var init_residency_service = __esm({
  "src/data-residency/residency-service.ts"() {
    "use strict";
    init_postgres();
    init_otel_tracing();
    toEncodedString = (value, encoding) => Buffer.from(value).toString(encoding);
    randomHex = (size) => toEncodedString(randomBytes(size), "hex");
    ResidencyConfigSchema = z2.object({
      region: z2.string(),
      country: z2.string(),
      jurisdiction: z2.string(),
      dataClassifications: z2.array(z2.string()),
      allowedTransfers: z2.array(z2.string()),
      retentionPolicyDays: z2.number().min(1).max(36500),
      // 1 day to 100 years
      encryptionRequired: z2.boolean()
    });
    KMSConfigSchema = z2.object({
      provider: z2.enum([
        "aws-kms",
        "azure-keyvault",
        "gcp-kms",
        "hashicorp-vault",
        "customer-managed"
      ]),
      keyId: z2.string(),
      region: z2.string(),
      endpoint: z2.string().optional(),
      credentials: z2.object({
        accessKey: z2.string().optional(),
        secretKey: z2.string().optional(),
        token: z2.string().optional()
      }).optional()
    });
    DataResidencyService = class {
      kmsProviders = /* @__PURE__ */ new Map();
      constructor() {
        this.initializeKMSProviders();
      }
      initializeKMSProviders() {
        if (process.env.AWS_KMS_ENABLED === "true") {
          this.kmsProviders.set("aws-kms", {
            encrypt: this.encryptWithAWSKMS.bind(this),
            decrypt: this.decryptWithAWSKMS.bind(this),
            generateDataKey: this.generateAWSDataKey.bind(this)
          });
        }
        if (process.env.AZURE_KEYVAULT_ENABLED === "true") {
          this.kmsProviders.set("azure-keyvault", {
            encrypt: this.encryptWithAzureKV.bind(this),
            decrypt: this.decryptWithAzureKV.bind(this),
            generateDataKey: this.generateAzureDataKey.bind(this)
          });
        }
        if (process.env.GCP_KMS_ENABLED === "true") {
          this.kmsProviders.set("gcp-kms", {
            encrypt: this.encryptWithGCPKMS.bind(this),
            decrypt: this.decryptWithGCPKMS.bind(this),
            generateDataKey: this.generateGCPDataKey.bind(this)
          });
        }
        this.kmsProviders.set("customer-managed", {
          encrypt: this.encryptWithCustomerKey.bind(this),
          decrypt: this.decryptWithCustomerKey.bind(this),
          generateDataKey: this.generateCustomerDataKey.bind(this)
        });
      }
      async configureDataResidency(tenantId, config9) {
        const span = otelService.createSpan("data-residency.configure");
        try {
          const validatedConfig = ResidencyConfigSchema.parse(config9);
          const pool4 = getPostgresPool();
          const configId = `residency-${tenantId}-${Date.now()}`;
          await pool4.query(
            `INSERT INTO data_residency_configs (
          id, tenant_id, region, country, jurisdiction, 
          data_classifications, allowed_transfers, retention_policy_days, 
          encryption_required, created_at, updated_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, now(), now())
        ON CONFLICT (tenant_id) 
        DO UPDATE SET 
          region = EXCLUDED.region,
          country = EXCLUDED.country,
          jurisdiction = EXCLUDED.jurisdiction,
          data_classifications = EXCLUDED.data_classifications,
          allowed_transfers = EXCLUDED.allowed_transfers,
          retention_policy_days = EXCLUDED.retention_policy_days,
          encryption_required = EXCLUDED.encryption_required,
          updated_at = now()`,
            [
              configId,
              tenantId,
              validatedConfig.region,
              validatedConfig.country,
              validatedConfig.jurisdiction,
              JSON.stringify(validatedConfig.dataClassifications),
              JSON.stringify(validatedConfig.allowedTransfers),
              validatedConfig.retentionPolicyDays,
              validatedConfig.encryptionRequired
            ]
          );
          await pool4.query(
            `INSERT INTO data_residency_audit (
          tenant_id, action, config_id, metadata, created_at
        ) VALUES ($1, $2, $3, $4, now())`,
            [
              tenantId,
              "residency_config_updated",
              configId,
              JSON.stringify({
                region: validatedConfig.region,
                country: validatedConfig.country,
                jurisdiction: validatedConfig.jurisdiction
              })
            ]
          );
          otelService.addSpanAttributes({
            "data-residency.tenant_id": tenantId,
            "data-residency.region": validatedConfig.region,
            "data-residency.country": validatedConfig.country
          });
          return configId;
        } catch (error) {
          console.error("Data residency configuration failed:", error);
          throw error;
        } finally {
          span?.end();
        }
      }
      async configureKMS(tenantId, config9) {
        const span = otelService.createSpan("data-residency.configure-kms");
        try {
          const validatedConfig = KMSConfigSchema.parse(config9);
          const pool4 = getPostgresPool();
          const kmsConfigId = `kms-${tenantId}-${Date.now()}`;
          await this.testKMSConnectivity(validatedConfig);
          const encryptedCredentials = config9.credentials ? await this.encryptCredentials(JSON.stringify(config9.credentials)) : null;
          await pool4.query(
            `INSERT INTO kms_configs (
          id, tenant_id, provider, key_id, region, endpoint, 
          encrypted_credentials, created_at, updated_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, now(), now())
        ON CONFLICT (tenant_id) 
        DO UPDATE SET 
          provider = EXCLUDED.provider,
          key_id = EXCLUDED.key_id,
          region = EXCLUDED.region,
          endpoint = EXCLUDED.endpoint,
          encrypted_credentials = EXCLUDED.encrypted_credentials,
          updated_at = now()`,
            [
              kmsConfigId,
              tenantId,
              validatedConfig.provider,
              validatedConfig.keyId,
              validatedConfig.region,
              validatedConfig.endpoint,
              encryptedCredentials
            ]
          );
          await pool4.query(
            `INSERT INTO data_residency_audit (
          tenant_id, action, config_id, metadata, created_at
        ) VALUES ($1, $2, $3, $4, now())`,
            [
              tenantId,
              "kms_config_updated",
              kmsConfigId,
              JSON.stringify({
                provider: validatedConfig.provider,
                region: validatedConfig.region,
                keyId: validatedConfig.keyId.substring(0, 8) + "***"
              })
            ]
          );
          otelService.addSpanAttributes({
            "data-residency.tenant_id": tenantId,
            "data-residency.kms_provider": validatedConfig.provider,
            "data-residency.kms_region": validatedConfig.region
          });
          return kmsConfigId;
        } catch (error) {
          console.error("KMS configuration failed:", error);
          throw error;
        } finally {
          span?.end();
        }
      }
      async encryptData(tenantId, data, dataClassification) {
        const span = otelService.createSpan("data-residency.encrypt");
        try {
          const pool4 = getPostgresPool();
          const region = process.env.SUMMIT_REGION || "us-east-1";
          const [residencyConfig, kmsConfig] = await Promise.all([
            this.getResidencyConfig(tenantId),
            this.getKMSConfig(tenantId, region)
          ]);
          const residencyCompliant = this.checkResidencyCompliance(
            dataClassification,
            residencyConfig
          );
          if (!residencyCompliant && residencyConfig?.encryptionRequired) {
            throw new Error(
              "Data classification not compatible with tenant residency requirements"
            );
          }
          let encryptionResult;
          if (kmsConfig && this.kmsProviders.has(kmsConfig.provider)) {
            const kmsProvider = this.kmsProviders.get(kmsConfig.provider);
            encryptionResult = await kmsProvider.encrypt(
              data,
              kmsConfig,
              dataClassification
            );
          } else {
            encryptionResult = await this.encryptLocally(data, dataClassification);
          }
          const encryptionMetadata = {
            algorithm: encryptionResult.algorithm,
            keyId: encryptionResult.keyId,
            encryptionTimestamp: (/* @__PURE__ */ new Date()).toISOString(),
            dataClassification: dataClassification.level,
            residencyRegion: residencyConfig?.region || "unknown",
            kmsProvider: kmsConfig?.provider || "local"
          };
          await pool4.query(
            `INSERT INTO encryption_audit (
          tenant_id, data_hash, classification_level, encryption_method,
          kms_provider, region, compliant, created_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, now())`,
            [
              tenantId,
              createHash2("sha256").update(data).digest("hex"),
              dataClassification.level,
              encryptionResult.algorithm,
              kmsConfig?.provider || "local",
              residencyConfig?.region || "unknown",
              residencyCompliant
            ]
          );
          otelService.addSpanAttributes({
            "data-residency.tenant_id": tenantId,
            "data-residency.classification": dataClassification.level,
            "data-residency.compliant": residencyCompliant,
            "data-residency.encryption_method": encryptionResult.algorithm
          });
          return {
            encryptedData: encryptionResult.encryptedData,
            encryptionMetadata,
            residencyCompliant
          };
        } catch (error) {
          console.error("Data encryption failed:", error);
          throw error;
        } finally {
          span?.end();
        }
      }
      async decryptData(tenantId, encryptedData, encryptionMetadata) {
        const span = otelService.createSpan("data-residency.decrypt");
        try {
          const pool4 = getPostgresPool();
          const region = process.env.SUMMIT_REGION || "us-east-1";
          const kmsConfig = await this.getKMSConfig(tenantId, region);
          let decryptedData;
          if (kmsConfig && this.kmsProviders.has(kmsConfig.provider)) {
            const kmsProvider = this.kmsProviders.get(kmsConfig.provider);
            decryptedData = await kmsProvider.decrypt(
              encryptedData,
              kmsConfig,
              encryptionMetadata
            );
          } else {
            decryptedData = await this.decryptLocally(
              encryptedData,
              encryptionMetadata
            );
          }
          await pool4.query(
            `INSERT INTO decryption_audit (
          tenant_id, data_hash, decryption_timestamp, successful, created_at
        ) VALUES ($1, $2, now(), $3, now())`,
            [
              tenantId,
              createHash2("sha256").update(decryptedData).digest("hex"),
              true
            ]
          );
          otelService.addSpanAttributes({
            "data-residency.tenant_id": tenantId,
            "data-residency.decryption_successful": true,
            "data-residency.kms_provider": kmsConfig?.provider || "local"
          });
          return decryptedData;
        } catch (error) {
          console.error("Data decryption failed:", error);
          const pool4 = getPostgresPool();
          await pool4.query(
            `INSERT INTO decryption_audit (
          tenant_id, decryption_timestamp, successful, error_message, created_at
        ) VALUES ($1, now(), $2, $3, now())`,
            [tenantId, false, error.message]
          ).catch(() => {
          });
          throw error;
        } finally {
          span?.end();
        }
      }
      async checkDataTransferCompliance(tenantId, sourceRegion, targetRegion, dataClassification) {
        const span = otelService.createSpan("data-residency.transfer-compliance");
        try {
          const residencyConfig = await this.getResidencyConfig(tenantId);
          if (!residencyConfig) {
            return {
              compliant: false,
              reason: "No data residency configuration found for tenant"
            };
          }
          if (!residencyConfig.allowedTransfers.includes(targetRegion)) {
            return {
              compliant: false,
              reason: `Transfer to ${targetRegion} not permitted by residency policy`
            };
          }
          const jurisdictionMapping = await this.getJurisdictionMapping();
          const sourceJurisdiction = jurisdictionMapping[sourceRegion];
          const targetJurisdiction = jurisdictionMapping[targetRegion];
          if (sourceJurisdiction !== targetJurisdiction) {
            const additionalControls = [];
            const requiredApprovals = [];
            if (["confidential", "restricted", "top-secret"].includes(
              dataClassification.level
            )) {
              additionalControls.push("encrypted-in-transit");
              additionalControls.push("encrypted-at-rest");
              requiredApprovals.push("data-protection-officer");
            }
            if (this.requiresLegalReview(sourceJurisdiction, targetJurisdiction)) {
              requiredApprovals.push("legal-counsel");
              additionalControls.push("data-processing-agreement");
            }
            return {
              compliant: true,
              additionalControls,
              requiredApprovals
            };
          }
          return { compliant: true };
        } catch (error) {
          console.error("Transfer compliance check failed:", error);
          return {
            compliant: false,
            reason: "Transfer compliance check failed: " + error.message
          };
        } finally {
          span?.end();
        }
      }
      async generateResidencyReport(tenantId) {
        const span = otelService.createSpan("data-residency.generate-report");
        try {
          const pool4 = getPostgresPool();
          const [residencyConfig, kmsConfig, auditData, encryptionStats] = await Promise.all([
            this.getResidencyConfig(tenantId),
            this.getKMSConfig(tenantId),
            this.getAuditData(tenantId),
            this.getEncryptionStats(tenantId)
          ]);
          const report = {
            metadata: {
              tenantId,
              generatedAt: (/* @__PURE__ */ new Date()).toISOString(),
              reportType: "data-residency-compliance",
              version: "1.0"
            },
            configuration: {
              residency: residencyConfig,
              kms: kmsConfig ? {
                provider: kmsConfig.provider,
                region: kmsConfig.region,
                keyId: kmsConfig.keyId.substring(0, 8) + "***"
              } : null
            },
            compliance: {
              overallStatus: this.calculateComplianceStatus(
                residencyConfig,
                kmsConfig,
                encryptionStats
              ),
              encryptionCoverage: encryptionStats.encryptionRate,
              dataClassificationBreakdown: encryptionStats.classificationBreakdown,
              crossBorderTransfers: auditData.transferCount
            },
            audit: {
              totalEvents: auditData.totalEvents,
              encryptionEvents: auditData.encryptionEvents,
              decryptionEvents: auditData.decryptionEvents,
              complianceViolations: auditData.violations
            },
            recommendations: this.generateComplianceRecommendations(
              residencyConfig,
              kmsConfig,
              encryptionStats
            )
          };
          await pool4.query(
            `INSERT INTO residency_reports (
          tenant_id, report_data, created_at
        ) VALUES ($1, $2, now())`,
            [tenantId, JSON.stringify(report)]
          );
          return report;
        } catch (error) {
          console.error("Residency report generation failed:", error);
          throw error;
        } finally {
          span?.end();
        }
      }
      // KMS Provider Implementations
      async encryptWithAWSKMS(data, config9, classification) {
        const dataKey = randomBytes(32);
        const cipher = createCipheriv("aes-256-gcm", dataKey, randomBytes(16));
        let encrypted = cipher.update(data, "utf8", "base64");
        encrypted += cipher.final("base64");
        return {
          encryptedData: encrypted,
          algorithm: "aes-256-gcm-aws-kms",
          keyId: config9.keyId,
          encryptedDataKey: toEncodedString(
            Buffer.from(`aws-kms-encrypted-${toEncodedString(dataKey, "hex")}`),
            "base64"
          ),
          authTag: toEncodedString(cipher.getAuthTag(), "base64")
        };
      }
      async decryptWithAWSKMS(encryptedData, config9, metadata) {
        const dataKey = Buffer.from(metadata.encryptedDataKey, "base64").toString().replace("aws-kms-encrypted-", "");
        const decipher = createDecipheriv(
          "aes-256-gcm",
          Buffer.from(dataKey, "hex"),
          Buffer.alloc(16)
        );
        decipher.setAuthTag(Buffer.from(metadata.authTag, "base64"));
        let decrypted = decipher.update(encryptedData, "base64", "utf8");
        decrypted += decipher.final("utf8");
        return decrypted;
      }
      async generateAWSDataKey(keyId) {
        return {
          plaintextDataKey: randomBytes(32),
          encryptedDataKey: Buffer.from(
            `aws-kms-encrypted-${randomHex(32)}`
          )
        };
      }
      async encryptWithAzureKV(data, config9, classification) {
        const dataKey = randomBytes(32);
        const cipher = createCipheriv("aes-256-cbc", dataKey, randomBytes(16));
        let encrypted = cipher.update(data, "utf8", "base64");
        encrypted += cipher.final("base64");
        return {
          encryptedData: encrypted,
          algorithm: "aes-256-cbc-azure-kv",
          keyId: config9.keyId,
          encryptedDataKey: toEncodedString(
            Buffer.from(`azure-kv-encrypted-${toEncodedString(dataKey, "hex")}`),
            "base64"
          )
        };
      }
      async decryptWithAzureKV(encryptedData, config9, metadata) {
        const dataKey = Buffer.from(metadata.encryptedDataKey, "base64").toString().replace("azure-kv-encrypted-", "");
        const decipher = createDecipheriv(
          "aes-256-cbc",
          Buffer.from(dataKey, "hex"),
          Buffer.alloc(16)
        );
        let decrypted = decipher.update(encryptedData, "base64", "utf8");
        decrypted += decipher.final("utf8");
        return decrypted;
      }
      async generateAzureDataKey(keyId) {
        return {
          plaintextDataKey: randomBytes(32),
          encryptedDataKey: Buffer.from(
            `azure-kv-encrypted-${randomHex(32)}`
          )
        };
      }
      async encryptWithGCPKMS(data, config9, classification) {
        const dataKey = randomBytes(32);
        const cipher = createCipheriv("aes-256-gcm", dataKey, randomBytes(12));
        let encrypted = cipher.update(data, "utf8", "base64");
        encrypted += cipher.final("base64");
        return {
          encryptedData: encrypted,
          algorithm: "aes-256-gcm-gcp-kms",
          keyId: config9.keyId,
          encryptedDataKey: toEncodedString(
            Buffer.from(`gcp-kms-encrypted-${toEncodedString(dataKey, "hex")}`),
            "base64"
          ),
          authTag: toEncodedString(cipher.getAuthTag(), "base64")
        };
      }
      async decryptWithGCPKMS(encryptedData, config9, metadata) {
        const dataKey = Buffer.from(metadata.encryptedDataKey, "base64").toString().replace("gcp-kms-encrypted-", "");
        const decipher = createDecipheriv(
          "aes-256-gcm",
          Buffer.from(dataKey, "hex"),
          Buffer.alloc(12)
        );
        decipher.setAuthTag(Buffer.from(metadata.authTag, "base64"));
        let decrypted = decipher.update(encryptedData, "base64", "utf8");
        decrypted += decipher.final("utf8");
        return decrypted;
      }
      async generateGCPDataKey(keyId) {
        return {
          plaintextDataKey: randomBytes(32),
          encryptedDataKey: Buffer.from(
            `gcp-kms-encrypted-${randomHex(32)}`
          )
        };
      }
      async encryptWithCustomerKey(data, config9, classification) {
        const dataKey = randomBytes(32);
        const iv = randomBytes(16);
        const cipher = createCipheriv("aes-256-gcm", dataKey, iv);
        let encrypted = cipher.update(data, "utf8", "base64");
        encrypted += cipher.final("base64");
        const rootKey = await this.getCustomerRootKey(config9.keyId);
        const keyEncryptionKey = createCipheriv(
          "aes-256-gcm",
          rootKey,
          randomBytes(16)
        );
        let encryptedDataKey = keyEncryptionKey.update(dataKey, void 0, "base64");
        encryptedDataKey += keyEncryptionKey.final("base64");
        return {
          encryptedData: encrypted,
          algorithm: "aes-256-gcm-customer-managed",
          keyId: config9.keyId,
          encryptedDataKey,
          iv: toEncodedString(iv, "base64"),
          authTag: toEncodedString(cipher.getAuthTag(), "base64"),
          keyAuthTag: toEncodedString(keyEncryptionKey.getAuthTag(), "base64")
        };
      }
      async decryptWithCustomerKey(encryptedData, config9, metadata) {
        const rootKey = await this.getCustomerRootKey(config9.keyId);
        const keyDecipher = createDecipheriv(
          "aes-256-gcm",
          rootKey,
          Buffer.alloc(16)
        );
        keyDecipher.setAuthTag(Buffer.from(metadata.keyAuthTag, "base64"));
        let dataKey = keyDecipher.update(metadata.encryptedDataKey, "base64");
        dataKey = Buffer.concat([dataKey, keyDecipher.final()]);
        const decipher = createDecipheriv(
          "aes-256-gcm",
          dataKey,
          Buffer.from(metadata.iv, "base64")
        );
        decipher.setAuthTag(Buffer.from(metadata.authTag, "base64"));
        let decrypted = decipher.update(encryptedData, "base64", "utf8");
        decrypted += decipher.final("utf8");
        return decrypted;
      }
      async generateCustomerDataKey(keyId) {
        const rootKey = await this.getCustomerRootKey(keyId);
        const dataKey = randomBytes(32);
        const cipher = createCipheriv("aes-256-gcm", rootKey, randomBytes(16));
        let encryptedDataKey = cipher.update(dataKey, void 0, "base64");
        encryptedDataKey += cipher.final("base64");
        return {
          plaintextDataKey: dataKey,
          encryptedDataKey: Buffer.from(encryptedDataKey, "base64")
        };
      }
      // Helper methods
      async getCustomerRootKey(keyId) {
        return Buffer.from(
          process.env.CUSTOMER_ROOT_KEY || "customer-root-key-32-byte-length!!!",
          "utf8"
        );
      }
      async testKMSConnectivity(config9) {
        try {
          switch (config9.provider) {
            case "aws-kms":
              return true;
            case "azure-keyvault":
              return true;
            case "gcp-kms":
              return true;
            case "customer-managed":
              return true;
            default:
              throw new Error("Unsupported KMS provider");
          }
        } catch (error) {
          console.error("KMS connectivity test failed:", error);
          return false;
        }
      }
      async encryptCredentials(credentials) {
        const systemKey = Buffer.from(
          process.env.SYSTEM_ENCRYPTION_KEY || "system-key-32-byte-length!!!!!!",
          "utf8"
        );
        const iv = randomBytes(16);
        const cipher = createCipheriv("aes-256-gcm", systemKey, iv);
        let encrypted = cipher.update(credentials, "utf8", "base64");
        encrypted += cipher.final("base64");
        return JSON.stringify({
          encrypted,
          iv: toEncodedString(iv, "base64"),
          authTag: toEncodedString(cipher.getAuthTag(), "base64")
        });
      }
      async getResidencyConfig(tenantId) {
        const pool4 = getPostgresPool();
        const result2 = await pool4.query(
          "SELECT * FROM data_residency_configs WHERE tenant_id = $1",
          [tenantId]
        );
        if (result2.rows.length === 0) return null;
        const row = result2.rows[0];
        return {
          region: row.region,
          country: row.country,
          jurisdiction: row.jurisdiction,
          dataClassifications: JSON.parse(row.data_classifications || "[]"),
          allowedTransfers: JSON.parse(row.allowed_transfers || "[]"),
          retentionPolicyDays: row.retention_policy_days,
          encryptionRequired: row.encryption_required
        };
      }
      async getKMSConfig(tenantId, region) {
        const pool4 = getPostgresPool();
        let query3 = "SELECT * FROM kms_configs WHERE tenant_id = $1";
        const params = [tenantId];
        if (region) {
          query3 += " AND (region = $2 OR region IS NULL)";
          params.push(region);
        }
        const result2 = await pool4.query(query3, params);
        if (result2.rows.length === 0) return null;
        const row = result2.rows.find((r) => r.region === region) || result2.rows[0];
        return {
          provider: row.provider,
          keyId: row.key_id,
          region: row.region,
          endpoint: row.endpoint
          // Don't return decrypted credentials in this context
        };
      }
      checkResidencyCompliance(classification, config9) {
        if (!config9) return false;
        if (classification.residencyRequirements.length > 0) {
          return classification.residencyRequirements.includes(config9.region) || classification.residencyRequirements.includes(config9.country) || classification.residencyRequirements.includes(config9.jurisdiction);
        }
        return true;
      }
      async encryptLocally(data, classification) {
        const key = randomBytes(32);
        const iv = randomBytes(16);
        const cipher = createCipheriv("aes-256-gcm", key, iv);
        let encrypted = cipher.update(data, "utf8", "base64");
        encrypted += cipher.final("base64");
        return {
          encryptedData: encrypted,
          algorithm: "aes-256-gcm-local",
          keyId: `local-${randomHex(8)}`,
          iv: toEncodedString(iv, "base64"),
          authTag: toEncodedString(cipher.getAuthTag(), "base64")
        };
      }
      async decryptLocally(encryptedData, metadata) {
        throw new Error(
          "Local decryption requires proper key storage implementation"
        );
      }
      async getJurisdictionMapping() {
        return {
          "us-east-1": "US",
          "us-west-2": "US",
          "eu-west-1": "EU",
          "eu-central-1": "EU",
          "ap-southeast-1": "APAC",
          "ca-central-1": "CANADA",
          "ap-northeast-1": "JAPAN"
        };
      }
      requiresLegalReview(sourceJurisdiction, targetJurisdiction) {
        const legalReviewRequired = /* @__PURE__ */ new Set([
          "US-EU",
          "EU-US",
          "US-APAC",
          "APAC-US",
          "EU-APAC",
          "APAC-EU"
        ]);
        return legalReviewRequired.has(
          `${sourceJurisdiction}-${targetJurisdiction}`
        );
      }
      async getAuditData(tenantId) {
        const pool4 = getPostgresPool();
        const [auditEvents2, encEvents, decEvents, violations] = await Promise.all([
          pool4.query(
            "SELECT COUNT(*) FROM data_residency_audit WHERE tenant_id = $1",
            [tenantId]
          ),
          pool4.query("SELECT COUNT(*) FROM encryption_audit WHERE tenant_id = $1", [
            tenantId
          ]),
          pool4.query("SELECT COUNT(*) FROM decryption_audit WHERE tenant_id = $1", [
            tenantId
          ]),
          pool4.query(
            "SELECT COUNT(*) FROM encryption_audit WHERE tenant_id = $1 AND compliant = false",
            [tenantId]
          )
        ]);
        return {
          totalEvents: parseInt(auditEvents2.rows[0].count),
          encryptionEvents: parseInt(encEvents.rows[0].count),
          decryptionEvents: parseInt(decEvents.rows[0].count),
          violations: parseInt(violations.rows[0].count),
          transferCount: 0
          // Would be calculated from transfer audit logs
        };
      }
      async getEncryptionStats(tenantId) {
        const pool4 = getPostgresPool();
        const stats = await pool4.query(
          `
      SELECT 
        classification_level,
        COUNT(*) as count,
        AVG(CASE WHEN compliant THEN 1.0 ELSE 0.0 END) as compliance_rate
      FROM encryption_audit 
      WHERE tenant_id = $1 
      GROUP BY classification_level
    `,
          [tenantId]
        );
        const totalEvents = await pool4.query(
          "SELECT COUNT(*) FROM encryption_audit WHERE tenant_id = $1",
          [tenantId]
        );
        const encryptedEvents = await pool4.query(
          "SELECT COUNT(*) FROM encryption_audit WHERE tenant_id = $1 AND encryption_method != 'none'",
          [tenantId]
        );
        const total = parseInt(totalEvents.rows[0].count);
        const encrypted = parseInt(encryptedEvents.rows[0].count);
        return {
          encryptionRate: total > 0 ? encrypted / total * 100 : 100,
          classificationBreakdown: stats.rows.reduce((acc, row) => {
            acc[row.classification_level] = {
              count: parseInt(row.count),
              complianceRate: parseFloat(row.compliance_rate) * 100
            };
            return acc;
          }, {})
        };
      }
      calculateComplianceStatus(residencyConfig, kmsConfig, encryptionStats) {
        if (!residencyConfig) return "non-compliant";
        if (residencyConfig.encryptionRequired && !kmsConfig) return "partial";
        if (encryptionStats.encryptionRate < 95) return "partial";
        return "compliant";
      }
      generateComplianceRecommendations(residencyConfig, kmsConfig, encryptionStats) {
        const recommendations = [];
        if (!residencyConfig) {
          recommendations.push(
            "Configure data residency requirements for your tenant"
          );
        }
        if (!kmsConfig) {
          recommendations.push(
            "Configure BYOK (Bring Your Own Key) for enhanced security"
          );
        }
        if (encryptionStats.encryptionRate < 100) {
          recommendations.push("Ensure all sensitive data is encrypted at rest");
        }
        if (Object.keys(encryptionStats.classificationBreakdown).length === 0) {
          recommendations.push("Implement data classification scheme");
        }
        if (recommendations.length === 0) {
          recommendations.push("Data residency configuration is compliant");
          recommendations.push("Consider regular compliance audits");
        }
        return recommendations;
      }
    };
  }
});

// src/config/regional.ts
var REGIONAL_CATALOG;
var init_regional = __esm({
  "src/config/regional.ts"() {
    "use strict";
    REGIONAL_CATALOG = {
      US: {
        countryCode: "US",
        region: "us-east-1",
        residency: {
          dataRegion: "us-east-1",
          allowedTransferTargets: ["EU", "UK"],
          // Just an example
          strictSovereignty: false
        },
        privacy: {
          requiresConsent: false,
          retentionYears: 7,
          rightToBeForgotten: true,
          dataMinimization: false
        },
        features: {
          aiFeatures: true,
          betaFeatures: true
        },
        sla: {
          uptimeTarget: 99.9,
          maxResponseTimeMs: 500
        },
        support: {
          hours: "24/7",
          escalationEmail: "escalations-us@summit.com",
          language: "en"
        }
      },
      DE: {
        countryCode: "DE",
        region: "eu-central-1",
        residency: {
          dataRegion: "eu-central-1",
          allowedTransferTargets: [],
          // Strict residency
          strictSovereignty: true
        },
        privacy: {
          requiresConsent: true,
          retentionYears: 10,
          rightToBeForgotten: true,
          dataMinimization: true
        },
        features: {
          aiFeatures: true,
          // Assuming enabled for now
          betaFeatures: false
        },
        sla: {
          uptimeTarget: 99.95,
          maxResponseTimeMs: 300
        },
        support: {
          hours: "09:00-17:00 CET",
          escalationEmail: "escalations-de@summit.com",
          language: "de"
        }
      },
      UK: {
        countryCode: "UK",
        region: "eu-west-2",
        residency: {
          dataRegion: "eu-west-2",
          allowedTransferTargets: ["EU"],
          strictSovereignty: false
        },
        privacy: {
          requiresConsent: true,
          retentionYears: 7,
          rightToBeForgotten: true,
          dataMinimization: true
        },
        features: {
          aiFeatures: true,
          betaFeatures: false
        },
        sla: {
          uptimeTarget: 99.9,
          maxResponseTimeMs: 400
        },
        support: {
          hours: "09:00-17:00 BST",
          escalationEmail: "escalations-uk@summit.com",
          language: "en"
        }
      }
    };
  }
});

// src/data-residency/residency-guard.ts
var ResidencyViolationError, ResidencyGuard;
var init_residency_guard = __esm({
  "src/data-residency/residency-guard.ts"() {
    "use strict";
    init_residency_service();
    init_postgres();
    init_otel_tracing();
    init_regional();
    ResidencyViolationError = class extends Error {
      constructor(message, tenantId, context4) {
        super(message);
        this.tenantId = tenantId;
        this.context = context4;
        this.name = "ResidencyViolationError";
      }
    };
    ResidencyGuard = class _ResidencyGuard {
      static instance;
      residencyService;
      currentRegion;
      configCache = /* @__PURE__ */ new Map();
      CACHE_TTL_MS = 60 * 1e3;
      // 1 minute
      constructor() {
        this.residencyService = new DataResidencyService();
        this.currentRegion = process.env.SUMMIT_REGION || process.env.REGION || "us-east-1";
      }
      static getInstance() {
        if (!_ResidencyGuard.instance) {
          _ResidencyGuard.instance = new _ResidencyGuard();
        }
        return _ResidencyGuard.instance;
      }
      /**
       * Enforces residency rules for a given operation.
       * Throws ResidencyViolationError if the operation violates the policy.
       */
      async enforce(tenantId, context4) {
        const span = otelService.createSpan("residency.enforce");
        try {
          const config9 = await this.getResidencyConfig(tenantId);
          if (!config9) {
            if (context4.targetRegion !== this.currentRegion) {
              throw new ResidencyViolationError(
                `No residency policy found. Strictly blocking cross-region access to ${context4.targetRegion}.`,
                tenantId,
                context4
              );
            }
            return;
          }
          const isStrict = config9.residencyMode === "strict";
          const isPrimary = config9.primaryRegion === context4.targetRegion;
          const isAllowed = config9.allowedRegions.includes(context4.targetRegion);
          if (!isPrimary && !isAllowed) {
            const activeException = await this.checkExceptions(tenantId, context4.targetRegion, context4.operation);
            if (!activeException) {
              if (context4.operation === "export" && config9.country) {
                const regionalPolicy = REGIONAL_CATALOG[config9.country];
                if (regionalPolicy && !regionalPolicy.residency.allowedTransferTargets.includes(context4.targetRegion)) {
                  throw new ResidencyViolationError(
                    `Export to ${context4.targetRegion} is prohibited by sovereign policy for ${config9.country}.`,
                    tenantId,
                    context4
                  );
                }
              }
              const errorMsg = `Region ${context4.targetRegion} is not allowed for tenant ${tenantId}.`;
              if (isStrict) {
                throw new ResidencyViolationError(errorMsg, tenantId, context4);
              } else {
                console.warn(`[Residency Warning] ${errorMsg} (Mode: Preferred)`);
                otelService.addSpanAttributes({ "residency.warning": errorMsg });
              }
            }
          }
          if (context4.dataClassification) {
            const classificationRules = config9.dataClassifications?.[context4.dataClassification];
            if (classificationRules) {
              const allowedForScope = classificationRules[context4.operation];
              if (allowedForScope && !allowedForScope.includes(context4.targetRegion)) {
                const errorMsg = `Data classification ${context4.dataClassification} prohibits ${context4.operation} in ${context4.targetRegion}.`;
                if (isStrict) {
                  throw new ResidencyViolationError(errorMsg, tenantId, context4);
                } else {
                  console.warn(`[Residency Classification Warning] ${errorMsg}`);
                }
              }
            }
          }
          otelService.addSpanAttributes({
            "residency.status": "allowed",
            "residency.tenant": tenantId,
            "residency.target_region": context4.targetRegion,
            "residency.mode": config9.residencyMode
          });
        } catch (error) {
          otelService.addSpanAttributes({
            "residency.status": "violated",
            "residency.error": error instanceof Error ? error.message : "Unknown error"
          });
          throw error;
        } finally {
          span?.end();
        }
      }
      /**
       * Checks if an agent is allowed to execute in the current region/target region.
       */
      async validateAgentExecution(tenantId) {
        return this.enforce(tenantId, {
          operation: "compute",
          targetRegion: this.currentRegion,
          dataClassification: "internal"
          // Default for agent execution unless specified
        });
      }
      /**
       * Checks if export to a target region is allowed.
       */
      async checkExportCompliance(tenantId, targetRegion, classification) {
        return this.enforce(tenantId, {
          operation: "export",
          targetRegion,
          dataClassification: classification
        });
      }
      /**
       * Checks if a region is allowed for a tenant without throwing.
       */
      async isRegionAllowed(tenantId, region, operation = "storage") {
        try {
          const config9 = await this.getResidencyConfig(tenantId);
          if (!config9) return region === this.currentRegion;
          const isAllowed = config9.primaryRegion === region || config9.allowedRegions.includes(region);
          if (isAllowed) return true;
          return await this.checkExceptions(tenantId, region, operation);
        } catch (error) {
          console.error("isRegionAllowed check failed:", error);
          return false;
        }
      }
      async getResidencyConfig(tenantId) {
        const now = Date.now();
        const cached = this.configCache.get(tenantId);
        if (cached && cached.expiresAt > now) {
          return cached.config;
        }
        const pool4 = getPostgresPool();
        const result2 = await pool4.query(
          `SELECT c.*, p.region as shard_region 
             FROM data_residency_configs c
             LEFT JOIN tenant_partitions p ON c.tenant_id = p.tenant_id
             WHERE c.tenant_id = $1`,
          [tenantId]
        );
        if (result2.rows.length === 0) return null;
        const row = result2.rows[0];
        const primaryRegion = row.shard_region || row.region;
        const parseSafe = (val) => {
          if (!val || val === "") return [];
          try {
            return typeof val === "string" ? JSON.parse(val) : val;
          } catch (e) {
            return [];
          }
        };
        const allowedRegions = parseSafe(row.allowed_regions || row.allowed_transfers);
        if (primaryRegion && !allowedRegions.includes(primaryRegion)) {
          allowedRegions.push(primaryRegion);
        }
        const config9 = {
          primaryRegion,
          allowedRegions,
          country: row.country,
          residencyMode: row.residency_mode || "strict",
          dataClassifications: {
            "confidential": {
              "storage": [primaryRegion],
              // Strict default
              "compute": [primaryRegion],
              "logs": [primaryRegion],
              "backups": [primaryRegion],
              "export": [primaryRegion]
            },
            "restricted": {
              "storage": [primaryRegion],
              "export": [primaryRegion]
            }
          }
        };
        this.configCache.set(tenantId, {
          config: config9,
          expiresAt: now + this.CACHE_TTL_MS
        });
        return config9;
      }
      /**
       * Checks if a specific feature is allowed for a tenant based on their regional policy.
       */
      async validateFeatureAccess(tenantId, feature) {
        const config9 = await this.getResidencyConfig(tenantId);
        if (!config9 || !config9.country) return true;
        const regionalPolicy = REGIONAL_CATALOG[config9.country];
        if (!regionalPolicy) return true;
        return !!regionalPolicy.features[feature];
      }
      async checkExceptions(tenantId, region, operation) {
        const pool4 = getPostgresPool();
        try {
          const result2 = await pool4.query(
            `SELECT * FROM residency_exceptions
                 WHERE tenant_id = $1
                 AND target_region = $2
                 AND (scope = $3 OR scope = '*')
                 AND (expires_at IS NULL OR expires_at > NOW())`,
            [tenantId, region, operation]
          );
          return result2.rows.length > 0;
        } catch (e) {
          const msg = e instanceof Error ? e.message : String(e);
          if (!msg.includes('relation "residency_exceptions" does not exist')) {
            console.error("Residency Exception Check Failed:", e);
          }
          return false;
        }
      }
    };
  }
});

// src/db/tenantRouter.ts
import { Pool } from "pg";
import { Counter, Gauge, register } from "prom-client";
var routeResolutionCounter, partitionGauge, ROUTER_TTL_MS, TenantRouter, tenantRouter;
var init_tenantRouter = __esm({
  "src/db/tenantRouter.ts"() {
    "use strict";
    init_logger();
    routeResolutionCounter = register.getSingleMetric("tenant_router_resolutions_total") || new Counter({
      name: "tenant_router_resolutions_total",
      help: "Tenant routing resolution decisions",
      labelNames: ["tenant_id", "partition_key", "source", "strategy"]
    });
    partitionGauge = register.getSingleMetric("tenant_partition_config_info") || new Gauge({
      name: "tenant_partition_config_info",
      help: "Active tenant partition definitions loaded into the router",
      labelNames: ["partition_key", "strategy", "schema", "is_default"]
    });
    ROUTER_TTL_MS = 6e4;
    TenantRouter = class {
      enabled = process.env.TENANT_ROUTING_V1 === "1";
      writePool = null;
      readPool = null;
      defaultPartition = "primary";
      partitions = /* @__PURE__ */ new Map();
      tenantToPartition = /* @__PURE__ */ new Map();
      poolCache = /* @__PURE__ */ new Map();
      lastLoadedAt = 0;
      configure(pools) {
        if (!this.writePool) {
          this.writePool = pools.writePool;
        }
        if (!this.readPool && pools.readPool) {
          this.readPool = pools.readPool;
        }
        this.ensureDefaultPartition();
      }
      isEnabled() {
        return this.enabled;
      }
      async refresh() {
        this.lastLoadedAt = 0;
        await this.ensureLoaded();
      }
      async resolve(tenantId) {
        if (!this.enabled || !this.writePool) {
          return null;
        }
        await this.ensureLoaded();
        const partitionKey = tenantId && this.tenantToPartition.get(tenantId) || this.defaultPartition;
        const partition = this.partitions.get(partitionKey) || this.partitions.get(this.defaultPartition);
        if (!partition) {
          return {
            tenantId: tenantId || "unknown",
            partitionKey: this.defaultPartition,
            schema: null,
            strategy: "shared-schema",
            writePool: this.writePool,
            readPool: this.readPool || this.writePool,
            source: "static"
          };
        }
        const writePool2 = this.getOrCreatePool(
          partition.partition_key,
          "write",
          partition.write_connection_url
        );
        const readPool2 = this.getOrCreatePool(
          partition.partition_key,
          "read",
          partition.read_connection_url || partition.write_connection_url
        );
        const strategy = partition.strategy === "schema" ? "schema-per-tenant" : "shared-schema";
        const source = this.tenantToPartition.has(tenantId || "") ? "mapping" : "default";
        routeResolutionCounter.inc({
          tenant_id: tenantId || "unknown",
          partition_key: partition.partition_key,
          source,
          strategy
        });
        return {
          tenantId: tenantId || "unknown",
          partitionKey: partition.partition_key,
          schema: partition.schema_name,
          strategy,
          writePool: writePool2,
          readPool: readPool2,
          source,
          region: partition.region
        };
      }
      async resolveRegionalRoute(tenantId, region) {
        if (!this.enabled || !this.writePool) {
          return null;
        }
        await this.ensureLoaded();
        const partitionKey = tenantId ? this.tenantToPartition.get(tenantId) : null;
        let partition;
        if (partitionKey) {
          partition = this.partitions.get(partitionKey);
        }
        if (!partition || partition.region && partition.region !== region) {
          const regionalPool = Array.from(this.partitions.values()).find(
            (p) => p.region === region && (p.status === "active" || p.status === null)
          );
          if (regionalPool) {
            partition = regionalPool;
          }
        }
        if (!partition) {
          partition = this.partitions.get(this.defaultPartition);
        }
        if (!partition) return null;
        const writePool2 = this.getOrCreatePool(
          partition.partition_key,
          "write",
          partition.write_connection_url
        );
        const readPool2 = this.getOrCreatePool(
          partition.partition_key,
          "read",
          partition.read_connection_url || partition.write_connection_url
        );
        const strategy = partition.strategy === "schema" ? "schema-per-tenant" : "shared-schema";
        const source = this.tenantToPartition.has(tenantId || "") ? "mapping" : "default";
        return {
          tenantId: tenantId || "unknown",
          partitionKey: partition.partition_key,
          schema: partition.schema_name,
          strategy,
          writePool: writePool2,
          readPool: readPool2,
          source,
          region: partition.region
        };
      }
      async assignTenantToPartition(client6, tenantId, requestedKey) {
        if (!this.enabled) {
          return this.defaultPartition;
        }
        await this.ensureLoaded();
        const partitionKey = requestedKey && this.partitions.get(requestedKey)?.partition_key || this.defaultPartition;
        try {
          await client6.query(
            `
          INSERT INTO tenant_partition_map (tenant_id, partition_key)
          VALUES ($1, $2)
          ON CONFLICT (tenant_id) DO UPDATE
            SET partition_key = EXCLUDED.partition_key,
                updated_at = NOW()
        `,
            [tenantId, partitionKey]
          );
          this.tenantToPartition.set(tenantId, partitionKey);
          return partitionKey;
        } catch (error) {
          logger_default.warn(
            { tenantId, partitionKey, err: error },
            "Tenant routing map not available; continuing without partition mapping"
          );
          return this.defaultPartition;
        }
      }
      getDefaultPartition() {
        return this.defaultPartition;
      }
      /**
       * Test hook: reset router cache between tests.
       */
      resetForTests() {
        this.partitions.clear();
        this.tenantToPartition.clear();
        this.poolCache.clear();
        this.lastLoadedAt = 0;
        this.defaultPartition = "primary";
        this.writePool = null;
        this.readPool = null;
        this.enabled = process.env.TENANT_ROUTING_V1 === "1";
      }
      /**
       * Test hook: seed partitions without hitting the database.
       */
      seedForTests(partitions, tenantMap = []) {
        this.partitions.clear();
        partitions.forEach((row) => this.partitions.set(row.partition_key, { region: "us-east-1", ...row }));
        tenantMap.forEach(
          ({ tenant_id, partition_key }) => this.tenantToPartition.set(tenant_id, partition_key)
        );
        const defaultRow = partitions.find((p) => p.is_default) || partitions[0] || null;
        if (defaultRow) {
          this.defaultPartition = defaultRow.partition_key;
          partitionGauge.set(
            {
              partition_key: defaultRow.partition_key,
              strategy: defaultRow.strategy,
              schema: defaultRow.schema_name || "public",
              is_default: "true"
            },
            1
          );
        } else {
          this.ensureDefaultPartition();
        }
        this.lastLoadedAt = Date.now();
      }
      ensureDefaultPartition() {
        if (!this.partitions.size) {
          this.partitions.set(this.defaultPartition, {
            partition_key: this.defaultPartition,
            strategy: "shared",
            schema_name: null,
            write_connection_url: null,
            read_connection_url: null,
            is_default: true,
            status: "active"
          });
        }
      }
      async ensureLoaded() {
        if (!this.enabled || !this.writePool || Date.now() - this.lastLoadedAt < ROUTER_TTL_MS && this.partitions.size > 0) {
          return;
        }
        try {
          const partitionResult = await this.writePool.query(
            `
          SELECT partition_key,
                 strategy,
                 schema_name,
                 write_connection_url,
                 read_connection_url,
                 is_default,
                 status,
                 region
          FROM tenant_partitions
          WHERE status IS NULL OR status = 'active'
        `
          );
          const mapResult = await this.writePool.query(
            `SELECT tenant_id, partition_key FROM tenant_partition_map`
          );
          this.partitions.clear();
          partitionResult.rows.forEach((row) => {
            this.partitions.set(row.partition_key, row);
            partitionGauge.set(
              {
                partition_key: row.partition_key,
                strategy: row.strategy,
                schema: row.schema_name || "public",
                is_default: row.is_default ? "true" : "false"
              },
              1
            );
            if (row.is_default) {
              this.defaultPartition = row.partition_key;
            }
          });
          if (!partitionResult.rows.length) {
            this.ensureDefaultPartition();
          }
          this.tenantToPartition.clear();
          mapResult.rows.forEach(
            (row) => this.tenantToPartition.set(row.tenant_id, row.partition_key)
          );
          this.lastLoadedAt = Date.now();
        } catch (error) {
          logger_default.debug(
            { err: error },
            "Tenant routing tables unavailable; using default partition"
          );
          this.ensureDefaultPartition();
          this.lastLoadedAt = Date.now();
        }
      }
      getOrCreatePool(partitionKey, type, connectionUrl) {
        if (!connectionUrl) {
          if (type === "read" && this.readPool) {
            return this.readPool;
          }
          return this.writePool;
        }
        const cacheKey = `${partitionKey}:${type}:${connectionUrl}`;
        const cached = this.poolCache.get(cacheKey);
        if (cached) {
          return cached;
        }
        const pool4 = new Pool({
          connectionString: connectionUrl,
          application_name: `summit-${type}-${partitionKey}`
        });
        this.poolCache.set(cacheKey, pool4);
        return pool4;
      }
    };
    tenantRouter = new TenantRouter();
  }
});

// src/lib/observability/metrics.ts
import * as client from "prom-client";
var register3, httpRequestDuration, httpRequestsTotal, businessUserSignupsTotal, businessApiCallsTotal, businessRevenueTotal, graphqlRequestDuration, graphqlRequestsTotal, graphqlErrors, graphqlResolverDurationSeconds, graphqlResolverErrorsTotal, graphqlResolverCallsTotal, dbConnectionsActive, dbQueryDuration, dbQueriesTotal, aiJobsQueued, aiJobsProcessing, aiJobDuration, aiJobsTotal, aiRequestTotal, copilotApiRequestTotal, copilotApiRequestDurationMs, graphNodesTotal, graphEdgesTotal, graphOperationDuration, graphExpandRequestsTotal, pipelineUptimeRatio, pipelineLatencySeconds, maestroDeploymentsTotal, maestroChangeFailureRate, agentExecutionsTotal, agentExecutionDuration, policyDecisionsTotal, applicationErrors, memoryUsage, tenantScopeViolationsTotal, metricsToRegister, metrics;
var init_metrics = __esm({
  "src/lib/observability/metrics.ts"() {
    "use strict";
    client.register.clear();
    register3 = new client.Registry();
    client.collectDefaultMetrics({
      register: register3,
      timeout: 5e3,
      gcDurationBuckets: [1e-3, 0.01, 0.1, 1, 2, 5]
    });
    httpRequestDuration = new client.Histogram({
      registers: [],
      name: "http_request_duration_seconds",
      help: "Duration of HTTP requests in seconds",
      labelNames: ["method", "route", "status_code"],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    httpRequestsTotal = new client.Counter({
      registers: [],
      name: "http_requests_total",
      help: "Total number of HTTP requests",
      labelNames: ["method", "route", "status_code"]
    });
    businessUserSignupsTotal = new client.Counter({
      registers: [],
      name: "business_user_signups_total",
      help: "Total number of customer or workspace signups",
      labelNames: ["tenant", "plan"]
    });
    businessApiCallsTotal = new client.Counter({
      registers: [],
      name: "business_api_calls_total",
      help: "API calls attributed to customer activity and billing",
      labelNames: ["service", "route", "status_code", "tenant"]
    });
    businessRevenueTotal = new client.Counter({
      registers: [],
      name: "business_revenue_total",
      help: "Recognized revenue amounts in the system's reporting currency",
      labelNames: ["tenant", "currency"]
    });
    graphqlRequestDuration = new client.Histogram({
      registers: [],
      name: "graphql_request_duration_seconds",
      help: "Duration of GraphQL requests in seconds",
      labelNames: ["operation", "operation_type"],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    graphqlRequestsTotal = new client.Counter({
      registers: [],
      name: "graphql_requests_total",
      help: "Total number of GraphQL requests",
      labelNames: ["operation", "operation_type", "status"]
    });
    graphqlErrors = new client.Counter({
      registers: [],
      name: "graphql_errors_total",
      help: "Total number of GraphQL errors",
      labelNames: ["operation", "error_type"]
    });
    graphqlResolverDurationSeconds = new client.Histogram({
      registers: [],
      name: "graphql_resolver_duration_seconds",
      help: "Duration of GraphQL resolver execution in seconds",
      labelNames: ["resolver_name", "field_name", "type_name", "status"],
      buckets: [1e-3, 5e-3, 0.01, 0.05, 0.1, 0.5, 1, 2, 5]
    });
    graphqlResolverErrorsTotal = new client.Counter({
      registers: [],
      name: "graphql_resolver_errors_total",
      help: "Total number of GraphQL resolver errors",
      labelNames: ["resolver_name", "field_name", "type_name", "error_type"]
    });
    graphqlResolverCallsTotal = new client.Counter({
      registers: [],
      name: "graphql_resolver_calls_total",
      help: "Total number of GraphQL resolver calls",
      labelNames: ["resolver_name", "field_name", "type_name"]
    });
    dbConnectionsActive = new client.Gauge({
      registers: [],
      name: "db_connections_active",
      help: "Number of active database connections",
      labelNames: ["database"]
    });
    dbQueryDuration = new client.Histogram({
      registers: [],
      name: "db_query_duration_seconds",
      help: "Duration of database queries in seconds",
      labelNames: ["database", "operation"],
      buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5]
    });
    dbQueriesTotal = new client.Counter({
      registers: [],
      name: "db_queries_total",
      help: "Total number of database queries",
      labelNames: ["database", "operation", "status"]
    });
    aiJobsQueued = new client.Gauge({
      registers: [],
      name: "ai_jobs_queued",
      help: "Number of AI/ML jobs in queue",
      labelNames: ["job_type"]
    });
    aiJobsProcessing = new client.Gauge({
      registers: [],
      name: "ai_jobs_processing",
      help: "Number of AI/ML jobs currently processing",
      labelNames: ["job_type"]
    });
    aiJobDuration = new client.Histogram({
      registers: [],
      name: "ai_job_duration_seconds",
      help: "Duration of AI/ML job processing in seconds",
      labelNames: ["job_type", "status"],
      buckets: [1, 5, 10, 30, 60, 300, 600]
    });
    aiJobsTotal = new client.Counter({
      registers: [],
      name: "ai_jobs_total",
      help: "Total number of AI/ML jobs processed",
      labelNames: ["job_type", "status"]
    });
    aiRequestTotal = new client.Counter({
      registers: [],
      name: "ai_request_total",
      help: "AI request events",
      labelNames: ["status"]
    });
    copilotApiRequestTotal = new client.Counter({
      registers: [],
      name: "copilot_api_request_total",
      help: "Total number of AI Copilot API requests",
      labelNames: ["endpoint", "mode", "status"]
    });
    copilotApiRequestDurationMs = new client.Histogram({
      registers: [],
      name: "copilot_api_request_duration_ms",
      help: "AI Copilot API request duration in milliseconds",
      labelNames: ["endpoint", "mode"],
      buckets: [50, 100, 250, 500, 1e3, 2e3, 5e3, 1e4, 3e4]
    });
    graphNodesTotal = new client.Gauge({
      registers: [],
      name: "graph_nodes_total",
      help: "Total number of nodes in the graph",
      labelNames: ["investigation_id"]
    });
    graphEdgesTotal = new client.Gauge({
      registers: [],
      name: "graph_edges_total",
      help: "Total number of edges in the graph",
      labelNames: ["investigation_id"]
    });
    graphOperationDuration = new client.Histogram({
      registers: [],
      name: "graph_operation_duration_seconds",
      help: "Duration of graph operations in seconds",
      labelNames: ["operation", "investigation_id"],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    graphExpandRequestsTotal = new client.Counter({
      registers: [],
      name: "graph_expand_requests_total",
      help: "Total expandNeighbors requests",
      labelNames: ["cached"]
    });
    pipelineUptimeRatio = new client.Gauge({
      registers: [],
      name: "pipeline_uptime_ratio",
      help: "Pipeline availability ratio (0..1) over current window",
      labelNames: ["source", "pipeline", "env"]
    });
    pipelineLatencySeconds = new client.Histogram({
      registers: [],
      name: "pipeline_latency_seconds",
      help: "End-to-end processing latency seconds",
      labelNames: ["source", "pipeline", "env"],
      buckets: [5, 15, 30, 60, 120, 300, 600, 1200]
    });
    maestroDeploymentsTotal = new client.Counter({
      registers: [],
      name: "maestro_deployments_total",
      help: "Total number of deployments",
      labelNames: ["environment", "status"]
    });
    maestroChangeFailureRate = new client.Gauge({
      registers: [],
      name: "maestro_change_failure_rate",
      help: "Change failure rate percentage"
    });
    agentExecutionsTotal = new client.Counter({
      registers: [],
      name: "agent_executions_total",
      help: "Total number of agent executions",
      labelNames: ["tenant", "status"]
    });
    agentExecutionDuration = new client.Histogram({
      registers: [],
      name: "agent_execution_duration_seconds",
      help: "Duration of agent execution in seconds",
      labelNames: ["tenant", "status"],
      buckets: [1, 5, 10, 30, 60, 120, 300]
    });
    policyDecisionsTotal = new client.Counter({
      registers: [],
      name: "policy_decisions_total",
      help: "Total number of policy decisions",
      labelNames: ["policy", "decision", "signal_type"]
    });
    applicationErrors = new client.Counter({
      registers: [],
      name: "application_errors_total",
      help: "Total number of application errors",
      labelNames: ["module", "error_type", "severity", "signal_type"]
    });
    memoryUsage = new client.Gauge({
      registers: [],
      name: "application_memory_usage_bytes",
      help: "Memory usage by application component",
      labelNames: ["component"]
    });
    tenantScopeViolationsTotal = new client.Counter({
      registers: [],
      name: "tenant_scope_violations_total",
      help: "Total number of tenant scope violations"
    });
    metricsToRegister = [
      httpRequestDuration,
      httpRequestsTotal,
      graphqlRequestDuration,
      graphqlRequestsTotal,
      graphqlErrors,
      graphqlResolverDurationSeconds,
      graphqlResolverErrorsTotal,
      graphqlResolverCallsTotal,
      dbConnectionsActive,
      dbQueryDuration,
      dbQueriesTotal,
      aiJobsQueued,
      aiJobsProcessing,
      aiJobDuration,
      aiJobsTotal,
      aiRequestTotal,
      copilotApiRequestTotal,
      copilotApiRequestDurationMs,
      graphNodesTotal,
      graphEdgesTotal,
      graphOperationDuration,
      graphExpandRequestsTotal,
      pipelineUptimeRatio,
      pipelineLatencySeconds,
      maestroDeploymentsTotal,
      maestroChangeFailureRate,
      agentExecutionsTotal,
      agentExecutionDuration,
      policyDecisionsTotal,
      applicationErrors,
      memoryUsage,
      tenantScopeViolationsTotal,
      businessUserSignupsTotal,
      businessApiCallsTotal,
      businessRevenueTotal
    ];
    metricsToRegister.forEach((m) => register3.registerMetric(m));
    setInterval(() => {
      const usage = process.memoryUsage();
      memoryUsage.set({ component: "heap_used" }, usage.heapUsed);
      memoryUsage.set({ component: "heap_total" }, usage.heapTotal);
      memoryUsage.set({ component: "external" }, usage.external);
      memoryUsage.set({ component: "rss" }, usage.rss);
    }, 3e4);
    metrics = {
      httpRequestsTotal,
      httpRequestDuration,
      graphqlRequestsTotal,
      graphqlRequestDuration,
      graphqlErrors,
      dbQueriesTotal,
      dbQueryDuration,
      aiJobsTotal,
      aiJobDuration,
      applicationErrors,
      businessApiCallsTotal,
      agentExecutionsTotal,
      agentExecutionDuration,
      policyDecisionsTotal
    };
  }
});

// src/lib/telemetry/comprehensive-telemetry.ts
import { MeterProvider } from "@opentelemetry/sdk-metrics";
import { Resource } from "@opentelemetry/resources";
import { SemanticResourceAttributes } from "@opentelemetry/semantic-conventions";
var ComprehensiveTelemetry, telemetry;
var init_comprehensive_telemetry = __esm({
  "src/lib/telemetry/comprehensive-telemetry.ts"() {
    "use strict";
    init_metrics();
    ComprehensiveTelemetry = class _ComprehensiveTelemetry {
      static instance;
      meter;
      // Performance counters
      subsystems;
      // Request/response timing
      requestDuration;
      activeConnections;
      activeConnectionsCount = 0;
      constructor() {
        const resource = new Resource({
          [SemanticResourceAttributes.SERVICE_NAME]: "intelgraph-server-legacy"
        });
        const meterProvider = new MeterProvider({ resource });
        this.meter = meterProvider.getMeter("intelgraph-server-telemetry-legacy");
        this.requestDuration = { record: () => {
        } };
        this.activeConnections = { add: (value) => {
        } };
        this.subsystems = {
          database: {
            queries: {
              add: (value = 1) => {
                if (metrics.dbQueriesTotal) {
                  metrics.dbQueriesTotal.labels("unknown", "query", "ok").inc(value);
                }
              }
            },
            errors: {
              add: (value = 1) => {
                if (metrics.dbQueriesTotal) {
                  metrics.dbQueriesTotal.labels("unknown", "query", "error").inc(value);
                }
              }
            },
            latency: {
              record: (value) => {
                if (metrics.dbQueryDuration) {
                  metrics.dbQueryDuration.labels("unknown", "query").observe(value);
                }
                if (metrics.intelgraphDatabaseQueryDuration) {
                  metrics.intelgraphDatabaseQueryDuration.labels("unknown", "query").observe(value);
                }
              }
            }
          },
          cache: {
            hits: {
              add: (value = 1) => {
                if (metrics.intelgraphCacheHits) metrics.intelgraphCacheHits.labels("redis").inc(value);
                if (metrics.cacheHits) metrics.cacheHits.inc(value);
              }
            },
            misses: {
              add: (value = 1) => {
                if (metrics.intelgraphCacheMisses) metrics.intelgraphCacheMisses.inc(value);
                if (metrics.cacheMisses) metrics.cacheMisses.inc(value);
              }
            },
            sets: { add: (value = 1) => {
            } },
            dels: { add: (value = 1) => {
            } }
          },
          api: {
            requests: {
              add: (value = 1) => {
                if (metrics.stdHttpRequestsTotal) {
                  metrics.stdHttpRequestsTotal.labels("GET", "unknown", "200").inc(value);
                }
              }
            },
            errors: {
              add: (value = 1) => {
                if (metrics.applicationErrors) {
                  metrics.applicationErrors.labels("api", "error", "high", "general").inc(value);
                }
              }
            }
          }
        };
      }
      static getInstance() {
        if (!_ComprehensiveTelemetry.instance) {
          _ComprehensiveTelemetry.instance = new _ComprehensiveTelemetry();
        }
        return _ComprehensiveTelemetry.instance;
      }
      recordRequest(duration, attributes) {
        const labels2 = {
          method: String(attributes.method || "GET"),
          route: String(attributes.route || "unknown"),
          status_code: String(attributes.status || 200)
        };
        if (metrics.stdHttpRequestDuration) {
          metrics.stdHttpRequestDuration.observe(labels2, duration);
        }
        if (metrics.stdHttpRequestsTotal) {
          metrics.stdHttpRequestsTotal.inc(labels2);
        }
        const legacyLabels = {
          method: String(attributes.method || "GET"),
          route: String(attributes.route || "unknown"),
          status_code: String(attributes.status || 200)
        };
        metrics.httpRequestDuration.observe(legacyLabels, duration);
      }
      incrementActiveConnections() {
        this.activeConnectionsCount++;
        if (metrics.websocketConnections) {
          metrics.websocketConnections.inc();
        }
        if (metrics.intelgraphActiveConnections) {
          metrics.intelgraphActiveConnections.set({ tenant: "unknown" }, this.activeConnectionsCount);
        }
      }
      decrementActiveConnections() {
        this.activeConnectionsCount--;
        if (metrics.websocketConnections) {
          metrics.websocketConnections.dec();
        }
        if (metrics.intelgraphActiveConnections) {
          metrics.intelgraphActiveConnections.set({ tenant: "unknown" }, this.activeConnectionsCount);
        }
      }
      onMetric(_listener) {
      }
    };
    telemetry = ComprehensiveTelemetry.getInstance();
  }
});

// src/db/postgres.ts
var postgres_exports = {};
__export(postgres_exports, {
  __private: () => __private,
  closePostgresPool: () => closePostgresPool,
  getPostgresPool: () => getPostgresPool
});
import crypto2 from "node:crypto";
import { performance } from "node:perf_hooks";
import { Pool as Pool2 } from "pg";
import * as dotenv from "dotenv";
function createPool(name, type, max) {
  const pool4 = new Pool2({
    ...dbConfig.connectionConfig,
    max,
    idleTimeoutMillis: dbConfig.idleTimeoutMs,
    connectionTimeoutMillis: dbConfig.connectionTimeoutMs,
    application_name: `summit-${type}-${process.env.CURRENT_REGION || "global"}`
  });
  pool4.on("error", (err) => {
    logger3.error({ pool: name, err }, "Unexpected PostgreSQL client error");
  });
  pool4.on("connect", (client6) => {
    client6.connectedAt = Date.now();
    logger3.debug({ pool: name }, "New PostgreSQL connection established");
  });
  return {
    name,
    type,
    pool: pool4,
    circuitBreaker: new CircuitBreaker(
      name,
      CIRCUIT_BREAKER_FAILURE_THRESHOLD,
      CIRCUIT_BREAKER_COOLDOWN_MS
    )
  };
}
function initializePools() {
  if (managedPool) {
    return;
  }
  if (QUERY_CAPTURE_ENABLED && !queryCaptureTimer) {
    queryCaptureTimer = setInterval(() => {
      logQueryCaptureSnapshot("interval");
    }, QUERY_CAPTURE_INTERVAL_MS);
    queryCaptureTimer.unref?.();
    logger3.info(
      { intervalMs: QUERY_CAPTURE_INTERVAL_MS },
      "DB query capture enabled"
    );
  }
  writePoolWrapper = createPool(
    "write-primary",
    "write",
    dbConfig.maxPoolSize
  );
  const readPool2 = createPool(
    "read-default",
    "read",
    dbConfig.readPoolSize
  );
  readPoolWrappers = [readPool2];
  managedPool = createManagedPool(writePoolWrapper, readPoolWrappers);
}
function createManagedPool(writePool2, readPools) {
  if (process.env.ZERO_FOOTPRINT === "true") {
    logger3.warn("ZERO_FOOTPRINT mode active: PostgreSQL queries will not be persisted.");
    const mockExecutor = async (queryInput) => {
      logger3.debug("Zero-Footprint: Skipping query execution");
      return { rowCount: 0, rows: [], command: "MOCK", oid: 0, fields: [] };
    };
    return {
      query: mockExecutor,
      read: mockExecutor,
      write: mockExecutor,
      transaction: async () => ({}),
      withTransaction: async () => ({}),
      connect: async () => writePool2.pool.connect(),
      end: async () => {
      },
      on: () => ({}),
      healthCheck: async () => [],
      slowQueryInsights: () => [],
      pool: writePool2.pool
    };
  }
  const query3 = (queryInput, params, options2 = {}) => executeManagedQuery({
    queryInput,
    params,
    options: options2,
    desiredType: "auto",
    writePool: writePool2,
    readPools
  });
  const read = (queryInput, params, options2 = {}) => executeManagedQuery({
    queryInput,
    params,
    options: { ...options2, forceWrite: false },
    desiredType: "read",
    writePool: writePool2,
    readPools
  });
  const write = (queryInput, params, options2 = {}) => executeManagedQuery({
    queryInput,
    params,
    options: { ...options2, forceWrite: true },
    desiredType: "write",
    writePool: writePool2,
    readPools
  });
  const connect = async () => {
    return writePool2.pool.connect();
  };
  const withTransaction = async (callback) => {
    const client6 = await connect();
    try {
      await client6.query("BEGIN");
      const result2 = await callback(client6);
      await client6.query("COMMIT");
      return result2;
    } catch (error) {
      await client6.query("ROLLBACK");
      throw error;
    } finally {
      client6.release();
    }
  };
  const end = async () => {
    logQueryCaptureSnapshot("shutdown");
    if (queryCaptureTimer) {
      clearInterval(queryCaptureTimer);
      queryCaptureTimer = null;
    }
    await Promise.all([
      writePool2.pool.end(),
      ...readPools.map((wrapper) => wrapper.pool.end())
    ]);
    managedPool = null;
    writePoolWrapper = null;
    readPoolWrappers = [];
  };
  const on = (event, listener) => {
    writePool2.pool.on(event, listener);
    readPools.forEach((wrapper) => wrapper.pool.on(event, listener));
    return managedPool;
  };
  const healthCheck = async () => {
    const pools = [writePool2, ...readPools];
    const checks = await Promise.all(
      pools.map(async (wrapper) => {
        const snapshot = {
          name: wrapper.name,
          type: wrapper.type,
          circuitState: wrapper.circuitBreaker.getState(),
          healthy: true,
          activeConnections: (wrapper.pool.totalCount ?? 0) - (wrapper.pool.idleCount ?? 0),
          idleConnections: wrapper.pool.idleCount ?? 0,
          queuedRequests: wrapper.pool.waitingCount ?? 0,
          totalConnections: wrapper.pool.totalCount ?? 0
        };
        try {
          await withManagedClient(wrapper, 1e3, async (client7) => {
            await client7.query("SELECT 1");
          });
          const client6 = await wrapper.pool.connect();
          try {
            await client6.query("SELECT 1");
          } finally {
            client6.release();
          }
        } catch (error) {
          snapshot.healthy = false;
          snapshot.lastError = error.message;
        }
        const breakerError = wrapper.circuitBreaker.getLastError();
        if (breakerError && !snapshot.lastError) {
          snapshot.lastError = breakerError.message;
        }
        return snapshot;
      })
    );
    return checks;
  };
  const slowQueryInsights = () => {
    const entries = Array.from(slowQueryStats.entries()).map(
      ([key, value]) => ({
        key,
        pool: value.pool,
        executions: value.count,
        avgDurationMs: value.totalDuration / Math.max(value.count, 1),
        maxDurationMs: value.maxDuration
      })
    );
    return entries.sort((a, b) => b.maxDurationMs - a.maxDurationMs);
  };
  return {
    query: query3,
    read,
    write,
    transaction: withTransaction,
    withTransaction,
    connect,
    end,
    on: (event, listener) => {
      writePool2.pool.on(event, listener);
      readPools.forEach((wrapper) => wrapper.pool.on(event, listener));
      return managedPool;
    },
    healthCheck,
    slowQueryInsights,
    queryCaptureSnapshot: snapshotQueryCapture,
    pool: writePool2.pool
  };
}
async function executeManagedQuery({
  queryInput,
  params,
  options: options2,
  desiredType,
  writePool: writePool2,
  readPools
}) {
  if (options2.tenantId) {
    const guard = ResidencyGuard.getInstance();
    await guard.enforce(options2.tenantId, {
      operation: "storage",
      targetRegion: process.env.SUMMIT_REGION || "us-east-1",
      dataClassification: options2.classification || "internal"
    });
  }
  let activeWritePool = writePool2;
  let activeReadPools = readPools;
  if (options2.tenantId) {
    const region = process.env.SUMMIT_REGION || "us-east-1";
    const route = await tenantRouter.resolveRegionalRoute(options2.tenantId, region);
    if (route) {
      activeWritePool = {
        name: route.partitionKey,
        type: "write",
        pool: route.writePool,
        circuitBreaker: new CircuitBreaker(route.partitionKey, CIRCUIT_BREAKER_FAILURE_THRESHOLD, CIRCUIT_BREAKER_COOLDOWN_MS)
      };
      activeReadPools = [{
        name: route.partitionKey,
        type: "read",
        pool: route.readPool,
        circuitBreaker: new CircuitBreaker(route.partitionKey, CIRCUIT_BREAKER_FAILURE_THRESHOLD, CIRCUIT_BREAKER_COOLDOWN_MS)
      }];
    }
  }
  const normalized = normalizeQuery(queryInput, params);
  const queryType = desiredType === "auto" ? inferQueryType(normalized.text) : desiredType;
  const poolCandidates = queryType === "write" ? [activeWritePool] : [...pickReadPoolSequence(activeReadPools), activeWritePool];
  const timeoutMs = options2.timeoutMs ?? dbConfig.statementTimeoutMs;
  const label = options2.label ?? inferOperation(normalized.text);
  let lastError;
  for (const candidate of poolCandidates) {
    if (candidate.type === "read" && !candidate.circuitBreaker.canExecute()) {
      lastError = candidate.circuitBreaker.getLastError();
      continue;
    }
    try {
      return await executeWithRetry(candidate, normalized, timeoutMs, label);
    } catch (error) {
      lastError = error;
      if (!isRetryableError(error)) {
        break;
      }
    }
  }
  throw lastError ?? new Error("Failed to execute PostgreSQL query");
}
async function executeWithRetry(wrapper, normalizedQuery, timeoutMs, label) {
  let attempt = 0;
  let delay2 = 40;
  while (attempt <= 3) {
    try {
      const client6 = await wrapper.pool.connect();
      try {
        return await executeQueryOnClient(client6, normalizedQuery, wrapper, label, timeoutMs);
      } finally {
        client6.release();
      }
    } catch (error) {
      const err = error;
      wrapper.circuitBreaker.recordFailure(err);
      if (!isRetryableError(err) || attempt === 3) {
        throw err;
      }
      const jitter = Math.random() * 10;
      await delayAsync(Math.min(delay2, 500) + jitter);
      delay2 = Math.min(delay2 * 2, 500);
      attempt += 1;
    }
  }
  throw new Error("PostgreSQL query exhausted retries");
}
async function executeQueryOnClient(client6, normalizedQuery, wrapper, label, timeoutMs) {
  const start = performance.now();
  const result2 = await client6.query({
    text: normalizedQuery.text,
    values: normalizedQuery.values,
    name: normalizedQuery.name
  });
  const duration = performance.now() - start;
  telemetry.subsystems.database.queries.add(1);
  telemetry.subsystems.database.latency.record(duration / 1e3);
  if (duration >= dbConfig.slowQueryThresholdMs) {
    recordSlowQuery(
      normalizedQuery.name,
      duration,
      wrapper.name,
      normalizedQuery.text
    );
  }
  recordQueryCapture(normalizedQuery, duration, wrapper.name, label);
  logger3.debug(
    {
      pool: wrapper.name,
      label,
      durationMs: duration,
      rows: result2.rowCount ?? 0
    },
    "PostgreSQL query executed"
  );
  return result2;
}
async function withManagedClient(poolWrapper, timeoutMs, fn, options2 = {}) {
  const startWait = performance.now();
  let client6 = await poolWrapper.pool.connect();
  const waitTime = performance.now() - startWait;
  if (client6.connectedAt && Date.now() - client6.connectedAt > MAX_LIFETIME_MS) {
    logger3.debug({ pool: poolWrapper.name }, "Closing expired PostgreSQL connection");
    client6.release(true);
    return withManagedClient(poolWrapper, timeoutMs, fn, options2);
  }
  const leakTimer = setTimeout(() => {
    logger3.error(
      { pool: poolWrapper.name },
      "Possible PostgreSQL connection leak detected"
    );
  }, CONNECTION_LEAK_THRESHOLD_MS);
  try {
    await client6.query("SET statement_timeout = $1", [timeoutMs]);
  } catch (error) {
    clearTimeout(leakTimer);
    client6.release(true);
    throw error;
  }
  try {
    const result2 = await fn(client6);
    if (!options2.skipRelease) {
    }
    return result2;
  } finally {
    if (!options2.skipRelease) {
      try {
        await client6.query("RESET statement_timeout");
        client6.release();
      } catch (error) {
        logger3.warn(
          { pool: poolWrapper.name, err: error },
          "Failed to reset statement timeout or release"
        );
        client6.release(true);
      }
    }
    clearTimeout(leakTimer);
  }
}
function normalizeQuery(query3, params) {
  if (typeof query3 === "string") {
    const trimmed = query3.trim();
    return {
      text: trimmed,
      values: params ?? [],
      name: getPreparedStatementName(trimmed)
    };
  }
  const text = query3.text.trim();
  const values = query3.values ?? params ?? [];
  return {
    text,
    values,
    name: query3.name ?? getPreparedStatementName(text)
  };
}
function getPreparedStatementName(queryText) {
  const normalized = queryText.replace(/\s+/g, " ").trim();
  const existing = preparedStatementCache.get(normalized);
  if (existing) {
    return existing;
  }
  const hash3 = crypto2.createHash("sha1").update(normalized).digest("hex").slice(0, 16);
  const name = `stmt_${hash3}`;
  preparedStatementCache.set(normalized, name);
  if (preparedStatementCache.size > MAX_PREPARED_STATEMENTS) {
    const firstKey = preparedStatementCache.keys().next().value;
    if (firstKey) {
      preparedStatementCache.delete(firstKey);
    }
  }
  return name;
}
function recordSlowQuery(statementName, duration, poolName, sql) {
  const key = `${poolName}:${statementName}`;
  const entry = slowQueryStats.get(key) ?? {
    count: 0,
    totalDuration: 0,
    maxDuration: 0,
    pool: poolName
  };
  entry.count += 1;
  entry.totalDuration += duration;
  entry.maxDuration = Math.max(entry.maxDuration, duration);
  slowQueryStats.set(key, entry);
  if (slowQueryStats.size > MAX_SLOW_QUERY_ENTRIES) {
    const iterator = slowQueryStats.keys().next();
    if (!iterator.done) {
      slowQueryStats.delete(iterator.value);
    }
  }
  const store = correlationStorage.getStore();
  const traceId = store?.get("traceId");
  const tenantId = store?.get("tenantId");
  logger3.warn(
    {
      pool: poolName,
      durationMs: duration,
      queryName: statementName,
      traceId,
      tenantId,
      sql
    },
    "Slow PostgreSQL query detected"
  );
}
function pickReadPoolSequence(readPools) {
  if (readPools.length === 0) {
    return [];
  }
  const sequence = [];
  for (let i = 0; i < readPools.length; i += 1) {
    const index = (readReplicaCursor + i) % readPools.length;
    sequence.push(readPools[index]);
  }
  readReplicaCursor = (readReplicaCursor + 1) % readPools.length;
  return sequence;
}
function inferQueryType(queryText) {
  const normalized = queryText.trim().toLowerCase();
  if (normalized.startsWith("with")) {
    const match = normalized.match(
      /with\s+[\s\S]*?\b(select|insert|update|delete|merge|create|alter|drop)\b/
    );
    if (match && match[1]) {
      return [
        "insert",
        "update",
        "delete",
        "merge",
        "create",
        "alter",
        "drop"
      ].includes(match[1]) ? "write" : "read";
    }
  }
  const firstToken = normalized.split(/\s+/)[0];
  if (["select", "show", "describe", "explain"].includes(firstToken) || normalized.startsWith("values")) {
    return "read";
  }
  return "write";
}
function inferOperation(queryText) {
  const normalized = queryText.trim().toLowerCase();
  const token = normalized.split(/\s+/)[0];
  if (token === "with") {
    return "cte";
  }
  return token;
}
function isRetryableError(error) {
  if (!error || typeof error !== "object") {
    return false;
  }
  const pgError = error;
  const nodeError = error;
  if (pgError.code && transientErrorCodes.has(pgError.code)) {
    return true;
  }
  if (nodeError.code && transientNodeErrors.has(nodeError.code)) {
    return true;
  }
  return false;
}
function delayAsync(duration) {
  return new Promise((resolve2) => {
    setTimeout(resolve2, duration);
  });
}
function percentile(values, p) {
  if (values.length === 0) return 0;
  const sorted = [...values].sort((a, b) => a - b);
  const idx = Math.min(sorted.length - 1, Math.floor(sorted.length * p));
  return sorted[idx];
}
function recordQueryCapture(normalizedQuery, duration, poolName, label) {
  if (!QUERY_CAPTURE_ENABLED) return;
  const key = normalizedQuery.name || getPreparedStatementName(normalizedQuery.text);
  const existing = queryCapture.get(key) ?? {
    sql: normalizedQuery.text.slice(0, 1e3),
    label,
    pool: poolName,
    count: 0,
    totalDurationMs: 0,
    maxDurationMs: 0,
    samples: []
  };
  existing.count += 1;
  existing.totalDurationMs += duration;
  existing.maxDurationMs = Math.max(existing.maxDurationMs, duration);
  if (existing.samples.length < QUERY_CAPTURE_MAX_SAMPLES) {
    existing.samples.push(duration);
  } else {
    const idx = Math.floor(Math.random() * existing.count);
    if (idx < QUERY_CAPTURE_MAX_SAMPLES) {
      existing.samples[idx] = duration;
    }
  }
  queryCapture.set(key, existing);
}
function snapshotQueryCapture() {
  const entries = Array.from(
    queryCapture.entries()
  ).map(([key, entry]) => ({
    key,
    sql: entry.sql,
    pool: entry.pool,
    label: entry.label,
    count: entry.count,
    totalDurationMs: entry.totalDurationMs,
    maxDurationMs: entry.maxDurationMs,
    avgDurationMs: entry.totalDurationMs / Math.max(entry.count, 1),
    p95DurationMs: percentile(entry.samples, 0.95)
  }));
  const topByTotalTime = [...entries].sort((a, b) => b.totalDurationMs - a.totalDurationMs).slice(0, 20);
  const topByP95 = [...entries].sort((a, b) => b.p95DurationMs - a.p95DurationMs).slice(0, 20);
  return { topByTotalTime, topByP95 };
}
function logQueryCaptureSnapshot(reason) {
  if (!QUERY_CAPTURE_ENABLED || queryCapture.size === 0) return;
  const snapshot = snapshotQueryCapture();
  logger3.info(
    { reason, queryCapture: snapshot },
    "DB query capture snapshot (top queries by total time and p95)"
  );
}
function getPostgresPool() {
  initializePools();
  if (!managedPool) {
    throw new Error("Failed to initialize PostgreSQL pool");
  }
  return managedPool;
}
async function closePostgresPool() {
  if (managedPool) {
    await managedPool.end();
    managedPool = null;
  }
}
var MAX_LIFETIME_MS, CONNECTION_LEAK_THRESHOLD_MS, logger3, QUERY_CAPTURE_ENABLED, QUERY_CAPTURE_INTERVAL_MS, QUERY_CAPTURE_MAX_SAMPLES, CIRCUIT_BREAKER_FAILURE_THRESHOLD, CIRCUIT_BREAKER_COOLDOWN_MS, MAX_PREPARED_STATEMENTS, MAX_SLOW_QUERY_ENTRIES, CircuitBreaker, preparedStatementCache, slowQueryStats, queryCapture, queryCaptureTimer, writePoolWrapper, readPoolWrappers, managedPool, readReplicaCursor, transientErrorCodes, transientNodeErrors, __private;
var init_postgres = __esm({
  "src/db/postgres.ts"() {
    "use strict";
    init_config2();
    init_logger();
    init_residency_guard();
    init_tenantRouter();
    init_comprehensive_telemetry();
    dotenv.config();
    MAX_LIFETIME_MS = 36e5;
    CONNECTION_LEAK_THRESHOLD_MS = 6e4;
    logger3 = logger.child({ name: "postgres-pool" });
    QUERY_CAPTURE_ENABLED = process.env.DB_QUERY_CAPTURE === "1";
    QUERY_CAPTURE_INTERVAL_MS = parseInt(
      process.env.DB_QUERY_CAPTURE_INTERVAL_MS ?? "30000",
      10
    );
    QUERY_CAPTURE_MAX_SAMPLES = 200;
    CIRCUIT_BREAKER_FAILURE_THRESHOLD = 5;
    CIRCUIT_BREAKER_COOLDOWN_MS = 3e4;
    MAX_PREPARED_STATEMENTS = 500;
    MAX_SLOW_QUERY_ENTRIES = 200;
    CircuitBreaker = class {
      constructor(name, failureThreshold, cooldownMs) {
        this.name = name;
        this.failureThreshold = failureThreshold;
        this.cooldownMs = cooldownMs;
      }
      failureCount = 0;
      state = "closed";
      openUntil = 0;
      lastError;
      canExecute() {
        if (this.state === "open") {
          if (Date.now() >= this.openUntil) {
            this.state = "half-open";
            logger3.warn(
              { pool: this.name },
              "PostgreSQL circuit breaker half-open"
            );
            return true;
          }
          return false;
        }
        return true;
      }
      recordSuccess() {
        if (this.state !== "closed" || this.failureCount !== 0) {
          logger3.info({ pool: this.name }, "PostgreSQL circuit breaker reset");
        }
        this.failureCount = 0;
        this.state = "closed";
        this.openUntil = 0;
        this.lastError = void 0;
      }
      recordFailure(error) {
        this.failureCount += 1;
        this.lastError = error;
        if (this.failureCount >= this.failureThreshold) {
          this.state = "open";
          this.openUntil = Date.now() + this.cooldownMs;
          logger3.error(
            { pool: this.name, failureCount: this.failureCount, err: error },
            "PostgreSQL circuit breaker opened"
          );
        } else if (this.state === "half-open") {
          this.state = "open";
          this.openUntil = Date.now() + this.cooldownMs;
          logger3.error(
            { pool: this.name, err: error },
            "PostgreSQL circuit breaker re-opened while half-open"
          );
        }
      }
      getState() {
        if (this.state === "open" && Date.now() >= this.openUntil) {
          return "half-open";
        }
        return this.state;
      }
      getFailureCount() {
        return this.failureCount;
      }
      getLastError() {
        return this.lastError;
      }
    };
    preparedStatementCache = /* @__PURE__ */ new Map();
    slowQueryStats = /* @__PURE__ */ new Map();
    queryCapture = /* @__PURE__ */ new Map();
    queryCaptureTimer = null;
    writePoolWrapper = null;
    readPoolWrappers = [];
    managedPool = null;
    readReplicaCursor = 0;
    transientErrorCodes = /* @__PURE__ */ new Set([
      "57P01",
      // admin_shutdown
      "57P02",
      // crash_shutdown
      "57P03",
      // cannot_connect_now
      "53300",
      // too_many_connections
      "08000",
      "08003",
      "08006",
      "08001",
      "08004",
      "08007",
      "08P01",
      "40001"
    ]);
    transientNodeErrors = /* @__PURE__ */ new Set([
      "ECONNRESET",
      "ECONNREFUSED",
      "ETIMEDOUT",
      "EHOSTUNREACH",
      "EPIPE"
    ]);
    __private = {
      initializePools,
      getPoolsSnapshot: () => ({ writePoolWrapper, readPoolWrappers }),
      getPreparedStatementName,
      inferQueryType,
      isRetryableError,
      CircuitBreaker,
      recordSlowQuery
    };
  }
});

// src/config/schema.ts
import * as z3 from "zod";
var ConfigSchema;
var init_schema = __esm({
  "src/config/schema.ts"() {
    "use strict";
    ConfigSchema = z3.object({
      env: z3.enum(["development", "test", "staging", "production"]).default("development"),
      port: z3.coerce.number().default(4e3),
      requireRealDbs: z3.coerce.boolean().default(false),
      neo4j: z3.object({
        uri: z3.string().default("bolt://localhost:7687"),
        username: z3.string().default("neo4j"),
        password: z3.string().default("devpassword"),
        database: z3.string().default("neo4j")
      }).default({}),
      postgres: z3.object({
        host: z3.string().default("localhost"),
        port: z3.coerce.number().default(5432),
        database: z3.string().default("intelgraph_dev"),
        username: z3.string().default("intelgraph"),
        password: z3.string().default("devpassword")
      }).default({}),
      redis: z3.object({
        host: z3.string().default("localhost"),
        port: z3.coerce.number().default(6379),
        password: z3.string().default("devpassword"),
        db: z3.coerce.number().default(0),
        useCluster: z3.coerce.boolean().default(false),
        clusterNodes: z3.array(z3.object({
          host: z3.string(),
          port: z3.coerce.number()
        })).default([]),
        tls: z3.coerce.boolean().default(false)
      }).default({}),
      jwt: z3.object({
        secret: z3.string().min(10).default("dev_jwt_secret_12345"),
        expiresIn: z3.string().default("24h"),
        refreshSecret: z3.string().min(10).default("dev_refresh_secret_67890"),
        refreshExpiresIn: z3.string().default("7d")
      }).default({}),
      bcrypt: z3.object({
        rounds: z3.coerce.number().default(12)
      }).default({}),
      rateLimit: z3.object({
        windowMs: z3.coerce.number().default(15 * 60 * 1e3),
        maxRequests: z3.coerce.number().default(100)
      }).default({}),
      cors: z3.object({
        origin: z3.string().default("http://localhost:3000")
      }).default({}),
      cache: z3.object({
        staleWhileRevalidateSeconds: z3.coerce.number().default(300)
      }).default({}),
      cdn: z3.object({
        enabled: z3.coerce.boolean().default(false),
        browserTtlSeconds: z3.coerce.number().default(60),
        edgeTtlSeconds: z3.coerce.number().default(300),
        surrogateKeyNamespace: z3.string().default("summit")
      }).default({}),
      features: z3.object({
        GRAPH_EXPAND_CACHE: z3.coerce.boolean().default(true),
        AI_REQUEST_ENABLED: z3.coerce.boolean().default(true)
      }).default({})
    });
  }
});

// src/config/load.ts
import Ajv from "ajv";
import addFormats from "ajv-formats";
import * as dotenv2 from "dotenv";
function loadConfig() {
  const rawConfig2 = {
    env: process.env.NODE_ENV,
    port: process.env.PORT,
    requireRealDbs: process.env.REQUIRE_REAL_DBS,
    neo4j: {
      uri: process.env.NEO4J_URI,
      username: process.env.NEO4J_USERNAME,
      password: process.env.NEO4J_PASSWORD,
      database: process.env.NEO4J_DATABASE
    },
    postgres: {
      host: process.env.POSTGRES_HOST,
      port: process.env.POSTGRES_PORT,
      database: process.env.POSTGRES_DB,
      username: process.env.POSTGRES_USER,
      password: process.env.POSTGRES_PASSWORD
    },
    redis: {
      host: process.env.REDIS_HOST,
      port: process.env.REDIS_PORT,
      password: process.env.REDIS_PASSWORD,
      db: process.env.REDIS_DB
    },
    jwt: {
      secret: process.env.JWT_SECRET,
      expiresIn: process.env.JWT_EXPIRES_IN,
      refreshSecret: process.env.JWT_REFRESH_SECRET,
      refreshExpiresIn: process.env.JWT_REFRESH_EXPIRES_IN
    },
    bcrypt: {
      rounds: process.env.BCRYPT_ROUNDS
    },
    rateLimit: {
      windowMs: process.env.RATE_LIMIT_WINDOW_MS,
      maxRequests: process.env.RATE_LIMIT_MAX_REQUESTS
    },
    cors: {
      origin: process.env.CORS_ORIGIN
    },
    features: {
      // Preserve specific logic from original index.ts
      GRAPH_EXPAND_CACHE: process.env.GRAPH_EXPAND_CACHE !== "0",
      AI_REQUEST_ENABLED: process.env.AI_REQUEST_ENABLED !== "0"
    }
  };
  const valid = validate(rawConfig2);
  const shouldValidateStrictly = process.env.CONFIG_VALIDATE_ON_START === "true";
  if (!valid) {
    const errors = validate.errors.map((err) => `${err.instancePath} ${err.message}`).join(", ");
    if (shouldValidateStrictly) {
      console.error(`\u274C Invalid Configuration (Strict Mode): ${errors}`);
      process.exit(1);
    } else {
      console.warn(`\u26A0\uFE0F Configuration validation failed (Non-strict mode): ${errors}`);
    }
  }
  return rawConfig2;
}
var schema, ajv, validate;
var init_load = __esm({
  "src/config/load.ts"() {
    "use strict";
    dotenv2.config();
    schema = {
      "$schema": "http://json-schema.org/draft-07/schema#",
      "type": "object",
      "properties": {
        "env": { "type": "string", "enum": ["development", "test", "staging", "production"], "default": "development" },
        "port": { "type": "integer", "default": 4e3 },
        "requireRealDbs": { "type": "boolean", "default": false },
        "neo4j": {
          "type": "object",
          "properties": {
            "uri": { "type": "string", "default": "bolt://localhost:7687" },
            "username": { "type": "string", "default": "neo4j" },
            "password": { "type": "string", "default": "devpassword" },
            "database": { "type": "string", "default": "neo4j" }
          },
          "required": ["uri", "username", "password", "database"],
          "default": {}
        },
        "postgres": {
          "type": "object",
          "properties": {
            "host": { "type": "string", "default": "localhost" },
            "port": { "type": "integer", "default": 5432 },
            "database": { "type": "string", "default": "intelgraph_dev" },
            "username": { "type": "string", "default": "intelgraph" },
            "password": { "type": "string", "default": "devpassword" }
          },
          "required": ["host", "port", "database", "username", "password"],
          "default": {}
        },
        "redis": {
          "type": "object",
          "properties": {
            "host": { "type": "string", "default": "localhost" },
            "port": { "type": "integer", "default": 6379 },
            "password": { "type": "string", "default": "devpassword" },
            "db": { "type": "integer", "default": 0 }
          },
          "required": ["host", "port", "password", "db"],
          "default": {}
        },
        "jwt": {
          "type": "object",
          "properties": {
            "secret": { "type": "string", "minLength": 10, "default": "dev_jwt_secret_12345" },
            "expiresIn": { "type": "string", "default": "24h" },
            "refreshSecret": { "type": "string", "minLength": 10, "default": "dev_refresh_secret_67890" },
            "refreshExpiresIn": { "type": "string", "default": "7d" }
          },
          "required": ["secret", "expiresIn", "refreshSecret", "refreshExpiresIn"],
          "default": {}
        },
        "bcrypt": {
          "type": "object",
          "properties": { "rounds": { "type": "integer", "default": 12 } },
          "required": ["rounds"],
          "default": {}
        },
        "rateLimit": {
          "type": "object",
          "properties": {
            "windowMs": { "type": "integer", "default": 9e5 },
            "maxRequests": { "type": "integer", "default": 100 }
          },
          "required": ["windowMs", "maxRequests"],
          "default": {}
        },
        "cors": {
          "type": "object",
          "properties": { "origin": { "type": "string", "default": "http://localhost:3000" } },
          "required": ["origin"],
          "default": {}
        },
        "features": {
          "type": "object",
          "properties": {
            "GRAPH_EXPAND_CACHE": { "type": "boolean", "default": true },
            "AI_REQUEST_ENABLED": { "type": "boolean", "default": true }
          },
          "required": ["GRAPH_EXPAND_CACHE", "AI_REQUEST_ENABLED"],
          "default": {}
        }
      },
      "required": ["env", "port", "neo4j", "postgres", "redis", "jwt", "bcrypt", "rateLimit", "cors", "features"],
      "additionalProperties": false
    };
    ajv = new Ajv({
      allErrors: true,
      coerceTypes: true,
      useDefaults: true,
      strict: false
    });
    addFormats(ajv);
    validate = ajv.compile(schema);
  }
});

// src/config/index.ts
import * as dotenv3 from "dotenv";
var rawConfig, config4, result, config_default;
var init_config3 = __esm({
  "src/config/index.ts"() {
    "use strict";
    init_schema();
    init_load();
    dotenv3.config();
    rawConfig = loadConfig();
    result = ConfigSchema.safeParse(rawConfig);
    if (!result.success) {
      const strict = process.env.CONFIG_VALIDATE_ON_START === "true";
      const errors = result.error.format();
      if (strict) {
        console.error("\u274C Invalid Configuration (Zod validation):", errors);
        process.exit(1);
      } else {
        console.warn("\u26A0\uFE0F Invalid Configuration detected by Zod (Non-strict mode). proceeding with potentially unstable config:", errors);
        config4 = rawConfig;
      }
    } else {
      config4 = result.data;
    }
    if (config4.requireRealDbs || config4.env === "production") {
      const devPasswords = [
        "devpassword",
        "dev_jwt_secret_12345",
        "dev_refresh_secret_67890"
      ];
      if (config4.neo4j && config4.postgres && config4.jwt && config4.redis) {
        if (devPasswords.includes(config4.neo4j.password) || devPasswords.includes(config4.postgres.password) || devPasswords.includes(config4.jwt.secret) || devPasswords.includes(config4.redis.password) || devPasswords.includes(config4.jwt.refreshSecret)) {
          console.error("\u274C Production/RealDBs mode security violations found.");
          process.exit(1);
        }
      }
    }
    config_default = config4;
  }
});

// src/monitoring/metrics.ts
import * as promClient from "prom-client";
function createHistogram(config9) {
  try {
    return new client2.Histogram(config9);
  } catch (e) {
    return {
      observe: () => {
      },
      startTimer: () => () => {
      },
      inc: () => {
      },
      dec: () => {
      },
      set: () => {
      },
      labels: () => ({ observe: () => {
      }, inc: () => {
      }, dec: () => {
      }, set: () => {
      } })
    };
  }
}
function createCounter(config9) {
  try {
    return new client2.Counter(config9);
  } catch (e) {
    return {
      inc: () => {
      },
      labels: () => ({ inc: () => {
      } })
    };
  }
}
function createGauge(config9) {
  try {
    return new client2.Gauge(config9);
  } catch (e) {
    return {
      inc: () => {
      },
      dec: () => {
      },
      set: () => {
      },
      labels: () => ({ inc: () => {
      }, dec: () => {
      }, set: () => {
      } })
    };
  }
}
var client2, register4, defaultMetricsInterval, httpRequestDuration2, httpRequestsTotal2, businessUserSignupsTotal2, businessApiCallsTotal2, businessRevenueTotal2, graphqlRequestDuration2, graphqlRequestsTotal2, graphqlErrors2, tenantScopeViolationsTotal2, dbConnectionsActive2, dbQueryDuration2, dbQueriesTotal2, vectorQueryDurationSeconds, vectorQueriesTotal, aiJobsQueued2, aiJobsProcessing2, aiJobDuration2, aiJobsTotal2, llmRequestDuration, llmTokensTotal, llmRequestsTotal, graphNodesTotal2, graphEdgesTotal2, graphOperationDuration2, websocketConnections, websocketMessages, investigationsActive, investigationOperations, erMergeOutcomesTotal, deploymentRollbacksTotal, approvalsPending, approvalsApprovedTotal, approvalsRejectedTotal, applicationErrors2, memoryUsage2, pipelineUptimeRatio2, pipelineFreshnessSeconds, pipelineCompletenessRatio, pipelineCorrectnessRatio, pipelineLatencySeconds2, graphragSchemaFailuresTotal, graphragCacheHitRatio, pbacDecisionsTotal, admissionDecisionsTotal, doclingInferenceDuration, doclingInferenceTotal, doclingCharactersProcessed, doclingCostUsd, graphExpandRequestsTotal2, aiRequestTotal2, resolverLatencyMs, neighborhoodCacheHitRatio, neighborhoodCacheLatencyMs, graphqlResolverDurationSeconds2, graphqlResolverErrorsTotal2, graphqlResolverCallsTotal2, webVitalValue, realtimeConflictsTotal, idempotentHitsTotal, serviceAutoRemediationsTotal, goldenPathStepTotal, uiErrorBoundaryCatchTotal, breakerState, intelgraphJobQueueDepth, maestroDeploymentsTotal2, maestroPrLeadTimeHours, maestroChangeFailureRate2, maestroMttrHours, maestroDagExecutionDurationSeconds, maestroJobExecutionDurationSeconds, narrativeSimulationActiveSimulations, narrativeSimulationTicksTotal, narrativeSimulationEventsTotal, narrativeSimulationDurationSeconds, shouldCollectMemory, intelgraphJobsProcessed, intelgraphOutboxSyncLatency, intelgraphActiveConnections, intelgraphDatabaseQueryDuration, intelgraphHttpRequestDuration, intelgraphGraphragQueryTotal, intelgraphGraphragQueryDurationMs, intelgraphQueryPreviewsTotal, intelgraphQueryPreviewLatencyMs, intelgraphQueryPreviewErrorsTotal, intelgraphQueryPreviewExecutionsTotal, intelgraphGlassBoxRunsTotal, intelgraphGlassBoxRunDurationMs, intelgraphGlassBoxCacheHits, intelgraphCacheHits, intelgraphCacheMisses, copilotApiRequestTotal2, copilotApiRequestDurationMs2, llmCostTotal, graphqlQueryCostHistogram, graphqlCostLimitExceededTotal, graphqlCostLimitRemaining, graphqlTenantCostUsage, graphqlCostRateLimitHits, graphqlPerTenantOverageCount, maestroOrchestrationRequests, maestroOrchestrationDuration, maestroOrchestrationErrors, maestroActiveConnections, maestroActiveSessions, maestroAiModelRequests, maestroAiModelDuration, maestroAiModelErrors, maestroAiModelCosts, maestroThompsonSamplingRewards, maestroGraphOperations, maestroGraphQueryDuration, maestroGraphConnections, maestroGraphEntities, maestroGraphRelations, maestroPremiumRoutingDecisions, maestroPremiumBudgetUtilization, maestroPremiumCostSavings, maestroSecurityEvents, maestroComplianceGateDecisions, maestroAuthenticationAttempts, maestroAuthorizationDecisions, maestroInvestigationsCreated, maestroDataSourcesActive, maestroWebScrapingRequests, maestroSynthesisOperations, metrics2;
var init_metrics2 = __esm({
  "src/monitoring/metrics.ts"() {
    "use strict";
    client2 = promClient.default || promClient;
    register4 = new client2.Registry();
    if (process.env.NODE_ENV !== "test" && process.env.ZERO_FOOTPRINT !== "true") {
      try {
        defaultMetricsInterval = client2.collectDefaultMetrics({
          register: register4,
          gcDurationBuckets: [1e-3, 0.01, 0.1, 1, 2, 5]
          // Garbage collection buckets
        });
      } catch (e) {
      }
    }
    httpRequestDuration2 = createHistogram({
      registers: [],
      name: "http_request_duration_seconds",
      help: "Duration of HTTP requests in seconds",
      labelNames: ["method", "route", "status_code"],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    httpRequestsTotal2 = createCounter({
      registers: [],
      name: "http_requests_total",
      help: "Total number of HTTP requests",
      labelNames: ["method", "route", "status_code"]
    });
    businessUserSignupsTotal2 = createCounter({
      registers: [],
      name: "business_user_signups_total",
      help: "Total number of customer or workspace signups",
      labelNames: ["tenant", "plan"]
    });
    businessApiCallsTotal2 = createCounter({
      registers: [],
      name: "business_api_calls_total",
      help: "API calls attributed to customer activity and billing",
      labelNames: ["service", "route", "status_code", "tenant"]
    });
    businessRevenueTotal2 = createCounter({
      registers: [],
      name: "business_revenue_total",
      help: "Recognized revenue amounts in the system's reporting currency",
      labelNames: ["tenant", "currency"]
    });
    graphqlRequestDuration2 = createHistogram({
      registers: [],
      name: "graphql_request_duration_seconds",
      help: "Duration of GraphQL requests in seconds",
      labelNames: ["operation", "operation_type"],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    graphqlRequestsTotal2 = createCounter({
      registers: [],
      name: "graphql_requests_total",
      help: "Total number of GraphQL requests",
      labelNames: ["operation", "operation_type", "status"]
    });
    graphqlErrors2 = createCounter({
      registers: [],
      name: "graphql_errors_total",
      help: "Total number of GraphQL errors",
      labelNames: ["operation", "error_type"]
    });
    tenantScopeViolationsTotal2 = createCounter({
      registers: [],
      name: "tenant_scope_violations_total",
      help: "Total number of tenant scope violations"
    });
    dbConnectionsActive2 = createGauge({
      registers: [],
      name: "db_connections_active",
      help: "Number of active database connections",
      labelNames: ["database"]
    });
    dbQueryDuration2 = createHistogram({
      registers: [],
      name: "db_query_duration_seconds",
      help: "Duration of database queries in seconds",
      labelNames: ["database", "operation"],
      buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5]
    });
    dbQueriesTotal2 = createCounter({
      registers: [],
      name: "db_queries_total",
      help: "Total number of database queries",
      labelNames: ["database", "operation", "status"]
    });
    vectorQueryDurationSeconds = createHistogram({
      registers: [],
      name: "vector_query_duration_seconds",
      help: "Latency of pgvector operations in seconds",
      labelNames: ["operation", "tenant_id"],
      buckets: [5e-3, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]
    });
    vectorQueriesTotal = createCounter({
      registers: [],
      name: "vector_queries_total",
      help: "Total pgvector operations by outcome",
      labelNames: ["operation", "tenant_id", "status"]
    });
    aiJobsQueued2 = createGauge({
      registers: [],
      name: "ai_jobs_queued",
      help: "Number of AI/ML jobs in queue",
      labelNames: ["job_type"]
    });
    aiJobsProcessing2 = createGauge({
      registers: [],
      name: "ai_jobs_processing",
      help: "Number of AI/ML jobs currently processing",
      labelNames: ["job_type"]
    });
    aiJobDuration2 = createHistogram({
      registers: [],
      name: "ai_job_duration_seconds",
      help: "Duration of AI/ML job processing in seconds",
      labelNames: ["job_type", "status"],
      buckets: [1, 5, 10, 30, 60, 300, 600]
    });
    aiJobsTotal2 = createCounter({
      registers: [],
      name: "ai_jobs_total",
      help: "Total number of AI/ML jobs processed",
      labelNames: ["job_type", "status"]
    });
    llmRequestDuration = createHistogram({
      registers: [],
      name: "llm_request_duration_seconds",
      help: "Duration of LLM requests in seconds",
      labelNames: ["provider", "model", "status"],
      buckets: [0.5, 1, 2, 5, 10, 20, 60]
    });
    llmTokensTotal = createCounter({
      registers: [],
      name: "llm_tokens_total",
      help: "Total number of tokens processed",
      labelNames: ["provider", "model", "type"]
      // type: prompt, completion
    });
    llmRequestsTotal = createCounter({
      registers: [],
      name: "llm_requests_total",
      help: "Total number of LLM requests",
      labelNames: ["provider", "model", "status"]
    });
    graphNodesTotal2 = createGauge({
      registers: [],
      name: "graph_nodes_total",
      help: "Total number of nodes in the graph",
      labelNames: ["investigation_id"]
    });
    graphEdgesTotal2 = createGauge({
      registers: [],
      name: "graph_edges_total",
      help: "Total number of edges in the graph",
      labelNames: ["investigation_id"]
    });
    graphOperationDuration2 = createHistogram({
      registers: [],
      name: "graph_operation_duration_seconds",
      help: "Duration of graph operations in seconds",
      labelNames: ["operation", "investigation_id"],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    websocketConnections = createGauge({
      registers: [],
      name: "websocket_connections_active",
      help: "Number of active WebSocket connections"
    });
    websocketMessages = createCounter({
      registers: [],
      name: "websocket_messages_total",
      help: "Total number of WebSocket messages",
      labelNames: ["direction", "event_type"]
    });
    investigationsActive = createGauge({
      registers: [],
      name: "investigations_active",
      help: "Number of active investigations"
    });
    investigationOperations = createCounter({
      registers: [],
      name: "investigation_operations_total",
      help: "Total number of investigation operations",
      labelNames: ["operation", "user_id"]
    });
    erMergeOutcomesTotal = createCounter({
      registers: [],
      name: "er_merge_outcomes_total",
      help: "Total number of ER merge outcomes recorded",
      labelNames: ["decision", "entity_type", "method"]
    });
    deploymentRollbacksTotal = createCounter({
      registers: [],
      name: "deployment_rollbacks_total",
      help: "Total number of deployment rollbacks",
      labelNames: ["service", "reason", "success"]
    });
    approvalsPending = createGauge({
      registers: [],
      name: "approvals_pending",
      help: "Current pending approvals requiring human review"
    });
    approvalsApprovedTotal = createCounter({
      registers: [],
      name: "approvals_approved_total",
      help: "Total approvals granted by human reviewers",
      labelNames: ["reviewer_role"]
    });
    approvalsRejectedTotal = createCounter({
      registers: [],
      name: "approvals_rejected_total",
      help: "Total approvals rejected by human reviewers",
      labelNames: ["reviewer_role"]
    });
    applicationErrors2 = createCounter({
      registers: [],
      name: "application_errors_total",
      help: "Total number of application errors",
      labelNames: ["module", "error_type", "severity"]
    });
    memoryUsage2 = createGauge({
      registers: [],
      name: "application_memory_usage_bytes",
      help: "Memory usage by application component",
      labelNames: ["component"]
    });
    pipelineUptimeRatio2 = createGauge({
      registers: [],
      name: "pipeline_uptime_ratio",
      help: "Pipeline availability ratio (0..1) over current window",
      labelNames: ["source", "pipeline", "env"]
    });
    pipelineFreshnessSeconds = createGauge({
      registers: [],
      name: "pipeline_freshness_seconds",
      help: "Freshness (seconds) from source event to load completion",
      labelNames: ["source", "pipeline", "env"]
    });
    pipelineCompletenessRatio = createGauge({
      registers: [],
      name: "pipeline_completeness_ratio",
      help: "Data completeness ratio (0..1) expected vs actual",
      labelNames: ["source", "pipeline", "env"]
    });
    pipelineCorrectnessRatio = createGauge({
      registers: [],
      name: "pipeline_correctness_ratio",
      help: "Validation pass rate ratio (0..1)",
      labelNames: ["source", "pipeline", "env"]
    });
    pipelineLatencySeconds2 = createHistogram({
      registers: [],
      name: "pipeline_latency_seconds",
      help: "End-to-end processing latency seconds",
      labelNames: ["source", "pipeline", "env"],
      buckets: [5, 15, 30, 60, 120, 300, 600, 1200]
    });
    try {
      register4.registerMetric(httpRequestDuration2);
      register4.registerMetric(httpRequestsTotal2);
      register4.registerMetric(graphqlRequestDuration2);
      register4.registerMetric(graphqlRequestsTotal2);
      register4.registerMetric(graphqlErrors2);
      register4.registerMetric(tenantScopeViolationsTotal2);
      register4.registerMetric(dbConnectionsActive2);
      register4.registerMetric(dbQueryDuration2);
      register4.registerMetric(dbQueriesTotal2);
      register4.registerMetric(vectorQueryDurationSeconds);
      register4.registerMetric(vectorQueriesTotal);
      register4.registerMetric(aiJobsQueued2);
      register4.registerMetric(aiJobsProcessing2);
      register4.registerMetric(aiJobDuration2);
      register4.registerMetric(aiJobsTotal2);
      register4.registerMetric(llmRequestDuration);
      register4.registerMetric(llmTokensTotal);
      register4.registerMetric(llmRequestsTotal);
      register4.registerMetric(graphNodesTotal2);
      register4.registerMetric(graphEdgesTotal2);
      register4.registerMetric(graphOperationDuration2);
      register4.registerMetric(websocketConnections);
      register4.registerMetric(websocketMessages);
      register4.registerMetric(investigationsActive);
      register4.registerMetric(investigationOperations);
      register4.registerMetric(erMergeOutcomesTotal);
      register4.registerMetric(deploymentRollbacksTotal);
      register4.registerMetric(approvalsPending);
      register4.registerMetric(approvalsApprovedTotal);
      register4.registerMetric(approvalsRejectedTotal);
      register4.registerMetric(applicationErrors2);
      register4.registerMetric(memoryUsage2);
      register4.registerMetric(pipelineUptimeRatio2);
      register4.registerMetric(pipelineFreshnessSeconds);
      register4.registerMetric(pipelineCompletenessRatio);
      register4.registerMetric(pipelineCorrectnessRatio);
      register4.registerMetric(pipelineLatencySeconds2);
    } catch (e) {
    }
    graphragSchemaFailuresTotal = createCounter({
      registers: [],
      name: "graphrag_schema_failures_total",
      help: "Total number of GraphRAG schema validation failures"
    });
    graphragCacheHitRatio = createGauge({
      registers: [],
      name: "graphrag_cache_hit_ratio",
      help: "Ratio of GraphRAG cache hits to total requests"
    });
    try {
      register4.registerMetric(graphragSchemaFailuresTotal);
      register4.registerMetric(graphragCacheHitRatio);
    } catch (e) {
    }
    pbacDecisionsTotal = createCounter({
      registers: [],
      name: "pbac_decisions_total",
      help: "Total PBAC access decisions",
      labelNames: ["decision"]
    });
    try {
      register4.registerMetric(pbacDecisionsTotal);
    } catch (e) {
    }
    admissionDecisionsTotal = createCounter({
      registers: [],
      name: "admission_decisions_total",
      help: "Total admission control decisions",
      labelNames: ["decision", "policy"]
    });
    try {
      register4.registerMetric(admissionDecisionsTotal);
    } catch (e) {
    }
    doclingInferenceDuration = createHistogram({
      registers: [],
      name: "docling_inference_duration_seconds",
      help: "Docling document inference duration in seconds",
      labelNames: ["model", "status"],
      buckets: [0.1, 0.5, 1, 2, 5, 10, 30, 60]
    });
    try {
      register4.registerMetric(doclingInferenceDuration);
    } catch (e) {
    }
    doclingInferenceTotal = createCounter({
      registers: [],
      name: "docling_inference_total",
      help: "Total Docling inference requests",
      labelNames: ["model", "status"]
    });
    try {
      register4.registerMetric(doclingInferenceTotal);
    } catch (e) {
    }
    doclingCharactersProcessed = createCounter({
      registers: [],
      name: "docling_characters_processed_total",
      help: "Total characters processed by Docling",
      labelNames: ["model"]
    });
    try {
      register4.registerMetric(doclingCharactersProcessed);
    } catch (e) {
    }
    doclingCostUsd = createCounter({
      registers: [],
      name: "docling_cost_usd_total",
      help: "Total cost in USD for Docling processing",
      labelNames: ["model"]
    });
    try {
      register4.registerMetric(doclingCostUsd);
    } catch (e) {
    }
    graphExpandRequestsTotal2 = createCounter({
      registers: [],
      name: "graph_expand_requests_total",
      help: "Total expandNeighbors requests",
      labelNames: ["cached"]
    });
    aiRequestTotal2 = createCounter({
      registers: [],
      name: "ai_request_total",
      help: "AI request events",
      labelNames: ["status"]
    });
    resolverLatencyMs = createHistogram({
      registers: [],
      name: "resolver_latency_ms",
      help: "Resolver latency in ms",
      labelNames: ["operation"],
      buckets: [5, 10, 25, 50, 100, 200, 400, 800, 1600]
    });
    neighborhoodCacheHitRatio = createGauge({
      registers: [],
      name: "neighborhood_cache_hit_ratio",
      help: "Neighborhood cache hit ratio"
    });
    neighborhoodCacheLatencyMs = createHistogram({
      registers: [],
      name: "neighborhood_cache_latency_ms",
      help: "Neighborhood cache lookup latency in ms",
      buckets: [1, 5, 10, 25, 50, 100, 250, 500, 1e3]
    });
    graphqlResolverDurationSeconds2 = createHistogram({
      registers: [],
      name: "graphql_resolver_duration_seconds",
      help: "Duration of GraphQL resolver execution in seconds",
      labelNames: ["resolver_name", "field_name", "type_name", "status"],
      buckets: [1e-3, 5e-3, 0.01, 0.05, 0.1, 0.5, 1, 2, 5]
    });
    graphqlResolverErrorsTotal2 = createCounter({
      registers: [],
      name: "graphql_resolver_errors_total",
      help: "Total number of GraphQL resolver errors",
      labelNames: ["resolver_name", "field_name", "type_name", "error_type"]
    });
    graphqlResolverCallsTotal2 = createCounter({
      registers: [],
      name: "graphql_resolver_calls_total",
      help: "Total number of GraphQL resolver calls",
      labelNames: ["resolver_name", "field_name", "type_name"]
    });
    webVitalValue = createGauge({
      registers: [],
      name: "web_vital_value",
      help: "Latest reported Web Vitals values",
      labelNames: ["metric", "id"]
    });
    realtimeConflictsTotal = createCounter({
      registers: [],
      name: "realtime_conflicts_total",
      help: "Total number of real-time update conflicts (LWW)"
    });
    idempotentHitsTotal = createCounter({
      registers: [],
      name: "idempotent_hits_total",
      help: "Total number of idempotent mutation hits"
    });
    serviceAutoRemediationsTotal = createCounter({
      registers: [],
      name: "service_auto_remediations_total",
      help: "Total number of automated remediation actions executed",
      labelNames: ["service", "action", "result"]
    });
    goldenPathStepTotal = createCounter({
      registers: [],
      name: "golden_path_step_total",
      help: "Completion of steps in the Golden Path user journey",
      labelNames: ["step", "status", "tenant_id"]
    });
    uiErrorBoundaryCatchTotal = createCounter({
      registers: [],
      name: "ui_error_boundary_catch_total",
      help: "Total number of UI errors caught by the React Error Boundary",
      labelNames: ["component", "tenant_id"]
    });
    breakerState = createGauge({
      registers: [],
      name: "circuit_breaker_state",
      help: "State of the circuit breaker (0 = Closed, 1 = Open)",
      labelNames: ["service"]
    });
    intelgraphJobQueueDepth = createGauge({
      registers: [],
      name: "intelgraph_job_queue_depth",
      help: "Current depth of the job queue",
      labelNames: ["queue"]
    });
    maestroDeploymentsTotal2 = createCounter({
      registers: [],
      name: "maestro_deployments_total",
      help: "Total number of deployments",
      labelNames: ["environment", "status"]
    });
    maestroPrLeadTimeHours = createHistogram({
      registers: [],
      name: "maestro_pr_lead_time_hours",
      help: "Lead time for changes in hours",
      buckets: [1, 4, 12, 24, 48, 168]
    });
    maestroChangeFailureRate2 = createGauge({
      registers: [],
      name: "maestro_change_failure_rate",
      help: "Change failure rate percentage"
    });
    maestroMttrHours = createHistogram({
      registers: [],
      name: "maestro_mttr_hours",
      help: "Mean time to recovery in hours",
      buckets: [0.1, 0.5, 1, 4, 24]
    });
    maestroDagExecutionDurationSeconds = createHistogram({
      registers: [],
      name: "maestro_dag_execution_duration_seconds",
      help: "Duration of Maestro DAG execution in seconds",
      labelNames: ["dag_id", "status", "tenant_id"],
      buckets: [1, 5, 10, 30, 60, 300, 600]
    });
    maestroJobExecutionDurationSeconds = createHistogram({
      registers: [],
      name: "maestro_job_execution_duration_seconds",
      help: "Duration of Maestro Job execution in seconds",
      labelNames: ["job_type", "status", "tenant_id"],
      buckets: [0.1, 0.5, 1, 5, 10, 30, 60]
    });
    try {
      register4.registerMetric(maestroDagExecutionDurationSeconds);
      register4.registerMetric(maestroJobExecutionDurationSeconds);
      register4.registerMetric(graphExpandRequestsTotal2);
      register4.registerMetric(aiRequestTotal2);
      register4.registerMetric(resolverLatencyMs);
      register4.registerMetric(neighborhoodCacheHitRatio);
      register4.registerMetric(neighborhoodCacheLatencyMs);
      register4.registerMetric(graphqlResolverDurationSeconds2);
      register4.registerMetric(graphqlResolverErrorsTotal2);
      register4.registerMetric(graphqlResolverCallsTotal2);
      register4.registerMetric(webVitalValue);
      register4.registerMetric(realtimeConflictsTotal);
      register4.registerMetric(idempotentHitsTotal);
      register4.registerMetric(businessUserSignupsTotal2);
      register4.registerMetric(businessApiCallsTotal2);
      register4.registerMetric(businessRevenueTotal2);
      register4.registerMetric(serviceAutoRemediationsTotal);
      register4.registerMetric(goldenPathStepTotal);
      register4.registerMetric(uiErrorBoundaryCatchTotal);
      register4.registerMetric(maestroDeploymentsTotal2);
      register4.registerMetric(maestroPrLeadTimeHours);
      register4.registerMetric(maestroChangeFailureRate2);
      register4.registerMetric(maestroMttrHours);
      register4.registerMetric(breakerState);
      register4.registerMetric(intelgraphJobQueueDepth);
    } catch (e) {
    }
    narrativeSimulationActiveSimulations = createGauge({
      name: "narrative_simulation_active_total",
      help: "Total number of active narrative simulations"
    });
    narrativeSimulationTicksTotal = createCounter({
      name: "narrative_simulation_ticks_total",
      help: "Total number of simulation ticks executed",
      labelNames: ["simulation_id"]
    });
    narrativeSimulationEventsTotal = createCounter({
      name: "narrative_simulation_events_total",
      help: "Total number of events processed in simulations",
      labelNames: ["simulation_id", "event_type"]
    });
    narrativeSimulationDurationSeconds = createHistogram({
      name: "narrative_simulation_tick_duration_seconds",
      help: "Duration of a single simulation tick cycle",
      labelNames: ["simulation_id"],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    try {
      register4.registerMetric(narrativeSimulationActiveSimulations);
      register4.registerMetric(narrativeSimulationTicksTotal);
      register4.registerMetric(narrativeSimulationEventsTotal);
      register4.registerMetric(narrativeSimulationDurationSeconds);
    } catch (e) {
    }
    shouldCollectMemory = process.env.NODE_ENV !== "test" && !process.env.JEST_WORKER_ID;
    if (shouldCollectMemory) {
      setInterval(() => {
        try {
          const usage = process.memoryUsage();
          memoryUsage2.set({ component: "heap_used" }, usage.heapUsed);
          memoryUsage2.set({ component: "heap_total" }, usage.heapTotal);
          memoryUsage2.set({ component: "external" }, usage.external);
          memoryUsage2.set({ component: "rss" }, usage.rss);
        } catch (e) {
        }
      }, 3e4);
    }
    intelgraphJobsProcessed = createCounter({
      name: "intelgraph_jobs_processed_total",
      help: "Total jobs processed by the system",
      labelNames: ["queue", "status"]
    });
    intelgraphOutboxSyncLatency = createHistogram({
      name: "intelgraph_outbox_sync_latency_seconds",
      help: "Latency of outbox to Neo4j sync operations",
      labelNames: ["operation"],
      buckets: [0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 5, 10]
    });
    intelgraphActiveConnections = createGauge({
      name: "intelgraph_active_connections",
      help: "Number of active WebSocket connections",
      labelNames: ["tenant"]
    });
    intelgraphDatabaseQueryDuration = createHistogram({
      name: "intelgraph_database_query_duration_seconds",
      help: "Database query execution time",
      labelNames: ["database", "operation"],
      buckets: [1e-3, 5e-3, 0.01, 0.05, 0.1, 0.5, 1, 5]
    });
    intelgraphHttpRequestDuration = createHistogram({
      name: "intelgraph_http_request_duration_seconds",
      help: "HTTP request duration in seconds",
      labelNames: ["method", "route", "status"],
      buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5]
    });
    intelgraphGraphragQueryTotal = createCounter({
      name: "intelgraph_graphrag_query_total",
      help: "Total GraphRAG queries executed",
      labelNames: ["status", "hasPreview", "redactionEnabled", "provenanceEnabled"]
    });
    intelgraphGraphragQueryDurationMs = createHistogram({
      name: "intelgraph_graphrag_query_duration_ms",
      help: "GraphRAG query execution duration in milliseconds",
      labelNames: ["hasPreview"],
      buckets: [100, 500, 1e3, 2e3, 5e3, 1e4, 3e4]
    });
    intelgraphQueryPreviewsTotal = createCounter({
      name: "intelgraph_query_previews_total",
      help: "Total query previews generated",
      labelNames: ["language", "status"]
    });
    intelgraphQueryPreviewLatencyMs = createHistogram({
      name: "intelgraph_query_preview_latency_ms",
      help: "Query preview generation latency in milliseconds",
      labelNames: ["language"],
      buckets: [50, 100, 250, 500, 1e3, 2e3, 5e3]
    });
    intelgraphQueryPreviewErrorsTotal = createCounter({
      name: "intelgraph_query_preview_errors_total",
      help: "Total query preview errors",
      labelNames: ["language"]
    });
    intelgraphQueryPreviewExecutionsTotal = createCounter({
      name: "intelgraph_query_preview_executions_total",
      help: "Total query preview executions",
      labelNames: ["language", "dryRun", "status"]
    });
    intelgraphGlassBoxRunsTotal = createCounter({
      name: "intelgraph_glass_box_runs_total",
      help: "Total glass-box runs created",
      labelNames: ["type", "status"]
    });
    intelgraphGlassBoxRunDurationMs = createHistogram({
      name: "intelgraph_glass_box_run_duration_ms",
      help: "Glass-box run duration in milliseconds",
      labelNames: ["type"],
      buckets: [100, 500, 1e3, 2e3, 5e3, 1e4, 3e4, 6e4]
    });
    intelgraphGlassBoxCacheHits = createCounter({
      name: "intelgraph_glass_box_cache_hits_total",
      help: "Total glass-box cache hits",
      labelNames: ["operation"]
    });
    intelgraphCacheHits = createCounter({
      name: "intelgraph_cache_hits_total",
      help: "Total cache hits",
      labelNames: ["level"]
    });
    intelgraphCacheMisses = createCounter({
      name: "intelgraph_cache_misses_total",
      help: "Total cache misses"
    });
    copilotApiRequestTotal2 = createCounter({
      name: "copilot_api_request_total",
      help: "Total number of AI Copilot API requests",
      labelNames: ["endpoint", "mode", "status"]
    });
    copilotApiRequestDurationMs2 = createHistogram({
      name: "copilot_api_request_duration_ms",
      help: "AI Copilot API request duration in milliseconds",
      labelNames: ["endpoint", "mode"],
      buckets: [50, 100, 250, 500, 1e3, 2e3, 5e3, 1e4, 3e4]
    });
    llmCostTotal = createCounter({
      name: "llm_cost_total_usd",
      help: "Total estimated cost of LLM calls in USD",
      labelNames: ["provider", "model"]
    });
    graphqlQueryCostHistogram = createHistogram({
      registers: [],
      name: "graphql_query_cost_total",
      help: "Distribution of GraphQL query costs",
      labelNames: ["tenant_id", "operation_name", "operation_type"],
      buckets: [1, 10, 50, 100, 250, 500, 1e3, 2e3, 5e3, 1e4]
    });
    graphqlCostLimitExceededTotal = createCounter({
      registers: [],
      name: "graphql_cost_limit_exceeded_total",
      help: "Total number of queries rejected due to cost limits",
      labelNames: ["tenant_id", "reason", "tier"]
    });
    graphqlCostLimitRemaining = createGauge({
      registers: [],
      name: "graphql_cost_limit_remaining",
      help: "Remaining cost capacity for tenant (per minute)",
      labelNames: ["tenant_id", "tier"]
    });
    graphqlTenantCostUsage = createCounter({
      registers: [],
      name: "graphql_tenant_cost_usage_total",
      help: "Total cost consumed by tenant",
      labelNames: ["tenant_id", "tier", "user_id"]
    });
    graphqlCostRateLimitHits = createCounter({
      registers: [],
      name: "graphql_cost_rate_limit_hits_total",
      help: "Number of times cost-based rate limit was hit",
      labelNames: ["tenant_id", "limit_type", "tier"]
    });
    graphqlPerTenantOverageCount = createCounter({
      registers: [],
      name: "graphql_per_tenant_overage_count_total",
      help: "Count of cost limit overages per tenant",
      labelNames: ["tenant_id", "tier"]
    });
    try {
      register4.registerMetric(llmCostTotal);
      register4.registerMetric(graphqlQueryCostHistogram);
      register4.registerMetric(graphqlCostLimitExceededTotal);
      register4.registerMetric(graphqlCostLimitRemaining);
      register4.registerMetric(graphqlTenantCostUsage);
      register4.registerMetric(graphqlCostRateLimitHits);
      register4.registerMetric(graphqlPerTenantOverageCount);
      register4.registerMetric(intelgraphJobsProcessed);
      register4.registerMetric(intelgraphOutboxSyncLatency);
      register4.registerMetric(intelgraphActiveConnections);
      register4.registerMetric(intelgraphDatabaseQueryDuration);
      register4.registerMetric(intelgraphHttpRequestDuration);
      register4.registerMetric(intelgraphGraphragQueryTotal);
      register4.registerMetric(intelgraphGraphragQueryDurationMs);
      register4.registerMetric(intelgraphQueryPreviewsTotal);
      register4.registerMetric(intelgraphQueryPreviewLatencyMs);
      register4.registerMetric(intelgraphQueryPreviewErrorsTotal);
      register4.registerMetric(intelgraphQueryPreviewExecutionsTotal);
      register4.registerMetric(intelgraphGlassBoxRunsTotal);
      register4.registerMetric(intelgraphGlassBoxRunDurationMs);
      register4.registerMetric(intelgraphGlassBoxCacheHits);
      register4.registerMetric(intelgraphCacheHits);
      register4.registerMetric(intelgraphCacheMisses);
      register4.registerMetric(copilotApiRequestTotal2);
      register4.registerMetric(copilotApiRequestDurationMs2);
    } catch (e) {
    }
    maestroOrchestrationRequests = createCounter({
      registers: [],
      name: "maestro_orchestration_requests_total",
      help: "Total number of orchestration requests",
      labelNames: ["method", "endpoint", "status"]
    });
    maestroOrchestrationDuration = createHistogram({
      registers: [],
      name: "maestro_orchestration_duration_seconds",
      help: "Duration of orchestration requests",
      labelNames: ["endpoint"],
      buckets: [0.1, 0.5, 1, 2, 5, 10, 30, 60, 120]
    });
    maestroOrchestrationErrors = createCounter({
      registers: [],
      name: "maestro_orchestration_errors_total",
      help: "Total number of orchestration errors",
      labelNames: ["error_type", "endpoint"]
    });
    maestroActiveConnections = createGauge({
      registers: [],
      name: "maestro_active_connections",
      help: "Number of active connections",
      labelNames: ["type"]
    });
    maestroActiveSessions = createGauge({
      registers: [],
      name: "maestro_active_sessions_total",
      help: "Number of active user sessions",
      labelNames: ["type"]
    });
    maestroAiModelRequests = createCounter({
      registers: [],
      name: "maestro_ai_model_requests_total",
      help: "Total AI model requests by model type",
      labelNames: ["model", "operation", "status"]
    });
    maestroAiModelDuration = createHistogram({
      registers: [],
      name: "maestro_ai_model_response_time_seconds",
      help: "AI model response time",
      labelNames: ["model", "operation"],
      buckets: [0.1, 0.5, 1, 2, 5, 10, 20, 30]
    });
    maestroAiModelErrors = createCounter({
      registers: [],
      name: "maestro_ai_model_errors_total",
      help: "Total AI model errors",
      labelNames: ["model"]
      // Simplified
    });
    maestroAiModelCosts = createHistogram({
      registers: [],
      name: "maestro_ai_model_cost_usd",
      help: "Cost per AI model request in USD",
      labelNames: ["model", "operation"],
      buckets: [1e-3, 0.01, 0.1, 1, 5, 10, 50]
    });
    maestroThompsonSamplingRewards = createGauge({
      registers: [],
      name: "maestro_thompson_sampling_reward_rate",
      help: "Thompson sampling reward rate by model",
      labelNames: ["model"]
    });
    maestroGraphOperations = createCounter({
      registers: [],
      name: "maestro_graph_operations_total",
      help: "Total graph database operations",
      labelNames: ["operation", "status"]
    });
    maestroGraphQueryDuration = createHistogram({
      registers: [],
      name: "maestro_graph_query_duration_seconds",
      help: "Graph query execution time",
      labelNames: ["operation"],
      buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]
    });
    maestroGraphConnections = createGauge({
      registers: [],
      name: "maestro_graph_connections_active",
      help: "Active Neo4j connections"
    });
    maestroGraphEntities = createGauge({
      registers: [],
      name: "maestro_graph_entities_total",
      help: "Total entities in graph database",
      labelNames: ["entity_type"]
    });
    maestroGraphRelations = createGauge({
      registers: [],
      name: "maestro_graph_relations_total",
      help: "Total relations in graph database"
    });
    maestroPremiumRoutingDecisions = createCounter({
      registers: [],
      name: "maestro_premium_routing_decisions_total",
      help: "Premium routing decisions",
      labelNames: ["decision", "model_tier"]
    });
    maestroPremiumBudgetUtilization = createGauge({
      registers: [],
      name: "maestro_premium_budget_utilization_percent",
      help: "Premium model budget utilization percentage"
    });
    maestroPremiumCostSavings = createCounter({
      registers: [],
      name: "maestro_premium_cost_savings_usd",
      help: "Cost savings from premium routing",
      labelNames: ["model_tier"]
    });
    maestroSecurityEvents = createCounter({
      registers: [],
      name: "maestro_security_events_total",
      help: "Security events by type",
      labelNames: ["event_type", "severity", "user_id"]
    });
    maestroComplianceGateDecisions = createCounter({
      registers: [],
      name: "maestro_compliance_gate_decisions_total",
      help: "Compliance gate decisions",
      labelNames: ["decision", "policy", "reason"]
    });
    maestroAuthenticationAttempts = createCounter({
      registers: [],
      name: "maestro_authentication_attempts_total",
      help: "Authentication attempts",
      labelNames: ["auth_method", "status", "user_id"]
    });
    maestroAuthorizationDecisions = createCounter({
      registers: [],
      name: "maestro_authorization_decisions_total",
      help: "Authorization decisions"
    });
    maestroInvestigationsCreated = createCounter({
      registers: [],
      name: "maestro_investigations_created_total",
      help: "Total investigations created",
      labelNames: ["investigation_type", "user_id"]
    });
    maestroDataSourcesActive = createGauge({
      registers: [],
      name: "maestro_data_sources_active_total",
      help: "Number of active data sources",
      labelNames: ["source_type"]
    });
    maestroWebScrapingRequests = createCounter({
      registers: [],
      name: "maestro_web_scraping_requests_total",
      help: "Web scraping requests",
      labelNames: ["status", "domain"]
    });
    maestroSynthesisOperations = createCounter({
      registers: [],
      name: "maestro_synthesis_operations_total",
      help: "Data synthesis operations"
    });
    try {
      register4.registerMetric(maestroOrchestrationRequests);
      register4.registerMetric(maestroOrchestrationDuration);
      register4.registerMetric(maestroOrchestrationErrors);
      register4.registerMetric(maestroActiveConnections);
      register4.registerMetric(maestroActiveSessions);
      register4.registerMetric(maestroAiModelRequests);
      register4.registerMetric(maestroAiModelDuration);
      register4.registerMetric(maestroAiModelErrors);
      register4.registerMetric(maestroAiModelCosts);
      register4.registerMetric(maestroThompsonSamplingRewards);
      register4.registerMetric(maestroGraphOperations);
      register4.registerMetric(maestroGraphQueryDuration);
      register4.registerMetric(maestroGraphConnections);
      register4.registerMetric(maestroGraphEntities);
      register4.registerMetric(maestroGraphRelations);
      register4.registerMetric(maestroPremiumRoutingDecisions);
      register4.registerMetric(maestroPremiumBudgetUtilization);
      register4.registerMetric(maestroPremiumCostSavings);
      register4.registerMetric(maestroSecurityEvents);
      register4.registerMetric(maestroComplianceGateDecisions);
      register4.registerMetric(maestroAuthenticationAttempts);
      register4.registerMetric(maestroAuthorizationDecisions);
      register4.registerMetric(maestroInvestigationsCreated);
      register4.registerMetric(maestroDataSourcesActive);
      register4.registerMetric(maestroWebScrapingRequests);
      register4.registerMetric(maestroSynthesisOperations);
    } catch (e) {
    }
    if (process.env.NODE_ENV === "test") {
    }
    metrics2 = {
      graphExpandRequestsTotal: graphExpandRequestsTotal2,
      aiRequestTotal: aiRequestTotal2,
      resolverLatencyMs,
      breakerState,
      intelgraphJobQueueDepth,
      graphragSchemaFailuresTotal,
      graphragCacheHitRatio,
      neighborhoodCacheHitRatio,
      neighborhoodCacheLatencyMs,
      pbacDecisionsTotal,
      businessUserSignupsTotal: businessUserSignupsTotal2,
      businessApiCallsTotal: businessApiCallsTotal2,
      businessRevenueTotal: businessRevenueTotal2,
      serviceAutoRemediationsTotal,
      goldenPathStepTotal,
      uiErrorBoundaryCatchTotal,
      maestroDeploymentsTotal: maestroDeploymentsTotal2,
      maestroPrLeadTimeHours,
      maestroChangeFailureRate: maestroChangeFailureRate2,
      maestroMttrHours,
      maestroDagExecutionDurationSeconds,
      maestroJobExecutionDurationSeconds,
      httpRequestDuration: httpRequestDuration2,
      httpRequestsTotal: httpRequestsTotal2,
      graphqlRequestDuration: graphqlRequestDuration2,
      graphqlRequestsTotal: graphqlRequestsTotal2,
      graphqlErrors: graphqlErrors2,
      dbConnectionsActive: dbConnectionsActive2,
      dbQueryDuration: dbQueryDuration2,
      dbQueriesTotal: dbQueriesTotal2,
      aiJobsQueued: aiJobsQueued2,
      aiJobsProcessing: aiJobsProcessing2,
      aiJobDuration: aiJobDuration2,
      aiJobsTotal: aiJobsTotal2,
      llmRequestDuration,
      llmTokensTotal,
      llmRequestsTotal,
      graphNodesTotal: graphNodesTotal2,
      graphEdgesTotal: graphEdgesTotal2,
      graphOperationDuration: graphOperationDuration2,
      websocketConnections,
      websocketMessages,
      investigationsActive,
      investigationOperations,
      approvalsPending,
      approvalsApprovedTotal,
      approvalsRejectedTotal,
      applicationErrors: applicationErrors2,
      tenantScopeViolationsTotal: tenantScopeViolationsTotal2,
      memoryUsage: memoryUsage2,
      admissionDecisionsTotal,
      doclingInferenceDuration,
      doclingInferenceTotal,
      doclingCharactersProcessed,
      doclingCostUsd,
      pipelineUptimeRatio: pipelineUptimeRatio2,
      pipelineFreshnessSeconds,
      pipelineCompletenessRatio,
      pipelineCorrectnessRatio,
      pipelineLatencySeconds: pipelineLatencySeconds2,
      graphqlResolverDurationSeconds: graphqlResolverDurationSeconds2,
      graphqlResolverErrorsTotal: graphqlResolverErrorsTotal2,
      graphqlResolverCallsTotal: graphqlResolverCallsTotal2,
      webVitalValue,
      realtimeConflictsTotal,
      idempotentHitsTotal,
      graphqlQueryCostHistogram,
      graphqlCostLimitExceededTotal,
      graphqlCostLimitRemaining,
      graphqlTenantCostUsage,
      graphqlCostRateLimitHits,
      graphqlPerTenantOverageCount,
      maestroAiModelRequests,
      maestroAiModelErrors,
      maestroOrchestrationDuration,
      maestroOrchestrationRequests,
      maestroActiveSessions,
      maestroOrchestrationErrors,
      narrativeSimulationActiveSimulations,
      narrativeSimulationTicksTotal,
      narrativeSimulationEventsTotal,
      narrativeSimulationDurationSeconds
    };
  }
});

// src/metrics/neo4jMetrics.ts
import * as promClient2 from "prom-client";
function createHistogram2(config9) {
  try {
    return new client3.Histogram(config9);
  } catch (e) {
    return {
      observe: () => {
      },
      startTimer: () => () => {
      },
      labels: () => ({ observe: () => {
      } }),
      reset: () => {
      }
    };
  }
}
function createCounter2(config9) {
  try {
    return new client3.Counter(config9);
  } catch (e) {
    return {
      inc: () => {
      },
      labels: () => ({ inc: () => {
      } }),
      reset: () => {
      }
    };
  }
}
function createGauge2(config9) {
  try {
    return new client3.Gauge(config9);
  } catch (e) {
    return {
      inc: () => {
      },
      dec: () => {
      },
      set: () => {
      },
      labels: () => ({ inc: () => {
      }, dec: () => {
      }, set: () => {
      } }),
      reset: () => {
      }
    };
  }
}
var client3, neo4jQueryTotal, neo4jQueryErrorsTotal, neo4jQueryLatencyMs, neo4jConnectivityUp, neo4jActiveConnections, neo4jIdleConnections;
var init_neo4jMetrics = __esm({
  "src/metrics/neo4jMetrics.ts"() {
    "use strict";
    init_metrics2();
    client3 = promClient2.default || promClient2;
    neo4jQueryTotal = createCounter2({
      name: "neo4j_query_total",
      help: "Total number of Neo4j queries executed",
      labelNames: ["operation", "label", "tenant_id"]
    });
    neo4jQueryErrorsTotal = createCounter2({
      name: "neo4j_query_errors_total",
      help: "Total number of Neo4j query errors",
      labelNames: ["operation", "label", "tenant_id"]
    });
    neo4jQueryLatencyMs = createHistogram2({
      name: "neo4j_query_latency_ms",
      help: "Latency of Neo4j queries in milliseconds",
      labelNames: ["operation", "label", "tenant_id"],
      buckets: [5, 10, 25, 50, 100, 250, 500, 1e3, 2500, 5e3, 1e4]
    });
    neo4jConnectivityUp = createGauge2({
      name: "neo4j_connectivity_up",
      help: "Neo4j connectivity status (1=up, 0=down)"
    });
    neo4jActiveConnections = createGauge2({
      name: "neo4j_active_connections",
      help: "Number of active connections in the Neo4j pool"
    });
    neo4jIdleConnections = createGauge2({
      name: "neo4j_idle_connections",
      help: "Number of idle connections in the Neo4j pool"
    });
    try {
      register4.registerMetric(neo4jQueryTotal);
      register4.registerMetric(neo4jQueryErrorsTotal);
      register4.registerMetric(neo4jQueryLatencyMs);
      register4.registerMetric(neo4jConnectivityUp);
      register4.registerMetric(neo4jActiveConnections);
      register4.registerMetric(neo4jIdleConnections);
    } catch (e) {
    }
  }
});

// src/db/neo4jPerformanceMonitor.ts
import pino4 from "pino";
var logger4, Neo4jPerformanceMonitor, defaultMonitor, neo4jPerformanceMonitor;
var init_neo4jPerformanceMonitor = __esm({
  "src/db/neo4jPerformanceMonitor.ts"() {
    "use strict";
    init_neo4jMetrics();
    logger4 = pino4({ name: "neo4j-performance-monitor" });
    Neo4jPerformanceMonitor = class {
      slowQueryThresholdMs;
      maxTrackedQueries;
      slowQueries = [];
      recentErrors = [];
      labelCache = /* @__PURE__ */ new Map();
      constructor(options2) {
        this.slowQueryThresholdMs = options2.slowQueryThresholdMs;
        this.maxTrackedQueries = options2.maxTrackedQueries;
      }
      recordSuccess(outcome) {
        const { cypher, durationMs } = outcome;
        const labels2 = this.normalizeLabels(outcome.labels);
        neo4jQueryTotal.inc(labels2);
        neo4jQueryLatencyMs.observe(labels2, durationMs);
        if (durationMs >= this.slowQueryThresholdMs) {
          const normalizedOutcome = { ...outcome, labels: labels2 };
          this.trackSlowQuery(normalizedOutcome);
        }
        if (durationMs >= this.slowQueryThresholdMs * 2) {
          logger4.warn(
            {
              cypher: cypher.slice(0, 240),
              durationMs,
              operation: labels2.operation,
              label: labels2.label
            },
            "Neo4j slow query detected"
          );
        }
      }
      recordError(outcome) {
        const { durationMs } = outcome;
        const labels2 = this.normalizeLabels(outcome.labels);
        neo4jQueryTotal.inc(labels2);
        neo4jQueryErrorsTotal.inc(labels2);
        neo4jQueryLatencyMs.observe(labels2, durationMs);
        this.recentErrors.unshift({ ...outcome, labels: labels2, timestamp: Date.now() });
        this.trimTracked(this.recentErrors);
      }
      getSlowQueries() {
        return [...this.slowQueries];
      }
      getRecentErrors() {
        return [...this.recentErrors];
      }
      reset() {
        this.slowQueries.length = 0;
        this.recentErrors.length = 0;
        neo4jQueryTotal.reset();
        neo4jQueryErrorsTotal.reset();
        neo4jQueryLatencyMs.reset();
      }
      trackSlowQuery(outcome) {
        this.slowQueries.unshift({ ...outcome, timestamp: Date.now() });
        this.trimTracked(this.slowQueries);
      }
      trimTracked(buffer) {
        if (buffer.length > this.maxTrackedQueries) {
          buffer.length = this.maxTrackedQueries;
        }
      }
      normalizeLabels(labels2) {
        const operation = labels2?.operation || "unknown";
        const label = labels2?.label || "unlabeled";
        const tenant_id = labels2?.tenant_id || "unknown";
        const cacheKey = `${operation}:${label}:${tenant_id}`;
        let cached = this.labelCache.get(cacheKey);
        if (!cached) {
          cached = { operation, label, tenant_id };
          if (this.labelCache.size < 1e3) {
            this.labelCache.set(cacheKey, cached);
          }
        }
        return cached;
      }
    };
    defaultMonitor = new Neo4jPerformanceMonitor({
      slowQueryThresholdMs: Number(process.env.NEO4J_SLOW_QUERY_THRESHOLD_MS) || 500,
      maxTrackedQueries: Number(process.env.NEO4J_SLOW_QUERY_BUFFER || 50)
    });
    neo4jPerformanceMonitor = defaultMonitor;
  }
});

// src/db/neo4j.ts
var neo4j_exports = {};
__export(neo4j_exports, {
  closeNeo4jDriver: () => closeNeo4jDriver,
  getNeo4jDriver: () => getNeo4jDriver,
  initializeNeo4jDriver: () => initializeNeo4jDriver,
  instrumentSession: () => instrumentSession,
  isNeo4jMockMode: () => isNeo4jMockMode,
  neo: () => neo,
  onNeo4jDriverReady: () => onNeo4jDriverReady,
  transformNeo4jIntegers: () => transformNeo4jIntegers
});
import neo4j from "neo4j-driver";
import * as dotenv4 from "dotenv";
import pino5 from "pino";
async function initializeNeo4jDriver() {
  if (initializationPromise) return initializationPromise;
  initializationPromise = (async () => {
    try {
      logger5.info("Initializing Neo4j driver...");
      realDriver = neo4j.driver(
        NEO4J_URI,
        neo4j.auth.basic(NEO4J_USER, NEO4J_PASSWORD),
        {
          maxConnectionPoolSize: MAX_CONNECTION_POOL_SIZE,
          connectionTimeout: CONNECTION_TIMEOUT_MS,
          connectionAcquisitionTimeout: ACQUISITION_TIMEOUT_MS
        }
      );
      await realDriver.verifyConnectivity();
      isMockMode = false;
      logger5.info("Neo4j driver initialized successfully.");
      neo4jConnectivityUp.set(1);
      await Promise.all(readyCallbacks.map((cb) => cb({ reason: "driver_initialized" })));
    } catch (error) {
      neo4jConnectivityUp.set(0);
      if (REQUIRE_REAL_DBS) {
        logger5.error("Neo4j connectivity required but failed.", error);
        throw error;
      } else {
        logger5.warn("Neo4j connection failed, falling back to mock mode.", error);
        isMockMode = true;
      }
    }
  })();
  return initializationPromise;
}
function getNeo4jDriver() {
  if (!realDriver && !isMockMode) {
    throw new Error("Neo4j driver not initialized. Call initializeNeo4jDriver() first.");
  }
  return realDriver;
}
function isNeo4jMockMode() {
  return isMockMode;
}
async function closeNeo4jDriver() {
  if (realDriver) {
    await realDriver.close();
    realDriver = null;
    initializationPromise = null;
    isMockMode = true;
    logger5.info("Neo4j driver closed.");
  }
}
function onNeo4jDriverReady(callback) {
  readyCallbacks.push(callback);
}
function inferLabels(cypher) {
  const operation = /\b(create|merge|delete|set)\b/i.test(cypher) ? "write" : "read";
  const labelMatch = cypher.match(/:\s*([A-Za-z0-9_]+)/);
  return {
    operation,
    label: labelMatch?.[1] || "unlabeled"
  };
}
function instrumentSession(session) {
  const run = async (cypher, params, txConfig) => {
    const start = Date.now();
    const labels2 = inferLabels(cypher);
    try {
      const result2 = await session.run(cypher, params, txConfig);
      neo4jPerformanceMonitor.recordSuccess({
        cypher,
        params,
        durationMs: Date.now() - start,
        labels: labels2
      });
      return result2;
    } catch (error) {
      neo4jPerformanceMonitor.recordError({
        cypher,
        params,
        durationMs: Date.now() - start,
        labels: labels2,
        error: error?.message ?? String(error)
      });
      throw error;
    }
  };
  return { ...session, run };
}
function transformNeo4jIntegers(obj) {
  if (obj === null || obj === void 0 || typeof obj !== "object") {
    return obj;
  }
  if (neo4j.isInt(obj)) {
    return obj.inSafeRange() ? obj.toNumber() : obj.toString();
  }
  if (obj instanceof neo4j.types.DateTime || obj instanceof neo4j.types.Date || obj instanceof neo4j.types.Time || obj instanceof neo4j.types.LocalDateTime || obj instanceof neo4j.types.LocalTime) {
    return obj.toString();
  }
  if (Array.isArray(obj)) {
    let newArr = null;
    for (let i = 0; i < obj.length; i++) {
      const v = obj[i];
      const t = transformNeo4jIntegers(v);
      if (t !== v && !newArr) {
        newArr = obj.slice(0, i);
      }
      if (newArr) {
        newArr.push(t);
      }
    }
    return newArr || obj;
  }
  if (typeof obj.toObject === "function") {
    return transformNeo4jIntegers(obj.toObject());
  }
  if (obj instanceof Date || obj instanceof RegExp || typeof Buffer !== "undefined" && Buffer.isBuffer(obj)) {
    return obj;
  }
  let newObj = null;
  for (const k in obj) {
    if (Object.prototype.hasOwnProperty.call(obj, k)) {
      const v = obj[k];
      const t = transformNeo4jIntegers(v);
      if (t !== v && !newObj) {
        newObj = { ...obj };
      }
      if (newObj) {
        newObj[k] = t;
      }
    }
  }
  return newObj || obj;
}
var logger5, NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, REQUIRE_REAL_DBS, MAX_CONNECTION_POOL_SIZE, CONNECTION_TIMEOUT_MS, ACQUISITION_TIMEOUT_MS, realDriver, initializationPromise, isMockMode, readyCallbacks, neo;
var init_neo4j = __esm({
  "src/db/neo4j.ts"() {
    "use strict";
    init_neo4jMetrics();
    init_neo4jPerformanceMonitor();
    dotenv4.config();
    logger5 = pino5();
    NEO4J_URI = process.env.NEO4J_URI || "bolt://neo4j:7687";
    NEO4J_USER = process.env.NEO4J_USER || process.env.NEO4J_USERNAME || "neo4j";
    NEO4J_PASSWORD = process.env.NEO4J_PASSWORD || "devpassword";
    REQUIRE_REAL_DBS = process.env.REQUIRE_REAL_DBS === "true";
    MAX_CONNECTION_POOL_SIZE = parseInt(process.env.NEO4J_MAX_CONNECTION_POOL_SIZE || "50", 10);
    CONNECTION_TIMEOUT_MS = parseInt(process.env.NEO4J_CONNECTION_TIMEOUT_MS || "30000", 10);
    ACQUISITION_TIMEOUT_MS = parseInt(process.env.NEO4J_POOL_ACQUISITION_TIMEOUT_MS || "5000", 10);
    realDriver = null;
    initializationPromise = null;
    isMockMode = true;
    readyCallbacks = [];
    neo = {
      session: () => {
        if (isMockMode || !realDriver) {
          if (realDriver) return realDriver.session();
          return getNeo4jDriver().session();
        }
        return realDriver.session();
      },
      run: async (query3, params) => {
        const session = neo.session();
        try {
          const result2 = await session.run(query3, params);
          return result2;
        } finally {
          await session.close();
        }
      }
    };
  }
});

// src/config/database.ts
var database_exports = {};
__export(database_exports, {
  closeConnections: () => closeConnections,
  connectNeo4j: () => connectNeo4j,
  connectPostgres: () => connectPostgres,
  connectRedis: () => connectRedis,
  getNeo4jDriver: () => getNeo4jDriver2,
  getPostgresPool: () => getPostgresPool2,
  getRedisClient: () => getRedisClient
});
import ioredis from "ioredis";
async function connectNeo4j() {
  if (neo4jDriver) {
    return neo4jDriver;
  }
  registerNeo4jReadyHook();
  try {
    await initializeNeo4jDriver();
  } catch (error) {
    logger_default2.error("\u274C Failed to establish Neo4j connectivity:", error);
    if (config_default.requireRealDbs) {
      throw error;
    }
  }
  neo4jDriver = getNeo4jDriver();
  if (isNeo4jMockMode()) {
    logger_default2.warn("Neo4j unavailable - operating in mock mode for development.");
    neo4jMigrationsCompleted = false;
    return neo4jDriver;
  }
  const session = neo4jDriver.session();
  try {
    await session.run("RETURN 1");
  } finally {
    await session.close();
  }
  if (!neo4jMigrationsCompleted) {
    await runNeo4jMigrations();
    neo4jMigrationsCompleted = true;
  }
  logger_default2.info("\u2705 Connected to Neo4j");
  return neo4jDriver;
}
async function runNeo4jMigrations() {
  if (isNeo4jMockMode()) {
    logger_default2.debug("Skipping Neo4j migrations in mock mode.");
    return;
  }
  try {
    const { migrationManager } = await import("../db/migrations/index.js");
    await migrationManager.migrate();
    logger_default2.info("Neo4j migrations completed successfully");
  } catch (error) {
    logger_default2.warn(
      "Migration system not available, falling back to legacy constraints"
    );
    await createNeo4jConstraints();
  }
}
function registerNeo4jReadyHook() {
  if (neo4jReadyHookRegistered) {
    return;
  }
  neo4jReadyHookRegistered = true;
  onNeo4jDriverReady(async ({ reason }) => {
    if (isNeo4jMockMode()) {
      neo4jMigrationsCompleted = false;
      return;
    }
    if (reason === "reconnected" || !neo4jMigrationsCompleted) {
      try {
        await runNeo4jMigrations();
        neo4jMigrationsCompleted = true;
        if (reason === "reconnected") {
          logger_default2.info("Neo4j migrations reapplied after driver reconnection.");
        }
      } catch (error) {
        logger_default2.error("Failed to run Neo4j migrations after driver recovery:", error);
      }
    }
  });
}
async function createNeo4jConstraints() {
  if (!neo4jDriver) throw new Error("Neo4j driver not initialized");
  if (isNeo4jMockMode()) {
    logger_default2.debug("Skipping Neo4j constraint creation in mock mode.");
    return;
  }
  const session = neo4jDriver.session();
  try {
    const constraints = [
      "CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE",
      "CREATE CONSTRAINT entity_uuid IF NOT EXISTS FOR (e:Entity) REQUIRE e.uuid IS UNIQUE",
      "CREATE CONSTRAINT user_id IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE",
      "CREATE CONSTRAINT user_email IF NOT EXISTS FOR (u:User) REQUIRE u.email IS UNIQUE",
      "CREATE CONSTRAINT investigation_id IF NOT EXISTS FOR (i:Investigation) REQUIRE i.id IS UNIQUE",
      "CREATE CONSTRAINT relationship_id IF NOT EXISTS FOR ()-[r:RELATIONSHIP]-() REQUIRE r.id IS UNIQUE"
    ];
    for (const constraint of constraints) {
      try {
        await session.run(constraint);
      } catch (error) {
        const err = error;
        if (!err.message.includes("already exists")) {
          logger_default2.warn(
            "Failed to create constraint:",
            constraint,
            err.message
          );
        }
      }
    }
    const indexes = [
      "CREATE INDEX entity_type IF NOT EXISTS FOR (e:Entity) ON (e.type)",
      "CREATE INDEX entity_label IF NOT EXISTS FOR (e:Entity) ON (e.label)",
      "CREATE INDEX entity_created IF NOT EXISTS FOR (e:Entity) ON (e.createdAt)",
      "CREATE INDEX investigation_status IF NOT EXISTS FOR (i:Investigation) ON (i.status)",
      "CREATE INDEX user_username IF NOT EXISTS FOR (u:User) ON (u.username)",
      "CREATE FULLTEXT INDEX entity_search IF NOT EXISTS FOR (e:Entity) ON EACH [e.label, e.description]",
      "CREATE FULLTEXT INDEX investigation_search IF NOT EXISTS FOR (i:Investigation) ON EACH [i.title, i.description]"
    ];
    for (const index of indexes) {
      try {
        await session.run(index);
      } catch (error) {
        const err = error;
        if (!err.message.includes("already exists")) {
          logger_default2.warn("Failed to create index:", index, err.message);
        }
      }
    }
    logger_default2.info("Neo4j constraints and indexes created");
  } catch (error) {
    logger_default2.error("Failed to create Neo4j constraints:", error);
  } finally {
    await session.close();
  }
}
async function connectPostgres() {
  try {
    postgresPool = getPostgresPool();
    const client6 = await postgresPool.connect();
    try {
      await client6.query("SELECT NOW()");
    } finally {
      client6.release();
    }
    logger_default2.info("\u2705 Connected to PostgreSQL");
    return postgresPool;
  } catch (error) {
    logger_default2.error("\u274C Failed to connect to PostgreSQL:", error);
    throw error;
  }
}
async function connectRedis() {
  try {
    const redisConfig = {
      host: config_default.redis.host,
      port: config_default.redis.port,
      db: config_default.redis.db,
      retryDelayOnFailover: 100,
      maxRetriesPerRequest: 3,
      connectTimeout: 1e4,
      lazyConnect: true,
      keepAlive: 3e4,
      family: 4,
      enableOfflineQueue: false,
      reconnectOnError: (err) => {
        const targetError = "READONLY";
        return err.message.includes(targetError);
      },
      retryStrategy: (times) => {
        const delay2 = Math.min(times * 50, 2e3);
        return delay2;
      }
    };
    if (config_default.redis.password) {
      redisConfig.password = config_default.redis.password;
    }
    redisClient = new Redis(redisConfig);
    redisClient.on("error", (error) => {
      logger_default2.error("Redis error:", error.message);
    });
    redisClient.on("connect", () => {
      logger_default2.info("Redis connected");
    });
    redisClient.on("ready", () => {
      logger_default2.info("\u2705 Redis ready");
    });
    redisClient.on("reconnecting", () => {
      logger_default2.info("Redis reconnecting...");
    });
    redisClient.on("end", () => {
      logger_default2.warn("Redis connection ended");
    });
    await redisClient.connect();
    await redisClient.ping();
    logger_default2.info("\u2705 Connected to Redis");
    return redisClient;
  } catch (error) {
    const err = error;
    logger_default2.error("\u274C Failed to connect to Redis:", err.message);
    logger_default2.warn("Server will continue without Redis caching");
    return null;
  }
}
function getNeo4jDriver2() {
  return getNeo4jDriver();
}
function getPostgresPool2() {
  if (!postgresPool) {
    postgresPool = getPostgresPool();
  }
  return postgresPool;
}
function getRedisClient() {
  if (!redisClient) {
    logger_default2.warn("Redis client not available");
    return null;
  }
  return redisClient;
}
async function closeConnections() {
  if (neo4jDriver) {
    await neo4jDriver.close();
    logger_default2.info("Neo4j connection closed");
    neo4jDriver = null;
    neo4jMigrationsCompleted = false;
  }
  if (postgresPool) {
    await closePostgresPool();
    postgresPool = null;
    logger_default2.info("PostgreSQL connection closed");
  }
  if (redisClient) {
    redisClient.disconnect();
    logger_default2.info("Redis connection closed");
  }
}
var Redis, neo4jDriver, postgresPool, redisClient, neo4jMigrationsCompleted, neo4jReadyHookRegistered;
var init_database = __esm({
  "src/config/database.ts"() {
    "use strict";
    init_config3();
    init_logger2();
    init_postgres();
    init_neo4j();
    Redis = ioredis;
    neo4jDriver = null;
    postgresPool = null;
    redisClient = null;
    neo4jMigrationsCompleted = false;
    neo4jReadyHookRegistered = false;
  }
});

// src/graphql/dataloaders/supportTicketLoader.ts
import DataLoader from "dataloader";
import pino6 from "pino";
var logger6, safeDeleteEnabled, safeRows, reviveDates, batchGetComments, createSupportTicketLoader;
var init_supportTicketLoader = __esm({
  "src/graphql/dataloaders/supportTicketLoader.ts"() {
    "use strict";
    init_database();
    init_database();
    logger6 = pino6();
    safeDeleteEnabled = process.env.SAFE_DELETE !== "false";
    safeRows = (result2) => Array.isArray(result2?.rows) ? result2.rows : [];
    reviveDates = (_key, value) => {
      if (typeof value === "string" && /^\d{4}-\d{2}-\d{2}T/.test(value)) {
        return new Date(value);
      }
      return value;
    };
    batchGetComments = async (ticketIds) => {
      if (ticketIds.length === 0) return [];
      const redis5 = getRedisClient();
      const cacheKeys = ticketIds.map((id) => `support:comments:${id}`);
      const results = new Array(ticketIds.length).fill(null);
      const missedIndices = [];
      const missedTicketIds = [];
      if (redis5) {
        try {
          const cachedValues = await redis5.mget(cacheKeys);
          cachedValues.forEach((val, index) => {
            if (val) {
              results[index] = JSON.parse(val, reviveDates);
            } else {
              missedIndices.push(index);
              missedTicketIds.push(ticketIds[index]);
            }
          });
        } catch (err) {
          logger6.warn({ err }, "Redis MGET failed for support ticket loader");
          ticketIds.forEach((id, index) => {
            missedIndices.push(index);
            missedTicketIds.push(id);
          });
        }
      } else {
        ticketIds.forEach((id, index) => {
          missedIndices.push(index);
          missedTicketIds.push(id);
        });
      }
      if (missedTicketIds.length > 0) {
        const pool4 = getPostgresPool2();
        const query3 = `
        SELECT * FROM support_ticket_comments
        WHERE ticket_id = ANY($1)
        ${safeDeleteEnabled ? "AND deleted_at IS NULL" : ""}
        ORDER BY created_at ASC
      `;
        try {
          const result2 = await pool4.query(query3, [missedTicketIds]);
          const allComments = safeRows(result2);
          const commentsByTicket = /* @__PURE__ */ new Map();
          missedTicketIds.forEach((id) => commentsByTicket.set(id, []));
          allComments.forEach((comment) => {
            const list = commentsByTicket.get(comment.ticket_id);
            if (list) {
              list.push(comment);
            }
          });
          const pipeline2 = redis5 ? redis5.pipeline() : null;
          missedTicketIds.forEach((id, i) => {
            const comments = commentsByTicket.get(id) || [];
            const originalIndex = missedIndices[i];
            results[originalIndex] = comments;
            if (pipeline2) {
              pipeline2.setex(`support:comments:${id}`, 60, JSON.stringify(comments));
            }
          });
          if (pipeline2) {
            pipeline2.exec().catch((err) => logger6.warn({ err }, "Redis pipeline write failed"));
          }
        } catch (err) {
          logger6.error({ err }, "DB Fetch failed in support ticket loader");
          missedIndices.forEach((index) => {
            results[index] = [];
          });
        }
      }
      return results;
    };
    createSupportTicketLoader = () => new DataLoader(batchGetComments);
  }
});

// src/graphql/loaders.ts
import DataLoader2 from "dataloader";
var batchUsers, batchEntities, createLoaders;
var init_loaders = __esm({
  "src/graphql/loaders.ts"() {
    "use strict";
    init_database();
    init_supportTicketLoader();
    batchUsers = async (userIds) => {
      const pool4 = getPostgresPool2();
      const query3 = "SELECT id, email, username, role FROM users WHERE id = ANY($1)";
      try {
        const result2 = await pool4.query(query3, [userIds]);
        const userMap = new Map(result2.rows.map((u) => [u.id, u]));
        return userIds.map((id) => userMap.get(id) || new Error(`User not found: ${id}`));
      } catch (err) {
        return userIds.map(() => err);
      }
    };
    batchEntities = async (entityIds) => {
      const driver3 = getNeo4jDriver2();
      const session = driver3.session();
      try {
        const result2 = await session.run(
          `
            MATCH (n) WHERE elementId(n) IN $ids OR n.id IN $ids
            RETURN n
            `,
          { ids: entityIds }
        );
        const entityMap = /* @__PURE__ */ new Map();
        result2.records.forEach((record2) => {
          const node = record2.get("n");
          const id = node.properties.id || node.elementId;
          entityMap.set(id, node.properties);
        });
        return entityIds.map((id) => entityMap.get(id) || new Error(`Entity not found: ${id}`));
      } catch (err) {
        return entityIds.map(() => err);
      } finally {
        await session.close();
      }
    };
    createLoaders = () => ({
      userLoader: new DataLoader2(batchUsers),
      entityLoader: new DataLoader2(batchEntities),
      supportTicketLoader: createSupportTicketLoader()
    });
  }
});

// src/security/tenantContext.ts
var DEFAULT_TENANT_HEADER, DEFAULT_ENV_HEADER, DEFAULT_PRIVILEGE_HEADER, normalizeRoles, normalizeEnvironment, normalizePrivilegeTier, extractAuthContext, extractTenantContext;
var init_tenantContext = __esm({
  "src/security/tenantContext.ts"() {
    "use strict";
    DEFAULT_TENANT_HEADER = "x-tenant-id";
    DEFAULT_ENV_HEADER = "x-tenant-environment";
    DEFAULT_PRIVILEGE_HEADER = "x-tenant-privilege-tier";
    normalizeRoles = (roles) => {
      if (!roles) return [];
      if (Array.isArray(roles)) return roles.map(String);
      return [String(roles)];
    };
    normalizeEnvironment = (environment, fallback) => {
      const env2 = String(environment || "").toLowerCase();
      if (env2.startsWith("prod")) {
        return { value: "prod", inferred: false };
      }
      if (env2.startsWith("stag")) {
        return { value: "staging", inferred: false };
      }
      if (env2) {
        return { value: "dev", inferred: true };
      }
      return { value: fallback, inferred: true };
    };
    normalizePrivilegeTier = (tier) => {
      const normalized = String(tier || "").toLowerCase();
      if (["break-glass", "breakglass"].includes(normalized)) {
        return { value: "break-glass", inferred: false };
      }
      if (["elevated", "admin"].includes(normalized)) {
        return { value: "elevated", inferred: false };
      }
      if (normalized) {
        return { value: "standard", inferred: true };
      }
      return { value: "standard", inferred: true };
    };
    extractAuthContext = (req) => {
      const authContext = req.auth || req.user || {};
      if (!Object.keys(authContext).length && req.headers?.authorization === "Bearer dev-token" && process.env.NODE_ENV === "development") {
        return {
          id: "dev-user-1",
          sub: "dev-user-1",
          email: "developer@intelgraph.com",
          role: "ADMIN",
          tenantId: req.headers["x-tenant-id"] || "default"
        };
      }
      return authContext;
    };
    extractTenantContext = (req, options2 = {}) => {
      const headerName = options2.headerName || DEFAULT_TENANT_HEADER;
      const environmentHeader = options2.environmentHeader || DEFAULT_ENV_HEADER;
      const privilegeHeader = options2.privilegeHeader || DEFAULT_PRIVILEGE_HEADER;
      const tenantFromHeader = req.headers[headerName] || req.headers["x-tenant"];
      const authContext = extractAuthContext(req);
      const tenantFromAuth = authContext.tenantId || authContext.tenant_id;
      const tenantId = tenantFromHeader || tenantFromAuth;
      if (!tenantId) {
        return null;
      }
      const subject = authContext.sub || authContext.id || authContext.userId || "";
      const roles = normalizeRoles(authContext.roles);
      const { value: environment, inferred: inferredEnvironment } = normalizeEnvironment(
        req.headers[environmentHeader] || authContext.environment || process.env.NODE_ENV || "dev",
        "dev"
      );
      const { value: privilegeTier, inferred: inferredPrivilege } = normalizePrivilegeTier(
        req.headers[privilegeHeader] || authContext.privilegeTier || authContext.tier
      );
      return {
        tenantId: String(tenantId),
        roles,
        subject: subject ? String(subject) : "",
        environment,
        privilegeTier,
        inferredEnvironment,
        inferredPrivilege
      };
    };
  }
});

// src/lib/auth.ts
import { GraphQLError } from "graphql";
import jwt from "jsonwebtoken";
import pino7 from "pino";
import { randomUUID } from "node:crypto";
function extractToken(req) {
  const authHeader = req.headers?.authorization;
  if (authHeader?.startsWith("Bearer ")) {
    return authHeader.substring(7);
  }
  return null;
}
var logger7, JWT_SECRET, getContext, verifyToken;
var init_auth = __esm({
  "src/lib/auth.ts"() {
    "use strict";
    init_postgres();
    init_loaders();
    init_tenantContext();
    init_config();
    logger7 = pino7();
    JWT_SECRET = cfg.JWT_SECRET;
    getContext = async ({
      req
    }) => {
      const requestId = randomUUID();
      const loaders = createLoaders();
      try {
        if (req.user) {
          logger7.info({ requestId, userId: req.user.id }, "Authenticated request (middleware)");
          return {
            user: req.user,
            isAuthenticated: true,
            requestId,
            loaders,
            tenantContext: req.tenantContext || extractTenantContext(req, { strict: false })
          };
        }
        const token = extractToken(req);
        if (!token) {
          if (process.env.ENABLE_INSECURE_DEV_AUTH === "true" && process.env.NODE_ENV === "development") {
            logger7.info({ requestId }, "Allowing unauthenticated request (ENABLE_INSECURE_DEV_AUTH)");
            return {
              user: {
                id: "dev-user-1",
                email: "developer@intelgraph.com",
                username: "developer",
                role: "ADMIN",
                token_version: 0,
                tenantId: "tenant_1"
              },
              isAuthenticated: true,
              requestId,
              loaders,
              tenantContext: req.tenantContext || extractTenantContext(req, { strict: false })
            };
          }
          logger7.info({ requestId }, "Unauthenticated request");
          return { isAuthenticated: false, requestId, loaders };
        }
        const user = await verifyToken(token);
        logger7.info({ requestId, userId: user.id }, "Authenticated request");
        return {
          user,
          isAuthenticated: true,
          requestId,
          loaders,
          tenantContext: req.tenantContext || extractTenantContext(req, { strict: false })
        };
      } catch (error) {
        logger7.warn(
          { requestId, error: error.message },
          "Authentication failed"
        );
        return { isAuthenticated: false, requestId, loaders };
      }
    };
    verifyToken = async (token) => {
      try {
        logger7.info({ token: token === "dev-token" ? "dev-token" : "[REDACTED]" }, "Verifying token");
        if (process.env.NODE_ENV === "development" && token === "dev-token") {
          logger7.info("Accepted dev-token");
          return {
            id: "dev-user-1",
            email: "developer@intelgraph.com",
            username: "developer",
            role: "ADMIN",
            token_version: 0,
            tenantId: "tenant_1"
          };
        }
        const decoded = jwt.verify(token, JWT_SECRET);
        const pool4 = getPostgresPool();
        const result2 = await pool4.query(
          "SELECT id, email, username, role, token_version FROM users WHERE id = $1",
          [decoded.userId]
        );
        if (result2.rows.length === 0) {
          throw new Error("User not found");
        }
        const user = result2.rows[0];
        if (user.token_version !== decoded.token_version) {
          throw new Error("Token is revoked");
        }
        return user;
      } catch (error) {
        throw new GraphQLError("Invalid or expired token", {
          extensions: {
            code: "UNAUTHENTICATED",
            http: { status: 401 }
          }
        });
      }
    };
  }
});

// src/graphql/schema.factgov.ts
import { gql } from "apollo-server-express";
var factGovSchema;
var init_schema_factgov = __esm({
  "src/graphql/schema.factgov.ts"() {
    "use strict";
    factGovSchema = gql`
  type Agency {
    id: ID!
    name: String!
    domain: String!
    createdAt: DateTime!
    updatedAt: DateTime
  }

  type Vendor {
    id: ID!
    name: String!
    description: String
    tags: [String!]!
    complianceStatus: String!
    createdAt: DateTime!
    updatedAt: DateTime
  }

  type RFP {
    id: ID!
    agencyId: ID!
    title: String!
    content: String!
    budgetRange: String
    status: String!
    createdAt: DateTime!
    updatedAt: DateTime
  }

  type Match {
    id: ID!
    rfpId: ID!
    vendorId: ID!
    score: Float
    matchDetails: JSON
    createdAt: DateTime!
  }

  extend type Query {
    factgovGetRfp(id: ID!): RFP @scope(requires: "factgov:read")
    factgovGetVendor(id: ID!): Vendor @scope(requires: "factgov:read")
    factgovGetMatches(rfpId: ID!): [Match!]! @scope(requires: "factgov:read")
  }

  extend type Mutation {
    factgovCreateAgency(name: String!, domain: String!): Agency! @scope(requires: "factgov:admin")
    factgovCreateVendor(name: String!, tags: [String!]!, description: String): Vendor! @scope(requires: "factgov:write")
    factgovCreateRfp(agencyId: ID!, title: String!, content: String!): RFP! @scope(requires: "factgov:write")
    factgovMatchRfp(rfpId: ID!): [Match!]! @scope(requires: "factgov:write")
  }
`;
  }
});

// src/graphql/schema.ts
var schema_exports = {};
__export(schema_exports, {
  typeDefs: () => typeDefs
});
var mainSchema, typeDefs;
var init_schema2 = __esm({
  "src/graphql/schema.ts"() {
    "use strict";
    init_schema_factgov();
    mainSchema = `
  scalar JSON
  scalar DateTime
  type Entity { id: ID!, type: String!, props: JSON, createdAt: DateTime!, updatedAt: DateTime, canonicalId: ID }
  type Relationship { id: ID!, from: ID!, to: ID!, type: String!, props: JSON, createdAt: DateTime! }
  type GeneratedEntitiesResult {
    entities: [Entity!]!
    relationships: [Relationship!]!
  }

  type AISuggestionExplanation {
    score: Float!
    factors: [String!]!
    featureImportances: JSON
  }

  type AIRecommendation {
    from: ID!
    to: ID!
    score: Float!
    explanation: AISuggestionExplanation
  }

  type User {
    id: ID!
    email: String!
    username: String
    firstName: String
    lastName: String
    fullName: String
    role: String
    isActive: Boolean
    lastLogin: DateTime
    preferences: JSON
    createdAt: DateTime!
    updatedAt: DateTime
  }

  input UserInput {
    email: String!
    username: String
  }

  directive @scope(requires: [String]) on FIELD_DEFINITION

  type AuthResponse {
    success: Boolean!
    message: String
    user: AuthUser
    token: String
    refreshToken: String
    expiresIn: Int
  }

  type AuthUser {
    id: ID!
    email: String!
    username: String
    firstName: String
    lastName: String
    fullName: String
    role: String!
    isActive: Boolean!
    lastLogin: DateTime
    createdAt: DateTime!
    updatedAt: DateTime
  }

  type RefreshTokenResponse {
    success: Boolean!
    token: String
    refreshToken: String
  }

  type AuthResult {
    success: Boolean!
    message: String
  }

  type TokenVerification {
    valid: Boolean!
    user: AuthUser
  }

  type ResetTokenVerification {
    valid: Boolean!
  }

  type Investigation {
    id: ID!
    name: String!
    description: String
    createdAt: DateTime!
    updatedAt: DateTime
    entities: [Entity!]
    relationships: [Relationship!]
    status: InvestigationStatus
    priority: Int
  }

  type InvestigationSnapshot {
    id: ID!
    investigationId: ID!
    data: JSON!
    snapshotLabel: String
    createdAt: DateTime!
    createdBy: String!
  }

  type Case {
    id: ID!
    tenantId: String!
    title: String!
    description: String
    status: String!
    priority: String!
    compartment: String
    policyLabels: [String!]
    metadata: JSON
    createdAt: DateTime!
    updatedAt: DateTime!
    createdBy: String!
    closedAt: DateTime
    closedBy: String
    slaTimers: [SLATimer!]
    comments(limit: Int, offset: Int): [Comment!]
  }

  type Comment {
    commentId: ID!
    tenantId: String!
    targetType: String!
    targetId: String!
    parentId: ID
    rootId: ID
    content: String!
    authorId: String!
    author: User
    createdAt: DateTime!
    updatedAt: DateTime!
    mentions: [String!]
    isEdited: Boolean!
    isDeleted: Boolean!
    metadata: JSON
  }

  type SLATimer {
    slaId: ID!
    caseId: ID!
    tenantId: String!
    type: String!
    name: String!
    startTime: DateTime!
    deadline: DateTime!
    completedAt: DateTime
    status: String!
    targetDurationSeconds: Int!
    metadata: JSON
  }

  input CaseInput {
    title: String!
    description: String
    status: String
    priority: String
    compartment: String
    policyLabels: [String!]
    metadata: JSON
    reason: String
    legalBasis: String
  }

  input CaseUpdateInput {
    id: ID!
    title: String
    description: String
    status: String
    priority: String
    compartment: String
    policyLabels: [String!]
    metadata: JSON
    reason: String
    legalBasis: String
  }

  input CommentInput {
    targetType: String!
    targetId: String!
    parentId: ID
    content: String!
    mentions: [String!]
    metadata: JSON
  }

  input InvestigationInput {
    name: String!
    description: String
    status: InvestigationStatus
    priority: Int
  }

  enum InvestigationStatus {
    ACTIVE
    ARCHIVED
    COMPLETED
    ON_HOLD
  }

  type AuditLog {
    id: ID!
    userId: String!
    action: String!
    resourceType: String!
    resourceId: String
    details: JSON
    investigationId: String
    createdAt: DateTime!
  }

  input AuditFilter {
    userId: String
    entityType: String
    from: DateTime
    to: DateTime
  }

  type RiskSignal {
    id: ID!
    tenantId: String!
    kind: String!
    severity: String!
    message: String!
    source: String!
    createdAt: DateTime!
    context: JSON
  }

  type EvidenceOk {
    ok: Boolean!
    reasons: [String!]!
  }

  type WarRoom {
    id: ID!
    name: String!
    status: String
    createdBy: User!
    createdAt: DateTime!
    participants: [WarRoomParticipant!]!
  }

  type WarRoomParticipant {
    user: User!
    role: String!
    joinedAt: DateTime!
  }

  type Ticket {
    id: ID!
    key: String!
    summary: String!
    status: String!
    link: String
    runs: [JSON!]
    deployments: [JSON!]
  }

  type EvidenceBundle {
    id: ID!
    releaseId: ID!
    service: String!
    createdAt: DateTime!
  }


  type CogSecClaim {
    id: ID!
    text: String!
    evidence: [Evidence!]
    relatedClaims: [CogSecClaim!]
    narratives: [CogSecNarrative!]
    actors: [JSON!]
    channels: [JSON!]
  }

  type CogSecCampaign {
    id: ID!
    name: String!
    narratives: [CogSecNarrative!]
    signals: [CoordinationSignal!]
    actors: [JSON!]
    channels: [JSON!]
    claims: [CogSecClaim!]
    playbooks: [JSON!]
    incident: CogSecIncident
  }

  type CogSecIncident {
    id: ID!
    name: String!
    campaigns: [CogSecCampaign!]
    playbooks: [JSON!]
    leadAnalyst: User
    investigation: Investigation
  }

  type VerificationAppeal {
    id: ID!
    status: String!
    claim: CogSecClaim
    appellant: User
    reviewer: User
  }

  type ResponseAction {
    id: ID!
    title: String!
  }

  type ResponseArtifact {
    id: ID!
    title: String!
  }

  type VerifiedBriefing {
    id: ID!
    title: String!
  }

  type TakedownPacket {
    id: ID!
    title: String!
  }

  type SupportTicket {
    id: ID!
    title: String!
    description: String
    status: String!
    priority: String!
    reporterId: String!
    createdAt: DateTime!
    updatedAt: DateTime!
    comments(limit: Int, offset: Int): [SupportTicketComment!]!
  }

  type SupportTicketComment {
    id: ID!
    authorId: String!
    content: String!
    isInternal: Boolean!
    createdAt: DateTime!
  }

  type SupportTicketConnection {
    data: [SupportTicket!]!
    total: Int!
  }

  type CoordinationSignal {
    id: ID!
    type: String!
  }

  type CogSecNarrative {
    id: ID!
    name: String!
    description: String
  }

  type Evidence {
    id: ID!
    title: String!
    content: String!
  }

  type AudienceSegment {
    id: ID!
    name: String!
    description: String
    size: Int
    resilienceScore: Float
    trustInInstitutions: Float
    polarizationIndex: Float
    fearSensitivity: Float
    identityClusters: [String!]
    narrativeIds: [ID!]
    cognitiveStates: [CognitiveStateSnapshot!]
    targetedByCampaigns: [CogSecCampaign!]
    createdAt: DateTime!
    updatedAt: DateTime
  }

  type CognitiveStateSnapshot {
    id: ID!
    segmentId: ID!
    timestamp: DateTime!
    beliefVector: JSON
    resilienceScore: Float
    emotionalValence: Float
    arousalLevel: Float
  }

  type NarrativeCascade {
    id: ID!
    narrativeId: ID!
    startTime: DateTime!
    originNodeId: ID
    totalHops: Int
    maxDepth: Int
    uniqueActors: Int
    velocity: Float
    viralityScore: Float
    narrative: CogSecNarrative
    originActor: JSON
  }

  type NarrativeConflict {
    competingNarrativeId: ID!
    conflictScore: Float!
    contradictingClaims: [ContradictingClaimPair!]!
    competingNarrative: CogSecNarrative
  }

  type ContradictingClaimPair {
    claim1: CogSecClaim!
    claim2: CogSecClaim!
  }

  enum RiskSeverity {
    LOW
    MEDIUM
    HIGH
    CRITICAL
  }

  type TrustScore {
    subjectId: ID!
    score: Float!
    reasons: [String!]!
    updatedAt: DateTime!
  }

  type RiskSignalPage {
    items: [RiskSignal!]!
    total: Int!
    nextOffset: Int
  }

  type TrustScorePage {
    items: [TrustScore!]!
    total: Int!
    nextOffset: Int
  }

  type IncidentBundle {
    id: ID!
    type: String!
    status: String!
    signals: [RiskSignal!]!
    actions: [String!]!
    createdAt: DateTime!
    evidenceId: ID
    notes: String
  }

  type ProvenanceExport {
    format: String!
    content: JSON!
    exportedAt: DateTime!
    tenantId: String!
  }

  type OsintScan {
    id: ID!
    target: String!
    status: String!
  }

  type SynintAgentFinding {
    agentName: String!
    success: Boolean!
    findings: JSON!
    errors: [String!]
    warnings: [String!]
    startedAt: DateTime
    completedAt: DateTime
  }

  type SynintSweep {
    target: String!
    startedAt: DateTime!
    completedAt: DateTime!
    agents: [SynintAgentFinding!]!
    meta: JSON
  }

  input CognitiveExposureInput {
    segmentId: ID!
    narrativeId: ID!
    reactionType: String
    sentimentShift: Float
  }

  type Query {
    healthScore: Float!
    evidenceOk(service: String!, releaseId: String!): JSON!
    me: AuthUser
    verifyToken(token: String!): TokenVerification!
    verifyResetToken(token: String!): ResetTokenVerification!
    entity(id: ID!): Entity
    entities(type: String, q: String, limit: Int = 25, offset: Int = 0): [Entity!]!
    user(id: ID!): User
    users(limit: Int = 25, offset: Int = 0): [User!]!
    investigation(id: ID!): Investigation
    investigations(limit: Int = 25, offset: Int = 0): [Investigation!]!
    investigationSnapshots(investigationId: ID!): [InvestigationSnapshot!]!
    investigationSnapshot(id: ID!): InvestigationSnapshot
    semanticSearch(query: String!, filters: JSON, limit: Int = 10, offset: Int = 0): [Entity!]!
    auditTrace(investigationId: ID!, filter: JSON): [JSON!]!
    exportProvenance(tenantId: String!, format: String): JSON!
    riskSignals(limit: Int = 50, offset: Int = 0): [RiskSignal!]!
    riskSignalsPage(tenantId: String!, limit: Int = 50, offset: Int = 0, kind: String, severity: RiskSeverity): RiskSignalPage!
    trustScore(subjectId: ID!): TrustScore
    trustScoresPage(tenantId: String!, limit: Int = 50, offset: Int = 0): TrustScorePage
    evidenceBundles(filter: JSON, limit: Int = 20, offset: Int = 0): [EvidenceBundle!]!
    warRooms(status: String): [WarRoom!]!
    warRoom(id: ID!): WarRoom
    activeCampaigns(limit: Int = 20): [CogSecCampaign!]!
    activeIncidents(limit: Int): [CogSecIncident!]!
    audienceCognitiveProfile(id: ID!): AudienceSegment
    influencePathways(narrativeId: ID!): [NarrativeCascade!]!
    narrativeConflicts(narrativeId: ID!): [NarrativeConflict!]!
    incidentBundle(id: ID!): IncidentBundle
    supportTicket(id: ID!): SupportTicket
    supportTickets(filter: JSON, limit: Int, offset: Int): SupportTicketConnection!
    tickets(provider: String!, externalId: String, limit: Int): [Ticket!]!
    cogSecClaim(id: ID!): CogSecClaim
    cogSecClaims(filter: JSON, limit: Int, offset: Int): [CogSecClaim!]
    searchCogSecClaims(query: String!, limit: Int): [CogSecClaim!]
    similarClaims(claimId: ID!, threshold: Float): [CogSecClaim!]
    cogSecEvidence(id: ID!): Evidence
    cogSecNarrative(id: ID!): CogSecNarrative
    narrativeGraph(narrativeId: ID!): JSON
    cogSecCampaign(id: ID!): CogSecCampaign
    campaignSignals(campaignId: ID!): [CoordinationSignal!]
    cogSecIncident(id: ID!): CogSecIncident
    responsePlaybook(id: ID!): JSON
    cogSecAuditLogs(resourceType: String, resourceId: String, limit: Int): [JSON!]
    pendingAppeals(limit: Int): [VerificationAppeal!]
    governancePolicies: [JSON!]
    transparencyReport(startDate: String!, endDate: String!): JSON
    cogSecMetrics(startDate: String!, endDate: String!): JSON
    benchmarkComparison(startDate: String!, endDate: String!): JSON
    riskAssessment: JSON
    contentCredential(id: ID!): JSON
    cognitiveRiskDashboard(filters: JSON): JSON
    narrativeEarlyWarnings(watchlistId: ID): [JSON!]
    ewBattleSpace: JSON
    ewAnalyzeEMP(lat: Float!, lon: Float!, yieldKt: Float!): JSON
    funnel(period: String!): [JSON!]!
    pilotKpis(workspaceId: String!): JSON
    pilotSuccess(workspaceId: String!): [JSON!]!
    deduplicationCandidates(investigationId: ID, threshold: Float): [JSON!]!
    getCrisisTelemetry(scenarioId: ID!): JSON
    getAdversaryIntentEstimates(scenarioId: ID!): JSON
    getNarrativeHeatmapData(scenarioId: ID!): JSON
    getStrategicResponsePlaybooks(scenarioId: ID!): JSON
    getCrisisScenario(id: ID!): JSON
    getAllCrisisScenarios: [JSON]
    case(id: ID!, reason: String!, legalBasis: String!): Case
    cases(status: String, compartment: String, limit: Int, offset: Int): [Case!]
    comments(targetType: String!, targetId: String!, limit: Int, offset: Int): [Comment!]
    osintScans: [OsintScan]
    osintScan(id: ID!): OsintScan
    health: JSON
  }

  type Mutation {
    login(input: JSON!): AuthResponse!
    register(input: JSON!): AuthResponse!
    requestPasswordReset(email: String!): JSON!
    resetPassword(input: JSON!): JSON!
    refreshToken(token: String!): AuthResponse!
    logout: JSON!
    changePassword(currentPassword: String!, newPassword: String!): JSON!
    revokeToken(token: String!): JSON!
    createInvestigation(input: InvestigationInput!): Investigation!
    updateInvestigation(id: ID!, input: InvestigationInput!): Investigation!
    deleteInvestigation(id: ID!): Boolean!
    createInvestigationSnapshot(investigationId: ID!, label: String): InvestigationSnapshot!
    publishEvidence(input: JSON!): EvidenceBundle!
    recordCognitiveEffect(exposure: CognitiveExposureInput!): CognitiveStateSnapshot!
    createEntity(input: JSON!): Entity!
    updateEntity(id: ID!, input: JSON!, lastSeenTimestamp: String): Entity!
    deleteEntity(id: ID!): Boolean!
    linkEntities(text: String!): [JSON!]!
    extractRelationships(text: String!, entities: [JSON!]!): [JSON!]!
    createRelationship(input: JSON!): Relationship!
    updateRelationship(id: ID!, input: JSON!, lastSeenTimestamp: String): Relationship!
    deleteRelationship(id: ID!): Boolean!
    createUser(input: JSON!): User!
    updateUser(id: ID!, input: JSON!): User!
    deleteUser(id: ID!): Boolean!
    updateUserPreferences(userId: ID!, preferences: JSON!): User!
    extractClaim(input: JSON!): CogSecClaim!
    updateClaimVerdict(input: JSON!): CogSecClaim!
    linkRelatedClaims(claimId1: ID!, claimId2: ID!, relationType: String!): Boolean!
    createEvidence(input: JSON!): Evidence!
    verifyEvidence(evidenceId: ID!, notes: String): Evidence!
    linkEvidenceToClaims(evidenceId: ID!, claimIds: [ID!]!): Boolean!
    createNarrative(input: JSON!): CogSecNarrative!
    updateNarrativeStatus(narrativeId: ID!, status: String!): CogSecNarrative!
    linkClaimsToNarrative(claimIds: [ID!]!, narrativeId: ID!): Boolean!
    runDetectionPipeline: [CoordinationSignal!]!
    clusterIntoCampaigns: [CogSecCampaign!]!
    updateCampaignStatus(campaignId: ID!, status: String!): CogSecCampaign!
    createIncident(input: JSON!): CogSecIncident!
    updateIncidentStatus(incidentId: ID!, status: String!): CogSecIncident!
    addIncidentTimelineEvent(incidentId: ID!, type: String!, description: String!): JSON
    generatePlaybook(input: JSON!): JSON
    executePlaybookAction(playbookId: ID!, actionId: ID!): JSON
    updatePlaybookStatus(playbookId: ID!, status: String!): JSON
    generateBriefing(campaignId: ID!): JSON
    generateStakeholderMessage(campaignId: ID!, stakeholder: String!): JSON
    generateTakedownPacket(input: JSON!): JSON
    createAppeal(input: JSON!): VerificationAppeal!
    reviewAppeal(appealId: ID!, decision: String!, notes: String!): VerificationAppeal!
    createContentCredential(assetId: ID!, mimeType: String!, sourceUrl: String): JSON
    addProvenanceLink(credentialId: ID!, source: String!, platform: String): JSON
    raiseRiskSignal(input: JSON!): RiskSignal!
    updateRiskSignalStatus(id: ID!, status: String!): RiskSignal!
    createIncidentBundle(input: JSON!): IncidentBundle!
    linkTrustScoreEvidence(tenantId: String!, subjectId: ID!, evidenceId: ID!): TrustScore!
    createWarRoom(name: String!): WarRoom!
    addParticipant(warRoomId: ID!, userId: ID!, role: String!): WarRoom!
    removeParticipant(warRoomId: ID!, userId: ID!): WarRoom!
    createSupportTicket(input: JSON!): SupportTicket!
    updateSupportTicket(id: ID!, input: JSON!): SupportTicket!
    deleteSupportTicket(id: ID!): Boolean!
    addSupportTicketComment(ticketId: ID!, content: String!, isInternal: Boolean): JSON
    ewRegisterAsset(id: ID!, name: String!, type: String!, lat: Float!, lon: Float!, capabilities: [JSON], maxPower: Float, minFreq: Float, maxFreq: Float): JSON
    ewDeployJammer(assetId: ID!, targetFrequency: Float!, bandwidth: Float!, effect: JSON, durationSeconds: Int): JSON
    ewStopJammer(missionId: ID!): Boolean!
    ewSimulateSignalDetection(frequency: Float!, bandwidth: Float!, power: Float!, modulation: String!, type: JSON, lat: Float, lon: Float): JSON
    ewTriangulateSignal(signalId: ID!): JSON
    ewActivateProtection(assetId: ID!, measure: JSON): Boolean!
    submitNps(score: Int!, comment: String): Boolean!
    recordEvent(name: String!, props: JSON): Boolean!
    startTrial(plan: String!, days: Int!): Boolean!
    upgradePlan(plan: String!): Boolean!
    runWarGameSimulation(input: JSON!): JSON
    createCrisisScenario(input: JSON!): JSON
    updateCrisisScenario(id: ID!, input: JSON!): JSON
    deleteCrisisScenario(id: ID!): Boolean!
    createCase(input: CaseInput!): Case!
    updateCase(input: CaseUpdateInput!): Case!
    archiveCase(id: ID!, reason: String!, legalBasis: String!): Case!
    addComment(input: CommentInput!): Comment!
    updateComment(id: ID!, content: String!): Comment!
    deleteComment(id: ID!): Boolean!
    startOsintScan(target: String!): OsintScan
    runSynintSweep(target: String!): SynintSweep!
  }

  type Subscription {
    participantAdded: JSON!
    participantRemoved: JSON!
    campaignDetected: JSON!
    coordinationSignalDetected: JSON!
    incidentUpdated(incidentId: ID!): JSON!
    claimVerdictUpdated(narrativeId: ID): JSON!
    playbookActionCompleted(playbookId: ID!): JSON!
  }
`;
    typeDefs = [mainSchema, factGovSchema];
  }
});

// src/graphql/ordered-pubsub.ts
import { PubSub } from "graphql-subscriptions";
import pino8 from "pino";
var logger8, OrderedPubSub;
var init_ordered_pubsub = __esm({
  "src/graphql/ordered-pubsub.ts"() {
    "use strict";
    logger8 = pino8();
    OrderedPubSub = class {
      pubsub;
      buffers;
      seq;
      bufferSize;
      constructor(bufferSize = 100) {
        this.pubsub = new PubSub();
        this.buffers = /* @__PURE__ */ new Map();
        this.seq = 0;
        this.bufferSize = bufferSize;
      }
      /**
       * Publish an event to a channel. Payloads that are null/undefined or not
       * objects are logged and discarded. Oldest events are dropped once the
       * per-channel buffer exceeds the configured size.
       */
      publish(trigger, payload) {
        if (payload === null || payload === void 0 || typeof payload !== "object") {
          logger8.warn(
            { trigger, payload },
            "Dropped malformed subscription event payload"
          );
          return;
        }
        const envelope = {
          seq: ++this.seq,
          ts: Date.now(),
          payload
        };
        const buf = this.buffers.get(trigger) ?? [];
        if (buf.length >= this.bufferSize) {
          const dropped = buf.shift();
          logger8.warn(
            { trigger, dropped },
            "Dropping oldest subscription event due to buffer overflow"
          );
        }
        buf.push(envelope);
        this.buffers.set(trigger, buf);
        this.pubsub.publish(trigger, envelope);
      }
      /**
       * Return an async iterator that replays buffered events before emitting
       * new events from the underlying PubSub instance.
       */
      asyncIterator(triggers) {
        const triggerList = Array.isArray(triggers) ? triggers : [triggers];
        const baseIterator = this.pubsub.asyncIterator(triggerList);
        const buffered = triggerList.flatMap((t) => this.buffers.get(t) ?? []).sort((a, b) => a.seq - b.seq);
        let index = 0;
        return {
          async next() {
            if (index < buffered.length) {
              return { value: buffered[index++], done: false };
            }
            return baseIterator.next();
          },
          return() {
            return baseIterator.return ? baseIterator.return() : Promise.resolve({ value: void 0, done: true });
          },
          throw(error) {
            return baseIterator.throw ? baseIterator.throw(error) : Promise.reject(error);
          },
          [Symbol.asyncIterator]() {
            return this;
          }
        };
      }
    };
  }
});

// src/middleware/withTenant.ts
import { GraphQLError as GraphQLError2 } from "graphql";
import pino9 from "pino";
var logger9;
var init_withTenant = __esm({
  "src/middleware/withTenant.ts"() {
    "use strict";
    logger9 = pino9({ name: "withTenant" });
  }
});

// src/graphql/subscriptions.ts
import pino10 from "pino";
var logger10, pubsub, ENTITY_CREATED, ENTITY_UPDATED, ENTITY_DELETED, RELATIONSHIP_CREATED, RELATIONSHIP_UPDATED, RELATIONSHIP_DELETED, tenantEvent;
var init_subscriptions = __esm({
  "src/graphql/subscriptions.ts"() {
    "use strict";
    init_ordered_pubsub();
    init_withTenant();
    logger10 = pino10();
    pubsub = new OrderedPubSub();
    ENTITY_CREATED = "ENTITY_CREATED";
    ENTITY_UPDATED = "ENTITY_UPDATED";
    ENTITY_DELETED = "ENTITY_DELETED";
    RELATIONSHIP_CREATED = "RELATIONSHIP_CREATED";
    RELATIONSHIP_UPDATED = "RELATIONSHIP_UPDATED";
    RELATIONSHIP_DELETED = "RELATIONSHIP_DELETED";
    tenantEvent = (base, tenantId) => `${base}_${tenantId}`;
  }
});

// src/graphql/resolvers/__mocks__/entityMocks.ts
function getMockEntities(type, q, limit = 25, offset = 0) {
  let filtered = MOCK_ENTITIES;
  if (type) {
    filtered = filtered.filter((entity) => entity.type === type);
  }
  if (q) {
    const query3 = q.toLowerCase();
    filtered = filtered.filter(
      (entity) => JSON.stringify(entity.props).toLowerCase().includes(query3) || entity.type.toLowerCase().includes(query3)
    );
  }
  return filtered.slice(offset, offset + limit);
}
function getMockEntity(id) {
  return MOCK_ENTITIES.find((entity) => entity.id === id) ?? null;
}
var MOCK_ENTITIES;
var init_entityMocks = __esm({
  "src/graphql/resolvers/__mocks__/entityMocks.ts"() {
    "use strict";
    MOCK_ENTITIES = [
      {
        id: "mock-entity-1",
        type: "PERSON",
        props: {
          name: "John Smith",
          email: "john.smith@example.com",
          phone: "+1-555-0101",
          location: "New York, NY"
        },
        createdAt: "2024-08-15T12:00:00Z",
        updatedAt: "2024-08-15T12:00:00Z"
      },
      {
        id: "mock-entity-2",
        type: "ORGANIZATION",
        props: {
          name: "Tech Corp Industries",
          industry: "Technology",
          headquarters: "San Francisco, CA",
          website: "https://techcorp.example.com"
        },
        createdAt: "2024-08-15T12:00:00Z",
        updatedAt: "2024-08-15T12:00:00Z"
      },
      {
        id: "mock-entity-3",
        type: "EVENT",
        props: {
          name: "Data Breach Incident",
          date: "2024-08-01",
          severity: "HIGH",
          status: "INVESTIGATING"
        },
        createdAt: "2024-08-15T12:00:00Z",
        updatedAt: "2024-08-15T12:00:00Z"
      },
      {
        id: "mock-entity-4",
        type: "LOCATION",
        props: {
          name: "Corporate Headquarters",
          address: "100 Market Street, San Francisco, CA 94105",
          coordinates: { lat: 37.7749, lng: -122.4194 }
        },
        createdAt: "2024-08-15T12:00:00Z",
        updatedAt: "2024-08-15T12:00:00Z"
      },
      {
        id: "mock-entity-5",
        type: "ASSET",
        props: {
          name: "Database Server DB-01",
          type: "SERVER",
          ip_address: "192.168.1.100",
          status: "ACTIVE"
        },
        createdAt: "2024-08-15T12:00:00Z",
        updatedAt: "2024-08-15T12:00:00Z"
      }
    ];
  }
});

// src/db/redis.ts
var redis_exports = {};
__export(redis_exports, {
  closeRedisClient: () => closeRedisClient,
  getRedisClient: () => getRedisClient2,
  redisHealthCheck: () => redisHealthCheck
});
import Redis2 from "ioredis";
import * as dotenv5 from "dotenv";
import pino11 from "pino";
function getConfig(name) {
  const prefix = name === "default" ? "REDIS" : `REDIS_${name.toUpperCase()}`;
  const getVar = (suffix) => process.env[`${prefix}_${suffix}`] || process.env[`REDIS_${suffix}`];
  const host = getVar("HOST") || "redis";
  const port = parseInt(getVar("PORT") || "6379", 10);
  const useCluster = getVar("USE_CLUSTER") === "true";
  const clusterNodes = getVar("CLUSTER_NODES") || "";
  const tlsEnabled = getVar("TLS_ENABLED") === "true";
  let password = getVar("PASSWORD");
  if (process.env.NODE_ENV === "production" && (!password || password === "devpassword")) {
    throw new Error(
      `Security Error: REDIS_PASSWORD (for ${name}) must be set and cannot be "devpassword" in production`
    );
  }
  return {
    host,
    port,
    password: password || "devpassword",
    useCluster,
    clusterNodes,
    tlsEnabled
  };
}
function getRedisClient2(name = "default") {
  if (!clients.has(name)) {
    try {
      const config9 = getConfig(name);
      let client6;
      if (config9.useCluster) {
        if (!config9.clusterNodes) {
          throw new Error(`Redis Cluster enabled for ${name} but CLUSTER_NODES is not defined`);
        }
        const nodes = config9.clusterNodes.split(",").map((node) => {
          const [host, port] = node.split(":");
          return { host, port: parseInt(port, 10) };
        });
        logger11.info({ nodes, name }, "Initializing Redis Cluster");
        client6 = new Redis2.Cluster(nodes, {
          redisOptions: {
            password: config9.password,
            tls: config9.tlsEnabled ? {} : void 0,
            connectTimeout: 1e4,
            maxRetriesPerRequest: null
          },
          scaleReads: "slave",
          clusterRetryStrategy: (times) => {
            const delay2 = Math.min(times * 100, 3e3);
            return delay2;
          },
          enableOfflineQueue: true
        });
      } else {
        logger11.info({ host: config9.host, port: config9.port, name }, "Initializing Redis Client");
        client6 = new Redis2({
          host: config9.host,
          port: config9.port,
          password: config9.password,
          tls: config9.tlsEnabled ? {} : void 0,
          connectTimeout: 1e4,
          lazyConnect: true,
          retryStrategy: (times) => {
            const delay2 = Math.min(times * 50, 2e3);
            return delay2;
          },
          maxRetriesPerRequest: null
        });
      }
      client6.on("connect", () => logger11.info(`Redis client '${name}' connected.`));
      client6.on("error", (err) => {
        logger11.warn(
          `Redis connection '${name}' failed - using mock responses. Error: ${err.message}`
        );
        clients.set(name, createMockRedisClient(name));
      });
      const originalGet = client6.get.bind(client6);
      client6.get = async (key) => {
        const value = await originalGet(key);
        if (value) {
          telemetry.subsystems.cache.hits.add(1);
        } else {
          telemetry.subsystems.cache.misses.add(1);
        }
        return value;
      };
      const originalSet = client6.set.bind(client6);
      client6.set = async (key, value) => {
        telemetry.subsystems.cache.sets.add(1);
        return await originalSet(key, value);
      };
      const originalDel = client6.del.bind(client6);
      client6.del = (async (...keys) => {
        telemetry.subsystems.cache.dels.add(1);
        return await originalDel(...keys);
      });
      clients.set(name, client6);
    } catch (error) {
      logger11.warn(
        `Redis initialization for '${name}' failed - using development mode. Error: ${error.message}`
      );
      clients.set(name, createMockRedisClient(name));
    }
  }
  return clients.get(name);
}
function createMockRedisClient(name) {
  return {
    get: async (key) => {
      logger11.debug(`Mock Redis (${name}) GET: Key: ${key}`);
      return null;
    },
    set: async (key, value, ...args) => {
      logger11.debug(`Mock Redis (${name}) SET: Key: ${key}, Value: ${value}`);
      return "OK";
    },
    del: async (...keys) => {
      logger11.debug(`Mock Redis (${name}) DEL: Keys: ${keys.join(", ")}`);
      return keys.length;
    },
    exists: async (...keys) => {
      logger11.debug(`Mock Redis (${name}) EXISTS: Keys: ${keys.join(", ")}`);
      return 0;
    },
    expire: async (key, seconds) => {
      logger11.debug(`Mock Redis (${name}) EXPIRE: Key: ${key}, Seconds: ${seconds}`);
      return 1;
    },
    quit: async () => {
    },
    on: () => {
    },
    connect: async () => {
    },
    options: { keyPrefix: "summit:" },
    duplicate: () => createMockRedisClient(name)
  };
}
async function redisHealthCheck() {
  const defaultClient2 = clients.get("default");
  if (!defaultClient2) return false;
  try {
    await defaultClient2.ping();
    return true;
  } catch {
    return false;
  }
}
async function closeRedisClient() {
  for (const [name, client6] of clients.entries()) {
    if (client6) {
      await client6.quit();
      logger11.info(`Redis client '${name}' closed.`);
    }
  }
  clients.clear();
}
var logger11, clients;
var init_redis = __esm({
  "src/db/redis.ts"() {
    "use strict";
    init_comprehensive_telemetry();
    dotenv5.config();
    logger11 = pino11();
    clients = /* @__PURE__ */ new Map();
  }
});

// src/cache/AdvancedCachingStrategy.ts
import crypto3 from "crypto";
import { EventEmitter } from "events";
var L1Cache, CacheManager;
var init_AdvancedCachingStrategy = __esm({
  "src/cache/AdvancedCachingStrategy.ts"() {
    "use strict";
    L1Cache = class {
      cache = /* @__PURE__ */ new Map();
      accessOrder = [];
      // For LRU
      maxEntries;
      evictionPolicy;
      constructor(maxEntries, evictionPolicy = "lru") {
        this.maxEntries = maxEntries;
        this.evictionPolicy = evictionPolicy;
      }
      get(key) {
        const entry = this.cache.get(key);
        if (!entry) return void 0;
        if (entry.expiresAt < Date.now()) {
          this.delete(key);
          return void 0;
        }
        entry.accessCount++;
        entry.lastAccessedAt = Date.now();
        if (this.evictionPolicy === "lru") {
          const index = this.accessOrder.indexOf(key);
          if (index > -1) {
            this.accessOrder.splice(index, 1);
          }
          this.accessOrder.push(key);
        }
        return entry;
      }
      set(key, entry) {
        while (this.cache.size >= this.maxEntries) {
          this.evict();
        }
        this.cache.set(key, entry);
        this.accessOrder.push(key);
      }
      delete(key) {
        const index = this.accessOrder.indexOf(key);
        if (index > -1) {
          this.accessOrder.splice(index, 1);
        }
        return this.cache.delete(key);
      }
      clear() {
        this.cache.clear();
        this.accessOrder = [];
      }
      size() {
        return this.cache.size;
      }
      keys() {
        return Array.from(this.cache.keys());
      }
      evict() {
        let keyToEvict;
        switch (this.evictionPolicy) {
          case "lru":
            keyToEvict = this.accessOrder.shift();
            break;
          case "lfu":
            keyToEvict = this.findLFUKey();
            break;
          case "fifo":
            keyToEvict = this.accessOrder.shift();
            break;
        }
        if (keyToEvict) {
          this.delete(keyToEvict);
        }
      }
      findLFUKey() {
        let minAccess = Infinity;
        let lfuKey;
        for (const [key, entry] of this.cache) {
          if (entry.accessCount < minAccess) {
            minAccess = entry.accessCount;
            lfuKey = key;
          }
        }
        return lfuKey;
      }
    };
    CacheManager = class extends EventEmitter {
      l1Cache;
      redisClient;
      config;
      metrics;
      circuitOpen = false;
      circuitOpenUntil = 0;
      consecutiveFailures = 0;
      circuitThreshold = 5;
      circuitCooldown = 3e4;
      pendingRevalidations = /* @__PURE__ */ new Set();
      constructor(redisClient4, config9 = {}) {
        super();
        this.redisClient = redisClient4;
        this.config = {
          defaultTtl: 300,
          // 5 minutes
          maxL1Entries: 1e4,
          l1EvictionPolicy: "lru",
          enableCompression: true,
          compressionThreshold: 1024,
          enableMetrics: true,
          keyPrefix: "ig:cache:",
          defaultStaleWhileRevalidate: 60,
          ...config9
        };
        this.l1Cache = new L1Cache(this.config.maxL1Entries, this.config.l1EvictionPolicy);
        this.metrics = this.initMetrics();
        this.setupCacheInvalidationSubscription();
      }
      initMetrics() {
        return {
          l1Hits: 0,
          l1Misses: 0,
          l2Hits: 0,
          l2Misses: 0,
          totalGets: 0,
          totalSets: 0,
          totalDeletes: 0,
          totalInvalidations: 0,
          evictions: 0,
          errors: 0,
          avgLatencyMs: 0,
          l1Size: 0,
          l2Size: 0
        };
      }
      /**
       * Generate cache key with prefix
       */
      generateKey(key) {
        return `${this.config.keyPrefix}${key}`;
      }
      /**
       * Generate cache key from object
       */
      generateKeyFromObject(obj) {
        const sorted = JSON.stringify(obj, Object.keys(obj).sort());
        const hash3 = crypto3.createHash("sha256").update(sorted).digest("hex").slice(0, 16);
        return hash3;
      }
      /**
       * Check if circuit breaker is open
       */
      isCircuitOpen() {
        if (this.circuitOpen && Date.now() >= this.circuitOpenUntil) {
          this.circuitOpen = false;
          this.consecutiveFailures = 0;
        }
        return this.circuitOpen;
      }
      /**
       * Record failure for circuit breaker
       */
      recordFailure() {
        this.consecutiveFailures++;
        this.metrics.errors++;
        if (this.consecutiveFailures >= this.circuitThreshold) {
          this.circuitOpen = true;
          this.circuitOpenUntil = Date.now() + this.circuitCooldown;
          this.emit("circuit:open");
        }
      }
      /**
       * Record success for circuit breaker
       */
      recordSuccess() {
        this.consecutiveFailures = 0;
      }
      /**
       * Get value from cache (cache-aside pattern)
       */
      async get(key, options2 = {}) {
        const startTime = Date.now();
        const fullKey = this.generateKey(key);
        this.metrics.totalGets++;
        try {
          if (!options2.skipL1 && !options2.forceRefresh) {
            const l1Entry = this.l1Cache.get(fullKey);
            if (l1Entry) {
              this.metrics.l1Hits++;
              this.updateLatency(startTime);
              return l1Entry.value;
            }
            this.metrics.l1Misses++;
          }
          if (!options2.skipL2 && !options2.forceRefresh && this.redisClient && !this.isCircuitOpen()) {
            try {
              const serialized = await this.redisClient.get(fullKey);
              if (serialized) {
                const entry = JSON.parse(serialized);
                const now = Date.now();
                if (entry.expiresAt < now) {
                  const staleWindow = (options2.staleWhileRevalidate ?? this.config.defaultStaleWhileRevalidate) * 1e3;
                  if (entry.expiresAt + staleWindow >= now) {
                    this.revalidateInBackground(key, options2);
                  } else {
                    await this.delete(key);
                    this.metrics.l2Misses++;
                    this.updateLatency(startTime);
                    return null;
                  }
                }
                this.metrics.l2Hits++;
                this.recordSuccess();
                if (!options2.skipL1) {
                  this.l1Cache.set(fullKey, entry);
                }
                this.updateLatency(startTime);
                return entry.value;
              }
              this.metrics.l2Misses++;
            } catch (error) {
              this.recordFailure();
              this.emit("error", { operation: "get", key, error });
            }
          }
          this.updateLatency(startTime);
          return null;
        } catch (error) {
          this.metrics.errors++;
          this.updateLatency(startTime);
          throw error;
        }
      }
      /**
       * Set value in cache (write-through pattern)
       */
      async set(key, value, options2 = {}) {
        const startTime = Date.now();
        const fullKey = this.generateKey(key);
        const ttl = options2.ttl ?? this.config.defaultTtl;
        const entry = {
          value,
          createdAt: Date.now(),
          expiresAt: Date.now() + ttl * 1e3,
          accessCount: 0,
          lastAccessedAt: Date.now(),
          tags: options2.tags ?? [],
          version: 1
        };
        this.metrics.totalSets++;
        try {
          if (!options2.skipL1) {
            this.l1Cache.set(fullKey, entry);
          }
          if (!options2.skipL2 && this.redisClient && !this.isCircuitOpen()) {
            try {
              const serialized = JSON.stringify(entry);
              await this.redisClient.setex(fullKey, ttl, serialized);
              if (options2.tags && options2.tags.length > 0) {
                await this.addToTagSets(fullKey, options2.tags, ttl);
              }
              this.recordSuccess();
            } catch (error) {
              this.recordFailure();
              this.emit("error", { operation: "set", key, error });
            }
          }
          this.updateLatency(startTime);
        } catch (error) {
          this.metrics.errors++;
          this.updateLatency(startTime);
          throw error;
        }
      }
      /**
       * Get or set with factory function (cache-aside with lazy loading)
       */
      async getOrSet(key, factory2, options2 = {}) {
        const cached = await this.get(key, options2);
        if (cached !== null) {
          return cached;
        }
        const value = await factory2();
        await this.set(key, value, options2);
        return value;
      }
      /**
       * Delete from cache
       */
      async delete(key) {
        const fullKey = this.generateKey(key);
        this.metrics.totalDeletes++;
        this.l1Cache.delete(fullKey);
        if (this.redisClient && !this.isCircuitOpen()) {
          try {
            await this.redisClient.del(fullKey);
            this.recordSuccess();
            await this.publishInvalidation([key]);
          } catch (error) {
            this.recordFailure();
            this.emit("error", { operation: "delete", key, error });
          }
        }
        return true;
      }
      /**
       * Delete multiple keys
       */
      async deleteMany(keys) {
        const fullKeys = keys.map((k) => this.generateKey(k));
        let deleted = 0;
        for (const key of fullKeys) {
          if (this.l1Cache.delete(key)) {
            deleted++;
          }
        }
        if (this.redisClient && !this.isCircuitOpen() && fullKeys.length > 0) {
          try {
            await this.redisClient.del(...fullKeys);
            this.recordSuccess();
            await this.publishInvalidation(keys);
          } catch (error) {
            this.recordFailure();
            this.emit("error", { operation: "deleteMany", keys, error });
          }
        }
        this.metrics.totalDeletes += keys.length;
        return deleted;
      }
      /**
       * Invalidate by tag
       */
      async invalidateByTag(tag) {
        const tagSetKey = `${this.config.keyPrefix}tag:${tag}`;
        let invalidated = 0;
        this.metrics.totalInvalidations++;
        if (this.redisClient && !this.isCircuitOpen()) {
          try {
            const keys = await this.redisClient.keys(`${tagSetKey}:*`);
            if (keys.length > 0) {
              const cacheKeys = keys.map((k) => k.replace(`${tagSetKey}:`, ""));
              await this.redisClient.del(...keys);
              await this.deleteMany(cacheKeys);
              invalidated = cacheKeys.length;
            }
            this.recordSuccess();
          } catch (error) {
            this.recordFailure();
            this.emit("error", { operation: "invalidateByTag", tag, error });
          }
        }
        const l1Keys = this.l1Cache.keys();
        for (const key of l1Keys) {
          const entry = this.l1Cache.get(key);
          if (entry && entry.tags.includes(tag)) {
            this.l1Cache.delete(key);
            invalidated++;
          }
        }
        return invalidated;
      }
      /**
       * Invalidate by pattern
       */
      async invalidateByPattern(pattern2) {
        const fullPattern = `${this.config.keyPrefix}${pattern2}`;
        let invalidated = 0;
        if (this.redisClient && !this.isCircuitOpen()) {
          try {
            let cursor = "0";
            const keysToDelete = [];
            do {
              const [nextCursor, keys] = await this.redisClient.scan(
                cursor,
                "MATCH",
                fullPattern,
                "COUNT",
                "100"
              );
              cursor = nextCursor;
              keysToDelete.push(...keys);
            } while (cursor !== "0");
            if (keysToDelete.length > 0) {
              await this.redisClient.del(...keysToDelete);
              invalidated = keysToDelete.length;
            }
            this.recordSuccess();
          } catch (error) {
            this.recordFailure();
            this.emit("error", { operation: "invalidateByPattern", pattern: pattern2, error });
          }
        }
        const regex = new RegExp(pattern2.replace(/\*/g, ".*"));
        const l1Keys = this.l1Cache.keys();
        for (const key of l1Keys) {
          if (regex.test(key)) {
            this.l1Cache.delete(key);
            invalidated++;
          }
        }
        this.metrics.totalInvalidations++;
        return invalidated;
      }
      /**
       * Clear all caches
       */
      async clear() {
        this.l1Cache.clear();
        if (this.redisClient && !this.isCircuitOpen()) {
          try {
            let cursor = "0";
            do {
              const [nextCursor, keys] = await this.redisClient.scan(
                cursor,
                "MATCH",
                `${this.config.keyPrefix}*`,
                "COUNT",
                "100"
              );
              cursor = nextCursor;
              if (keys.length > 0) {
                await this.redisClient.del(...keys);
              }
            } while (cursor !== "0");
            this.recordSuccess();
          } catch (error) {
            this.recordFailure();
            this.emit("error", { operation: "clear", error });
          }
        }
        this.metrics = this.initMetrics();
      }
      /**
       * Get cache metrics
       */
      getMetrics() {
        return {
          ...this.metrics,
          l1Size: this.l1Cache.size()
        };
      }
      /**
       * Warm cache with data
       */
      async warm(entries) {
        const pipeline2 = this.redisClient?.pipeline?.();
        for (const { key, value, options: options2 } of entries) {
          const fullKey = this.generateKey(key);
          const ttl = options2?.ttl ?? this.config.defaultTtl;
          const entry = {
            value,
            createdAt: Date.now(),
            expiresAt: Date.now() + ttl * 1e3,
            accessCount: 0,
            lastAccessedAt: Date.now(),
            tags: options2?.tags ?? [],
            version: 1
          };
          this.l1Cache.set(fullKey, entry);
          if (pipeline2) {
            pipeline2.setex(fullKey, ttl, JSON.stringify(entry));
          }
        }
        if (pipeline2 && !this.isCircuitOpen()) {
          try {
            await pipeline2.exec();
            this.recordSuccess();
          } catch (error) {
            this.recordFailure();
            this.emit("error", { operation: "warm", error });
          }
        }
        this.emit("cache:warmed", { count: entries.length });
      }
      /**
       * Add key to tag sets for invalidation tracking
       */
      async addToTagSets(key, tags, ttl) {
        if (!this.redisClient) return;
        const pipeline2 = this.redisClient.pipeline?.();
        if (!pipeline2) return;
        for (const tag of tags) {
          const tagSetKey = `${this.config.keyPrefix}tag:${tag}:${key}`;
          pipeline2.setex(tagSetKey, ttl, "1");
        }
        await pipeline2.exec();
      }
      /**
       * Publish invalidation event to other instances
       */
      async publishInvalidation(keys) {
        if (!this.redisClient) return;
        try {
          await this.redisClient.publish(
            `${this.config.keyPrefix}invalidation`,
            JSON.stringify({ keys, timestamp: Date.now() })
          );
        } catch (error) {
          this.emit("error", { operation: "publishInvalidation", error });
        }
      }
      /**
       * Setup subscription for cache invalidation from other instances
       */
      setupCacheInvalidationSubscription() {
        if (!this.redisClient) return;
        this.redisClient.on("message", (channel, message) => {
          if (channel === `${this.config.keyPrefix}invalidation`) {
            try {
              const { keys } = JSON.parse(message);
              for (const key of keys) {
                this.l1Cache.delete(this.generateKey(key));
              }
              this.emit("cache:invalidated", { keys });
            } catch (error) {
              this.emit("error", { operation: "invalidationSubscription", error });
            }
          }
        });
      }
      /**
       * Revalidate cache entry in background
       */
      async revalidateInBackground(key, options2) {
        if (this.pendingRevalidations.has(key)) {
          return;
        }
        this.pendingRevalidations.add(key);
        this.emit("cache:revalidating", { key });
        setTimeout(() => {
          this.pendingRevalidations.delete(key);
        }, 5e3);
      }
      /**
       * Update average latency metric
       */
      updateLatency(startTime) {
        const latency = Date.now() - startTime;
        const totalOps = this.metrics.l1Hits + this.metrics.l1Misses + this.metrics.l2Hits + this.metrics.l2Misses;
        this.metrics.avgLatencyMs = (this.metrics.avgLatencyMs * (totalOps - 1) + latency) / totalOps;
      }
      /**
       * Shutdown cache manager
       */
      async shutdown() {
        this.l1Cache.clear();
        if (this.redisClient) {
          await this.redisClient.quit();
        }
        this.emit("cache:shutdown");
      }
    };
  }
});

// src/types/data-envelope.ts
import { createHash as createHash3 } from "crypto";
function createDataEnvelope(data, options2) {
  if (!options2.governanceVerdict) {
    throw new Error("GA ENFORCEMENT: GovernanceVerdict is required (SOC 2 CC6.1, CC7.2)");
  }
  const provenanceId = generateProvenanceId();
  const generatedAt = /* @__PURE__ */ new Date();
  const dataString = JSON.stringify(data);
  const dataHash = createHash3("sha256").update(dataString).digest("hex");
  if (options2.confidence !== void 0) {
    if (options2.confidence < 0 || options2.confidence > 1) {
      throw new Error("Confidence score must be between 0 and 1");
    }
  }
  const provenance = {
    source: options2.source,
    generatedAt,
    lineage: options2.lineage || [],
    actor: options2.actor,
    version: options2.version,
    provenanceId
  };
  return {
    data,
    provenance,
    confidence: options2.confidence,
    isSimulated: options2.isSimulated || false,
    governanceVerdict: options2.governanceVerdict,
    classification: options2.classification || "INTERNAL" /* INTERNAL */,
    dataHash,
    warnings: options2.warnings || []
  };
}
function generateProvenanceId() {
  const timestamp = Date.now();
  const random = Math.random().toString(36).substring(2, 15);
  return `prov-${timestamp}-${random}`;
}
var init_data_envelope = __esm({
  "src/types/data-envelope.ts"() {
    "use strict";
  }
});

// src/cache/DistributedCacheService.ts
import { v4 as uuidv42 } from "uuid";
import { LRUCache } from "lru-cache";
var init_DistributedCacheService = __esm({
  "src/cache/DistributedCacheService.ts"() {
    "use strict";
    init_data_envelope();
    init_logger2();
  }
});

// src/cache/factory.ts
function getCacheManager() {
  if (!cacheManagerInstance) {
    const redisClient4 = getRedisClient2("cache");
    cacheManagerInstance = new CacheManager(redisClient4, {
      keyPrefix: "summit:cache:",
      defaultTtl: 600,
      // 10 minutes
      enableMetrics: true
    });
  }
  return cacheManagerInstance;
}
var cacheManagerInstance;
var init_factory = __esm({
  "src/cache/factory.ts"() {
    "use strict";
    init_redis();
    init_AdvancedCachingStrategy();
    init_DistributedCacheService();
    init_logger();
    cacheManagerInstance = null;
  }
});

// src/utils/metrics.ts
var PrometheusMetrics;
var init_metrics3 = __esm({
  "src/utils/metrics.ts"() {
    "use strict";
    PrometheusMetrics = class {
      namespace;
      gauges = /* @__PURE__ */ new Map();
      counters = /* @__PURE__ */ new Map();
      histograms = /* @__PURE__ */ new Map();
      gaugeDefinitions = /* @__PURE__ */ new Map();
      counterDefinitions = /* @__PURE__ */ new Map();
      histogramDefinitions = /* @__PURE__ */ new Map();
      /**
       * Initializes a new instance of PrometheusMetrics.
       *
       * @param namespace - A namespace prefix to apply to all metric keys.
       */
      constructor(namespace) {
        this.namespace = namespace;
      }
      /**
       * Defines a new gauge metric.
       *
       * @param name - The name of the gauge.
       * @param help - A help string describing the metric.
       * @param labelNames - Optional list of label names that will be used with this metric.
       */
      createGauge(name, help, labelNames = []) {
        this.gaugeDefinitions.set(name, { name, help, labelNames });
      }
      /**
       * Defines a new counter metric.
       *
       * @param name - The name of the counter.
       * @param help - A help string describing the metric.
       * @param labelNames - Optional list of label names that will be used with this metric.
       */
      createCounter(name, help, labelNames = []) {
        this.counterDefinitions.set(name, { name, help, labelNames });
      }
      /**
       * Defines a new histogram metric.
       *
       * @param name - The name of the histogram.
       * @param help - A help string describing the metric.
       * @param options - Configuration options, such as custom buckets.
       */
      createHistogram(name, help, options2 = {}) {
        this.histogramDefinitions.set(name, {
          name,
          help,
          buckets: options2.buckets
        });
      }
      /**
       * Sets the value of a gauge.
       *
       * @param name - The name of the gauge to update.
       * @param value - The new value.
       * @param labels - Optional labels to identify the specific metric series.
       */
      setGauge(name, value, labels2 = {}) {
        const key = this.metricKey(name, labels2);
        this.gauges.set(key, value);
      }
      /**
       * Increments a counter.
       *
       * @param name - The name of the counter to increment.
       * @param labels - Optional labels to identify the specific metric series.
       * @param value - The amount to increment by (default: 1).
       */
      incrementCounter(name, labels2 = {}, value = 1) {
        const key = this.metricKey(name, labels2);
        const current = this.counters.get(key) ?? 0;
        this.counters.set(key, current + value);
      }
      /**
       * Observes a value in a histogram.
       *
       * @param name - The name of the histogram.
       * @param value - The value to observe.
       * @param labels - Optional labels to identify the specific metric series.
       */
      observeHistogram(name, value, labels2 = {}) {
        const key = this.metricKey(name, labels2);
        const values = this.histograms.get(key) ?? [];
        values.push(value);
        if (values.length > 1e3) {
          values.shift();
        }
        this.histograms.set(key, values);
      }
      metricKey(name, labels2) {
        const keys = Object.keys(labels2);
        if (keys.length === 0) {
          return `${this.namespace}:${name}`;
        }
        keys.sort();
        let labelString = "";
        for (let i = 0; i < keys.length; i++) {
          const key = keys[i];
          if (i > 0) labelString += "|";
          labelString += `${key}=${labels2[key]}`;
        }
        return `${this.namespace}:${name}:{${labelString}}`;
      }
    };
  }
});

// src/services/CacheService.ts
var CacheService, cacheService;
var init_CacheService = __esm({
  "src/services/CacheService.ts"() {
    "use strict";
    init_factory();
    init_metrics3();
    init_config();
    init_logger();
    CacheService = class {
      metrics;
      namespace = "cache";
      defaultTtl;
      enabled;
      cacheManager;
      /**
       * @constructor
       * @description Initializes the CacheService, setting up metrics and configuration.
       */
      constructor() {
        this.metrics = new PrometheusMetrics("cache_service");
        this.metrics.createCounter("ops_total", "Total cache operations", ["operation", "status"]);
        this.defaultTtl = cfg.CACHE_TTL_DEFAULT;
        this.enabled = cfg.CACHE_ENABLED;
        this.cacheManager = getCacheManager();
      }
      /**
       * @private
       * @method getKey
       * @description Prepends the namespace to the cache key.
       * @param {string} key - The original cache key.
       * @returns {string} The namespaced cache key.
       */
      getKey(key) {
        return `${this.namespace}:${key}`;
      }
      /**
       * @method get
       * @description Retrieves a value from the cache.
       * @template T
       * @param {string} key - The key of the item to retrieve.
       * @returns {Promise<T | null>} The cached value, or null if it doesn't exist or an error occurs.
       *
       * @example
       * ```typescript
       * const user = await cacheService.get<User>('user:123');
       * ```
       */
      async get(key) {
        if (!this.enabled) return null;
        try {
          const data = await this.cacheManager.get(this.getKey(key));
          if (data) {
            this.metrics.incrementCounter("ops_total", { operation: "get", status: "hit" });
            return data;
          }
          this.metrics.incrementCounter("ops_total", { operation: "get", status: "miss" });
          return null;
        } catch (error) {
          logger_default.error({ err: error, key }, "Cache get error");
          this.metrics.incrementCounter("ops_total", { operation: "get", status: "error" });
          return null;
        }
      }
      /**
       * @method set
       * @description Sets a value in the cache with an optional TTL.
       * @param {string} key - The key for the cache entry.
       * @param {*} value - The value to cache.
       * @param {number} [ttl] - The time-to-live for the cache entry in seconds. Defaults to the system's default TTL.
       * @returns {Promise<void>}
       *
       * @example
       * ```typescript
       * await cacheService.set('user:123', { name: 'John Doe' }, 3600);
       * ```
       */
      async set(key, value, ttl) {
        if (!this.enabled) return;
        try {
          const expiry = ttl || this.defaultTtl;
          await this.cacheManager.set(this.getKey(key), value, { ttl: expiry });
          this.metrics.incrementCounter("ops_total", { operation: "set", status: "success" });
        } catch (error) {
          logger_default.error({ err: error, key }, "Cache set error");
          this.metrics.incrementCounter("ops_total", { operation: "set", status: "error" });
        }
      }
      /**
       * @method del
       * @description Deletes a value from the cache.
       * @param {string} key - The key of the item to delete.
       * @returns {Promise<void>}
       *
       * @example
       * ```typescript
       * await cacheService.del('user:123');
       * ```
       */
      async del(key) {
        if (!this.enabled) return;
        try {
          await this.cacheManager.delete(this.getKey(key));
          this.metrics.incrementCounter("ops_total", { operation: "del", status: "success" });
        } catch (error) {
          logger_default.error({ err: error, key }, "Cache del error");
        }
      }
      /**
       * @method invalidatePattern
       * @description Invalidates cache keys matching a given pattern.
       * @param {string} pattern - The pattern to match keys against (e.g., 'users:*').
       * @returns {Promise<void>}
       *
       * @example
       * ```typescript
       * await cacheService.invalidatePattern('user:*');
       * ```
       */
      async invalidatePattern(pattern2) {
        if (!this.enabled) return;
        try {
          const fullPattern = this.getKey(pattern2);
          await this.cacheManager.invalidateByPattern(fullPattern);
          logger_default.info({ pattern: fullPattern }, "Cache pattern invalidated");
        } catch (error) {
          logger_default.error({ err: error, pattern: pattern2 }, "Cache pattern invalidation error");
        }
      }
      /**
       * @method getOrSet
       * @description A helper function that attempts to get a value from the cache and,
       * if it's not present, calls a factory function to generate the value,
       * sets it in the cache, and then returns it.
       * @template T
       * @param {string} key - The cache key.
       * @param {() => Promise<T>} factory - A function that returns a promise resolving to the value to be cached.
       * @param {number} [ttl] - Optional TTL in seconds for the new cache entry.
       * @returns {Promise<T>} The cached or newly generated value.
       *
       * @example
       * ```typescript
       * const user = await cacheService.getOrSet('user:123', () => findUserInDb('123'), 3600);
       * ```
       */
      async getOrSet(key, factory2, ttl) {
        return this.cacheManager.getOrSet(
          this.getKey(key),
          factory2,
          { ttl: ttl || this.defaultTtl }
        );
      }
    };
    cacheService = new CacheService();
  }
});

// src/utils/cacheHelper.ts
var withCache, listCacheKey;
var init_cacheHelper = __esm({
  "src/utils/cacheHelper.ts"() {
    "use strict";
    init_CacheService();
    withCache = (keyGenerator, resolver, ttl) => {
      return async (parent, args, context4, info) => {
        const key = keyGenerator(parent, args, context4, info);
        return cacheService.getOrSet(
          key,
          () => resolver(parent, args, context4, info),
          ttl
        );
      };
    };
    listCacheKey = (prefix, params) => {
      const sortedParams = Object.keys(params).sort().reduce((acc, key) => {
        acc[key] = params[key];
        return acc;
      }, {});
      return `${prefix}:${JSON.stringify(sortedParams)}`;
    };
  }
});

// src/graphql/utils/auth.ts
import { GraphQLError as GraphQLError3 } from "graphql";
var authGuard;
var init_auth2 = __esm({
  "src/graphql/utils/auth.ts"() {
    "use strict";
    authGuard = (resolver, requiredPermission) => {
      return async (parent, args, context4, info) => {
        if (!context4.user) {
          throw new GraphQLError3("Not authenticated", {
            extensions: { code: "UNAUTHENTICATED" }
          });
        }
        if (!context4.user.tenantId) {
          throw new GraphQLError3("Tenant context missing", {
            extensions: { code: "FORBIDDEN" }
          });
        }
        if (requiredPermission) {
          const permissions = context4.user.permissions || [];
          if (!permissions.includes(requiredPermission)) {
            if (!permissions.includes("*") && !context4.user.isSuperAdmin) {
              throw new GraphQLError3(`Missing permission: ${requiredPermission}`, {
                extensions: { code: "FORBIDDEN" }
              });
            }
          }
        }
        return resolver(parent, args, context4, info);
      };
    };
  }
});

// src/cache/redis.ts
import Redis3, { Cluster as Cluster2 } from "ioredis";
var RedisService;
var init_redis2 = __esm({
  "src/cache/redis.ts"() {
    "use strict";
    init_config3();
    init_logger();
    RedisService = class _RedisService {
      client;
      subscriber;
      static instance;
      static getInstance() {
        if (!_RedisService.instance) {
          _RedisService.instance = new _RedisService();
        }
        return _RedisService.instance;
      }
      constructor() {
        const redisConfig = config_default.redis || {};
        const useCluster = redisConfig.useCluster;
        const password = redisConfig.password;
        const tls = redisConfig.tls ? {} : void 0;
        const commonOptions = {
          password,
          connectTimeout: 1e4,
          // 10s
          maxRetriesPerRequest: 3,
          retryStrategy: (times) => {
            const delay2 = Math.min(times * 50, 2e3);
            return delay2;
          },
          tls,
          lazyConnect: true
          // Don't connect immediately in constructor
        };
        if (useCluster) {
          const nodes = (redisConfig.clusterNodes || []).map((n) => ({
            host: n.host,
            port: n.port
          }));
          if (nodes.length === 0) {
            nodes.push({
              host: redisConfig.host || "localhost",
              port: redisConfig.port || 6379
            });
          }
          const clusterOptions = {
            redisOptions: commonOptions,
            dnsLookup: (address, callback) => callback(null, address),
            scaleReads: "slave"
            // Read from slaves if possible
          };
          logger.info({ nodes }, "Initializing Redis Cluster");
          this.client = new Cluster2(nodes, clusterOptions);
          this.subscriber = new Cluster2(nodes, clusterOptions);
        } else {
          const host = redisConfig.host || "localhost";
          const port = redisConfig.port || 6379;
          const db2 = redisConfig.db || 0;
          logger.info({ host, port, db: db2 }, "Initializing Redis Standalone");
          this.client = new Redis3({
            ...commonOptions,
            host,
            port,
            db: db2
          });
          this.subscriber = new Redis3({
            ...commonOptions,
            host,
            port,
            db: db2
          });
        }
        this.handleErrors(this.client, "Client");
        this.handleErrors(this.subscriber, "Subscriber");
      }
      handleErrors(client6, name) {
        client6.on("error", (err) => {
          logger.error({ err, client: name }, "Redis connection error");
        });
        client6.on("connect", () => {
          logger.info({ client: name }, "Redis connected");
        });
      }
      getClient() {
        return this.client;
      }
      async publish(channel, message) {
        return this.client.publish(channel, message);
      }
      async hgetall(key) {
        return this.client.hgetall(key);
      }
      async hincrby(key, field, increment) {
        return this.client.hincrby(key, field, increment);
      }
      async hdel(key, field) {
        return this.client.hdel(key, field);
      }
      async get(key) {
        return this.client.get(key);
      }
      async setex(key, seconds, value) {
        return this.client.setex(key, seconds, value);
      }
      async ping() {
        return this.client.ping();
      }
      async close() {
        await Promise.all([this.client.quit(), this.subscriber.quit()]);
      }
      async del(key) {
        return this.client.del(key);
      }
      async set(key, value, ttlSeconds) {
        if (ttlSeconds !== void 0) {
          return this.client.set(key, value, "EX", ttlSeconds);
        }
        return this.client.set(key, value);
      }
      async getKeysByPattern(pattern2) {
        return this.client.keys(pattern2);
      }
    };
  }
});

// src/utils/lruCache.ts
var LRUCache2, lruCache_default;
var init_lruCache = __esm({
  "src/utils/lruCache.ts"() {
    "use strict";
    LRUCache2 = class {
      cache;
      capacity;
      /**
       * Creates an instance of LRUCache.
       *
       * @param capacity - The maximum number of items the cache can hold.
       */
      constructor(capacity) {
        this.cache = /* @__PURE__ */ new Map();
        this.capacity = capacity;
      }
      /**
       * Retrieves an item from the cache.
       *
       * Accessing an item moves it to the most recently used position.
       *
       * @param key - The key of the item to retrieve.
       * @returns The value associated with the key, or undefined if not found.
       */
      get(key) {
        if (!this.cache.has(key)) {
          return void 0;
        }
        const value = this.cache.get(key);
        this.cache.delete(key);
        this.cache.set(key, value);
        return value;
      }
      /**
       * Adds or updates an item in the cache.
       *
       * If the key already exists, its value is updated and it becomes the most recently used.
       * If the key does not exist and the cache is full, the least recently used item is evicted.
       *
       * @param key - The key of the item to add.
       * @param value - The value to associate with the key.
       */
      put(key, value) {
        if (this.cache.has(key)) {
          this.cache.delete(key);
        } else if (this.cache.size >= this.capacity) {
          const firstKey = this.cache.keys().next().value;
          if (firstKey !== void 0) {
            this.cache.delete(firstKey);
          }
        }
        this.cache.set(key, value);
      }
      /**
       * Returns the current number of items in the cache.
       *
       * @returns The size of the cache.
       */
      size() {
        return this.cache.size;
      }
      /**
       * Clears all items from the cache.
       */
      clear() {
        this.cache.clear();
      }
    };
    lruCache_default = LRUCache2;
  }
});

// src/db/query-scope.ts
function validateAndScopeQuery(query3, params, tenantId) {
  let analysis = queryAnalysisCache.get(query3);
  if (!analysis) {
    const lowerQuery = query3.toLowerCase().trim();
    const tenantScopedTables = [
      "coherence_scores",
      "audit_logs",
      "user_sessions",
      "api_keys"
    ];
    const affectedTable2 = tenantScopedTables.find(
      (table) => lowerQuery.includes(table)
    );
    const isAlreadyScoped2 = !!(lowerQuery.includes("tenant_id") && lowerQuery.includes("$"));
    const hasWhereClause2 = lowerQuery.includes("where");
    let operationType2;
    if (lowerQuery.startsWith("select")) operationType2 = "select";
    else if (lowerQuery.startsWith("insert")) operationType2 = "insert";
    else if (lowerQuery.startsWith("update")) operationType2 = "update";
    else if (lowerQuery.startsWith("delete")) operationType2 = "delete";
    analysis = {
      affectedTable: affectedTable2,
      isAlreadyScoped: isAlreadyScoped2,
      operationType: operationType2,
      hasWhereClause: hasWhereClause2
    };
    queryAnalysisCache.put(query3, analysis);
  }
  const { affectedTable, isAlreadyScoped, operationType, hasWhereClause } = analysis;
  if (!affectedTable) {
    return { query: query3, params, wasScoped: false };
  }
  if (!tenantId) {
    throw new Error(`Tenant ID required for queries on ${affectedTable}`);
  }
  if (isAlreadyScoped) {
    return { query: query3, params, wasScoped: true };
  }
  if (operationType === "select") {
    return applyScope(query3, params, tenantId, hasWhereClause, "AND");
  } else if (operationType === "insert") {
    console.warn(
      `INSERT query tenant scoping needs manual verification: ${query3}`
    );
    return { query: query3, params, wasScoped: false };
  } else if (operationType === "update") {
    return applyScope(query3, params, tenantId, hasWhereClause, "AND");
  } else if (operationType === "delete") {
    return applyScope(query3, params, tenantId, hasWhereClause, "AND");
  }
  console.warn(
    `Unable to auto-scope query for table ${affectedTable}: ${query3}`
  );
  return { query: query3, params, wasScoped: false };
}
function applyScope(query3, params, tenantId, hasWhereClause, connector) {
  if (hasWhereClause) {
    const scopedQuery = query3 + ` AND tenant_id = $${params.length + 1}`;
    return {
      query: scopedQuery,
      params: [...params, tenantId],
      wasScoped: true
    };
  } else {
    const scopedQuery = query3 + ` WHERE tenant_id = $${params.length + 1}`;
    return {
      query: scopedQuery,
      params: [...params, tenantId],
      wasScoped: true
    };
  }
}
var queryAnalysisCache;
var init_query_scope = __esm({
  "src/db/query-scope.ts"() {
    "use strict";
    init_lruCache();
    queryAnalysisCache = new lruCache_default(1e3);
  }
});

// src/db/pg.ts
import { Pool as Pool3 } from "pg";
import { trace } from "@opentelemetry/api";
import { Counter as Counter3, Histogram as Histogram2, register as register5 } from "prom-client";
function createCounter3(config9) {
  try {
    return new Counter3(config9);
  } catch (e) {
    return { inc: () => {
    }, labels: () => ({ inc: () => {
    } }) };
  }
}
function createHistogram3(config9) {
  try {
    return new Histogram2(config9);
  } catch (e) {
    return { observe: () => {
    }, labels: () => ({ observe: () => {
    } }) };
  }
}
function getPoolForQuery(query3, forceWrite = false) {
  const operation = query3.trim().split(" ")[0].toLowerCase();
  const isWriteOperation = [
    "insert",
    "update",
    "delete",
    "create",
    "alter",
    "drop",
    "truncate"
  ].includes(operation);
  if (forceWrite || isWriteOperation || process.env.READ_ONLY_REGION !== "true") {
    return { pool: writePool, poolType: "write" };
  } else {
    return { pool: readPool, poolType: "read" };
  }
}
async function _executeQuery(spanName, query3, params, options2, returnMany) {
  return tracer2.startActiveSpan(spanName, async (span) => {
    const operation = query3.split(" ")[0].toLowerCase();
    const { pool: selectedPool, poolType } = options2.poolType ? {
      pool: options2.poolType === "read" ? readPool : writePool,
      poolType: options2.poolType
    } : getPoolForQuery(query3, options2?.forceWrite);
    const currentRegion = process.env.CURRENT_REGION || "unknown";
    span.setAttributes({
      "db.system": "postgresql",
      "db.statement": query3,
      "db.operation": operation,
      "db.pool_type": poolType,
      "db.region": currentRegion,
      tenant_id: options2?.tenantId || "unknown"
    });
    const scopedQuery = validateAndScopeQuery(
      query3,
      params,
      options2?.tenantId
    );
    const startTime = Date.now();
    try {
      dbConnectionsActive3.inc({
        region: currentRegion,
        pool_type: poolType,
        tenant_id: options2?.tenantId || "unknown"
      });
      const result2 = await selectedPool.query(
        scopedQuery.query,
        scopedQuery.params
      );
      const duration = (Date.now() - startTime) / 1e3;
      dbQueryDuration3.observe(
        {
          region: currentRegion,
          pool_type: poolType,
          operation,
          tenant_id: options2?.tenantId || "unknown"
        },
        duration
      );
      span.setAttributes({
        "db.rows_affected": result2.rowCount || 0,
        "db.tenant_scoped": scopedQuery.wasScoped,
        "db.query_duration": duration
      });
      return returnMany ? result2.rows : result2.rows[0] || null;
    } catch (error) {
      span.recordException(error);
      span.setStatus({ code: 2, message: error.message });
      throw error;
    } finally {
      span.end();
    }
  });
}
var tracer2, jsonDateReviver, toInt, writePoolSize, readPoolSize, idleTimeoutMs, connectionTimeoutMs, maxUses, statementTimeoutMs, dbConnectionsActive3, dbQueryDuration3, dbReplicationLag, writePool, readPool, pool, pg;
var init_pg = __esm({
  "src/db/pg.ts"() {
    "use strict";
    init_redis2();
    init_query_scope();
    tracer2 = trace.getTracer("maestro-postgres", "24.3.0");
    jsonDateReviver = (key, value) => {
      if (typeof value === "string" && /^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d{1,3})?Z$/.test(value)) {
        const date = new Date(value);
        if (!isNaN(date.getTime())) return date;
      }
      return value;
    };
    toInt = (value, fallback) => {
      const parsed = Number.parseInt(value || "", 10);
      return Number.isNaN(parsed) ? fallback : parsed;
    };
    writePoolSize = toInt(process.env.PG_WRITE_POOL_SIZE, 20);
    readPoolSize = toInt(process.env.PG_READ_POOL_SIZE, 30);
    idleTimeoutMs = toInt(process.env.PG_IDLE_TIMEOUT_MS, 3e4);
    connectionTimeoutMs = toInt(process.env.PG_CONNECTION_TIMEOUT_MS, 5e3);
    maxUses = toInt(process.env.PG_POOL_MAX_USES, 5e3);
    statementTimeoutMs = toInt(process.env.PG_STATEMENT_TIMEOUT_MS, 0);
    dbConnectionsActive3 = register5?.getSingleMetric(
      "db_connections_active_total"
    ) || createCounter3({
      name: "db_connections_active_total",
      help: "Total active database connections",
      labelNames: ["region", "pool_type", "tenant_id"]
    });
    dbQueryDuration3 = register5?.getSingleMetric(
      "db_query_duration_seconds"
    ) || createHistogram3({
      name: "db_query_duration_seconds",
      help: "Database query duration",
      labelNames: ["region", "pool_type", "operation", "tenant_id"],
      buckets: [1e-3, 5e-3, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5]
    });
    dbReplicationLag = register5?.getSingleMetric(
      "db_replication_lag_seconds"
    ) || createHistogram3({
      name: "db_replication_lag_seconds",
      help: "Database replication lag in seconds",
      labelNames: ["region", "primary_region"],
      buckets: [0.1, 0.5, 1, 5, 10, 30, 60, 300]
    });
    writePool = new Pool3({
      connectionString: process.env.DATABASE_URL || process.env.DATABASE_WRITE_URL,
      ssl: process.env.NODE_ENV === "production" ? { rejectUnauthorized: true } : false,
      max: writePoolSize,
      idleTimeoutMillis: idleTimeoutMs,
      connectionTimeoutMillis: connectionTimeoutMs,
      maxUses,
      keepAlive: true,
      ...statementTimeoutMs > 0 ? { statement_timeout: statementTimeoutMs } : {},
      application_name: `maestro-write-${process.env.CURRENT_REGION || "unknown"}`
    });
    readPool = new Pool3({
      connectionString: process.env.DATABASE_READ_URL || process.env.DATABASE_URL,
      ssl: process.env.NODE_ENV === "production" ? { rejectUnauthorized: true } : false,
      max: readPoolSize,
      // More read connections
      idleTimeoutMillis: idleTimeoutMs,
      connectionTimeoutMillis: connectionTimeoutMs,
      maxUses,
      keepAlive: true,
      ...statementTimeoutMs > 0 ? { statement_timeout: statementTimeoutMs } : {},
      application_name: `maestro-read-${process.env.CURRENT_REGION || "unknown"}`
    });
    pool = writePool;
    pg = {
      // Legacy method with automatic pool selection
      oneOrNone: async (query3, params = [], options2) => {
        return _executeQuery("postgres.query", query3, params, options2 || {}, false);
      },
      // Explicit read-only method
      read: async (query3, params = [], options2) => {
        if (options2?.cache) {
          try {
            const redis5 = RedisService.getInstance();
            const cached = await redis5.get(options2.cache.key);
            if (cached) {
              return JSON.parse(cached, jsonDateReviver);
            }
          } catch (e) {
            console.warn("Cache retrieval failed:", e);
          }
        }
        const result2 = await _executeQuery(
          "postgres.read",
          query3,
          params,
          { ...options2, poolType: "read" },
          false
        );
        if (options2?.cache && result2) {
          try {
            const redis5 = RedisService.getInstance();
            await redis5.set(options2.cache.key, JSON.stringify(result2), options2.cache.ttl);
          } catch (e) {
            console.warn("Cache set failed:", e);
          }
        }
        return result2;
      },
      // Explicit write method
      write: async (query3, params = [], options2) => {
        return _executeQuery(
          "postgres.write",
          query3,
          params,
          { ...options2, poolType: "write" },
          false
        );
      },
      // Read many records
      readMany: async (query3, params = [], options2) => {
        if (options2?.cache) {
          try {
            const redis5 = RedisService.getInstance();
            const cached = await redis5.get(options2.cache.key);
            if (cached) {
              return JSON.parse(cached);
            }
          } catch (e) {
            console.warn("Cache retrieval failed:", e);
          }
        }
        const result2 = await _executeQuery(
          "postgres.read_many",
          query3,
          params,
          { ...options2, poolType: "read" },
          true
        );
        if (options2?.cache && result2) {
          try {
            const redis5 = RedisService.getInstance();
            await redis5.set(options2.cache.key, JSON.stringify(result2), options2.cache.ttl);
          } catch (e) {
            console.warn("Cache set failed:", e);
          }
        }
        return result2;
      },
      many: async (query3, params = [], options2) => {
        return tracer2.startActiveSpan("postgres.query.many", async (span) => {
          span.setAttributes({
            "db.system": "postgresql",
            "db.statement": query3,
            "db.operation": query3.split(" ")[0].toLowerCase(),
            tenant_id: options2?.tenantId || "unknown"
          });
          const scopedQuery = validateAndScopeQuery(
            query3,
            params,
            options2?.tenantId
          );
          try {
            const result2 = await pool.query(scopedQuery.query, scopedQuery.params);
            span.setAttributes({
              "db.rows_affected": result2.rowCount || 0,
              "db.tenant_scoped": scopedQuery.wasScoped
            });
            return result2.rows;
          } catch (error) {
            span.recordException(error);
            span.setStatus({ code: 2, message: error.message });
            throw error;
          } finally {
            span.end();
          }
        });
      },
      // Tenant-scoped transaction support
      withTenant: async (tenantId, callback) => {
        return tracer2.startActiveSpan(
          "postgres.with_tenant",
          async (span) => {
            span.setAttributes({
              tenant_id: tenantId,
              "db.system": "postgresql"
            });
            const scopedPg = {
              oneOrNone: (query3, params = []) => pg.oneOrNone(query3, params, { tenantId }),
              many: (query3, params = []) => pg.many(query3, params, { tenantId })
            };
            try {
              return await callback(scopedPg);
            } catch (error) {
              span.recordException(error);
              span.setStatus({ code: 2, message: error.message });
              throw error;
            } finally {
              span.end();
            }
          }
        );
      },
      // Generic transaction support
      transaction: async (callback) => {
        const client6 = await pool.connect();
        try {
          await client6.query("BEGIN");
          const tx = {
            query: async (text, params = []) => {
              return (await client6.query(text, params)).rows;
            }
          };
          const result2 = await callback(tx);
          await client6.query("COMMIT");
          return result2;
        } catch (e) {
          await client6.query("ROLLBACK");
          throw e;
        } finally {
          client6.release();
        }
      },
      healthCheck: async () => {
        try {
          await pool.query("SELECT 1");
          return true;
        } catch (error) {
          console.error("PostgreSQL health check failed:", error);
          return false;
        }
      },
      close: async () => {
        await pool.end();
      }
    };
  }
});

// src/security/crypto/types.ts
var init_types = __esm({
  "src/security/crypto/types.ts"() {
    "use strict";
  }
});

// src/security/crypto/keyStore.ts
var InMemoryKeyStore, KeyManager;
var init_keyStore = __esm({
  "src/security/crypto/keyStore.ts"() {
    "use strict";
    InMemoryKeyStore = class {
      store = /* @__PURE__ */ new Map();
      async listKeys(keyId) {
        if (keyId) {
          return Array.from(this.store.get(keyId)?.values() ?? []);
        }
        const result2 = [];
        for (const versions of Array.from(this.store.values())) {
          result2.push(...versions.values());
        }
        return result2;
      }
      async getKey(keyId, version) {
        return this.store.get(keyId)?.get(version);
      }
      async saveKey(key) {
        const versions = this.store.get(key.id) ?? /* @__PURE__ */ new Map();
        versions.set(key.version, { ...key });
        this.store.set(key.id, versions);
      }
      async updateKey(key) {
        const versions = this.store.get(key.id) ?? /* @__PURE__ */ new Map();
        versions.set(key.version, { ...key });
        this.store.set(key.id, versions);
      }
    };
    KeyManager = class {
      constructor(store) {
        this.store = store;
      }
      async getActiveKey(keyId) {
        const keys = await this.store.listKeys(keyId);
        return keys.filter((k) => k.isActive).sort((a, b) => b.version - a.version)[0];
      }
      async getKey(keyId, version) {
        return this.store.getKey(keyId, version);
      }
      async listKeyVersions(keyId) {
        const keys = await this.store.listKeys(keyId);
        return keys.sort((a, b) => b.version - a.version);
      }
      async rotateKey(keyId, nextKey) {
        const existing = await this.store.listKeys(keyId);
        const currentActive = existing.find((k) => k.isActive);
        if (currentActive) {
          currentActive.isActive = false;
          currentActive.rotatedAt = /* @__PURE__ */ new Date();
          await this.store.updateKey(currentActive);
        }
        const nextVersion = nextKey.version ?? (existing.length ? Math.max(...existing.map((k) => k.version)) + 1 : 1);
        const version = {
          ...nextKey,
          id: keyId,
          version: nextVersion,
          isActive: true,
          createdAt: nextKey.createdAt ?? /* @__PURE__ */ new Date()
        };
        await this.store.saveKey(version);
        return version;
      }
      async registerKeyVersion(key) {
        const existing = await this.store.listKeys(key.id);
        if (key.isActive) {
          for (const version of existing.filter(
            (v) => v.isActive && v.version !== key.version
          )) {
            version.isActive = false;
            version.rotatedAt = /* @__PURE__ */ new Date();
            await this.store.updateKey(version);
          }
        }
        await this.store.saveKey({ ...key });
      }
    };
  }
});

// src/utils/audit.ts
import * as crypto4 from "crypto";
function deepDiff(before = {}, after = {}) {
  const changed = {};
  const keys = /* @__PURE__ */ new Set([
    ...Object.keys(before || {}),
    ...Object.keys(after || {})
  ]);
  for (const k of Array.from(keys)) {
    const bv = before?.[k];
    const av = after?.[k];
    const bothObjects = bv && av && typeof bv === "object" && typeof av === "object";
    if (bothObjects) {
      const nested = deepDiff(bv, av);
      if (nested && Object.keys(nested).length) changed[k] = nested;
    } else if (JSON.stringify(bv) !== JSON.stringify(av)) {
      changed[k] = { before: bv, after: av };
    }
  }
  return changed;
}
function signAuditPayload(payload, secret) {
  try {
    const h = crypto4.createHmac("sha256", String(secret));
    h.update(Buffer.from(JSON.stringify(payload)));
    return h.digest("base64");
  } catch (_2) {
    return null;
  }
}
async function writeAudit({
  userId,
  action,
  resourceType,
  resourceId,
  details = {},
  ip,
  userAgent,
  actorRole,
  sessionId,
  before,
  after
}) {
  try {
    const pool4 = getPostgresPool2();
    const enrichedDetails = { ...details };
    if (before || after) {
      enrichedDetails.before = before ?? null;
      enrichedDetails.after = after ?? null;
      enrichedDetails.diff = deepDiff(before || {}, after || {});
    }
    if (actorRole) enrichedDetails.actorRole = actorRole;
    if (sessionId) enrichedDetails.sessionId = sessionId;
    if (ip) enrichedDetails.ip = ip;
    const secret = process.env.AUDIT_SIGNING_SECRET;
    if (secret) {
      enrichedDetails.signature = signAuditPayload(
        {
          userId: userId || null,
          action,
          resourceType: resourceType || null,
          resourceId: resourceId || null,
          before: enrichedDetails.before ?? null,
          after: enrichedDetails.after ?? null,
          at: (/* @__PURE__ */ new Date()).toISOString()
        },
        secret
      );
    }
    await pool4.query(
      `INSERT INTO audit_logs (user_id, action, resource_type, resource_id, details, ip_address, user_agent)
       VALUES ($1,$2,$3,$4,$5,$6,$7)`,
      [
        userId || null,
        action,
        resourceType || null,
        resourceId || null,
        enrichedDetails,
        ip || null,
        userAgent || null
      ]
    );
  } catch (e) {
  }
}
var init_audit = __esm({
  "src/utils/audit.ts"() {
    "use strict";
    init_database();
  }
});

// src/security/crypto/services.ts
import crypto5 from "node:crypto";
var SoftwareHSM, Rfc3161TimestampingService, DatabaseAuditLogger;
var init_services = __esm({
  "src/security/crypto/services.ts"() {
    "use strict";
    init_audit();
    SoftwareHSM = class {
      async sign(data, key) {
        if (!key.privateKeyPem) {
          throw new Error(
            `Key ${key.id} version ${key.version} missing private key material`
          );
        }
        switch (key.algorithm) {
          case "RSA_SHA256": {
            const signer = crypto5.createSign("RSA-SHA256");
            signer.update(data);
            signer.end();
            return signer.sign(key.privateKeyPem);
          }
          case "ECDSA_P256_SHA256": {
            const signer = crypto5.createSign("SHA256");
            signer.update(data);
            signer.end();
            return signer.sign({
              key: key.privateKeyPem,
              dsaEncoding: "ieee-p1363"
            });
          }
          case "ECDSA_P384_SHA384": {
            const signer = crypto5.createSign("SHA384");
            signer.update(data);
            signer.end();
            return signer.sign({
              key: key.privateKeyPem,
              dsaEncoding: "ieee-p1363"
            });
          }
          case "EdDSA_ED25519": {
            return crypto5.sign(null, data, key.privateKeyPem);
          }
          default:
            throw new Error(`Unsupported algorithm ${key.algorithm}`);
        }
      }
      async exportPublicKey(key) {
        if (key.publicKeyPem) {
          return key.publicKeyPem;
        }
        if (!key.privateKeyPem) {
          throw new Error(
            `Key ${key.id} version ${key.version} missing material to derive public key`
          );
        }
        const privateKey = crypto5.createPrivateKey(key.privateKeyPem);
        const publicKey = crypto5.createPublicKey(privateKey);
        return publicKey.export({ type: "spki", format: "pem" }).toString();
      }
    };
    Rfc3161TimestampingService = class {
      constructor(endpoint, options2 = {}) {
        this.endpoint = endpoint;
        this.options = options2;
      }
      async getTimestampToken(payload) {
        const fetchFn = global.fetch;
        const response = await fetchFn(this.endpoint, {
          method: "POST",
          headers: {
            "content-type": "application/octet-stream",
            ...this.options.apiKeyHeader && this.options.apiKeyValue ? { [this.options.apiKeyHeader]: this.options.apiKeyValue } : {}
          },
          body: new Uint8Array(payload)
        });
        if (!response.ok) {
          const text = await response.text();
          throw new Error(
            `Timestamping service responded with ${response.status}: ${text}`
          );
        }
        const contentType = response.headers.get("content-type") ?? "";
        if (contentType.includes("application/json")) {
          const body4 = await response.json();
          if (!body4.token) {
            throw new Error(
              "Timestamping service returned JSON without token field"
            );
          }
          return body4.token;
        }
        const buffer = Buffer.from(await response.arrayBuffer());
        return buffer.toString("base64");
      }
      async verify(payload, token) {
        if (!this.options.verifyEndpoint) {
          try {
            Buffer.from(token, "base64");
            return true;
          } catch (error) {
            return false;
          }
        }
        const fetchFn = global.fetch;
        const response = await fetchFn(this.options.verifyEndpoint, {
          method: "POST",
          headers: {
            "content-type": "application/json",
            ...this.options.apiKeyHeader && this.options.apiKeyValue ? { [this.options.apiKeyHeader]: this.options.apiKeyValue } : {}
          },
          body: JSON.stringify({ token, payload: payload.toString("base64") })
        });
        if (!response.ok) {
          return false;
        }
        const body4 = await response.json().catch(() => ({}));
        return body4.valid === true;
      }
    };
    DatabaseAuditLogger = class {
      constructor(subsystem) {
        this.subsystem = subsystem;
      }
      async log(event) {
        try {
          const resourceId = `${event.keyId}:${event.keyVersion ?? "unknown"}`;
          const details = {
            subsystem: this.subsystem,
            algorithm: event.algorithm ?? null,
            success: event.success,
            reason: event.reason ?? null,
            metadata: event.metadata ?? null
          };
          await writeAudit({
            action: `crypto.${event.action}`,
            resourceType: "crypto-key",
            resourceId,
            details
          });
        } catch (error) {
          console.warn("Failed to write crypto audit log", error);
        }
      }
    };
  }
});

// src/security/crypto/certificates.ts
import * as crypto6 from "node:crypto";
function normalizeDn(dn) {
  return dn.trim().replace(/\s+/g, " ").toLowerCase();
}
var CertificateValidator;
var init_certificates = __esm({
  "src/security/crypto/certificates.ts"() {
    "use strict";
    CertificateValidator = class {
      trustAnchors = /* @__PURE__ */ new Map();
      constructor(anchors = []) {
        anchors.forEach((anchor) => this.addTrustAnchor(anchor));
      }
      addTrustAnchor(pem) {
        const cert = new crypto6.X509Certificate(pem);
        this.trustAnchors.set(normalizeDn(cert.subject), cert);
        this.trustAnchors.set(cert.fingerprint256, cert);
      }
      validate(chain = []) {
        if (!chain.length) {
          return {
            valid: this.trustAnchors.size === 0,
            errors: this.trustAnchors.size === 0 ? [] : ["No certificates supplied"],
            depth: 0
          };
        }
        const errors = [];
        let depth = 0;
        let issuer;
        for (let i = 0; i < chain.length; i += 1) {
          const pem = chain[i];
          let cert;
          try {
            cert = new crypto6.X509Certificate(pem);
          } catch (error) {
            errors.push(`Invalid certificate at position ${i}: ${error.message}`);
            continue;
          }
          depth += 1;
          const now = Date.now();
          if (new Date(cert.validFrom).getTime() > now) {
            errors.push(`Certificate ${cert.subject} not yet valid`);
          }
          if (new Date(cert.validTo).getTime() < now) {
            errors.push(`Certificate ${cert.subject} expired on ${cert.validTo}`);
          }
          if (i + 1 < chain.length) {
            issuer = new crypto6.X509Certificate(chain[i + 1]);
          } else {
            const normalizedIssuer = normalizeDn(cert.issuer);
            issuer = this.trustAnchors.get(normalizedIssuer) || this.trustAnchors.get(cert.fingerprint256) || void 0;
          }
          if (!issuer) {
            errors.push(`Unable to locate issuer for ${cert.subject}`);
            continue;
          }
          try {
            const verified = cert.verify(issuer.publicKey);
            if (!verified) {
              errors.push(`Signature validation failed for ${cert.subject}`);
            }
          } catch (error) {
            errors.push(
              `Certificate verification error for ${cert.subject}: ${error.message}`
            );
          }
        }
        let leafSubject;
        let rootSubject;
        try {
          leafSubject = new crypto6.X509Certificate(chain[0]).subject;
        } catch {
          leafSubject = void 0;
        }
        try {
          rootSubject = (issuer ?? new crypto6.X509Certificate(chain[chain.length - 1])).subject;
        } catch {
          rootSubject = issuer?.subject;
        }
        return {
          valid: errors.length === 0,
          errors,
          depth,
          leafSubject,
          rootSubject
        };
      }
    };
  }
});

// src/security/crypto/pipeline.ts
import * as crypto7 from "node:crypto";
async function createDefaultCryptoPipeline(options2 = {}) {
  const raw = process.env.CRYPTO_SIGNING_KEYS;
  if (!raw) {
    return null;
  }
  let parsed;
  try {
    parsed = JSON.parse(raw);
  } catch (error) {
    console.error("Failed to parse CRYPTO_SIGNING_KEYS", error);
    return null;
  }
  if (!Array.isArray(parsed) || !parsed.length) {
    console.error("CRYPTO_SIGNING_KEYS must be a non-empty array");
    return null;
  }
  const keyStore = new InMemoryKeyStore();
  const auditLogger2 = new DatabaseAuditLogger(
    options2.auditSubsystem ?? "crypto-pipeline"
  );
  const trustAnchors = options2.trustAnchorsEnv ? process.env[options2.trustAnchorsEnv]?.split(":::").filter(Boolean) : void 0;
  let timestampingService;
  if (options2.timestampingEndpointEnv) {
    const endpoint = process.env[options2.timestampingEndpointEnv];
    if (endpoint) {
      timestampingService = new Rfc3161TimestampingService(endpoint);
    }
  }
  const pipeline2 = new CryptoPipeline({
    keyStore,
    auditLogger: auditLogger2,
    timestampingService,
    trustAnchors
  });
  for (const definition of parsed) {
    const version = {
      id: definition.id,
      version: definition.version ?? 1,
      algorithm: definition.algorithm,
      publicKeyPem: definition.publicKeyPem ?? "",
      privateKeyPem: definition.privateKeyPem,
      certificateChain: definition.certificateChain,
      metadata: definition.metadata,
      createdAt: /* @__PURE__ */ new Date(),
      validFrom: /* @__PURE__ */ new Date(),
      isActive: definition.active ?? true
    };
    if (!version.publicKeyPem && version.privateKeyPem) {
      const keyObject = crypto7.createPrivateKey(version.privateKeyPem);
      version.publicKeyPem = crypto7.createPublicKey(keyObject).export({ type: "spki", format: "pem" }).toString();
    }
    await pipeline2.registerKeyVersion(version);
  }
  return pipeline2;
}
var CryptoPipeline;
var init_pipeline = __esm({
  "src/security/crypto/pipeline.ts"() {
    "use strict";
    init_certificates();
    init_keyStore();
    init_services();
    CryptoPipeline = class {
      keyManager;
      certificateValidator;
      hsm;
      timestampingService;
      auditLogger;
      defaultKeyId;
      constructor(options2 = {}) {
        this.keyManager = new KeyManager(
          options2.keyStore ?? new InMemoryKeyStore()
        );
        this.certificateValidator = new CertificateValidator(
          options2.trustAnchors ?? []
        );
        this.hsm = options2.hsm ?? new SoftwareHSM();
        this.timestampingService = options2.timestampingService;
        this.auditLogger = options2.auditLogger;
        this.defaultKeyId = options2.defaultKeyId;
      }
      async registerKeyVersion(key) {
        await this.keyManager.registerKeyVersion(key);
      }
      async rotateKey(keyId, key) {
        const next = await this.keyManager.rotateKey(keyId, key);
        await this.logAudit({
          action: "rotate",
          keyId,
          keyVersion: next.version,
          algorithm: next.algorithm,
          success: true,
          metadata: key.metadata ?? void 0
        });
        return next;
      }
      async signPayload(payload, keyId, options2 = {}) {
        const material = Buffer.isBuffer(payload) ? payload : Buffer.from(payload);
        const effectiveKeyId = keyId ?? this.defaultKeyId;
        if (!effectiveKeyId) {
          throw new Error("No keyId specified for signing payload");
        }
        const key = await this.keyManager.getActiveKey(effectiveKeyId);
        if (!key) {
          throw new Error(`Active key not found for ${effectiveKeyId}`);
        }
        const signature = await this.hsm.sign(material, key);
        let timestampToken;
        if (options2.includeTimestamp && this.timestampingService) {
          timestampToken = await this.timestampingService.getTimestampToken(material);
        }
        const bundle = {
          keyId: effectiveKeyId,
          keyVersion: key.version,
          algorithm: key.algorithm,
          signature: signature.toString("base64"),
          timestampToken,
          certificateChain: key.certificateChain,
          metadata: options2.metadata ?? void 0
        };
        await this.logAudit({
          action: "sign",
          keyId: effectiveKeyId,
          keyVersion: key.version,
          algorithm: key.algorithm,
          success: true,
          metadata: options2.metadata
        });
        return bundle;
      }
      async verifySignature(payload, bundle, context4 = {}) {
        const errors = [];
        const material = Buffer.isBuffer(payload) ? payload : Buffer.from(payload);
        if (context4.expectedKeyId && context4.expectedKeyId !== bundle.keyId) {
          errors.push(
            `Unexpected key id ${bundle.keyId}, expected ${context4.expectedKeyId}`
          );
        }
        if (context4.expectedAlgorithm && context4.expectedAlgorithm !== bundle.algorithm) {
          errors.push(`Unexpected algorithm ${bundle.algorithm}`);
        }
        let key = await this.keyManager.getKey(bundle.keyId, bundle.keyVersion);
        if (!key) {
          key = await this.keyManager.getActiveKey(bundle.keyId);
        }
        let publicKeyPem = key?.publicKeyPem;
        let certificateResult;
        if (key && key.validTo && key.validTo.getTime() < Date.now() && !context4.allowExpiredKeys) {
          errors.push(`Key ${key.id} version ${key.version} is expired`);
        }
        if (bundle.certificateChain?.length) {
          certificateResult = this.certificateValidator.validate(
            bundle.certificateChain
          );
          if (!certificateResult.valid) {
            errors.push(...certificateResult.errors);
          }
        }
        if (!publicKeyPem) {
          if (bundle.certificateChain?.length) {
            try {
              const leaf = new crypto7.X509Certificate(bundle.certificateChain[0]);
              publicKeyPem = leaf.publicKey.export({ type: "spki", format: "pem" }).toString();
            } catch (error) {
              errors.push(
                `Unable to extract public key from leaf certificate: ${error.message}`
              );
            }
          } else if (key?.privateKeyPem) {
            const privateKey = crypto7.createPrivateKey(key.privateKeyPem);
            publicKeyPem = crypto7.createPublicKey(privateKey).export({ type: "spki", format: "pem" }).toString();
          } else {
            errors.push(`No public key material found for ${bundle.keyId}`);
          }
        }
        let signatureValid = false;
        if (publicKeyPem) {
          signatureValid = this.verifyWithAlgorithm(
            bundle.algorithm,
            material,
            Buffer.from(bundle.signature, "base64"),
            publicKeyPem
          );
          if (!signatureValid) {
            errors.push("Signature verification failed");
          }
        }
        let timestampValid;
        if (bundle.timestampToken && this.timestampingService?.verify) {
          timestampValid = await this.timestampingService.verify(
            material,
            bundle.timestampToken
          );
          if (!timestampValid) {
            errors.push("Timestamp token verification failed");
          }
        }
        const result2 = {
          valid: errors.length === 0 && signatureValid,
          keyId: bundle.keyId,
          keyVersion: bundle.keyVersion,
          algorithm: bundle.algorithm,
          chainValidated: certificateResult?.valid,
          timestampVerified: timestampValid,
          errors: errors.length ? errors : void 0
        };
        await this.logAudit({
          action: "verify",
          keyId: bundle.keyId,
          keyVersion: bundle.keyVersion,
          algorithm: bundle.algorithm,
          success: result2.valid,
          reason: result2.errors?.join("; "),
          metadata: context4
        });
        return result2;
      }
      verifyWithAlgorithm(algorithm, payload, signature, publicKey) {
        try {
          switch (algorithm) {
            case "RSA_SHA256": {
              const verifier = crypto7.createVerify("RSA-SHA256");
              verifier.update(payload);
              verifier.end();
              return verifier.verify(publicKey, signature);
            }
            case "ECDSA_P256_SHA256": {
              const verifier = crypto7.createVerify("SHA256");
              verifier.update(payload);
              verifier.end();
              return verifier.verify(
                { key: publicKey, dsaEncoding: "ieee-p1363" },
                signature
              );
            }
            case "ECDSA_P384_SHA384": {
              const verifier = crypto7.createVerify("SHA384");
              verifier.update(payload);
              verifier.end();
              return verifier.verify(
                { key: publicKey, dsaEncoding: "ieee-p1363" },
                signature
              );
            }
            case "EdDSA_ED25519":
              return crypto7.verify(null, payload, publicKey, signature);
            default:
              throw new Error(`Unsupported verification algorithm ${algorithm}`);
          }
        } catch (error) {
          console.warn("Signature verification error", error);
          return false;
        }
      }
      async logAudit(event) {
        if (!this.auditLogger) return;
        await this.auditLogger.log(event);
      }
    };
  }
});

// src/security/crypto/byok-hsm-orchestrator.ts
var init_byok_hsm_orchestrator = __esm({
  "src/security/crypto/byok-hsm-orchestrator.ts"() {
    "use strict";
    init_keyStore();
    init_services();
  }
});

// src/security/crypto/index.ts
var init_crypto = __esm({
  "src/security/crypto/index.ts"() {
    "use strict";
    init_types();
    init_keyStore();
    init_services();
    init_certificates();
    init_pipeline();
    init_byok_hsm_orchestrator();
  }
});

// src/provenance/witness.ts
import { createHash as createHash4, createSign, createVerify as createVerify2, generateKeyPairSync } from "crypto";
var MutationWitnessService, mutationWitness, WitnessRegistry, witnessRegistry;
var init_witness = __esm({
  "src/provenance/witness.ts"() {
    "use strict";
    MutationWitnessService = class _MutationWitnessService {
      static instance;
      witnessId;
      privateKey;
      publicKey;
      constructor() {
        this.witnessId = process.env.WITNESS_ID || `witness-${Date.now()}`;
        const { privateKey, publicKey } = generateKeyPairSync("rsa", {
          modulusLength: 2048
        });
        this.privateKey = process.env.WITNESS_PRIVATE_KEY || privateKey.export({ type: "pkcs1", format: "pem" }).toString();
        this.publicKey = process.env.WITNESS_PUBLIC_KEY || publicKey.export({ type: "pkcs1", format: "pem" }).toString();
      }
      static getInstance() {
        if (!_MutationWitnessService.instance) {
          _MutationWitnessService.instance = new _MutationWitnessService();
        }
        return _MutationWitnessService.instance;
      }
      /**
       * Validates and witnesses a mutation.
       * This is the "Proof of Correctness" step.
       */
      async witnessMutation(payload, context4) {
        const validationResult4 = await this.validateMutation(payload, context4);
        if (!validationResult4.valid) {
          throw new Error(`Mutation rejected by witness: ${JSON.stringify(validationResult4.checks)}`);
        }
        const canonicalString = this.createCanonicalString(payload, validationResult4);
        const sign4 = createSign("SHA256");
        sign4.update(canonicalString);
        const signature = sign4.sign(this.privateKey, "hex");
        return {
          witnessId: this.witnessId,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          signature,
          algorithm: "RSA-SHA256",
          validationResult: validationResult4
        };
      }
      async validateMutation(payload, context4) {
        const checks = [];
        const hasValidStructure = payload.mutationType === "CREATE" && payload.newState && !payload.previousState || payload.mutationType === "UPDATE" && payload.newState && payload.previousState || payload.mutationType === "DELETE" && !payload.newState && payload.previousState;
        checks.push({
          check: "Schema Integrity",
          passed: !!hasValidStructure,
          message: hasValidStructure ? "Structure matches mutation type" : "Invalid state combination for mutation type"
        });
        checks.push({
          check: "Tenant Isolation",
          passed: true,
          message: "Tenant context validated"
        });
        checks.push({
          check: "Business Rules",
          passed: true,
          message: "No blocking business rules"
        });
        const valid = checks.every((c) => c.passed);
        return {
          valid,
          policyId: "default-witness-policy-v1",
          checks
        };
      }
      createCanonicalString(payload, validation) {
        return JSON.stringify({
          payload: {
            type: payload.mutationType,
            entity: payload.entityId,
            diffHash: createHash4("sha256").update(JSON.stringify(payload.diff || {})).digest("hex")
          },
          validationHash: createHash4("sha256").update(JSON.stringify(validation)).digest("hex")
        });
      }
      verifyWitness(witness, payload) {
        const canonicalString = this.createCanonicalString(payload, witness.validationResult);
        const verify6 = createVerify2("SHA256");
        verify6.update(canonicalString);
        return verify6.verify(this.publicKey, witness.signature, "hex");
      }
      getPublicKey() {
        return this.publicKey;
      }
    };
    mutationWitness = MutationWitnessService.getInstance();
    WitnessRegistry = class {
      witnesses = /* @__PURE__ */ new Map();
      register(witness) {
        this.witnesses.set(witness.id, witness);
      }
      get(id) {
        return this.witnesses.get(id);
      }
      getAll() {
        return Array.from(this.witnesses.values());
      }
    };
    witnessRegistry = new WitnessRegistry();
  }
});

// src/audit/AuditTimelineRollupService.ts
var AuditTimelineRollupService;
var init_AuditTimelineRollupService = __esm({
  "src/audit/AuditTimelineRollupService.ts"() {
    "use strict";
    init_database();
    init_logger2();
    AuditTimelineRollupService = class {
      pool;
      logger = logger_default2.child({ module: "audit-timeline-rollups" });
      constructor(pool4) {
        this.pool = pool4 ?? getPostgresPool2();
      }
      async refreshRollups(options2 = {}) {
        const client6 = await this.pool.connect();
        const startedAt2 = /* @__PURE__ */ new Date();
        try {
          await client6.query("BEGIN");
          const columns = await this.getAuditColumns(client6);
          const windowStart = options2.from ?? await this.getWatermark(client6, columns);
          const windowEnd = options2.to ?? /* @__PURE__ */ new Date();
          if (!windowStart) {
            await this.updateState(client6, startedAt2, windowEnd, 0, "noop", null);
            await client6.query("COMMIT");
            return {
              processedEvents: 0,
              dailyBuckets: 0,
              weeklyBuckets: 0
            };
          }
          const aggregates = await this.aggregateEvents(
            client6,
            columns,
            windowStart,
            windowEnd
          );
          const dailyBuckets = await this.writeRollups(
            client6,
            "audit_event_rollups_daily",
            "day",
            aggregates.daily
          );
          const weeklyBuckets = await this.writeRollups(
            client6,
            "audit_event_rollups_weekly",
            "week",
            aggregates.weekly
          );
          await this.updateState(
            client6,
            startedAt2,
            windowEnd,
            aggregates.processed,
            "ok",
            null
          );
          await client6.query("COMMIT");
          return {
            windowStart,
            windowEnd,
            processedEvents: aggregates.processed,
            dailyBuckets,
            weeklyBuckets
          };
        } catch (error) {
          await client6.query("ROLLBACK");
          await this.updateState(
            client6,
            startedAt2,
            options2.to ?? /* @__PURE__ */ new Date(),
            0,
            "error",
            error?.message ?? "unknown error"
          );
          this.logger.error(
            { err: error?.message ?? error },
            "Failed to refresh audit timeline rollups"
          );
          throw error;
        } finally {
          client6.release();
        }
      }
      async getTimelineBuckets(params) {
        const granularity = params.granularity ?? "day";
        const useRollups = process.env.TIMELINE_ROLLUPS_V1 === "1";
        if (useRollups) {
          return this.readFromRollups({
            ...params,
            granularity
          });
        }
        return this.readFromBase({
          ...params,
          granularity
        });
      }
      async getAuditColumns(client6) {
        const { rows } = await client6.query(
          `
      SELECT column_name
      FROM information_schema.columns
      WHERE table_schema = 'public' AND table_name = 'audit_events'
    `
        );
        const names = new Set(rows.map((row) => row.column_name));
        const timestampColumn = names.has("timestamp") ? '"timestamp"' : names.has("created_at") ? "created_at" : null;
        if (!timestampColumn) {
          throw new Error(
            "audit_events table is missing a timestamp column for rollups"
          );
        }
        return {
          timestampColumn,
          tenantColumn: names.has("tenant_id") ? "tenant_id" : void 0,
          eventTypeColumn: names.has("event_type") ? "event_type" : names.has("action") ? "action" : void 0,
          levelColumn: names.has("level") ? "level" : void 0,
          serviceIdColumn: names.has("service_id") ? "service_id" : void 0
        };
      }
      async getWatermark(client6, columns) {
        const state = await client6.query(
          `
      SELECT last_processed_at
      FROM audit_event_rollup_state
      WHERE rollup_name = 'audit_events'
      FOR UPDATE
    `
        );
        if (state.rowCount && state.rows[0].last_processed_at) {
          return new Date(state.rows[0].last_processed_at);
        }
        const { rows } = await client6.query(
          `SELECT MIN(${columns.timestampColumn}) AS min_ts FROM audit_events`
        );
        const minTs = rows[0]?.min_ts;
        return minTs ? new Date(minTs) : void 0;
      }
      async aggregateEvents(client6, columns, start, end) {
        const tenantExpr = columns.tenantColumn ? `COALESCE(${columns.tenantColumn}, 'unknown')` : `'unknown'`;
        const eventTypeExpr = columns.eventTypeColumn ? `COALESCE(${columns.eventTypeColumn}, 'event')` : `'event'`;
        const levelExpr = columns.levelColumn ? `COALESCE(${columns.levelColumn}, 'info')` : `'info'`;
        const serviceExpr = columns.serviceIdColumn ? `COALESCE(${columns.serviceIdColumn}, NULL)` : "NULL::text";
        const sourceQuery = `
      SELECT
        ${tenantExpr} AS tenant_id,
        ${eventTypeExpr} AS event_type,
        ${levelExpr} AS level,
        ${serviceExpr} AS service_id,
        ${columns.timestampColumn} AS event_ts
      FROM audit_events
      WHERE ${columns.timestampColumn} >= $1 AND ${columns.timestampColumn} < $2
    `;
        const daily = await client6.query(
          `
      SELECT
        date_trunc('day', event_ts)::date AS bucket_start,
        (date_trunc('day', event_ts) + INTERVAL '1 day')::date AS bucket_end,
        tenant_id,
        event_type,
        level,
        service_id,
        COUNT(*)::bigint AS event_count,
        MAX(event_ts) AS last_event_timestamp
      FROM (${sourceQuery}) AS src
      GROUP BY 1,2,3,4,5,6
    `,
          [start, end]
        );
        const weekly = await client6.query(
          `
      SELECT
        date_trunc('week', event_ts)::date AS bucket_start,
        (date_trunc('week', event_ts) + INTERVAL '1 week')::date AS bucket_end,
        tenant_id,
        event_type,
        level,
        service_id,
        COUNT(*)::bigint AS event_count,
        MAX(event_ts) AS last_event_timestamp
      FROM (${sourceQuery}) AS src
      GROUP BY 1,2,3,4,5,6
    `,
          [start, end]
        );
        const processed = daily.rows.reduce(
          (sum, row) => sum + Number(row.event_count ?? 0),
          0
        );
        return { processed, daily: daily.rows, weekly: weekly.rows };
      }
      async writeRollups(client6, table, granularity, rows) {
        if (!rows.length) return 0;
        for (const row of rows) {
          await this.ensurePartition(
            client6,
            table,
            granularity,
            row.bucket_start,
            row.bucket_end
          );
        }
        const columns = [
          "bucket_start",
          "bucket_end",
          "tenant_id",
          "event_type",
          "level",
          "service_id",
          "event_count",
          "last_event_timestamp"
        ];
        const values = [];
        const placeholders = rows.map((row, idx) => {
          const offset = idx * columns.length;
          values.push(
            row.bucket_start,
            row.bucket_end,
            row.tenant_id,
            row.event_type,
            row.level,
            row.service_id,
            Number(row.event_count ?? 0),
            row.last_event_timestamp ? new Date(row.last_event_timestamp) : null
          );
          const base = Array.from({ length: columns.length }, (_2, i) => `$${offset + i + 1}`);
          return `(${base.join(", ")})`;
        });
        const insertSql = `
      INSERT INTO ${table} (${columns.join(", ")})
      VALUES ${placeholders.join(", ")}
      ON CONFLICT (bucket_start, tenant_id, event_type, level)
      DO UPDATE SET
        event_count = ${table}.event_count + EXCLUDED.event_count,
        last_event_timestamp = GREATEST(${table}.last_event_timestamp, EXCLUDED.last_event_timestamp)
    `;
        await client6.query(insertSql, values);
        return rows.length;
      }
      async ensurePartition(client6, table, granularity, bucketStart, bucketEnd) {
        const suffix = bucketStart.replace(/-/g, "");
        const partitionName = `${table}_${granularity}_${suffix}`;
        const sql = `
      CREATE TABLE IF NOT EXISTS ${partitionName}
      PARTITION OF ${table}
      FOR VALUES FROM ('${bucketStart}') TO ('${bucketEnd}')
    `;
        await client6.query(sql);
      }
      async updateState(client6, startedAt2, completedAt, processed, status, lastError) {
        await client6.query(
          `
      INSERT INTO audit_event_rollup_state (
        rollup_name,
        last_processed_at,
        last_run_started_at,
        last_run_completed_at,
        last_run_status,
        last_error,
        rows_processed,
        updated_at
      ) VALUES ('audit_events', $1, $2, $3, $4, $5, $6, NOW())
      ON CONFLICT (rollup_name) DO UPDATE SET
        last_processed_at = EXCLUDED.last_processed_at,
        last_run_started_at = EXCLUDED.last_run_started_at,
        last_run_completed_at = EXCLUDED.last_run_completed_at,
        last_run_status = EXCLUDED.last_run_status,
        last_error = EXCLUDED.last_error,
        rows_processed = audit_event_rollup_state.rows_processed + EXCLUDED.rows_processed,
        updated_at = NOW()
    `,
          [completedAt, startedAt2, completedAt, status, lastError, processed]
        );
      }
      async readFromRollups(params) {
        const table = params.granularity === "week" ? "audit_event_rollups_weekly" : "audit_event_rollups_daily";
        const startDate = this.truncateDate(params.rangeStart, params.granularity);
        const endDate = this.truncateDate(params.rangeEnd, params.granularity);
        const conditions = ["bucket_start >= $1", "bucket_start < $2"];
        const values = [startDate, endDate];
        if (params.tenantId) {
          conditions.push(`tenant_id = $${values.length + 1}`);
          values.push(params.tenantId);
        }
        if (params.eventTypes?.length) {
          conditions.push(`event_type = ANY($${values.length + 1})`);
          values.push(params.eventTypes);
        }
        if (params.levels?.length) {
          conditions.push(`level = ANY($${values.length + 1})`);
          values.push(params.levels);
        }
        const { rows } = await this.pool.query(
          `
      SELECT bucket_start, bucket_end, tenant_id, event_type, level, service_id, event_count, last_event_timestamp
      FROM ${table}
      WHERE ${conditions.join(" AND ")}
      ORDER BY bucket_start ASC
    `,
          values
        );
        return rows.map((row) => ({
          bucketStart: new Date(row.bucket_start),
          bucketEnd: new Date(row.bucket_end),
          tenantId: row.tenant_id,
          eventType: row.event_type,
          level: row.level,
          serviceId: row.service_id,
          eventCount: Number(row.event_count ?? 0)
        }));
      }
      async readFromBase(params) {
        const client6 = await this.pool.connect();
        try {
          const columns = await this.getAuditColumns(client6);
          const trunc = params.granularity === "week" ? "week" : "day";
          const conditions = [
            `${columns.timestampColumn} >= $1`,
            `${columns.timestampColumn} < $2`
          ];
          const values = [params.rangeStart, params.rangeEnd];
          if (params.tenantId && columns.tenantColumn) {
            conditions.push(`${columns.tenantColumn} = $${values.length + 1}`);
            values.push(params.tenantId);
          }
          if (params.eventTypes?.length && columns.eventTypeColumn) {
            conditions.push(`${columns.eventTypeColumn} = ANY($${values.length + 1})`);
            values.push(params.eventTypes);
          }
          if (params.levels?.length && columns.levelColumn) {
            conditions.push(`${columns.levelColumn} = ANY($${values.length + 1})`);
            values.push(params.levels);
          }
          const tenantExpr = columns.tenantColumn ? `COALESCE(${columns.tenantColumn}, 'unknown')` : `'unknown'`;
          const eventTypeExpr = columns.eventTypeColumn ? `COALESCE(${columns.eventTypeColumn}, 'event')` : `'event'`;
          const levelExpr = columns.levelColumn ? `COALESCE(${columns.levelColumn}, 'info')` : `'info'`;
          const serviceExpr = columns.serviceIdColumn ? columns.serviceIdColumn : "NULL::text";
          const { rows } = await client6.query(
            `
        SELECT
          date_trunc('${trunc}', ${columns.timestampColumn})::date AS bucket_start,
          (date_trunc('${trunc}', ${columns.timestampColumn}) + INTERVAL '1 ${trunc}')::date AS bucket_end,
          ${tenantExpr} AS tenant_id,
          ${eventTypeExpr} AS event_type,
          ${levelExpr} AS level,
          ${serviceExpr} AS service_id,
          COUNT(*)::bigint AS event_count
        FROM audit_events
        WHERE ${conditions.join(" AND ")}
        GROUP BY 1,2,3,4,5,6
        ORDER BY bucket_start ASC
      `,
            values
          );
          return rows.map((row) => ({
            bucketStart: new Date(row.bucket_start),
            bucketEnd: new Date(row.bucket_end),
            tenantId: row.tenant_id,
            eventType: row.event_type,
            level: row.level,
            serviceId: row.service_id,
            eventCount: Number(row.event_count ?? 0)
          }));
        } finally {
          client6.release();
        }
      }
      truncateDate(date, granularity) {
        const copy = new Date(date);
        if (granularity === "week") {
          const day = copy.getUTCDay();
          const diff = (day === 0 ? -6 : 1) - day;
          copy.setUTCDate(copy.getUTCDate() + diff);
          copy.setUTCHours(0, 0, 0, 0);
          return copy;
        }
        copy.setUTCHours(0, 0, 0, 0);
        return copy;
      }
    };
  }
});

// src/services/AuditArchivingService.ts
import { createGzip } from "zlib";
import { pipeline } from "stream/promises";
import { Readable } from "stream";
import { createWriteStream, mkdirSync } from "fs";
import path from "path";
var AuditArchivingService, auditArchivingService;
var init_AuditArchivingService = __esm({
  "src/services/AuditArchivingService.ts"() {
    "use strict";
    init_logger2();
    init_database();
    AuditArchivingService = class _AuditArchivingService {
      static instance;
      db;
      archiveRoot;
      constructor() {
        this.db = getPostgresPool2();
        this.archiveRoot = path.join(process.cwd(), "archive/audit");
        mkdirSync(this.archiveRoot, { recursive: true });
      }
      static getInstance() {
        if (!_AuditArchivingService.instance) {
          _AuditArchivingService.instance = new _AuditArchivingService();
        }
        return _AuditArchivingService.instance;
      }
      /**
       * Extracts and archives audit events within the specified date range.
       */
      async archiveRange(startDate, endDate, tier) {
        logger_default2.info(`Starting audit archival for range ${startDate.toISOString()} - ${endDate.toISOString()} (Tier: ${tier})`);
        const bundleId = `audit_${tier.toLowerCase()}_${startDate.getTime()}_${endDate.getTime()}`;
        const bundlePath = path.join(this.archiveRoot, `${bundleId}.json.gz`);
        const query3 = `
            SELECT * FROM audit_events 
            WHERE timestamp >= $1 AND timestamp < $2
        `;
        const result2 = await this.db.query(query3, [startDate, endDate]);
        const recordCount = result2.rowCount ?? 0;
        if (recordCount === 0) {
          logger_default2.info("No records found for archival in this range.");
          return null;
        }
        const jsonData = JSON.stringify(result2.rows);
        const writeStream = createWriteStream(bundlePath);
        const gzip3 = createGzip();
        const readable = Readable.from([jsonData]);
        await pipeline(
          readable,
          gzip3,
          writeStream
        );
        const stats = await import("fs/promises").then((fs44) => fs44.stat(bundlePath));
        logger_default2.info(`Archived ${recordCount} records to ${bundlePath} (${(stats.size / 1024).toFixed(2)} KB)`);
        return {
          tier,
          bundlePath,
          recordCount,
          compressedSize: stats.size
        };
      }
    };
    auditArchivingService = AuditArchivingService.getInstance();
  }
});

// src/audit/advanced-audit-system.ts
import { randomUUID as randomUUID2, createHash as createHash5 } from "crypto";
import { EventEmitter as EventEmitter2 } from "events";
import { z as z4 } from "zod";
import jwt2 from "jsonwebtoken";
var sign, verify2, AuditEventSchema, AdvancedAuditSystem, getAuditSystem;
var init_advanced_audit_system = __esm({
  "src/audit/advanced-audit-system.ts"() {
    "use strict";
    init_database();
    init_logger2();
    init_logger();
    init_AuditTimelineRollupService();
    init_AuditArchivingService();
    ({ sign, verify: verify2 } = jwt2);
    AuditEventSchema = z4.object({
      eventType: z4.string(),
      level: z4.enum(["debug", "info", "warn", "error", "critical"]),
      correlationId: z4.string(),
      tenantId: z4.string(),
      serviceId: z4.string(),
      action: z4.string(),
      outcome: z4.enum(["success", "failure", "partial"]),
      message: z4.string(),
      details: z4.record(z4.string(), z4.any()),
      complianceRelevant: z4.boolean(),
      complianceFrameworks: z4.array(z4.string())
    });
    AdvancedAuditSystem = class _AdvancedAuditSystem extends EventEmitter2 {
      db;
      redis;
      logger;
      signingKey;
      encryptionKey;
      lastEventHash = "";
      // Configuration
      retentionPeriodDays = 2555;
      // 7 years for compliance
      retentionEnabled = true;
      retentionIntervalHours = 24;
      batchSize = 100;
      compressionEnabled = true;
      realTimeAlerting = true;
      archiveThresholdDays = 90;
      // Archive events older than 90 days
      // Caching
      eventBuffer = [];
      flushInterval;
      retentionInterval;
      rollupService;
      static instance;
      constructor(db2, redis5, logger72, signingKey, encryptionKey) {
        super();
        this.db = db2;
        this.redis = redis5;
        this.logger = logger72;
        this.rollupService = new AuditTimelineRollupService(db2);
        this.signingKey = signingKey;
        this.encryptionKey = encryptionKey;
        this.retentionEnabled = process.env.AUDIT_RETENTION_ENABLED !== "false";
        const retentionDays = Number(process.env.AUDIT_RETENTION_DAYS);
        if (!Number.isNaN(retentionDays) && retentionDays > 0) {
          this.retentionPeriodDays = retentionDays;
        }
        const retentionIntervalHours = Number(
          process.env.AUDIT_RETENTION_SWEEP_HOURS
        );
        if (!Number.isNaN(retentionIntervalHours) && retentionIntervalHours > 0) {
          this.retentionIntervalHours = retentionIntervalHours;
        }
        const archiveThreshold = Number(process.env.AUDIT_ARCHIVE_THRESHOLD_DAYS);
        if (!Number.isNaN(archiveThreshold) && archiveThreshold > 0) {
          this.archiveThresholdDays = archiveThreshold;
        }
        this.initializeSchema().catch((err) => {
          this.logger.error(
            { error: err.message },
            "Failed to initialize audit schema"
          );
        });
        this.flushInterval = setInterval(() => {
          this.flushEventBuffer().catch((err) => {
            this.logger.error(
              { error: err.message },
              "Failed to flush audit events"
            );
          });
        }, 5e3);
        const scheduleRetention = this.retentionEnabled && process.env.AUDIT_RETENTION_SCHEDULE_ENABLED !== "false" && process.env.NODE_ENV !== "test";
        if (scheduleRetention) {
          this.retentionInterval = setInterval(() => {
            this.pruneExpiredEvents().catch((err) => {
              this.logger.error(
                { error: err.message },
                "Failed to prune audit events by retention policy"
              );
            });
          }, this.retentionIntervalHours * 60 * 60 * 1e3);
        }
        process.on("SIGTERM", () => this.gracefulShutdown());
        process.on("SIGINT", () => this.gracefulShutdown());
      }
      static getInstance() {
        if (!_AdvancedAuditSystem.instance) {
          const db2 = getPostgresPool2();
          const redis5 = getRedisClient();
          const signingKey = process.env.AUDIT_SIGNING_KEY;
          const encryptionKey = process.env.AUDIT_ENCRYPTION_KEY;
          if (!signingKey || !encryptionKey) {
            if (process.env.NODE_ENV === "production") {
              throw new Error(
                "AUDIT_SIGNING_KEY and AUDIT_ENCRYPTION_KEY environment variables must be set in production. These keys are critical for audit trail integrity and compliance."
              );
            }
            logger_default2.warn(
              "AUDIT_SIGNING_KEY and/or AUDIT_ENCRYPTION_KEY not set - using insecure defaults for development only. NEVER use these defaults in production!"
            );
          }
          if (!redis5) {
            logger_default2.warn("AdvancedAuditSystem initialized without Redis. Real-time alerting will be disabled.");
          }
          _AdvancedAuditSystem.instance = new _AdvancedAuditSystem(
            db2,
            redis5,
            // If null, we'll need to handle it in methods
            logger_default2,
            signingKey || "dev-signing-key-insecure",
            encryptionKey || "dev-encryption-key-insecure"
          );
        }
        return _AdvancedAuditSystem.instance;
      }
      static createForTest(options2) {
        return new _AdvancedAuditSystem(
          options2.db,
          options2.redis ?? null,
          options2.logger,
          options2.signingKey ?? "test-signing-key",
          options2.encryptionKey ?? "test-encryption-key"
        );
      }
      async shutdown() {
        await this.gracefulShutdown();
      }
      /**
       * Record an audit event
       */
      async recordEvent(eventData) {
        try {
          const store = correlationStorage.getStore();
          const defaults = {
            correlationId: store?.get("correlationId") || randomUUID2(),
            tenantId: store?.get("tenantId") || "unknown",
            requestId: store?.get("requestId"),
            serviceId: "intelgraph-server",
            outcome: "success",
            complianceRelevant: false,
            complianceFrameworks: []
          };
          const cleanEventData = Object.fromEntries(
            Object.entries(eventData).filter(([_2, v]) => v !== void 0)
          );
          const mergedData = { ...defaults, ...cleanEventData };
          const validation = AuditEventSchema.safeParse(mergedData);
          if (!validation.success) {
            throw new Error(`Invalid audit event: ${validation.error.message}`);
          }
          const event = {
            id: randomUUID2(),
            timestamp: /* @__PURE__ */ new Date(),
            sessionId: eventData.sessionId,
            userId: eventData.userId,
            resourceType: eventData.resourceType,
            resourceId: eventData.resourceId,
            resourcePath: eventData.resourcePath,
            ipAddress: eventData.ipAddress,
            userAgent: eventData.userAgent,
            dataClassification: eventData.dataClassification,
            ...validation.data
          };
          event.hash = this.calculateEventHash(event);
          event.previousEventHash = this.lastEventHash;
          this.lastEventHash = event.hash;
          event.signature = this.signEvent(event);
          this.eventBuffer.push(event);
          if (event.level === "critical" || event.complianceRelevant) {
            await this.flushEventBuffer();
          }
          if (this.realTimeAlerting) {
            await this.processRealTimeAlerts(event);
          }
          this.emit("eventRecorded", event);
          this.logger.debug(
            {
              eventId: event.id,
              eventType: event.eventType,
              level: event.level
            },
            "Audit event recorded"
          );
          return event.id;
        } catch (error) {
          this.logger.error(
            {
              error: error.message,
              eventData
            },
            "Failed to record audit event"
          );
          throw error;
        }
      }
      /**
       * Query audit events with advanced filtering
       */
      async queryEvents(query3) {
        try {
          let sql = `
        SELECT * FROM audit_events 
        WHERE 1=1
      `;
          const params = [];
          let paramIndex = 1;
          if (query3.startTime) {
            sql += ` AND timestamp >= $${paramIndex++}`;
            params.push(query3.startTime);
          }
          if (query3.endTime) {
            sql += ` AND timestamp <= $${paramIndex++}`;
            params.push(query3.endTime);
          }
          if (query3.eventTypes?.length) {
            sql += ` AND event_type = ANY($${paramIndex++})`;
            params.push(query3.eventTypes);
          }
          if (query3.levels?.length) {
            sql += ` AND level = ANY($${paramIndex++})`;
            params.push(query3.levels);
          }
          if (query3.userIds?.length) {
            sql += ` AND user_id = ANY($${paramIndex++})`;
            params.push(query3.userIds);
          }
          if (query3.tenantIds?.length) {
            sql += ` AND tenant_id = ANY($${paramIndex++})`;
            params.push(query3.tenantIds);
          }
          if (query3.resourceTypes?.length) {
            sql += ` AND resource_type = ANY($${paramIndex++})`;
            params.push(query3.resourceTypes);
          }
          if (query3.correlationIds?.length) {
            sql += ` AND correlation_id = ANY($${paramIndex++})`;
            params.push(query3.correlationIds);
          }
          if (query3.complianceFrameworks?.length) {
            sql += ` AND compliance_frameworks && $${paramIndex++}`;
            params.push(query3.complianceFrameworks);
          }
          sql += ` ORDER BY timestamp DESC`;
          if (query3.limit) {
            sql += ` LIMIT $${paramIndex++}`;
            params.push(query3.limit);
          }
          if (query3.offset) {
            sql += ` OFFSET $${paramIndex++}`;
            params.push(query3.offset);
          }
          const result2 = await this.db.query(sql, params);
          return result2.rows.map((row) => this.deserializeEvent(row));
        } catch (error) {
          this.logger.error(
            {
              error: error.message,
              query: query3
            },
            "Failed to query audit events"
          );
          throw error;
        }
      }
      /**
       * Materialize rollup tables for timeline views (resumable + observable)
       */
      async refreshTimelineRollups(options2 = {}) {
        return this.rollupService.refreshRollups(options2);
      }
      /**
       * Read timeline buckets, switching to rollups when TIMELINE_ROLLUPS_V1=1
       */
      async getTimelineBuckets(rangeStart, rangeEnd, granularity = "day", filters = {}) {
        return this.rollupService.getTimelineBuckets({
          rangeStart,
          rangeEnd,
          granularity,
          tenantId: filters.tenantId,
          eventTypes: filters.eventTypes,
          levels: filters.levels
        });
      }
      async pruneExpiredEvents(retentionDays = this.retentionPeriodDays) {
        if (!this.retentionEnabled) {
          return 0;
        }
        const effectiveDays = Math.max(0, retentionDays);
        if (effectiveDays === 0) {
          return 0;
        }
        if (this.archiveThresholdDays < effectiveDays) {
          const archiver6 = AuditArchivingService.getInstance();
          const archiveEnd = new Date(Date.now() - this.archiveThresholdDays * 24 * 60 * 60 * 1e3);
          const archiveStart = new Date(Date.now() - effectiveDays * 24 * 60 * 60 * 1e3);
          try {
            await archiver6.archiveRange(archiveStart, archiveEnd, "COLD");
          } catch (err) {
            this.logger.error({ error: err.message }, "Audit archival failed - continuing with pruning");
          }
        }
        const result2 = await this.db.query(
          `DELETE FROM audit_events
       WHERE timestamp < NOW() - ($1 * INTERVAL '1 day')`,
          [effectiveDays]
        );
        const deleted = result2.rowCount ?? 0;
        this.logger.info(
          { deleted, retentionDays: effectiveDays },
          "Audit retention cleanup completed"
        );
        return deleted;
      }
      /**
       * Generate compliance report
       */
      async generateComplianceReport(framework, startDate, endDate) {
        try {
          const events = await this.queryEvents({
            startTime: startDate,
            endTime: endDate,
            complianceFrameworks: [framework]
          });
          const violations = this.analyzeComplianceViolations(events, framework);
          const complianceScore = this.calculateComplianceScore(
            events,
            violations,
            framework
          );
          const recommendations = this.generateComplianceRecommendations(
            violations,
            framework
          );
          const report = {
            framework,
            period: { start: startDate, end: endDate },
            summary: {
              totalEvents: events.length,
              criticalEvents: events.filter((e) => e.level === "critical").length,
              violations: violations.length,
              complianceScore
            },
            violations,
            recommendations
          };
          await this.storeComplianceReport(report);
          this.logger.info(
            {
              framework,
              period: { start: startDate, end: endDate },
              score: complianceScore,
              violations: violations.length
            },
            "Compliance report generated"
          );
          return report;
        } catch (error) {
          this.logger.error(
            {
              error: error.message,
              framework,
              period: { start: startDate, end: endDate }
            },
            "Failed to generate compliance report"
          );
          throw error;
        }
      }
      /**
       * Perform forensic analysis on a correlation ID
       */
      async performForensicAnalysis(correlationId) {
        try {
          const events = await this.queryEvents({
            correlationIds: [correlationId]
          });
          if (events.length === 0) {
            throw new Error(`No events found for correlation ID: ${correlationId}`);
          }
          const actorMap = /* @__PURE__ */ new Map();
          for (const event of events) {
            if (event.userId) {
              const existing = actorMap.get(event.userId) || {
                actions: 0,
                events: []
              };
              existing.actions++;
              existing.events.push(event);
              actorMap.set(event.userId, existing);
            }
          }
          const actors = Array.from(actorMap.entries()).map(([userId, data]) => ({
            userId,
            actions: data.actions,
            riskScore: this.calculateActorRiskScore(data.events)
          }));
          const resourceMap = /* @__PURE__ */ new Map();
          for (const event of events) {
            if (event.resourceId) {
              const existing = resourceMap.get(event.resourceId) || {
                accessCount: 0,
                lastAccessed: /* @__PURE__ */ new Date(0)
              };
              existing.accessCount++;
              if (event.timestamp > existing.lastAccessed) {
                existing.lastAccessed = event.timestamp;
              }
              resourceMap.set(event.resourceId, existing);
            }
          }
          const resources2 = Array.from(resourceMap.entries()).map(
            ([resourceId, data]) => ({
              resourceId,
              accessCount: data.accessCount,
              lastAccessed: data.lastAccessed
            })
          );
          const anomalies = await this.detectAnomalies(events);
          const analysis = {
            correlationId,
            timeline: events.sort(
              (a, b) => a.timestamp.getTime() - b.timestamp.getTime()
            ),
            actors,
            resources: resources2,
            anomalies
          };
          await this.storeForensicAnalysis(analysis);
          this.logger.info(
            {
              correlationId,
              eventCount: events.length,
              actorCount: actors.length,
              resourceCount: resources2.length,
              anomalyCount: anomalies.length
            },
            "Forensic analysis completed"
          );
          return analysis;
        } catch (error) {
          this.logger.error(
            {
              error: error.message,
              correlationId
            },
            "Failed to perform forensic analysis"
          );
          throw error;
        }
      }
      /**
       * Verify audit trail integrity
       */
      async verifyIntegrity(startDate, endDate) {
        try {
          const events = await this.queryEvents({
            startTime: startDate,
            endTime: endDate
          });
          let validEvents = 0;
          const invalidEvents = [];
          let expectedPreviousHash = "";
          for (const event of events) {
            const calculatedHash = this.calculateEventHash(event);
            if (event.hash !== calculatedHash) {
              invalidEvents.push({
                eventId: event.id,
                issue: "Hash mismatch - possible tampering"
              });
              continue;
            }
            if (!this.verifyEventSignature(event)) {
              invalidEvents.push({
                eventId: event.id,
                issue: "Invalid signature"
              });
              continue;
            }
            if (expectedPreviousHash && event.hash !== expectedPreviousHash) {
              invalidEvents.push({
                eventId: event.id,
                issue: "Chain integrity violation: Hash mismatch with successor record"
              });
            }
            expectedPreviousHash = event.previousEventHash || "";
            validEvents++;
          }
          const result2 = {
            valid: invalidEvents.length === 0,
            totalEvents: events.length,
            validEvents,
            invalidEvents
          };
          this.logger.info(result2, "Audit trail integrity verification completed");
          return result2;
        } catch (error) {
          this.logger.error(
            { error: error.message },
            "Failed to verify audit trail integrity"
          );
          throw error;
        }
      }
      /**
       * Private helper methods
       */
      async initializeSchema() {
        const schema2 = `
      CREATE TABLE IF NOT EXISTS audit_events (
        id UUID PRIMARY KEY,
        event_type TEXT NOT NULL,
        level TEXT NOT NULL,
        timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
        correlation_id UUID,
        session_id UUID,
        request_id UUID,
        user_id TEXT,
        tenant_id TEXT NOT NULL,
        service_id TEXT NOT NULL,
        resource_type TEXT,
        resource_id TEXT,
        resource_path TEXT,
        action TEXT NOT NULL,
        outcome TEXT NOT NULL,
        message TEXT NOT NULL,
        details JSONB DEFAULT '{}',
        ip_address INET,
        user_agent TEXT,
        compliance_relevant BOOLEAN DEFAULT FALSE,
        compliance_frameworks TEXT[] DEFAULT '{}',
        data_classification TEXT,
        hash TEXT,
        signature TEXT,
        previous_event_hash TEXT,
        created_at TIMESTAMPTZ DEFAULT NOW()
      );

      CREATE INDEX IF NOT EXISTS idx_audit_events_timestamp ON audit_events(timestamp DESC);
      CREATE INDEX IF NOT EXISTS idx_audit_events_correlation_id ON audit_events(correlation_id);
      CREATE INDEX IF NOT EXISTS idx_audit_events_user_id ON audit_events(user_id);
      CREATE INDEX IF NOT EXISTS idx_audit_events_tenant_id ON audit_events(tenant_id);
      CREATE INDEX IF NOT EXISTS idx_audit_events_event_type ON audit_events(event_type);
      CREATE INDEX IF NOT EXISTS idx_audit_events_level ON audit_events(level);
      CREATE INDEX IF NOT EXISTS idx_audit_events_compliance ON audit_events(compliance_relevant) WHERE compliance_relevant = true;

      CREATE TABLE IF NOT EXISTS compliance_reports (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        framework TEXT NOT NULL,
        period_start TIMESTAMPTZ NOT NULL,
        period_end TIMESTAMPTZ NOT NULL,
        report_data JSONB NOT NULL,
        generated_at TIMESTAMPTZ DEFAULT NOW()
      );

      CREATE TABLE IF NOT EXISTS forensic_analyses (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        correlation_id UUID NOT NULL,
        analysis_data JSONB NOT NULL,
        created_at TIMESTAMPTZ DEFAULT NOW()
      );
    `;
        await this.db.query(schema2);
      }
      async flushEventBuffer() {
        if (this.eventBuffer.length === 0) return;
        const eventsToFlush = this.eventBuffer.splice(0);
        try {
          const values = eventsToFlush.map((event) => [
            event.id,
            event.eventType,
            event.level,
            event.timestamp,
            event.correlationId,
            event.sessionId,
            event.requestId,
            event.userId,
            event.tenantId,
            event.serviceId,
            event.resourceType,
            event.resourceId,
            event.resourcePath,
            event.action,
            event.outcome,
            event.message,
            JSON.stringify(event.details),
            event.ipAddress,
            event.userAgent,
            event.complianceRelevant,
            event.complianceFrameworks,
            event.dataClassification,
            event.hash,
            event.signature,
            event.previousEventHash
          ]);
          const placeholders = values.map(
            (_2, i) => `($${i * 25 + 1}, $${i * 25 + 2}, $${i * 25 + 3}, $${i * 25 + 4}, $${i * 25 + 5}, 
         $${i * 25 + 6}, $${i * 25 + 7}, $${i * 25 + 8}, $${i * 25 + 9}, $${i * 25 + 10},
         $${i * 25 + 11}, $${i * 25 + 12}, $${i * 25 + 13}, $${i * 25 + 14}, $${i * 25 + 15},
         $${i * 25 + 16}, $${i * 25 + 17}, $${i * 25 + 18}, $${i * 25 + 19}, $${i * 25 + 20},
         $${i * 25 + 21}, $${i * 25 + 22}, $${i * 25 + 23}, $${i * 25 + 24}, $${i * 25 + 25})`
          ).join(", ");
          const query3 = `
        INSERT INTO audit_events (
          id, event_type, level, timestamp, correlation_id, session_id, request_id,
          user_id, tenant_id, service_id, resource_type, resource_id, resource_path,
          action, outcome, message, details, ip_address, user_agent, compliance_relevant,
          compliance_frameworks, data_classification, hash, signature, previous_event_hash
        ) VALUES ${placeholders}
      `;
          await this.db.query(query3, values.flat());
          this.logger.debug(
            {
              flushedEvents: eventsToFlush.length
            },
            "Audit events flushed to database"
          );
        } catch (error) {
          this.eventBuffer.unshift(...eventsToFlush);
          throw error;
        }
      }
      calculateEventHash(event) {
        const hashableData = {
          id: event.id,
          eventType: event.eventType,
          timestamp: event.timestamp.toISOString(),
          correlationId: event.correlationId,
          tenantId: event.tenantId,
          serviceId: event.serviceId,
          action: event.action,
          message: event.message,
          details: event.details
        };
        return createHash5("sha256").update(JSON.stringify(hashableData, Object.keys(hashableData).sort())).digest("hex");
      }
      signEvent(event) {
        return sign(
          {
            id: event.id,
            hash: event.hash,
            timestamp: event.timestamp.toISOString()
          },
          this.signingKey,
          { algorithm: "HS256" }
        );
      }
      verifyEventSignature(event) {
        try {
          const payload = verify2(event.signature, this.signingKey);
          return payload.id === event.id && payload.hash === event.hash;
        } catch {
          return false;
        }
      }
      deserializeEvent(row) {
        return {
          id: row.id,
          eventType: row.event_type,
          level: row.level,
          timestamp: row.timestamp,
          correlationId: row.correlation_id,
          sessionId: row.session_id,
          requestId: row.request_id,
          userId: row.user_id,
          tenantId: row.tenant_id,
          serviceId: row.service_id,
          resourceType: row.resource_type,
          resourceId: row.resource_id,
          resourcePath: row.resource_path,
          action: row.action,
          outcome: row.outcome,
          message: row.message,
          details: row.details,
          ipAddress: row.ip_address,
          userAgent: row.user_agent,
          complianceRelevant: row.compliance_relevant,
          complianceFrameworks: row.compliance_frameworks,
          dataClassification: row.data_classification,
          hash: row.hash,
          signature: row.signature,
          previousEventHash: row.previous_event_hash
        };
      }
      analyzeComplianceViolations(events, framework) {
        const violations = [];
        switch (framework) {
          case "SOX":
            violations.push(...this.detectSoxViolations(events));
            break;
          case "GDPR":
            violations.push(...this.detectGdprViolations(events));
            break;
          case "SOC2":
            violations.push(...this.detectSoc2Violations(events));
            break;
        }
        return violations;
      }
      detectSoxViolations(events) {
        const violations = [];
        const financialAccess = events.filter(
          (e) => e.resourceType === "financial_data" && e.outcome === "failure"
        );
        for (const event of financialAccess) {
          violations.push({
            eventId: event.id,
            violationType: "unauthorized_financial_access",
            severity: "high",
            description: "Unauthorized access attempt to financial data",
            remediation: "Review user permissions and implement additional access controls"
          });
        }
        return violations;
      }
      detectGdprViolations(events) {
        const violations = [];
        const dataExports = events.filter(
          (e) => e.eventType === "data_export" && e.dataClassification === "restricted"
        );
        for (const event of dataExports) {
          violations.push({
            eventId: event.id,
            violationType: "unauthorized_data_export",
            severity: "critical",
            description: "Export of restricted personal data without proper approval",
            remediation: "Implement data export approval workflow and review data handling procedures"
          });
        }
        return violations;
      }
      detectSoc2Violations(events) {
        return [];
      }
      calculateComplianceScore(events, violations, framework) {
        if (events.length === 0) return 100;
        const criticalViolations = violations.filter(
          (v) => v.severity === "critical"
        ).length;
        const highViolations = violations.filter(
          (v) => v.severity === "high"
        ).length;
        const mediumViolations = violations.filter(
          (v) => v.severity === "medium"
        ).length;
        const lowViolations = violations.filter((v) => v.severity === "low").length;
        const totalPenalty = criticalViolations * 20 + highViolations * 10 + mediumViolations * 5 + lowViolations * 1;
        const score = Math.max(0, 100 - totalPenalty / events.length * 100);
        return Math.round(score * 100) / 100;
      }
      generateComplianceRecommendations(violations, framework) {
        const recommendations = [];
        const criticalCount = violations.filter(
          (v) => v.severity === "critical"
        ).length;
        if (criticalCount > 0) {
          recommendations.push(
            `Address ${criticalCount} critical violations immediately`
          );
        }
        const highCount = violations.filter((v) => v.severity === "high").length;
        if (highCount > 0) {
          recommendations.push(
            `Review and remediate ${highCount} high-severity violations`
          );
        }
        switch (framework) {
          case "GDPR":
            recommendations.push("Implement data processing impact assessments");
            recommendations.push("Review consent management procedures");
            break;
          case "SOX":
            recommendations.push("Strengthen financial data access controls");
            recommendations.push("Implement segregation of duties");
            break;
        }
        return recommendations;
      }
      calculateActorRiskScore(events) {
        let riskScore = 0;
        const failures = events.filter((e) => e.outcome === "failure").length;
        riskScore += failures * 10;
        const afterHours = events.filter((e) => {
          const hour = e.timestamp.getHours();
          return hour < 8 || hour > 18;
        }).length;
        riskScore += afterHours * 5;
        const sensitiveAccess = events.filter(
          (e) => e.dataClassification === "restricted" || e.dataClassification === "confidential"
        ).length;
        riskScore += sensitiveAccess * 15;
        return Math.min(100, riskScore);
      }
      async detectAnomalies(events) {
        const anomalies = [];
        const timeSpan = events.length > 0 ? events[events.length - 1].timestamp.getTime() - events[0].timestamp.getTime() : 0;
        if (timeSpan > 0) {
          const avgInterval = timeSpan / events.length;
          let burstCount = 0;
          for (let i = 1; i < events.length; i++) {
            const interval = events[i].timestamp.getTime() - events[i - 1].timestamp.getTime();
            if (interval < avgInterval * 0.1) {
              burstCount++;
            }
          }
          if (burstCount > events.length * 0.3) {
            anomalies.push({
              type: "burst_activity",
              description: "Unusual burst of rapid consecutive actions detected",
              severity: 70,
              events: events.map((e) => e.id)
            });
          }
        }
        const failures = events.filter((e) => e.outcome === "failure");
        if (failures.length > events.length * 0.5) {
          anomalies.push({
            type: "repeated_failures",
            description: "High rate of failed operations indicating possible attack",
            severity: 85,
            events: failures.map((e) => e.id)
          });
        }
        return anomalies;
      }
      async processRealTimeAlerts(event) {
        if (!this.redis) return;
        if (event.level === "critical" || event.eventType === "security_alert") {
          await this.redis.publish("audit:critical", JSON.stringify(event));
        }
        if (event.complianceRelevant) {
          await this.redis.publish("audit:compliance", JSON.stringify(event));
        }
      }
      async storeComplianceReport(report) {
        await this.db.query(
          `
      INSERT INTO compliance_reports (framework, period_start, period_end, report_data)
      VALUES ($1, $2, $3, $4)
    `,
          [
            report.framework,
            report.period.start,
            report.period.end,
            JSON.stringify(report)
          ]
        );
      }
      async storeForensicAnalysis(analysis) {
        await this.db.query(
          `
      INSERT INTO forensic_analyses (correlation_id, analysis_data)
      VALUES ($1, $2)
    `,
          [analysis.correlationId, JSON.stringify(analysis)]
        );
      }
      async gracefulShutdown() {
        this.logger.info("Shutting down audit system gracefully");
        clearInterval(this.flushInterval);
        if (this.retentionInterval) {
          clearInterval(this.retentionInterval);
        }
        await this.flushEventBuffer();
        this.logger.info("Audit system shutdown complete");
      }
    };
    getAuditSystem = () => AdvancedAuditSystem.getInstance();
  }
});

// src/audit/emit.ts
async function emitAuditEvent(event, options2 = {}) {
  const outcome = event.action.outcome ?? "success";
  const details = {
    auditVersion: "audit.v1",
    eventId: event.eventId,
    occurredAt: event.occurredAt,
    actor: event.actor,
    target: event.target,
    traceId: event.traceId,
    metadata: event.metadata ?? {}
  };
  return advancedAuditSystem.recordEvent({
    eventType: event.action.type,
    level: options2.level ?? "info",
    correlationId: options2.correlationId ?? event.traceId ?? event.eventId,
    tenantId: event.tenantId,
    serviceId: options2.serviceId ?? "server",
    action: event.action.type,
    outcome,
    message: typeof event.metadata?.message === "string" ? event.metadata.message : `audit.${event.action.type}`,
    details,
    complianceRelevant: options2.complianceRelevant ?? false,
    complianceFrameworks: options2.complianceFrameworks ?? [],
    userId: event.actor.id,
    resourceType: event.target?.type,
    resourceId: event.target?.id,
    resourcePath: event.target?.path,
    ipAddress: event.actor.ipAddress,
    requestId: typeof event.metadata?.requestId === "string" ? event.metadata.requestId : void 0,
    sessionId: typeof event.metadata?.sessionId === "string" ? event.metadata.sessionId : void 0,
    userAgent: typeof event.metadata?.userAgent === "string" ? event.metadata.userAgent : void 0
  });
}
var init_emit = __esm({
  "src/audit/emit.ts"() {
    "use strict";
    init_audit2();
  }
});

// src/audit/index.ts
import { Pool as Pool4 } from "pg";
import Redis4 from "ioredis";
import pino12 from "pino";
var instance, getAuditSystem2, advancedAuditSystem;
var init_audit2 = __esm({
  "src/audit/index.ts"() {
    "use strict";
    init_advanced_audit_system();
    init_emit();
    init_config();
    instance = null;
    getAuditSystem2 = () => {
      if (!instance) {
        const logger72 = pino12({ name: "audit-system" });
        const db2 = new Pool4({ connectionString: dbUrls.postgres });
        const redis5 = new Redis4(dbUrls.redis, {
          password: cfg.REDIS_PASSWORD || void 0
        });
        const signingKey = process.env.AUDIT_SIGNING_KEY || cfg.JWT_SECRET;
        const encryptionKey = process.env.AUDIT_ENCRYPTION_KEY || cfg.JWT_REFRESH_SECRET;
        instance = new AdvancedAuditSystem(db2, redis5, logger72, signingKey, encryptionKey);
      }
      return instance;
    };
    advancedAuditSystem = {
      logEvent: async (event) => {
        const sys = getAuditSystem2();
        return sys.recordEvent(event);
      },
      // Expose other methods if needed, or consumers can use getAuditSystem()
      recordEvent: async (event) => {
        return getAuditSystem2().recordEvent(event);
      },
      queryEvents: async (query3) => {
        return getAuditSystem2().queryEvents(query3);
      },
      refreshTimelineRollups: async (options2) => {
        return getAuditSystem2().refreshTimelineRollups(options2 ?? {});
      },
      getTimelineBuckets: async (rangeStart, rangeEnd, granularity = "day", filters = {}) => {
        return getAuditSystem2().getTimelineBuckets(
          rangeStart,
          rangeEnd,
          granularity,
          filters
        );
      }
    };
  }
});

// src/audit/worm.ts
import * as fs from "fs";
import * as path2 from "path";
async function putLocked(bucket, key, body4, days = Number(process.env.AUDIT_RETENTION_DAYS || 365)) {
  if (s3) {
    const retainUntil = new Date(Date.now() + days * 86400 * 1e3);
    const { PutObjectCommand: PutObjectCommand2 } = __require("@aws-sdk/client-s3");
    await s3.send(
      new PutObjectCommand2({
        Bucket: bucket,
        Key: key,
        Body: body4,
        ObjectLockMode: "COMPLIANCE",
        // Strict compliance mode
        ObjectLockRetainUntilDate: retainUntil
      })
    );
    return `s3://${bucket}/${key}`;
  } else {
    const filePath = path2.join(LOCAL_WORM_DIR, key.replace(/\//g, "_"));
    if (fs.existsSync(filePath)) {
      throw new Error(`WORM Violation: File ${key} already exists and cannot be overwritten.`);
    }
    fs.writeFileSync(filePath, body4);
    fs.chmodSync(filePath, 292);
    return `file://${filePath}`;
  }
}
var s3, LOCAL_WORM_DIR;
var init_worm = __esm({
  "src/audit/worm.ts"() {
    "use strict";
    s3 = process.env.AWS_S3_BUCKET ? (() => {
      try {
        const { S3Client: S3Client2 } = __require("@aws-sdk/client-s3");
        return new S3Client2({});
      } catch (err) {
        console.warn(
          "AWS SDK not installed. S3 WORM storage unavailable. Install @aws-sdk/client-s3 for S3 support."
        );
        return null;
      }
    })() : null;
    LOCAL_WORM_DIR = path2.resolve(process.cwd(), "worm_storage");
    if (!s3) {
      if (!fs.existsSync(LOCAL_WORM_DIR)) {
        fs.mkdirSync(LOCAL_WORM_DIR, { recursive: true });
      }
    }
  }
});

// src/provenance/CanonicalGraphService.ts
import * as crypto8 from "crypto";
var CanonicalGraphService;
var init_CanonicalGraphService = __esm({
  "src/provenance/CanonicalGraphService.ts"() {
    "use strict";
    init_neo4j();
    CanonicalGraphService = class _CanonicalGraphService {
      static instance;
      constructor() {
      }
      static getInstance() {
        if (!_CanonicalGraphService.instance) {
          _CanonicalGraphService.instance = new _CanonicalGraphService();
        }
        return _CanonicalGraphService.instance;
      }
      // Ensure indices exist
      static async initializeIndices() {
        const session = neo.session();
        try {
          await session.run(`CREATE INDEX canonical_id IF NOT EXISTS FOR (n:CanonicalNode) ON (n.id)`);
          await session.run(`CREATE INDEX canonical_tenant IF NOT EXISTS FOR (n:CanonicalNode) ON (n.tenantId)`);
        } catch (e) {
          console.warn("Failed to create indices", e);
        } finally {
          await session.close();
        }
      }
      /**
       * Projects a raw ProvenanceEntry from the Ledger into the Canonical Graph.
       * This is an idempotent operation.
       */
      async projectEntry(entry) {
        const nodeType = this.determineNodeType(entry);
        if (!nodeType) return;
        const session = neo.session();
        try {
          await session.run(`
        MERGE (n:CanonicalNode {id: $id})
        SET n.tenantId = $tenantId,
            n.nodeType = $nodeType,
            n.subType = $subType,
            n.label = $label,
            n.timestamp = $timestamp,
            n.metadata = $metadata,
            n.properties = $properties,
            n.hash = $hash,
            n.sourceEntryId = $entryId

        WITH n
        CALL apoc.create.addLabels(n, [$nodeType]) YIELD node
        RETURN node
      `, {
            id: entry.resourceId,
            tenantId: entry.tenantId,
            nodeType,
            subType: entry.resourceType,
            label: `${entry.resourceType}:${entry.resourceId}`,
            timestamp: entry.timestamp.toISOString(),
            metadata: JSON.stringify(entry.metadata || {}),
            properties: JSON.stringify(this.extractProperties(entry, nodeType)),
            hash: entry.currentHash,
            entryId: entry.id
          });
          const sourceIds = this.extractSourceIds(entry);
          if (sourceIds.length > 0) {
            for (const sourceId of sourceIds) {
              const sourceNodeResult = await session.run(`MATCH (n:CanonicalNode {id: $id}) RETURN n.nodeType as type`, { id: sourceId });
              let relationType = "DERIVED_FROM";
              if (sourceNodeResult.records.length > 0) {
                const sourceType = sourceNodeResult.records[0].get("type");
                relationType = this.determineRelationType(sourceType, nodeType);
              }
              await session.run(`
            MERGE (source:CanonicalNode {id: $sourceId, tenantId: $tenantId})
            ON CREATE SET source.nodeType = 'Unknown', source.label = 'Unknown:' + $sourceId, source.timestamp = $timestamp

            MATCH (target:CanonicalNode {id: $targetId})
            WHERE source.tenantId = target.tenantId // Tenant Isolation Check
            MERGE (source)-[r:${relationType} {timestamp: $timestamp}]->(target)
            SET r.isTentative = ${relationType === "DERIVED_FROM" ? "true" : "false"}
           `, {
                sourceId,
                targetId: entry.resourceId,
                timestamp: entry.timestamp.toISOString(),
                tenantId: entry.tenantId
              });
            }
          }
          const outgoingResult = await session.run(`
        MATCH (a:CanonicalNode {id: $id})-[r:DERIVED_FROM]->(b:CanonicalNode)
        WHERE a.tenantId = $tenantId AND r.isTentative = true
        RETURN b.id as targetId, b.nodeType as targetType
      `, { id: entry.resourceId, tenantId: entry.tenantId });
          for (const rec of outgoingResult.records) {
            const targetType = rec.get("targetType");
            const targetId = rec.get("targetId");
            if (targetType && targetType !== "Unknown") {
              const correctRelation = this.determineRelationType(nodeType, targetType);
              if (correctRelation !== "DERIVED_FROM") {
                await session.run(`
                    MATCH (a:CanonicalNode {id: $id})-[old:DERIVED_FROM]->(b:CanonicalNode {id: $targetId})
                    WHERE a.tenantId = $tenantId
                    MERGE (a)-[new:${correctRelation}]->(b)
                    SET new = old
                    SET new.isTentative = false
                    DELETE old
                  `, { id: entry.resourceId, tenantId: entry.tenantId, targetId });
              }
            }
          }
        } catch (error) {
          console.error("Failed to project provenance entry to graph", error);
          throw error;
        } finally {
          await session.close();
        }
      }
      /**
       * Explains the causality of a node by tracing back to Inputs.
       */
      async explainCausality(nodeId, tenantId, depth = 5) {
        const maxDepth = Math.min(depth || 5, 20);
        const session = neo.session();
        try {
          const result2 = await session.run(`
        MATCH path = (start:CanonicalNode)-[*1..${maxDepth}]->(end:CanonicalNode {id: $nodeId, tenantId: $tenantId})
        RETURN path
      `, { nodeId, tenantId });
          const nodes = [];
          const edges = [];
          const seenNodes = /* @__PURE__ */ new Set();
          result2.records.forEach((record2) => {
            const path55 = record2.get("path");
            path55.segments.forEach((segment) => {
              if (!seenNodes.has(segment.start.properties.id)) {
                nodes.push(this.mapNeo4jNode(segment.start.properties));
                seenNodes.add(segment.start.properties.id);
              }
              if (!seenNodes.has(segment.end.properties.id)) {
                nodes.push(this.mapNeo4jNode(segment.end.properties));
                seenNodes.add(segment.end.properties.id);
              }
              edges.push({
                sourceId: segment.start.properties.id,
                targetId: segment.end.properties.id,
                relation: segment.relationship.type,
                timestamp: segment.relationship.properties.timestamp,
                properties: segment.relationship.properties
              });
            });
          });
          return { nodes, edges };
        } finally {
          await session.close();
        }
      }
      /**
       * Calculates the diff between two nodes.
       */
      async getGraphDiff(startNodeId, endNodeId, tenantId) {
        const startGraph = await this.explainCausality(startNodeId, tenantId, 1);
        const endGraph = await this.explainCausality(endNodeId, tenantId, 1);
        const startNodesMap = new Map(startGraph.nodes.map((n) => [n.id, n]));
        const endNodesMap = new Map(endGraph.nodes.map((n) => [n.id, n]));
        const additions = [];
        const deletions = [];
        const modifications = [];
        endGraph.nodes.forEach((node) => {
          if (!startNodesMap.has(node.id)) {
            additions.push(node);
          }
        });
        startGraph.nodes.forEach((node) => {
          if (!endNodesMap.has(node.id)) {
            deletions.push(node);
          } else {
            const endNode = endNodesMap.get(node.id);
            const diffs = this.compareNodeProperties(node, endNode);
            modifications.push(...diffs);
          }
        });
        return { additions, deletions, modifications };
      }
      compareNodeProperties(startNode, endNode) {
        const changes = [];
        const startProps = startNode.properties || {};
        const endProps = endNode.properties || {};
        const allKeys = /* @__PURE__ */ new Set([...Object.keys(startProps), ...Object.keys(endProps)]);
        allKeys.forEach((key) => {
          const val1 = startProps[key];
          const val2 = endProps[key];
          if (JSON.stringify(val1) !== JSON.stringify(val2)) {
            changes.push({
              nodeId: startNode.id,
              field: key,
              oldValue: val1,
              newValue: val2
            });
          }
        });
        const systemFields = /* @__PURE__ */ new Set(["id", "tenantId", "nodeType", "subType", "label", "timestamp", "metadata", "hash", "sourceEntryId"]);
        Object.keys(startNode).forEach((key) => {
          if (systemFields.has(key)) return;
          const val1 = startNode[key];
          const val2 = endNode[key];
          if (JSON.stringify(val1) !== JSON.stringify(val2)) {
            if (!changes.some((c) => c.field === key)) {
              changes.push({
                nodeId: startNode.id,
                field: key,
                oldValue: val1,
                newValue: val2
              });
            }
          }
        });
        return changes;
      }
      /**
       * Export the full provenance graph for a tenant.
       */
      async exportGraph(tenantId, options2 = {}) {
        const session = neo.session();
        try {
          let query3 = `MATCH (n:CanonicalNode {tenantId: $tenantId})`;
          const params = { tenantId };
          if (options2.from) {
            query3 += ` WHERE datetime(n.timestamp) >= datetime($from)`;
            params.from = options2.from.toISOString();
          }
          if (options2.to) {
            query3 += options2.from ? ` AND ` : ` WHERE `;
            query3 += ` datetime(n.timestamp) <= datetime($to)`;
            params.to = options2.to.toISOString();
          }
          query3 += ` OPTIONAL MATCH (n)-[r]->(m) RETURN n, r, m`;
          const result2 = await session.run(query3, params);
          const nodes = /* @__PURE__ */ new Map();
          const edges = [];
          result2.records.forEach((rec) => {
            const nProps = rec.get("n").properties;
            nodes.set(nProps.id, this.mapNeo4jNode(nProps));
            const m = rec.get("m");
            const r = rec.get("r");
            if (m && r) {
              const mProps = m.properties;
              nodes.set(mProps.id, this.mapNeo4jNode(mProps));
              edges.push({
                sourceId: nProps.id,
                targetId: mProps.id,
                relation: r.type,
                timestamp: r.properties.timestamp,
                properties: r.properties
              });
            }
          });
          const exportData2 = {
            nodes: Array.from(nodes.values()),
            edges,
            generatedAt: (/* @__PURE__ */ new Date()).toISOString()
          };
          const sortedNodes = exportData2.nodes.sort((a, b) => a.id.localeCompare(b.id));
          const sortedEdges = exportData2.edges.sort(
            (a, b) => (a.sourceId + a.targetId + a.relation + a.timestamp).localeCompare(b.sourceId + b.targetId + b.relation + b.timestamp)
          );
          const checksum = crypto8.createHash("sha256").update(JSON.stringify({ nodes: sortedNodes, edges: sortedEdges })).digest("hex");
          return {
            ...exportData2,
            checksum: `sha256:${checksum}`
          };
        } finally {
          await session.close();
        }
      }
      /**
       * Integrity Check: Finds nodes with no incoming or outgoing relationships.
       */
      async checkIntegrity(tenantId) {
        const session = neo.session();
        try {
          const orphansResult = await session.run(`
        MATCH (n:CanonicalNode {tenantId: $tenantId})
        WHERE NOT (n)--() AND n.nodeType <> 'Input'
        RETURN n.id as id
      `, { tenantId });
          const orphans = orphansResult.records.map((r) => r.get("id"));
          const crossTenantResult = await session.run(`
        MATCH (a:CanonicalNode)-[r]-(b:CanonicalNode)
        WHERE a.tenantId <> b.tenantId
        RETURN count(r) as violations
      `);
          const crossTenantViolations = crossTenantResult.records[0].get("violations").toNumber();
          return { orphans, crossTenantViolations };
        } finally {
          await session.close();
        }
      }
      mapNeo4jNode(props) {
        const baseNode = {
          id: props.id,
          tenantId: props.tenantId,
          nodeType: props.nodeType,
          subType: props.subType,
          label: props.label,
          timestamp: props.timestamp,
          metadata: props.metadata ? JSON.parse(props.metadata) : {},
          hash: props.hash,
          sourceEntryId: props.sourceEntryId
        };
        if (props.properties) {
          const extraProps = JSON.parse(props.properties);
          return { ...baseNode, ...extraProps };
        }
        return baseNode;
      }
      determineNodeType(entry) {
        const type = entry.resourceType.toLowerCase();
        if (["document", "configuration", "policy", "prompt"].includes(type)) return "Input";
        if (["evaluation", "approval", "decision", "prediction"].includes(type)) return "Decision";
        if (["run", "job", "task", "action", "ingest"].includes(type)) return "Action";
        if (["metric", "alert", "report", "state", "outcome"].includes(type)) return "Outcome";
        if (entry.actionType === "EVALUATE") return "Decision";
        if (entry.actionType === "EXECUTE") return "Action";
        return null;
      }
      extractSourceIds(entry) {
        const ids = [];
        const payload = entry.payload;
        if (payload.sourceId) ids.push(payload.sourceId);
        if (Array.isArray(payload.sourceIds)) ids.push(...payload.sourceIds);
        if (payload.previousState?.id) ids.push(payload.previousState.id);
        if (entry.metadata?.derivedFrom) {
          if (Array.isArray(entry.metadata.derivedFrom)) {
            ids.push(...entry.metadata.derivedFrom);
          } else {
            ids.push(entry.metadata.derivedFrom);
          }
        }
        return ids;
      }
      determineRelationType(sourceType, targetType) {
        if (sourceType === "Input" && targetType === "Decision") return "FED_INTO";
        if (sourceType === "Input" && targetType === "Action") return "USED_BY";
        if (sourceType === "Decision" && targetType === "Action") return "TRIGGERED";
        if (sourceType === "Action" && targetType === "Outcome") return "PRODUCED";
        if (sourceType === "Outcome" && targetType === "Decision") return "AFFECTED";
        if (sourceType === "Action" && targetType === "Input") return "GENERATED";
        return "DERIVED_FROM";
      }
      extractProperties(entry, nodeType) {
        const payload = entry.payload;
        const props = {};
        if (nodeType === "Decision") {
          if (payload.result) props.result = payload.result;
          if (payload.confidence) props.confidence = payload.confidence;
          if (payload.evaluator) props.evaluator = payload.evaluator;
          if (payload.outcome && !props.result) props.result = payload.outcome;
        }
        if (nodeType === "Action") {
          if (payload.status) props.status = payload.status;
          if (payload.durationMs) props.durationMs = payload.durationMs;
          if (payload.startedAt) props.startedAt = payload.startedAt;
          if (payload.completedAt) props.completedAt = payload.completedAt;
        }
        if (nodeType === "Input") {
          if (payload.uri) props.uri = payload.uri;
          if (payload.version) props.version = payload.version;
          if (payload.path) props.uri = payload.path;
        }
        if (nodeType === "Outcome") {
          if (payload.value) props.value = payload.value;
          if (payload.dimension) props.dimension = payload.dimension;
        }
        return props;
      }
    };
  }
});

// src/provenance/ledger.ts
import { Counter as Counter4, Histogram as Histogram3, Gauge as Gauge3 } from "prom-client";
import { EventEmitter as EventEmitter3 } from "events";
import * as crypto9 from "crypto";
import { execSync } from "child_process";
var tracer3, ledgerEntries, ledgerOperationTime, ledgerChainHeight, ledgerIntegrityStatus, ProvenanceLedgerV2, provenanceLedger;
var init_ledger = __esm({
  "src/provenance/ledger.ts"() {
    "use strict";
    init_pg();
    init_crypto();
    init_witness();
    init_audit2();
    init_worm();
    init_CanonicalGraphService();
    tracer3 = {
      startActiveSpan: async (_name, fn) => {
        const span = {
          setAttributes: (_a) => {
          },
          recordException: (_e) => {
          },
          setStatus: (_s) => {
          },
          end: () => {
          }
        };
        return await fn(span);
      }
    };
    ledgerEntries = new Counter4({
      name: "provenance_ledger_entries_total",
      help: "Total provenance ledger entries",
      labelNames: ["tenant_id", "action_type", "resource_type"]
    });
    ledgerOperationTime = new Histogram3({
      name: "provenance_ledger_operation_duration_seconds",
      help: "Provenance ledger operation duration",
      buckets: [1e-3, 5e-3, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1],
      labelNames: ["operation", "batch_size"]
    });
    ledgerChainHeight = new Gauge3({
      name: "provenance_ledger_chain_height",
      help: "Current height of the provenance chain",
      labelNames: ["tenant_id"]
    });
    ledgerIntegrityStatus = new Gauge3({
      name: "provenance_ledger_integrity_status",
      help: "Provenance ledger integrity status (1 = valid, 0 = corrupted)",
      labelNames: ["tenant_id"]
    });
    ProvenanceLedgerV2 = class _ProvenanceLedgerV2 extends EventEmitter3 {
      genesisHash = "0000000000000000000000000000000000000000000000000000000000000000";
      rootSigningInterval = null;
      cryptoPipeline;
      cryptoPipelineInit;
      pool = pool;
      static instance;
      static getInstance() {
        if (!_ProvenanceLedgerV2.instance) {
          _ProvenanceLedgerV2.instance = new _ProvenanceLedgerV2();
        }
        return _ProvenanceLedgerV2.instance;
      }
      constructor() {
        super();
        this.startRootSigning();
      }
      setPool(pool4) {
        this.pool = pool4;
      }
      setCryptoPipeline(pipeline2) {
        this.cryptoPipeline = pipeline2 ?? void 0;
        this.cryptoPipelineInit = Promise.resolve();
      }
      initializeCryptoPipeline() {
        if (this.cryptoPipelineInit) return;
        this.cryptoPipelineInit = createDefaultCryptoPipeline({
          timestampingEndpointEnv: "CRYPTO_TIMESTAMP_ENDPOINT",
          auditSubsystem: "provenance-ledger",
          trustAnchorsEnv: "CRYPTO_TRUST_ANCHORS"
        }).then((pipeline2) => {
          this.cryptoPipeline = pipeline2 ?? void 0;
        }).catch((error) => {
          console.warn("Failed to initialize cryptographic pipeline", error);
          this.cryptoPipeline = void 0;
        });
      }
      async ensureCryptoPipeline() {
        this.initializeCryptoPipeline();
        try {
          await this.cryptoPipelineInit;
        } catch {
        }
      }
      async appendEntry(entry) {
        return tracer3.startActiveSpan(
          "provenance_ledger.append_entry",
          async (span) => {
            span.setAttributes?.({
              tenant_id: entry.tenantId,
              action_type: entry.actionType,
              resource_type: entry.resourceType,
              actor_type: entry.actorType,
              provided_id: entry.id
            });
            if (entry.id) {
              const existing = await this.getEntryById(entry.id);
              if (existing) {
                span.setAttributes?.({ idempotent_skip: true });
                return existing;
              }
            }
            const { createHash: createHash40 } = await import("crypto");
            const startTime = Date.now();
            try {
              let witness;
              if (this.isMutationPayload(entry.payload)) {
                witness = await mutationWitness.witnessMutation(
                  entry.payload,
                  { tenantId: entry.tenantId, actorId: entry.actorId }
                );
              }
              const client6 = await this.pool.connect();
              try {
                await client6.query("BEGIN");
                const previousEntry = await this.getLastEntry(
                  entry.tenantId,
                  client6
                );
                const previousHash = previousEntry?.currentHash || this.genesisHash;
                const sequenceNumber = previousEntry ? previousEntry.sequenceNumber + 1n : 1n;
                const id = entry.id || `prov_${Date.now()}_${Math.random().toString(36).substring(7)}`;
                const entryData = {
                  ...entry,
                  id,
                  sequenceNumber,
                  previousHash,
                  witness
                  // Include witness in hash calculation
                };
                const currentHash = this.computeEntryHash(entryData);
                const completeEntry = {
                  ...entryData,
                  currentHash,
                  payload: entry.payload
                };
                const storageMetadata = {
                  ...completeEntry.metadata,
                  witness: completeEntry.witness,
                  attribution: completeEntry.attribution
                };
                const insertQuery = `
            INSERT INTO provenance_ledger_v2 (
              id, tenant_id, sequence_number, previous_hash, current_hash,
              timestamp, action_type, resource_type, resource_id,
              actor_id, actor_type, payload, metadata, signature, attestation
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
            ON CONFLICT (id) DO NOTHING
            RETURNING *
          `;
                const result2 = await client6.query(insertQuery, [
                  completeEntry.id,
                  completeEntry.tenantId,
                  completeEntry.sequenceNumber.toString(),
                  completeEntry.previousHash,
                  completeEntry.currentHash,
                  completeEntry.timestamp,
                  completeEntry.actionType,
                  completeEntry.resourceType,
                  completeEntry.resourceId,
                  completeEntry.actorId,
                  completeEntry.actorType,
                  JSON.stringify(completeEntry.payload),
                  JSON.stringify(storageMetadata),
                  // Store witness in metadata column
                  completeEntry.signature,
                  completeEntry.attestation ? JSON.stringify(completeEntry.attestation) : null
                ]);
                if (result2.rows.length === 0) {
                  await client6.query("ROLLBACK");
                  const existing = await client6.query("SELECT * FROM provenance_ledger_v2 WHERE id = $1", [id]);
                  if (existing.rows.length > 0) {
                    return this.mapRowToEntry(existing.rows[0]);
                  }
                  throw new Error(`Failed to insert provenance entry ${id} and it was not found`);
                }
                await client6.query("COMMIT");
                ledgerEntries.inc({
                  tenant_id: entry.tenantId,
                  action_type: entry.actionType,
                  resource_type: entry.resourceType
                });
                ledgerChainHeight.set(
                  { tenant_id: entry.tenantId },
                  Number(completeEntry.sequenceNumber)
                );
                ledgerOperationTime.observe(
                  { operation: "append", batch_size: "1" },
                  (Date.now() - startTime) / 1e3
                );
                span.setAttributes?.({
                  sequence_number: completeEntry.sequenceNumber.toString(),
                  current_hash: completeEntry.currentHash.substring(0, 16),
                  chain_height: Number(completeEntry.sequenceNumber)
                });
                this.emit("entryAppended", completeEntry);
                CanonicalGraphService.getInstance().projectEntry(completeEntry).catch((err) => {
                  console.error("Failed to project entry to Canonical Graph", {
                    entryId: completeEntry.id,
                    error: err
                  });
                });
                advancedAuditSystem.logEvent?.({
                  eventType: "resource_modify",
                  // Generic mapping, could be more specific
                  action: entry.actionType,
                  tenantId: entry.tenantId,
                  userId: entry.actorId,
                  resourceId: entry.resourceId,
                  resourceType: entry.resourceType,
                  message: `Provenance entry appended: ${entry.actionType} on ${entry.resourceType}`,
                  details: {
                    provenanceId: completeEntry.id,
                    sequence: completeEntry.sequenceNumber.toString(),
                    witnessId: witness?.witnessId
                  },
                  level: "info",
                  complianceRelevant: true
                  // Provenance is always compliance relevant
                });
                return completeEntry;
              } catch (error) {
                await client6.query("ROLLBACK");
                throw error;
              } finally {
                client6.release();
              }
            } catch (error) {
              span.recordException?.(error);
              span.setStatus?.({ message: error.message });
              throw error;
            } finally {
              span.end?.();
            }
          }
        );
      }
      async recordReceiptEvidence(receipt) {
        return this.appendEntry({
          tenantId: receipt.tenantId,
          actionType: "RECEIPT_ISSUED",
          resourceType: "receipt",
          resourceId: receipt.receiptId,
          actorId: receipt.actorId,
          actorType: "system",
          payload: { receipt },
          metadata: {
            receipt,
            entryId: receipt.entryId,
            signerKeyId: receipt.signerKeyId
          },
          timestamp: new Date(receipt.issuedAt)
        });
      }
      async recordRecoveryEvidence(params) {
        const { tenantId, actorId, actorType, evidence } = params;
        return this.appendEntry({
          tenantId,
          actionType: "RECOVERY_EVIDENCE_CAPTURED",
          resourceType: "recovery",
          resourceId: evidence.drillId,
          actorId,
          actorType: actorType || "system",
          payload: { recovery: evidence },
          metadata: {
            recovery: evidence
          },
          timestamp: new Date(evidence.completedAt || evidence.startedAt)
        });
      }
      isMutationPayload(payload) {
        return payload && typeof payload === "object" && "mutationType" in payload;
      }
      async batchAppendEntries(entries) {
        return tracer3.startActiveSpan(
          "provenance_ledger.batch_append",
          async (span) => {
            span.setAttributes?.({
              batch_size: entries.length,
              tenant_ids: [...new Set(entries.map((e) => e.tenantId))].join(",")
            });
            const startTime = Date.now();
            const results = [];
            try {
              const client6 = await this.pool.connect();
              try {
                await client6.query("BEGIN");
                const entriesByTenant = /* @__PURE__ */ new Map();
                for (const entry of entries) {
                  if (!entriesByTenant.has(entry.tenantId)) {
                    entriesByTenant.set(entry.tenantId, []);
                  }
                  entriesByTenant.get(entry.tenantId).push(entry);
                }
                for (const [tenantId, tenantEntries] of entriesByTenant) {
                  let previousEntry = await this.getLastEntry(tenantId, client6);
                  for (const entry of tenantEntries) {
                    const previousHash = previousEntry?.currentHash || this.genesisHash;
                    const sequenceNumber = previousEntry ? previousEntry.sequenceNumber + 1n : 1n;
                    const id = `prov_${Date.now()}_${Math.random().toString(36).substring(7)}`;
                    const currentHash = this.computeEntryHash({
                      id,
                      sequenceNumber,
                      previousHash,
                      ...entry
                    });
                    const completeEntry = {
                      id,
                      sequenceNumber,
                      previousHash,
                      currentHash,
                      ...entry
                    };
                    const insertQuery = `
                INSERT INTO provenance_ledger_v2 (
                  id, tenant_id, sequence_number, previous_hash, current_hash,
                  timestamp, action_type, resource_type, resource_id,
                  actor_id, actor_type, payload, metadata, signature, attestation
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
              `;
                    await client6.query(insertQuery, [
                      completeEntry.id,
                      completeEntry.tenantId,
                      completeEntry.sequenceNumber.toString(),
                      completeEntry.previousHash,
                      completeEntry.currentHash,
                      completeEntry.timestamp,
                      completeEntry.actionType,
                      completeEntry.resourceType,
                      completeEntry.resourceId,
                      completeEntry.actorId,
                      completeEntry.actorType,
                      JSON.stringify(completeEntry.payload),
                      JSON.stringify(completeEntry.metadata),
                      completeEntry.signature,
                      completeEntry.attestation ? JSON.stringify(completeEntry.attestation) : null
                    ]);
                    results.push(completeEntry);
                    previousEntry = completeEntry;
                  }
                }
                await client6.query("COMMIT");
                for (const entry of results) {
                  ledgerEntries.inc({
                    tenant_id: entry.tenantId,
                    action_type: entry.actionType,
                    resource_type: entry.resourceType
                  });
                }
                ledgerOperationTime.observe(
                  {
                    operation: "batch_append",
                    batch_size: entries.length.toString()
                  },
                  (Date.now() - startTime) / 1e3
                );
                span.setAttributes?.({
                  entries_processed: results.length,
                  execution_time_ms: Date.now() - startTime
                });
                this.emit("batchAppended", results);
                return results;
              } catch (error) {
                await client6.query("ROLLBACK");
                throw error;
              } finally {
                client6.release();
              }
            } catch (error) {
              span.recordException(error);
              span.setStatus({ code: 2, message: error.message });
              throw error;
            } finally {
              span.end();
            }
          }
        );
      }
      computeEntryHash(entry) {
        const hashData = {
          id: entry.id,
          tenantId: entry.tenantId,
          sequenceNumber: entry.sequenceNumber?.toString(),
          previousHash: entry.previousHash,
          timestamp: entry.timestamp?.toISOString(),
          actionType: entry.actionType,
          resourceType: entry.resourceType,
          resourceId: entry.resourceId,
          actorId: entry.actorId,
          actorType: entry.actorType,
          payload: entry.payload,
          metadata: entry.metadata
        };
        return crypto9.createHash("sha256").update(JSON.stringify(hashData, Object.keys(hashData).sort())).digest("hex");
      }
      async getLastEntry(tenantId, client6) {
        const queryClient = client6 || this.pool;
        const result2 = await queryClient.query(
          `SELECT * FROM provenance_ledger_v2 
       WHERE tenant_id = $1 
       ORDER BY sequence_number DESC 
       LIMIT 1`,
          [tenantId]
        );
        if (result2.rows.length === 0) {
          return null;
        }
        return this.mapRowToEntry(result2.rows[0]);
      }
      async verifyChainIntegrity(tenantId) {
        return tracer3.startActiveSpan(
          "provenance_ledger.verify_integrity",
          async (span) => {
            span.setAttributes?.({
              tenant_id: tenantId || "all"
            });
            const startTime = Date.now();
            const verification2 = {
              valid: true,
              totalEntries: 0,
              brokenChains: 0,
              invalidHashes: 0,
              missingEntries: 0,
              lastVerifiedSequence: 0n,
              verificationTime: 0,
              errors: []
            };
            try {
              const query3 = `
          SELECT * FROM provenance_ledger_v2 
          ${tenantId ? "WHERE tenant_id = $1" : ""}
          ORDER BY tenant_id, sequence_number
        `;
              const result2 = tenantId ? await this.pool.query(query3, [tenantId]) : await this.pool.query(query3);
              verification2.totalEntries = result2.rows.length;
              let currentTenantId = "";
              let expectedSequence = 1n;
              let previousHash = this.genesisHash;
              for (const row of result2.rows) {
                const entry = this.mapRowToEntry(row);
                if (entry.tenantId !== currentTenantId) {
                  currentTenantId = entry.tenantId;
                  expectedSequence = 1n;
                  previousHash = this.genesisHash;
                }
                if (entry.sequenceNumber !== expectedSequence) {
                  verification2.missingEntries++;
                  verification2.errors.push({
                    sequenceNumber: entry.sequenceNumber,
                    error: `Expected sequence ${expectedSequence}, got ${entry.sequenceNumber}`,
                    severity: "critical"
                  });
                  verification2.valid = false;
                }
                if (entry.previousHash !== previousHash) {
                  verification2.brokenChains++;
                  verification2.errors.push({
                    sequenceNumber: entry.sequenceNumber,
                    error: `Hash chain broken: expected previous ${previousHash}, got ${entry.previousHash}`,
                    severity: "critical"
                  });
                  verification2.valid = false;
                }
                const computedHash = this.computeEntryHash(entry);
                if (entry.currentHash !== computedHash) {
                  verification2.invalidHashes++;
                  verification2.errors.push({
                    sequenceNumber: entry.sequenceNumber,
                    error: `Invalid hash: expected ${computedHash}, got ${entry.currentHash}`,
                    severity: "critical"
                  });
                  verification2.valid = false;
                }
                expectedSequence = entry.sequenceNumber + 1n;
                previousHash = entry.currentHash;
                verification2.lastVerifiedSequence = entry.sequenceNumber;
              }
              verification2.verificationTime = Date.now() - startTime;
              const status = verification2.valid ? 1 : 0;
              if (tenantId) {
                ledgerIntegrityStatus.set({ tenant_id: tenantId }, status);
              }
              span.setAttributes?.({
                entries_verified: verification2.totalEntries,
                chain_valid: verification2.valid,
                broken_chains: verification2.brokenChains,
                invalid_hashes: verification2.invalidHashes,
                verification_time_ms: verification2.verificationTime
              });
              this.emit("chainVerified", { tenantId, verification: verification2 });
              return verification2;
            } catch (error) {
              span.recordException(error);
              span.setStatus({ code: 2, message: error.message });
              throw error;
            } finally {
              span.end();
            }
          }
        );
      }
      async createSignedRoot(tenantId) {
        return tracer3.startActiveSpan(
          "provenance_ledger.create_signed_root",
          async (span) => {
            span.setAttributes?.({
              tenant_id: tenantId || "all"
            });
            try {
              const roots = [];
              const tenants = tenantId ? [tenantId] : await this.getTenantList();
              for (const tid of tenants) {
                const root = await this.createTenantSignedRoot(tid);
                roots.push(root);
              }
              span.setAttributes?.({
                roots_created: roots.length,
                tenant_count: tenants.length
              });
              this.emit("rootsSigned", roots);
              return roots;
            } catch (error) {
              span.recordException(error);
              span.setStatus({ code: 2, message: error.message });
              throw error;
            } finally {
              span.end();
            }
          }
        );
      }
      async createTenantSignedRoot(tenantId) {
        const rangeQuery = `
      SELECT 
        MIN(sequence_number) as start_seq,
        MAX(sequence_number) as end_seq,
        COUNT(*) as entry_count
      FROM provenance_ledger_v2 
      WHERE tenant_id = $1
      AND NOT EXISTS (
        SELECT 1 FROM provenance_ledger_roots 
        WHERE tenant_id = $1 
        AND end_sequence >= provenance_ledger_v2.sequence_number
      )
    `;
        const rangeResult = await this.pool.query(rangeQuery, [tenantId]);
        const range = rangeResult.rows[0];
        if (!range.start_seq) {
          throw new Error(`No new entries found for tenant ${tenantId}`);
        }
        const entriesQuery = `
      SELECT current_hash FROM provenance_ledger_v2
      WHERE tenant_id = $1 
      AND sequence_number BETWEEN $2 AND $3
      ORDER BY sequence_number
    `;
        const entriesResult = await this.pool.query(entriesQuery, [
          tenantId,
          range.start_seq,
          range.end_seq
        ]);
        const hashes = entriesResult.rows.map((row) => row.current_hash);
        const rootHash = this.computeMerkleRoot(hashes);
        const signature = await this.signWithCosign(rootHash);
        const root = {
          id: `root_${Date.now()}_${Math.random().toString(36).substring(7)}`,
          tenantId,
          rootHash,
          startSequence: BigInt(range.start_seq),
          endSequence: BigInt(range.end_seq),
          entryCount: parseInt(range.entry_count),
          timestamp: /* @__PURE__ */ new Date(),
          signature,
          merkleProof: this.generateMerkleProof(hashes, rootHash)
        };
        await this.pool.query(
          `
      INSERT INTO provenance_ledger_roots (
        id, tenant_id, root_hash, start_sequence, end_sequence,
        entry_count, timestamp, signature, cosign_bundle, merkle_proof
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
    `,
          [
            root.id,
            root.tenantId,
            root.rootHash,
            root.startSequence.toString(),
            root.endSequence.toString(),
            root.entryCount,
            root.timestamp,
            root.signature,
            root.cosignBundle,
            JSON.stringify(root.merkleProof)
          ]
        );
        try {
          const wormKey = `roots/${root.tenantId}/${root.id}.json`;
          const location = await putLocked(
            process.env.AUDIT_BUCKET || "provenance-logs",
            wormKey,
            JSON.stringify(root, null, 2)
          );
          console.log(`Archived provenance root to WORM storage: ${location}`);
        } catch (e) {
          console.error("Failed to archive root to WORM storage", e);
        }
        return root;
      }
      computeMerkleRoot(hashes) {
        if (hashes.length === 0) return this.genesisHash;
        if (hashes.length === 1) return hashes[0];
        let currentLevel = hashes;
        while (currentLevel.length > 1) {
          const nextLevel = [];
          for (let i = 0; i < currentLevel.length; i += 2) {
            const left = currentLevel[i];
            const right = i + 1 < currentLevel.length ? currentLevel[i + 1] : left;
            const combined = crypto9.createHash("sha256").update(left + right).digest("hex");
            nextLevel.push(combined);
          }
          currentLevel = nextLevel;
        }
        return currentLevel[0];
      }
      generateMerkleProof(hashes, rootHash) {
        return hashes.slice(0, Math.min(10, hashes.length));
      }
      async signWithCosign(data) {
        await this.ensureCryptoPipeline();
        if (this.cryptoPipeline) {
          try {
            const keyId = process.env.LEDGER_SIGNING_KEY_ID || "ledger-root";
            const bundle = await this.cryptoPipeline.signPayload(
              Buffer.from(data),
              keyId,
              {
                includeTimestamp: true,
                metadata: {
                  subsystem: "provenance-ledger"
                }
              }
            );
            return JSON.stringify(bundle);
          } catch (error) {
            console.warn(
              "Crypto pipeline signing failed, falling back to cosign/HMAC:",
              error
            );
          }
        }
        try {
          const tempFile = `/tmp/provenance_${Date.now()}.txt`;
          __require("fs").writeFileSync(tempFile, data);
          const signCommand = `cosign sign-blob --bundle=/tmp/bundle.json ${tempFile}`;
          const signature = execSync(signCommand, { encoding: "utf-8" });
          __require("fs").unlinkSync(tempFile);
          return signature.trim();
        } catch (error) {
          console.warn("Cosign signing failed, using fallback signature:", error);
          return crypto9.createHmac("sha256", process.env.LEDGER_SECRET || "default-secret").update(data).digest("hex");
        }
      }
      async verifySignature(rootHash, signature) {
        await this.ensureCryptoPipeline();
        if (this.cryptoPipeline) {
          try {
            const maybeBundle = JSON.parse(signature);
            if (maybeBundle?.signature && maybeBundle?.keyId) {
              const result2 = await this.cryptoPipeline.verifySignature(
                Buffer.from(rootHash),
                maybeBundle,
                {
                  expectedKeyId: maybeBundle.keyId,
                  payloadDescription: "provenance-ledger-root"
                }
              );
              if (result2.valid) {
                return true;
              }
            }
          } catch (error) {
            if (signature.trim().startsWith("{")) {
              console.warn(
                "Failed to verify cryptographic pipeline signature, falling back:",
                error
              );
            }
          }
        }
        try {
          const tempFile = `/tmp/verify_${Date.now()}.txt`;
          const tempSig = `/tmp/verify_${Date.now()}.sig`;
          __require("fs").writeFileSync(tempFile, rootHash);
          __require("fs").writeFileSync(tempSig, signature);
          const verifyCommand = `cosign verify-blob --signature=${tempSig} ${tempFile}`;
          execSync(verifyCommand, { encoding: "utf-8" });
          __require("fs").unlinkSync(tempFile);
          __require("fs").unlinkSync(tempSig);
          return true;
        } catch (error) {
          console.warn("Cosign verification failed, trying fallback:", error);
          const expectedSignature = crypto9.createHmac("sha256", process.env.LEDGER_SECRET || "default-secret").update(rootHash).digest("hex");
          return signature === expectedSignature;
        }
      }
      async getTenantList() {
        const result2 = await this.pool.query(
          "SELECT DISTINCT tenant_id FROM provenance_ledger_v2 ORDER BY tenant_id"
        );
        return result2.rows.map((row) => row.tenant_id);
      }
      mapRowToEntry(row) {
        return {
          id: row.id,
          tenantId: row.tenant_id,
          sequenceNumber: BigInt(row.sequence_number),
          previousHash: row.previous_hash,
          currentHash: row.current_hash,
          timestamp: row.timestamp,
          actionType: row.action_type,
          resourceType: row.resource_type,
          resourceId: row.resource_id,
          actorId: row.actor_id,
          actorType: row.actor_type,
          payload: typeof row.payload === "string" ? JSON.parse(row.payload) : row.payload,
          metadata: typeof row.metadata === "string" ? JSON.parse(row.metadata) : row.metadata,
          signature: row.signature,
          attestation: row.attestation ? typeof row.attestation === "string" ? JSON.parse(row.attestation) : row.attestation : void 0
        };
      }
      async initializeTables() {
      }
      startRootSigning() {
        if (process.env.ZERO_FOOTPRINT === "true") {
          return;
        }
        const dailySigningHour = 2;
        const now = /* @__PURE__ */ new Date();
        const nextRun = /* @__PURE__ */ new Date();
        nextRun.setUTCHours(dailySigningHour, 0, 0, 0);
        if (nextRun <= now) {
          nextRun.setUTCDate(nextRun.getUTCDate() + 1);
        }
        const msUntilNextRun = nextRun.getTime() - now.getTime();
        const timer3 = setTimeout(() => {
          this.performDailyRootSigning();
          this.rootSigningInterval = setInterval(
            () => {
              this.performDailyRootSigning();
            },
            24 * 60 * 60 * 1e3
          );
          this.rootSigningInterval?.unref();
        }, msUntilNextRun);
        timer3.unref();
      }
      async performDailyRootSigning() {
        try {
          console.log("Performing daily root signing...");
          const roots = await this.createSignedRoot();
          console.log(`Signed ${roots.length} tenant roots`);
          this.emit("dailySigningCompleted", {
            timestamp: /* @__PURE__ */ new Date(),
            rootCount: roots.length,
            roots
          });
        } catch (error) {
          console.error("Daily root signing failed:", error);
          this.emit("dailySigningFailed", { error, timestamp: /* @__PURE__ */ new Date() });
        }
      }
      async getEntryById(id) {
        const result2 = await this.pool.query(
          `SELECT * FROM provenance_ledger_v2 WHERE id = $1`,
          [id]
        );
        if (result2.rows.length === 0) return null;
        return this.mapRowToEntry(result2.rows[0]);
      }
      async getEntries(tenantId, options2 = {}) {
        const whereConditions = ["tenant_id = $1"];
        const params = [tenantId];
        let paramIndex = 2;
        if (options2.fromSequence !== void 0) {
          whereConditions.push(`sequence_number >= $${paramIndex}`);
          params.push(options2.fromSequence.toString());
          paramIndex++;
        }
        if (options2.toSequence !== void 0) {
          whereConditions.push(`sequence_number <= $${paramIndex}`);
          params.push(options2.toSequence.toString());
          paramIndex++;
        }
        if (options2.actionType) {
          whereConditions.push(`action_type = $${paramIndex}`);
          params.push(options2.actionType);
          paramIndex++;
        }
        if (options2.resourceType) {
          whereConditions.push(`resource_type = $${paramIndex}`);
          params.push(options2.resourceType);
          paramIndex++;
        }
        if (options2.resourceId) {
          whereConditions.push(`resource_id = $${paramIndex}`);
          params.push(options2.resourceId);
          paramIndex++;
        }
        const sortOrder = options2.order === "DESC" ? "DESC" : "ASC";
        const query3 = `
      SELECT * FROM provenance_ledger_v2
      WHERE ${whereConditions.join(" AND ")}
      ORDER BY sequence_number ${sortOrder}
      ${options2.limit ? `LIMIT ${options2.limit}` : ""}
      ${options2.offset ? `OFFSET ${options2.offset}` : ""}
    `;
        const result2 = await this.pool.query(query3, params);
        return result2.rows.map((row) => this.mapRowToEntry(row));
      }
      async generateExportManifest(tenantId, exportId, resourceIds) {
        return tracer3.startActiveSpan(
          "provenance_ledger.generate_manifest",
          async (span) => {
            span.setAttributes?.({
              tenant_id: tenantId,
              export_id: exportId,
              resource_count: resourceIds.length
            });
            const entries = [];
            for (const rid of resourceIds) {
              const resEntries = await this.getEntries(tenantId, {
                resourceType: "Evidence",
                // Assuming Evidence type for now
                limit: 100
                // Reasonable limit for manifest proof
              });
              const relevant = resEntries.filter((e) => e.resourceId === rid);
              entries.push(...relevant);
            }
            const hashes = entries.map((e) => e.currentHash).sort();
            const rootHash = this.computeMerkleRoot(hashes);
            const roots = await this.getTenantSignedRoots(tenantId);
            const latestRoot = roots[roots.length - 1];
            const manifest = {
              exportId,
              tenantId,
              timestamp: (/* @__PURE__ */ new Date()).toISOString(),
              content: {
                resourceCount: resourceIds.length,
                resources: resourceIds
              },
              provenance: {
                entryCount: entries.length,
                rootHash,
                // Hash of the exported items
                chainTip: latestRoot?.rootHash || "genesis",
                // Anchor to global chain
                signatures: [
                  {
                    signer: "provenance-ledger-v2",
                    timestamp: (/* @__PURE__ */ new Date()).toISOString(),
                    signature: await this.signWithCosign(rootHash)
                    // Sign the specific export root
                  }
                ]
              }
            };
            span.setAttributes?.({
              manifest_root_hash: rootHash
            });
            return JSON.stringify(manifest, null, 2);
          }
        );
      }
      async getTenantSignedRoots(tenantId) {
        const result2 = await this.pool.query(
          `SELECT * FROM provenance_ledger_roots
       WHERE tenant_id = $1
       ORDER BY end_sequence ASC`,
          [tenantId]
        );
        return result2.rows.map((row) => ({
          id: row.id,
          tenantId: row.tenant_id,
          rootHash: row.root_hash,
          startSequence: BigInt(row.start_sequence),
          endSequence: BigInt(row.end_sequence),
          entryCount: row.entry_count,
          timestamp: row.timestamp,
          signature: row.signature,
          cosignBundle: row.cosign_bundle,
          merkleProof: row.merkle_proof ? typeof row.merkle_proof === "string" ? JSON.parse(row.merkle_proof) : row.merkle_proof : void 0
        }));
      }
      async exportLedger(tenantId, format = "json") {
        const entries = await this.getEntries(tenantId);
        if (format === "json") {
          return JSON.stringify(entries, null, 2);
        } else {
          const headers = [
            "id",
            "sequence_number",
            "timestamp",
            "action_type",
            "resource_type",
            "resource_id",
            "actor_id",
            "actor_type",
            "current_hash"
          ];
          let csv = headers.join(",") + "\n";
          for (const entry of entries) {
            const row = headers.map((header) => {
              const value = entry[header === "sequence_number" ? "sequenceNumber" : header === "action_type" ? "actionType" : header === "resource_type" ? "resourceType" : header === "resource_id" ? "resourceId" : header === "actor_id" ? "actorId" : header === "actor_type" ? "actorType" : header === "current_hash" ? "currentHash" : header];
              return typeof value === "string" ? `"${value.replace(/"/g, '""')}"` : value;
            });
            csv += row.join(",") + "\n";
          }
          return csv;
        }
      }
      cleanup() {
        if (this.rootSigningInterval) {
          clearInterval(this.rootSigningInterval);
          this.rootSigningInterval = null;
        }
      }
    };
    provenanceLedger = new ProvenanceLedgerV2();
  }
});

// src/services/EmbeddingService.ts
import crypto10 from "crypto";
var EmbeddingService, EmbeddingService_default;
var init_EmbeddingService = __esm({
  "src/services/EmbeddingService.ts"() {
    "use strict";
    init_logger2();
    init_metrics2();
    EmbeddingService = class {
      config;
      logger;
      metrics;
      constructor(config9 = {}) {
        this.config = {
          provider: process.env.AIR_GAPPED_MODE === "true" ? "local" : process.env.EMBEDDING_PROVIDER || "openai",
          apiKey: process.env.OPENAI_API_KEY || process.env.EMBEDDING_API_KEY,
          model: process.env.EMBEDDING_MODEL || "text-embedding-3-large",
          dimension: parseInt(process.env.EMBEDDING_DIMENSION || "") || 3072,
          batchSize: parseInt(process.env.EMBEDDING_BATCH_SIZE || "") || 10,
          timeout: parseInt(process.env.EMBEDDING_TIMEOUT || "") || 3e4,
          maxRetries: parseInt(process.env.EMBEDDING_MAX_RETRIES || "") || 3,
          ...config9
        };
        this.logger = logger_default2;
        this.metrics = {
          totalEmbeddings: 0,
          averageLatency: 0,
          errorCount: 0,
          batchCount: 0
        };
      }
      /**
       * Generate embedding for a single text
       */
      async generateEmbedding(params) {
        const { text, model = this.config.model } = params;
        if (!text || typeof text !== "string") {
          throw new Error("Valid text string is required");
        }
        const startTime = Date.now();
        try {
          let embedding;
          switch (this.config.provider) {
            case "openai":
              embedding = await this.generateOpenAIEmbedding(text, model);
              break;
            case "huggingface":
              embedding = await this.generateHuggingFaceEmbedding(text, model);
              break;
            case "local":
              embedding = await this.generateLocalEmbedding(text, model);
              break;
            default:
              throw new Error(
                `Unsupported embedding provider: ${this.config.provider}`
              );
          }
          const latency = Date.now() - startTime;
          this.updateMetrics(latency);
          logger_default2.debug("Embedding generated", {
            provider: this.config.provider,
            model,
            textLength: text.length,
            dimension: embedding.length,
            latency
          });
          return embedding;
        } catch (error) {
          this.metrics.errorCount++;
          applicationErrors2.labels("embedding_service", "GenerationError", "error").inc();
          logger_default2.error("Embedding generation failed", {
            provider: this.config.provider,
            model,
            textLength: text.length,
            error: error.message
          });
          throw error;
        }
      }
      /**
       * Generate embeddings for multiple texts in batch
       */
      async generateEmbeddings(texts, model = this.config.model) {
        if (!Array.isArray(texts) || texts.length === 0) {
          throw new Error("Valid array of texts is required");
        }
        const batches = this.createBatches(texts, this.config.batchSize);
        const allEmbeddings = [];
        for (let i = 0; i < batches.length; i++) {
          const batch = batches[i];
          try {
            const batchEmbeddings = await this.generateBatchEmbeddings(
              batch,
              model
            );
            allEmbeddings.push(...batchEmbeddings);
            this.metrics.batchCount++;
            logger_default2.debug("Batch embeddings generated", {
              batchIndex: i + 1,
              totalBatches: batches.length,
              batchSize: batch.length,
              provider: this.config.provider
            });
          } catch (error) {
            logger_default2.error("Batch embedding generation failed", {
              batchIndex: i + 1,
              batchSize: batch.length,
              error: error.message
            });
            for (const text of batch) {
              try {
                const embedding = await this.generateEmbedding({ text, model });
                allEmbeddings.push(embedding);
              } catch (individualError) {
                logger_default2.warn("Individual embedding failed, using zero vector", {
                  textLength: text.length,
                  error: individualError.message
                });
                allEmbeddings.push(new Array(this.config.dimension).fill(0));
              }
            }
          }
        }
        return allEmbeddings;
      }
      /**
       * Generate OpenAI embeddings
       */
      async generateOpenAIEmbedding(text, model) {
        if (!this.config.apiKey) {
          throw new Error("OpenAI API key not configured");
        }
        const response = await fetch("https://api.openai.com/v1/embeddings", {
          method: "POST",
          headers: {
            Authorization: `Bearer ${this.config.apiKey}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            input: text,
            model,
            encoding_format: "float"
          }),
          signal: AbortSignal.timeout(this.config.timeout)
        });
        if (!response.ok) {
          const errorData = await response.text();
          throw new Error(`OpenAI API error: ${response.status} - ${errorData}`);
        }
        const data = await response.json();
        if (!data.data || data.data.length === 0) {
          throw new Error("No embedding data returned from OpenAI");
        }
        return data.data[0].embedding;
      }
      /**
       * Generate batch OpenAI embeddings
       */
      async generateBatchEmbeddings(texts, model) {
        if (this.config.provider !== "openai") {
          const embeddings = [];
          for (const text of texts) {
            const embedding = await this.generateEmbedding({ text, model });
            embeddings.push(embedding);
          }
          return embeddings;
        }
        const response = await fetch("https://api.openai.com/v1/embeddings", {
          method: "POST",
          headers: {
            Authorization: `Bearer ${this.config.apiKey}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            input: texts,
            model,
            encoding_format: "float"
          }),
          signal: AbortSignal.timeout(this.config.timeout)
        });
        if (!response.ok) {
          const errorData = await response.text();
          throw new Error(
            `OpenAI Batch API error: ${response.status} - ${errorData}`
          );
        }
        const data = await response.json();
        if (!data.data || data.data.length !== texts.length) {
          throw new Error("Batch embedding response length mismatch");
        }
        return data.data.map((item) => item.embedding);
      }
      /**
       * Generate HuggingFace embeddings (placeholder)
       */
      async generateHuggingFaceEmbedding(text, model) {
        throw new Error("HuggingFace provider not yet implemented");
      }
      /**
       * Generate local embeddings (deterministic simulation for air-gapped mode)
       * Note: These embeddings are structurally valid (correct dimension) but semantically meaningless.
       * Similar texts will NOT produce similar vectors. This is for testing logic flow and
       * prototypes where actual model weights are not available.
       */
      async generateLocalEmbedding(text, model) {
        const hash3 = crypto10.createHash("sha256").update(text).digest("hex");
        const vector = [];
        for (let i = 0; i < this.config.dimension; i++) {
          const charCode = hash3.charCodeAt(i % hash3.length);
          const modifier = i * 1337 % 255;
          const val = (charCode + modifier) % 256 / 127.5 - 1;
          vector.push(val);
        }
        const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));
        if (magnitude === 0) return new Array(this.config.dimension).fill(0);
        return vector.map((val) => val / magnitude);
      }
      /**
       * Calculate semantic similarity between two texts
       */
      async calculateSimilarity(text1, text2, model) {
        const [embedding1, embedding2] = await Promise.all([
          this.generateEmbedding({ text: text1, model }),
          this.generateEmbedding({ text: text2, model })
        ]);
        return this.cosineSimilarity(embedding1, embedding2);
      }
      /**
       * Calculate cosine similarity between two vectors
       */
      cosineSimilarity(vec1, vec2) {
        if (vec1.length !== vec2.length) {
          throw new Error("Vectors must have same dimensions");
        }
        const dotProduct = vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);
        const magnitude1 = Math.sqrt(vec1.reduce((sum, val) => sum + val * val, 0));
        const magnitude2 = Math.sqrt(vec2.reduce((sum, val) => sum + val * val, 0));
        if (magnitude1 === 0 || magnitude2 === 0) {
          return 0;
        }
        return dotProduct / (magnitude1 * magnitude2);
      }
      /**
       * Find most similar texts from a corpus
       */
      async findSimilar(queryText, corpusTexts, options2 = {}) {
        const { topK = 5, threshold = 0, model = this.config.model } = options2;
        const queryEmbedding = await this.generateEmbedding({
          text: queryText,
          model
        });
        const corpusEmbeddings = await this.generateEmbeddings(corpusTexts, model);
        const similarities = corpusEmbeddings.map((embedding, index) => ({
          index,
          text: corpusTexts[index],
          similarity: this.cosineSimilarity(queryEmbedding, embedding)
        }));
        return similarities.filter((item) => item.similarity >= threshold).sort((a, b) => b.similarity - a.similarity).slice(0, topK);
      }
      /**
       * Utility methods
       */
      createBatches(items, batchSize) {
        const batches = [];
        for (let i = 0; i < items.length; i += batchSize) {
          batches.push(items.slice(i, i + batchSize));
        }
        return batches;
      }
      updateMetrics(latency) {
        this.metrics.totalEmbeddings++;
        const currentAvg = this.metrics.averageLatency;
        this.metrics.averageLatency = currentAvg ? (currentAvg + latency) / 2 : latency;
      }
      /**
       * Health check
       */
      getHealth() {
        return {
          status: "healthy",
          provider: this.config.provider,
          model: this.config.model,
          dimension: this.config.dimension,
          metrics: {
            totalEmbeddings: this.metrics.totalEmbeddings,
            averageLatency: Math.round(this.metrics.averageLatency),
            errorCount: this.metrics.errorCount,
            batchCount: this.metrics.batchCount,
            successRate: this.metrics.totalEmbeddings > 0 ? ((this.metrics.totalEmbeddings - this.metrics.errorCount) / this.metrics.totalEmbeddings * 100).toFixed(1) + "%" : "100%"
          }
        };
      }
      /**
       * Test embedding generation
       */
      async test() {
        try {
          const testText = "This is a test sentence for embedding generation.";
          const embedding = await this.generateEmbedding({ text: testText });
          return {
            success: true,
            dimension: embedding.length,
            sampleValues: embedding.slice(0, 5)
          };
        } catch (error) {
          return {
            success: false,
            error: error.message
          };
        }
      }
    };
    EmbeddingService_default = EmbeddingService;
  }
});

// src/services/SynonymService.ts
import fs2 from "fs";
import path3 from "path";
import pino13 from "pino";
var logger12, SynonymService, synonymService;
var init_SynonymService = __esm({
  "src/services/SynonymService.ts"() {
    "use strict";
    logger12 = pino13({ name: "SynonymService" });
    SynonymService = class {
      synonyms = {};
      loaded = false;
      constructor() {
        this.loadSynonyms();
      }
      loadSynonyms() {
        try {
          const candidatePaths = [
            path3.resolve(process.cwd(), "server/src/config/synonyms.json"),
            path3.resolve(process.cwd(), "src/config/synonyms.json")
          ];
          const configPath = candidatePaths.find((candidate) => fs2.existsSync(candidate));
          if (configPath) {
            const raw = fs2.readFileSync(configPath, "utf-8");
            this.synonyms = JSON.parse(raw);
            this.loaded = true;
            logger12.info(`Loaded synonyms for ${Object.keys(this.synonyms).length} terms.`);
          } else {
            logger12.warn("Synonym file not found in expected locations");
          }
        } catch (err) {
          const error = err instanceof Error ? err : new Error(String(err));
          logger12.error({ err: error }, "Failed to load synonyms");
        }
      }
      /**
       * Expands a query string by adding synonyms for known terms.
       * Returns the expanded query string.
       */
      expandQuery(query3) {
        if (!query3 || !this.loaded) return query3;
        const terms = query3.toLowerCase().split(/\s+/);
        const expandedTerms = /* @__PURE__ */ new Set();
        for (const term of terms) {
          expandedTerms.add(term);
          if (this.synonyms[term]) {
            this.synonyms[term].forEach((syn) => expandedTerms.add(syn));
          }
        }
        for (const key of Object.keys(this.synonyms)) {
          if (query3.toLowerCase().includes(key) && !terms.includes(key)) {
            this.synonyms[key].forEach((syn) => expandedTerms.add(syn));
          }
        }
        return Array.from(expandedTerms).join(" ");
      }
      getSynonyms(term) {
        return this.synonyms[term.toLowerCase()] || [];
      }
    };
    synonymService = new SynonymService();
  }
});

// src/services/SemanticSearchService.ts
var SemanticSearchService_exports = {};
__export(SemanticSearchService_exports, {
  default: () => SemanticSearchService
});
import { Pool as Pool5 } from "pg";
import pino14 from "pino";
var SemanticSearchService;
var init_SemanticSearchService = __esm({
  "src/services/SemanticSearchService.ts"() {
    "use strict";
    init_EmbeddingService();
    init_SynonymService();
    SemanticSearchService = class {
      embeddingService;
      logger = pino14({ name: "SemanticSearchService" });
      pool = null;
      ownsPool;
      poolFactory;
      healthCheckIntervalMs;
      healthCheckTimeoutMs;
      lastHealthCheckAt = 0;
      constructor(options2 = {}) {
        this.pool = options2.pool ?? null;
        this.ownsPool = !options2.pool;
        this.poolFactory = options2.poolFactory ?? (() => {
          if (!process.env.DATABASE_URL) {
            throw new Error("DATABASE_URL must be set to initialize SemanticSearchService pool");
          }
          return new Pool5({ connectionString: process.env.DATABASE_URL });
        });
        this.healthCheckIntervalMs = options2.healthCheckIntervalMs ?? 3e4;
        this.healthCheckTimeoutMs = options2.healthCheckTimeoutMs ?? 3e3;
        this.embeddingService = options2.embeddingService ?? new EmbeddingService_default();
      }
      // Use a managed pool or create one if needed
      async getPool() {
        if (!this.pool || this.pool.ended) {
          this.pool = this.poolFactory();
          this.lastHealthCheckAt = 0;
        }
        await this.runHealthCheckIfStale(this.pool);
        return this.pool;
      }
      async runHealthCheckIfStale(pool4) {
        const now = Date.now();
        if (now - this.lastHealthCheckAt < this.healthCheckIntervalMs) {
          return;
        }
        const healthCheck = pool4.query("SELECT 1");
        const timeout = new Promise(
          (_2, reject) => setTimeout(() => reject(new Error("Semantic search health check timed out")), this.healthCheckTimeoutMs)
        );
        await Promise.race([healthCheck, timeout]);
        this.lastHealthCheckAt = now;
      }
      // Ensure we close the pool if we created it
      async close() {
        if (this.pool && this.ownsPool) {
          await this.pool.end();
        }
        this.pool = null;
      }
      // Deprecated indexDocument for backward compatibility
      async indexDocument(doc) {
        this.logger.warn("indexDocument is deprecated in SemanticSearchService. Use specific indexing methods.");
        if (doc.id && doc.text) {
          await this.indexCase(doc.id, doc.text);
        }
      }
      // Deprecated search method for backward compatibility
      async search(query3, filters = {}, limit = 10) {
        this.logger.warn("search() is deprecated in SemanticSearchService. Use searchCases() or update usage.");
        const results = await this.searchCases(query3, {
          status: void 0
        }, limit);
        return results.map((r) => ({
          id: r.id,
          text: r.title,
          score: r.score,
          metadata: { status: r.status, date: r.created_at }
        }));
      }
      async indexCase(caseId, text) {
        try {
          const vector = await this.embeddingService.generateEmbedding({ text });
          const vectorStr = `[${vector.join(",")}]`;
          const pool4 = await this.getPool();
          const client6 = await pool4.connect();
          try {
            await client6.query(
              `UPDATE cases SET embedding = $1::vector WHERE id = $2`,
              [vectorStr, caseId]
            );
          } finally {
            client6.release();
          }
        } catch (err) {
          this.logger.error({ err, caseId }, "Failed to index case");
        }
      }
      async searchCases(query3, filters = {}, limit = 20) {
        try {
          const expandedQuery = synonymService.expandQuery(query3);
          this.logger.debug({ query: query3, expandedQuery }, "Expanded query with synonyms");
          const vector = await this.embeddingService.generateEmbedding({ text: expandedQuery });
          const vectorStr = `[${vector.join(",")}]`;
          const pool4 = await this.getPool();
          const client6 = await pool4.connect();
          try {
            const whereClauses = [`embedding IS NOT NULL`];
            const params = [vectorStr];
            let pIdx = 2;
            if (filters.status && filters.status.length) {
              whereClauses.push(`status = ANY($${pIdx})`);
              params.push(filters.status);
              pIdx++;
            }
            if (filters.dateFrom) {
              whereClauses.push(`created_at >= $${pIdx}`);
              params.push(filters.dateFrom);
              pIdx++;
            }
            if (filters.dateTo) {
              whereClauses.push(`created_at <= $${pIdx}`);
              params.push(filters.dateTo);
              pIdx++;
            }
            params.push(query3);
            const qIdx = pIdx;
            const sql = `
          SELECT
            id,
            title,
            status,
            created_at,
            1 - (embedding <=> $1::vector) as similarity,
            ts_rank(to_tsvector('english', title || ' ' || coalesce(description,'')), plainto_tsquery('english', $${qIdx})) as text_rank
          FROM cases
          WHERE ${whereClauses.join(" AND ")}
          ORDER BY ( (1 - (embedding <=> $1::vector)) * 0.7 + (ts_rank(to_tsvector('english', title || ' ' || coalesce(description,'')), plainto_tsquery('english', $${qIdx})) / 10.0) * 0.3 ) DESC
          LIMIT ${limit}
        `;
            const res = await client6.query(sql, params);
            return res.rows.map((r) => ({
              id: r.id,
              title: r.title,
              status: r.status,
              created_at: r.created_at,
              similarity: parseFloat(r.similarity),
              score: parseFloat(r.similarity)
            }));
          } finally {
            client6.release();
          }
        } catch (err) {
          this.logger.error({ err }, "Semantic search failed");
          return [];
        }
      }
      async searchDocs(query3, limit = 10) {
        try {
          const vector = await this.embeddingService.generateEmbedding({ text: query3 });
          const vectorStr = `[${vector.join(",")}]`;
          const pool4 = await this.getPool();
          const sql = `
        SELECT
          path,
          title,
          metadata,
          1 - (embedding <=> $1::vector) as similarity
        FROM knowledge_articles
        WHERE embedding IS NOT NULL
        ORDER BY similarity DESC
        LIMIT $2
      `;
          const res = await pool4.query(sql, [vectorStr, limit]);
          return res.rows.map((r) => ({
            path: r.path,
            title: r.title,
            metadata: r.metadata,
            similarity: parseFloat(r.similarity)
          }));
        } catch (err) {
          this.logger.error({ err }, "Doc search failed");
          return [];
        }
      }
    };
  }
});

// src/graphql/resolvers/entity.ts
import neo4j2 from "neo4j-driver";
import { randomUUID as uuidv43 } from "node:crypto";
import pino15 from "pino";
import axios from "axios";
import { GraphQLError as GraphQLError4 } from "graphql";
var logger13, entityResolvers, entity_default;
var init_entity = __esm({
  "src/graphql/resolvers/entity.ts"() {
    "use strict";
    init_neo4j();
    init_subscriptions();
    init_postgres();
    init_entityMocks();
    init_cacheHelper();
    init_auth2();
    init_ledger();
    logger13 = pino15();
    entityResolvers = {
      Query: {
        entity: authGuard(async (_2, { id }, context4) => {
          if (isNeo4jMockMode()) {
            return getMockEntity(id);
          }
          try {
            const entity = await context4.loaders.entityLoader.load(id);
            return transformNeo4jIntegers(entity);
          } catch (error) {
            logger13.error({ error, id }, "Error fetching entity by ID");
            logger13.warn("Falling back to mock entity data");
            return getMockEntity(id);
          }
        }),
        entities: authGuard(withCache(
          // Cache key generator
          (_parent, args, context4) => {
            const tenantId = context4?.user?.tenantId || "default";
            return listCacheKey("entities", { ...args, tenantId });
          },
          // Resolver implementation
          async (_2, {
            type,
            q,
            limit,
            offset
          }, context4) => {
            if (isNeo4jMockMode()) {
              return getMockEntities(type, q, limit, offset);
            }
            const driver3 = getNeo4jDriver();
            const session = driver3.session();
            try {
              const tenantId = context4.user.tenantId;
              let query3 = "MATCH (n:Entity";
              const params = { tenantId };
              if (type) {
                if (/^[a-zA-Z0-9_]+$/.test(type)) {
                  query3 += `:${type}`;
                }
              }
              query3 += ") WHERE n.tenantId = $tenantId";
              if (q) {
                try {
                  const fulltextQuery = `

                CALL db.index.fulltext.queryNodes("entity_fulltext_idx", $q) YIELD node, score

                WHERE node.tenantId = $tenantId

                ${type ? `AND $type IN labels(node)` : ""}

                RETURN node SKIP $offset LIMIT $limit

              `;
                  const result3 = await session.run(fulltextQuery, { ...params, q, type, offset: neo4j2.int(offset), limit: neo4j2.int(limit) });
                  return result3.records.map((record2) => {
                    const entity = record2.get("node");
                    return transformNeo4jIntegers({
                      id: entity.properties.id,
                      type: entity.labels[0],
                      props: entity.properties,
                      createdAt: entity.properties.createdAt,
                      updatedAt: entity.properties.updatedAt
                    });
                  });
                } catch (err) {
                  logger13.warn({ err }, "Fulltext search failed, falling back to legacy CONTAINS search");
                  query3 += " AND (ANY(prop IN keys(n) WHERE toString(n[prop]) CONTAINS $q))";
                  params.q = q;
                  query3 += " RETURN n SKIP $offset LIMIT $limit";
                }
              } else {
                query3 += " RETURN n SKIP $offset LIMIT $limit";
              }
              params.limit = neo4j2.int(limit);
              params.offset = neo4j2.int(offset);
              const result2 = await session.run(query3, params);
              return result2.records.map((record2) => {
                const entity = record2.get("n");
                return transformNeo4jIntegers({
                  id: entity.properties.id,
                  type: entity.labels[0],
                  props: entity.properties,
                  createdAt: entity.properties.createdAt,
                  updatedAt: entity.properties.updatedAt
                });
              });
            } catch (error) {
              logger13.error(
                { error, type, q, limit, offset },
                "Error fetching entities"
              );
              const message = error instanceof Error ? error.message : "Unknown error";
              throw new Error(`Failed to fetch entities: ${message}`);
            } finally {
              await session.close();
            }
          },
          // Cache options
          30
          // 30s TTL
        )),
        semanticSearch: authGuard(withCache(
          (_p, args, ctx) => listCacheKey("semanticSearch", { ...args, tenantId: ctx.user.tenantId }),
          async (_2, {
            query: query3,
            filters,
            limit,
            offset
          }, context4) => {
            const pgPool = getPostgresPool();
            const neo4jSession = driver.session();
            let pgClient;
            try {
              const tenantId = context4.user.tenantId;
              const mlServiceUrl = process.env.ML_SERVICE_URL || "http://localhost:8081";
              const embeddingResponse = await axios.post(
                `${mlServiceUrl}/gnn/generate_embeddings`,
                {
                  graph_data: { nodes: [{ id: "query", features: [] }] },
                  // Dummy graph for query embedding
                  node_features: { query: [0] },
                  // Placeholder for actual query features
                  model_name: "text_embedding_model",
                  // Assuming a text embedding model in ML service
                  job_id: `semantic-search-${Date.now()}`
                }
              );
              const queryEmbedding = embeddingResponse.data.node_embeddings.query;
              if (!queryEmbedding || queryEmbedding.length === 0) {
                throw new Error("Failed to get embedding for query from ML service.");
              }
              pgClient = await pgPool.connect();
              const embeddingVectorString = `[${queryEmbedding.join(",")}]`;
              let pgQuery = `SELECT ee.entity_id FROM entity_embeddings ee`;
              const pgQueryParams = [embeddingVectorString];
              let paramIndex = 2;
              const type = filters?.type;
              const props = filters?.props;
              pgQuery += ` JOIN entities e ON ee.entity_id = e.id WHERE e.tenant_id = $${paramIndex}`;
              pgQueryParams.push(tenantId);
              paramIndex++;
              if (type) {
                pgQuery += ` AND e.type = $${paramIndex}`;
                pgQueryParams.push(type);
                paramIndex++;
              }
              if (props) {
                pgQuery += ` AND e.props @> $${paramIndex}`;
                pgQueryParams.push(JSON.stringify(props));
                paramIndex++;
              }
              pgQuery += ` ORDER BY ee.embedding <-> $1 LIMIT ${paramIndex} OFFSET ${paramIndex + 1}`;
              pgQueryParams.push(limit);
              pgQueryParams.push(offset);
              const pgResult = await pgClient.query(pgQuery, pgQueryParams);
              const entityIds = pgResult.rows.map((row) => row.entity_id);
              if (entityIds.length === 0) {
                return [];
              }
              const searchService = new (await Promise.resolve().then(() => (init_SemanticSearchService(), SemanticSearchService_exports))).default();
              const docs = await searchService.search(
                query3,
                filters || {},
                limit + offset
              );
              const sliced = docs.slice(offset);
              const ids = sliced.map((d) => d.metadata.graphId).filter(Boolean);
              if (ids.length === 0) return [];
              const entities = await Promise.all(
                ids.map((id) => context4.loaders.entityLoader.load(id))
              );
              return entities.filter((entity) => !(entity instanceof Error));
            } catch (error) {
              logger13.error(
                { error, query: query3, filters },
                "Error performing semantic search with filters"
              );
              const message = error instanceof Error ? error.message : "Unknown error";
              throw new Error(`Failed to perform semantic search: ${message}`);
            } finally {
              await neo4jSession.close();
              if (pgClient) pgClient.release();
            }
          },
          60
        ))
      },
      Mutation: {
        createEntity: authGuard(async (_2, { input }, context4) => {
          if (!input.type || typeof input.type !== "string" || input.type.trim() === "") {
            throw new GraphQLError4("Entity type is required and must be a non-empty string", {
              extensions: {
                code: "BAD_USER_INPUT",
                http: { status: 400 }
              }
            });
          }
          const session = driver.session();
          try {
            const tenantId = context4.user.tenantId;
            const id = uuidv43();
            const createdAt = (/* @__PURE__ */ new Date()).toISOString();
            const type = input.type;
            const props = {
              ...input.props,
              id,
              createdAt,
              updatedAt: createdAt,
              tenantId
            };
            const result2 = await session.run(
              `CREATE (n:Entity:${type} $props) RETURN n`,
              { props }
            );
            const record2 = result2.records[0].get("n");
            const entity = {
              id: record2.properties.id,
              type: record2.labels[0],
              props: record2.properties,
              createdAt: record2.properties.createdAt,
              updatedAt: record2.properties.updatedAt,
              tenantId: record2.properties.tenantId
            };
            pubsub.publish(tenantEvent(ENTITY_CREATED, tenantId), {
              payload: entity
            });
            await provenanceLedger.appendEntry({
              tenantId,
              actionType: "CREATE_ENTITY",
              resourceType: "Entity",
              resourceId: entity.id,
              actorId: context4.user.id,
              actorType: "user",
              timestamp: /* @__PURE__ */ new Date(),
              payload: {
                mutationType: "CREATE",
                entityId: entity.id,
                entityType: "Entity",
                newState: entity
              },
              metadata: {
                correlationId: context4.telemetry.traceId,
                timestamp: (/* @__PURE__ */ new Date()).toISOString()
              }
            }).catch((err) => logger13.error({ err }, "Failed to append audit log"));
            return entity;
          } catch (error) {
            logger13.error({ error, input }, "Error creating entity");
            if (error instanceof GraphQLError4) {
              throw error;
            }
            const message = error instanceof Error ? error.message : "Unknown error";
            throw new GraphQLError4(`Failed to create entity: ${message}`, {
              extensions: {
                code: "INTERNAL_SERVER_ERROR",
                http: { status: 500 }
              }
            });
          } finally {
            await session.close();
          }
        }),
        updateEntity: authGuard(async (_2, {
          id,
          input,
          lastSeenTimestamp
        }, context4) => {
          const session = driver.session();
          try {
            const tenantId = context4.user.tenantId;
            const existing = await session.run(
              "MATCH (n:Entity {id: $id, tenantId: $tenantId}) RETURN n",
              { id, tenantId }
            );
            if (existing.records.length === 0) {
              return null;
            }
            const current = existing.records[0].get("n").properties;
            if (current.updatedAt && new Date(current.updatedAt).toISOString() !== new Date(lastSeenTimestamp).toISOString()) {
              const err = new Error("Conflict: Entity has been modified");
              err.extensions = { code: "CONFLICT", server: current };
              throw err;
            }
            const updatedAt = (/* @__PURE__ */ new Date()).toISOString();
            let query3 = "MATCH (n:Entity {id: $id, tenantId: $tenantId})";
            const params = { id, updatedAt, tenantId };
            if (input.type) {
              query3 += ` REMOVE n:${input.type} SET n:${input.type}`;
            }
            if (input.props) {
              query3 += " SET n += $props, n.updatedAt = $updatedAt";
              params.props = input.props;
            } else {
              query3 += " SET n.updatedAt = $updatedAt";
            }
            query3 += " RETURN n";
            const result2 = await session.run(query3, params);
            if (result2.records.length === 0) {
              return null;
            }
            const record2 = result2.records[0].get("n");
            const entity = {
              id: record2.properties.id,
              type: record2.labels[0],
              props: record2.properties,
              createdAt: record2.properties.createdAt,
              updatedAt: record2.properties.updatedAt,
              tenantId: record2.properties.tenantId
            };
            pubsub.publish(tenantEvent(ENTITY_UPDATED, tenantId), {
              payload: entity
            });
            await provenanceLedger.appendEntry({
              tenantId,
              actionType: "UPDATE_ENTITY",
              resourceType: "Entity",
              resourceId: entity.id,
              actorId: context4.user.id,
              actorType: "user",
              timestamp: /* @__PURE__ */ new Date(),
              payload: {
                mutationType: "UPDATE",
                entityId: entity.id,
                entityType: "Entity",
                newState: entity
              },
              metadata: {
                correlationId: context4.telemetry.traceId,
                timestamp: (/* @__PURE__ */ new Date()).toISOString()
              }
            }).catch((err) => logger13.error({ err }, "Failed to append audit log"));
            return entity;
          } catch (error) {
            logger13.error({ error, id, input }, "Error updating entity");
            const message = error instanceof Error ? error.message : "Unknown error";
            throw new Error(`Failed to update entity: ${message}`);
          } finally {
            await session.close();
          }
        }),
        deleteEntity: authGuard(async (_2, { id }, context4) => {
          const session = driver.session();
          try {
            const tenantId = context4.user.tenantId;
            const deletedAt = (/* @__PURE__ */ new Date()).toISOString();
            const result2 = await session.run(
              "MATCH (n:Entity {id: $id, tenantId: $tenantId}) SET n.deletedAt = $deletedAt RETURN n",
              { id, deletedAt, tenantId }
            );
            if (result2.records.length === 0) {
              return false;
            }
            pubsub.publish(tenantEvent(ENTITY_DELETED, tenantId), { payload: id });
            await provenanceLedger.appendEntry({
              tenantId,
              actionType: "DELETE_ENTITY",
              resourceType: "Entity",
              resourceId: id,
              actorId: context4.user.id,
              actorType: "user",
              timestamp: /* @__PURE__ */ new Date(),
              payload: {
                mutationType: "DELETE",
                entityId: id,
                entityType: "Entity"
              },
              metadata: {
                correlationId: context4.telemetry.traceId,
                timestamp: (/* @__PURE__ */ new Date()).toISOString()
              }
            }).catch((err) => logger13.error({ err }, "Failed to append audit log"));
            return true;
          } catch (error) {
            logger13.error({ error, id }, "Error deleting entity");
            const message = error instanceof Error ? error.message : "Unknown error";
            throw new Error(`Failed to delete entity: ${message}`);
          } finally {
            await session.close();
          }
        }),
        linkEntities: authGuard(async (_2, { text }, context4) => {
          try {
            const mlServiceUrl = process.env.ML_SERVICE_URL || "http://localhost:8081";
            const response = await axios.post(
              `${mlServiceUrl}/nlp/entity_linking`,
              {
                text,
                job_id: `entity-linking-${Date.now()}`
              }
            );
            if (response.data.status === "completed" && response.data.entities) {
              return response.data.entities.map((entity) => ({
                text: entity.text,
                label: entity.label,
                startChar: entity.start_char,
                endChar: entity.end_char,
                entityId: entity.entity_id
              }));
            } else {
              throw new Error(
                `ML service did not return completed entities: ${response.data.status}`
              );
            }
          } catch (error) {
            logger13.error({ error, text }, "Error linking entities");
            const message = error instanceof Error ? error.message : "Unknown error";
            throw new Error(`Failed to link entities: ${message}`);
          }
        }),
        extractRelationships: authGuard(async (_2, { text, entities }, context4) => {
          const neo4jSession = driver.session();
          try {
            const mlServiceUrl = process.env.ML_SERVICE_URL || "http://localhost:8081";
            const response = await axios.post(
              `${mlServiceUrl}/nlp/relationship_extraction`,
              {
                text,
                entities,
                job_id: `relationship-extraction-${Date.now()}`
              }
            );
            if (response.data.status === "completed" && response.data.relationships) {
              const tenantId = context4.user.tenantId;
              const extractedRelationships = response.data.relationships.map(
                (rel) => ({
                  sourceEntityId: rel.source_entity_id,
                  targetEntityId: rel.target_entity_id,
                  type: rel.type,
                  confidence: rel.confidence,
                  textSpan: rel.text_span
                })
              );
              for (const rel of extractedRelationships) {
                const createdAt = (/* @__PURE__ */ new Date()).toISOString();
                const props = {
                  id: uuidv43(),
                  // Generate a new ID for the relationship
                  confidence: rel.confidence,
                  textSpan: rel.textSpan,
                  createdAt,
                  tenantId
                };
                await neo4jSession.run(
                  `
              MATCH (source:Entity {id: $sourceId, tenantId: $tenantId})
              MATCH (target:Entity {id: $targetId, tenantId: $tenantId})
              CREATE (source)-[r:${rel.type} $props]->(target)
              RETURN r
              `,
                  {
                    sourceId: rel.sourceEntityId,
                    targetId: rel.targetEntityId,
                    tenantId,
                    props
                  }
                );
                logger13.info(
                  `Created relationship: ${rel.sourceEntityId}-[${rel.type}]->${rel.targetEntityId}`
                );
              }
              return extractedRelationships;
            } else {
              throw new Error(
                `ML service did not return completed relationships: ${response.data.status}`
              );
            }
          } catch (error) {
            logger13.error(
              { error, text, entities },
              "Error extracting relationships"
            );
            const message = error instanceof Error ? error.message : "Unknown error";
            throw new Error(`Failed to extract relationships: ${message}`);
          } finally {
            await neo4jSession.close();
          }
        })
      }
    };
    entity_default = entityResolvers;
  }
});

// src/graphql/resolvers/relationship.ts
import { randomUUID as randomUUID3 } from "node:crypto";
import pino16 from "pino";
import { GraphQLError as GraphQLError5 } from "graphql";
var logger14, driver2, relationshipResolvers, relationship_default;
var init_relationship = __esm({
  "src/graphql/resolvers/relationship.ts"() {
    "use strict";
    init_neo4j();
    init_subscriptions();
    init_auth2();
    logger14 = pino16();
    driver2 = getNeo4jDriver();
    relationshipResolvers = {
      Mutation: {
        createRelationship: authGuard(async (_2, {
          input
        }, context4) => {
          const session = driver2.session();
          try {
            const tenantId = context4.user.tenantId;
            const id = randomUUID3();
            const createdAt = (/* @__PURE__ */ new Date()).toISOString();
            const props = {
              ...input.props,
              id,
              createdAt,
              updatedAt: createdAt,
              tenantId
            };
            const result2 = await session.run(
              `
          MATCH (fromNode:Entity {id: $from, tenantId: $tenantId})
          MATCH (toNode:Entity {id: $to, tenantId: $tenantId})
          CREATE (fromNode)-[r:${input.type} $props]->(toNode)
          RETURN r
          `,
              { from: input.from, to: input.to, tenantId, props }
            );
            if (result2.records.length === 0) {
              throw new GraphQLError5("One or both entities not found", {
                extensions: {
                  code: "BAD_USER_INPUT",
                  http: { status: 400 },
                  fromEntityId: input.from,
                  toEntityId: input.to
                }
              });
            }
            const record2 = result2.records[0].get("r");
            const relationship = {
              id: record2.properties.id,
              from: input.from,
              to: input.to,
              type: record2.type,
              props: record2.properties,
              createdAt: record2.properties.createdAt,
              tenantId: record2.properties.tenantId
            };
            pubsub.publish(tenantEvent(RELATIONSHIP_CREATED, tenantId), {
              payload: relationship
            });
            return relationship;
          } catch (error) {
            logger14.error({ error, input }, "Error creating relationship");
            if (error instanceof GraphQLError5) {
              throw error;
            }
            const message = error instanceof Error ? error.message : "Unknown error";
            throw new GraphQLError5(`Failed to create relationship: ${message}`, {
              extensions: {
                code: "INTERNAL_SERVER_ERROR",
                http: { status: 500 }
              }
            });
          } finally {
            await session.close();
          }
        }),
        updateRelationship: authGuard(async (_2, {
          id,
          input,
          lastSeenTimestamp
        }, context4) => {
          const session = driver2.session();
          try {
            const tenantId = context4.user.tenantId;
            const existing = await session.run(
              "MATCH ()-[r:Relationship {id: $id, tenantId: $tenantId}]->() RETURN r",
              { id, tenantId }
            );
            if (existing.records.length === 0) {
              return null;
            }
            const current = existing.records[0].get("r").properties;
            if (current.updatedAt && new Date(current.updatedAt).toISOString() !== new Date(lastSeenTimestamp).toISOString()) {
              const err = new Error(
                "Conflict: Relationship has been modified"
              );
              err.extensions = { code: "CONFLICT", server: current };
              throw err;
            }
            const updatedAt = (/* @__PURE__ */ new Date()).toISOString();
            let query3 = "MATCH ()-[r:Relationship {id: $id, tenantId: $tenantId}]->()";
            const params = { id, updatedAt, tenantId };
            if (input.type) {
              logger14.warn(
                "Changing relationship type is not fully supported in updateRelationship resolver."
              );
            }
            if (input.props) {
              query3 += " SET r += $props, r.updatedAt = $updatedAt";
              params.props = input.props;
            } else {
              query3 += " SET r.updatedAt = $updatedAt";
            }
            query3 += " RETURN r";
            const result2 = await session.run(query3, params);
            if (result2.records.length === 0) {
              return null;
            }
            const record2 = result2.records[0].get("r");
            const relationship = {
              id: record2.properties.id,
              from: record2.properties.from,
              to: record2.properties.to,
              type: record2.type,
              props: record2.properties,
              createdAt: record2.properties.createdAt,
              updatedAt: record2.properties.updatedAt,
              tenantId: record2.properties.tenantId
            };
            pubsub.publish(tenantEvent(RELATIONSHIP_UPDATED, tenantId), {
              payload: relationship
            });
            return relationship;
          } catch (error) {
            logger14.error({ error, id, input }, "Error updating relationship");
            const message = error instanceof Error ? error.message : "Unknown error";
            throw new Error(`Failed to update relationship: ${message}`);
          } finally {
            await session.close();
          }
        }),
        deleteRelationship: authGuard(async (_2, { id }, context4) => {
          const session = driver2.session();
          try {
            const tenantId = context4.user.tenantId;
            const deletedAt = (/* @__PURE__ */ new Date()).toISOString();
            const result2 = await session.run(
              "MATCH ()-[r:Relationship {id: $id, tenantId: $tenantId}]->() SET r.deletedAt = $deletedAt RETURN r",
              { id, deletedAt, tenantId }
            );
            if (result2.records.length === 0) {
              return false;
            }
            pubsub.publish(tenantEvent(RELATIONSHIP_DELETED, tenantId), {
              payload: id
            });
            return true;
          } catch (error) {
            logger14.error({ error, id }, "Error deleting relationship");
            const message = error instanceof Error ? error.message : "Unknown error";
            throw new Error(`Failed to delete relationship: ${message}`);
          } finally {
            await session.close();
          }
        })
      }
    };
    relationship_default = relationshipResolvers;
  }
});

// src/monitoring/businessMetrics.ts
import pino17 from "pino";
function normalize(value, fallback) {
  if (!value) {
    return fallback;
  }
  return value.toLowerCase().replace(/[^a-z0-9-_\.]/g, "-");
}
function recordUserSignup(event) {
  const tenant = normalize(event.tenant, "unknown");
  const plan = normalize(event.plan, "unspecified");
  businessUserSignupsTotal2.inc({ tenant, plan });
  logger15.debug(
    { tenant, plan, metadata: event.metadata },
    "Recorded user signup metric"
  );
}
function recordApiCall(event) {
  const tenant = normalize(event.tenant, "unknown");
  const service11 = normalize(event.service, "core");
  const route = event.route ?? "unknown";
  const statusCode = event.statusCode ?? 200;
  businessApiCallsTotal2.inc({
    tenant,
    service: service11,
    route,
    status_code: String(statusCode)
  });
  logger15.debug(
    { tenant, service: service11, route, statusCode, metadata: event.metadata },
    "Recorded API call metric"
  );
}
function recordRevenue(event) {
  const tenant = normalize(event.tenant, "unknown");
  const currency = normalize(event.currency, "usd").toUpperCase();
  const amount = Number(event.amount ?? 0);
  if (!Number.isFinite(amount) || amount < 0) {
    logger15.warn(
      { tenant, currency, amount },
      "Ignored revenue metric with invalid amount"
    );
    return;
  }
  businessRevenueTotal2.inc({ tenant, currency }, amount);
  logger15.debug(
    { tenant, currency, amount, metadata: event.metadata },
    "Recorded revenue metric"
  );
}
function recordBusinessEvent(event) {
  switch (event.type) {
    case "user_signup":
      recordUserSignup(event);
      break;
    case "api_call":
      recordApiCall(event);
      break;
    case "revenue":
      recordRevenue(event);
      break;
    default:
      logger15.warn({ event }, "Unsupported business metric event type received");
  }
}
var logger15;
var init_businessMetrics = __esm({
  "src/monitoring/businessMetrics.ts"() {
    "use strict";
    init_metrics2();
    logger15 = pino17({ name: "business-metrics" });
  }
});

// src/observability/metrics.ts
var metrics3;
var init_metrics4 = __esm({
  "src/observability/metrics.ts"() {
    "use strict";
    init_metrics2();
    metrics3 = {
      jobsProcessed: intelgraphJobsProcessed,
      outboxSyncLatency: intelgraphOutboxSyncLatency,
      activeConnections: intelgraphActiveConnections,
      databaseQueryDuration: intelgraphDatabaseQueryDuration,
      httpRequestDuration: intelgraphHttpRequestDuration,
      graphragQueryTotal: intelgraphGraphragQueryTotal,
      graphragQueryDurationMs: intelgraphGraphragQueryDurationMs,
      queryPreviewsTotal: intelgraphQueryPreviewsTotal,
      queryPreviewLatencyMs: intelgraphQueryPreviewLatencyMs,
      queryPreviewErrorsTotal: intelgraphQueryPreviewErrorsTotal,
      queryPreviewExecutionsTotal: intelgraphQueryPreviewExecutionsTotal,
      glassBoxRunsTotal: intelgraphGlassBoxRunsTotal,
      glassBoxRunDurationMs: intelgraphGlassBoxRunDurationMs,
      glassBoxCacheHits: intelgraphGlassBoxCacheHits,
      cacheHits: intelgraphCacheHits,
      cacheMisses: intelgraphCacheMisses,
      copilotApiRequestTotal: copilotApiRequestTotal2,
      copilotApiRequestDurationMs: copilotApiRequestDurationMs2,
      maestroDagExecutionDurationSeconds,
      maestroJobExecutionDurationSeconds,
      llmTokensTotal,
      llmRequestDuration,
      intelgraphCacheHits,
      intelgraphCacheMisses,
      intelgraphActiveConnections,
      intelgraphDatabaseQueryDuration,
      intelgraphHttpRequestDuration,
      intelgraphGraphragQueryTotal,
      intelgraphGraphragQueryDurationMs,
      intelgraphQueryPreviewsTotal,
      intelgraphQueryPreviewLatencyMs,
      intelgraphQueryPreviewErrorsTotal,
      intelgraphQueryPreviewExecutionsTotal,
      intelgraphGlassBoxRunsTotal,
      intelgraphGlassBoxRunDurationMs,
      intelgraphGlassBoxCacheHits,
      goldenPathStepTotal,
      uiErrorBoundaryCatchTotal,
      maestroDeploymentsTotal: maestroDeploymentsTotal2,
      maestroPrLeadTimeHours,
      maestroChangeFailureRate: maestroChangeFailureRate2,
      maestroMttrHours,
      stdHttpRequestsTotal: httpRequestsTotal2,
      stdHttpRequestDuration: httpRequestDuration2,
      dbQueriesTotal: dbQueriesTotal2,
      dbQueryDuration: dbQueryDuration2,
      websocketConnections,
      narrativeSimulationActiveSimulations,
      narrativeSimulationTicksTotal,
      narrativeSimulationEventsTotal,
      narrativeSimulationDurationSeconds
    };
  }
});

// src/lib/cache/multi-tier-cache.ts
import Redis5 from "ioredis";
import pino18 from "pino";
function createRedisClient() {
  try {
    const client6 = new Redis5({
      host: cfg.REDIS_HOST,
      port: cfg.REDIS_PORT,
      password: cfg.REDIS_PASSWORD
    });
    client6.on(
      "connect",
      () => logger16.info("Redis client connected for multi-tier cache.")
    );
    client6.on(
      "error",
      (err) => logger16.error({ err }, "Redis cache client error.")
    );
    return client6;
  } catch (error) {
    logger16.warn({ error }, "Redis unavailable, using in-memory cache only.");
    return null;
  }
}
function buildPayload(value, ttlSeconds, tags) {
  return {
    value,
    tags,
    expiresAt: Date.now() + ttlSeconds * 1e3
  };
}
var logger16, INVALIDATION_CHANNEL, SimpleLRUCache, MultiTierCache;
var init_multi_tier_cache = __esm({
  "src/lib/cache/multi-tier-cache.ts"() {
    "use strict";
    init_config();
    init_metrics4();
    logger16 = pino18();
    INVALIDATION_CHANNEL = "cache:invalidation";
    SimpleLRUCache = class {
      maxSize;
      currentSize;
      cache;
      constructor(options2) {
        this.maxSize = options2.maxSize;
        this.currentSize = 0;
        this.cache = /* @__PURE__ */ new Map();
      }
      calculateSize(key, value) {
        const keyString = String(key);
        const valueString = JSON.stringify(value);
        return Buffer.byteLength(keyString, "utf8") + Buffer.byteLength(valueString, "utf8");
      }
      get(key) {
        const entry = this.cache.get(key);
        if (!entry) return void 0;
        if (Date.now() > entry.expiry) {
          this.currentSize -= entry.size;
          this.cache.delete(key);
          return void 0;
        }
        this.cache.delete(key);
        this.cache.set(key, entry);
        return entry.value;
      }
      delete(key) {
        const entry = this.cache.get(key);
        if (entry) {
          this.currentSize -= entry.size;
          this.cache.delete(key);
        }
      }
      set(key, value, options2) {
        const ttl = options2?.ttl ?? Infinity;
        const size = this.calculateSize(key, value);
        if (size > this.maxSize) {
          logger16.warn(
            { key, size, maxSize: this.maxSize },
            "Item is larger than the cache size and will not be stored."
          );
          return;
        }
        if (this.cache.has(key)) {
          const oldEntry = this.cache.get(key);
          this.currentSize -= oldEntry.size;
          this.cache.delete(key);
        }
        while (this.currentSize + size > this.maxSize && this.cache.size > 0) {
          const oldestKey = this.cache.keys().next().value;
          const oldestEntry = this.cache.get(oldestKey);
          this.currentSize -= oldestEntry.size;
          this.cache.delete(oldestKey);
        }
        const expiry = ttl === Infinity ? Infinity : Date.now() + ttl;
        this.cache.set(key, { value, expiry, size });
        this.currentSize += size;
      }
    };
    MultiTierCache = class {
      namespace;
      defaultTtlSeconds;
      l1FallbackTtlMs;
      cacheEnabled;
      l1Cache;
      redis;
      subscriber;
      inflight = /* @__PURE__ */ new Map();
      keyTags = /* @__PURE__ */ new Map();
      tagIndex = /* @__PURE__ */ new Map();
      constructor(options2 = {}) {
        this.namespace = options2.namespace ?? "cache";
        this.defaultTtlSeconds = options2.defaultTtlSeconds ?? cfg.CACHE_TTL_DEFAULT;
        this.l1FallbackTtlMs = (options2.l1FallbackTtlSeconds ?? cfg.L1_CACHE_FALLBACK_TTL_SECONDS) * 1e3;
        this.cacheEnabled = options2.cacheEnabled ?? cfg.CACHE_ENABLED;
        this.l1Cache = new SimpleLRUCache({
          maxSize: options2.l1MaxBytes ?? cfg.L1_CACHE_MAX_BYTES
        });
        this.redis = options2.redisClient ?? createRedisClient();
        this.subscriber = options2.subscriberClient ?? this.redis?.duplicate?.() ?? null;
        this.setupPubSub();
      }
      async get(key) {
        if (!this.cacheEnabled) return null;
        const namespacedKey = this.namespaced(key);
        const l1Value = this.l1Cache.get(namespacedKey);
        if (l1Value) {
          metrics3.cacheHits.inc({ level: "l1" });
          return l1Value.value;
        }
        const payload = await this.readFromRedis(namespacedKey);
        if (payload) {
          metrics3.cacheHits.inc({ level: "l3" });
          const remainingMs = payload.expiresAt - Date.now();
          const ttlMs = Math.min(remainingMs, this.l1FallbackTtlMs);
          this.l1Cache.set(namespacedKey, payload, { ttl: ttlMs });
          this.trackTags(namespacedKey, payload.tags);
          return payload.value;
        }
        metrics3.cacheMisses.inc();
        return null;
      }
      async set(key, value, ttlOrOptions, tags) {
        if (!this.cacheEnabled) return;
        const { ttlSeconds, tags: resolvedTags } = this.normalizeSetOptions(
          ttlOrOptions,
          tags
        );
        const namespacedKey = this.namespaced(key);
        const payload = buildPayload(value, ttlSeconds, resolvedTags);
        this.l1Cache.set(namespacedKey, payload, { ttl: ttlSeconds * 1e3 });
        this.trackTags(namespacedKey, resolvedTags);
        if (!this.redis) return;
        try {
          await this.redis.setex(namespacedKey, ttlSeconds, JSON.stringify(payload));
          await this.indexTags(namespacedKey, resolvedTags, ttlSeconds);
          await this.publishInvalidation({ type: "key", keys: [namespacedKey] });
        } catch (error) {
          logger16.error({ err: error, key: namespacedKey }, "Failed to set key in cache");
        }
      }
      async wrap(key, factory2, options2) {
        const cached = await this.get(key);
        if (cached !== null) return cached;
        const namespacedKey = this.namespaced(key);
        if (this.inflight.has(namespacedKey)) {
          return await this.inflight.get(namespacedKey);
        }
        const inflightPromise = (async () => {
          const value = await factory2();
          await this.set(key, value, options2);
          return value;
        })();
        this.inflight.set(namespacedKey, inflightPromise);
        try {
          return await inflightPromise;
        } finally {
          this.inflight.delete(namespacedKey);
        }
      }
      async invalidate(key) {
        const namespacedKey = this.namespaced(key);
        const cachedTags = this.keyTags.get(namespacedKey);
        this.evictLocal(namespacedKey);
        if (!this.redis) return;
        try {
          const tags = await this.readTags(namespacedKey) ?? cachedTags;
          await this.redis.del(namespacedKey);
          await this.cleanupTagIndexes(namespacedKey, tags);
          await this.publishInvalidation({ type: "key", keys: [namespacedKey] });
        } catch (error) {
          logger16.error({ err: error, key: namespacedKey }, "Failed to invalidate key in cache");
        }
      }
      async invalidateByTag(tag) {
        const tagKey = this.tagIndexKey(tag);
        const keys = await this.collectKeysForTag(tagKey);
        keys.forEach((k) => this.evictLocal(k));
        if (!this.redis) return;
        try {
          if (keys.length > 0) {
            await this.redis.del(...keys);
          }
          await this.redis.del(tagKey);
          await this.publishInvalidation({ type: "tag", tag, keys });
        } catch (error) {
          logger16.error({ err: error, tag }, "Failed to invalidate tag cache");
        }
      }
      async shutdown() {
        if (this.subscriber) {
          await this.subscriber.quit();
        }
        if (this.redis) {
          await this.redis.quit();
        }
      }
      namespaced(key) {
        return `${this.namespace}:${key}`;
      }
      tagIndexKey(tag) {
        return `${this.namespace}:tag:${tag}`;
      }
      normalizeSetOptions(ttlOrOptions, tags) {
        if (typeof ttlOrOptions === "number") {
          return {
            ttlSeconds: ttlOrOptions,
            tags
          };
        }
        return {
          ttlSeconds: ttlOrOptions?.ttlSeconds ?? this.defaultTtlSeconds,
          tags: ttlOrOptions?.tags ?? tags
        };
      }
      trackTags(key, tags) {
        if (!tags || tags.length === 0) return;
        this.dropTagTracking(key);
        this.keyTags.set(key, tags);
        tags.forEach((tag) => {
          const entry = this.tagIndex.get(tag) ?? /* @__PURE__ */ new Set();
          entry.add(key);
          this.tagIndex.set(tag, entry);
        });
      }
      evictLocal(key) {
        this.l1Cache.delete(key);
        this.dropTagTracking(key);
      }
      dropTagTracking(key) {
        const tags = this.keyTags.get(key);
        if (tags) {
          tags.forEach((tag) => {
            const entry = this.tagIndex.get(tag);
            if (!entry) return;
            entry.delete(key);
            if (entry.size === 0) {
              this.tagIndex.delete(tag);
            }
          });
        }
        this.keyTags.delete(key);
      }
      async readFromRedis(key) {
        if (!this.redis) return null;
        try {
          const raw = await this.redis.get(key);
          if (!raw) return null;
          const payload = JSON.parse(raw);
          if (payload.expiresAt <= Date.now()) {
            await this.redis.del(key);
            return null;
          }
          this.trackTags(key, payload.tags);
          return payload;
        } catch (error) {
          logger16.error({ err: error, key }, "Failed to read from cache");
          return null;
        }
      }
      async readTags(key) {
        if (!this.redis) return this.keyTags.get(key);
        try {
          const raw = await this.redis.get(key);
          if (!raw) return this.keyTags.get(key);
          const payload = JSON.parse(raw);
          return payload.tags;
        } catch (error) {
          logger16.warn({ err: error, key }, "Unable to read tags for key");
          return this.keyTags.get(key);
        }
      }
      async indexTags(key, tags, ttlSeconds) {
        if (!this.redis || !tags || tags.length === 0) return;
        await Promise.all(
          tags.map(async (tag) => {
            const tagKey = this.tagIndexKey(tag);
            await this.redis.sadd(tagKey, key);
            await this.redis.expire(tagKey, ttlSeconds);
          })
        );
      }
      async cleanupTagIndexes(key, tags) {
        if (!this.redis || !tags || tags.length === 0) return;
        await Promise.all(
          tags.map(async (tag) => {
            const tagKey = this.tagIndexKey(tag);
            await this.redis.srem(tagKey, key);
            const remaining = await this.redis.smembers(tagKey);
            if (remaining.length === 0) {
              await this.redis.del(tagKey);
            }
          })
        );
      }
      async collectKeysForTag(tagKey) {
        if (!this.redis) {
          const tag = tagKey.split(":").at(-1);
          if (!tag) return [];
          return Array.from(this.tagIndex.get(tag) ?? []);
        }
        try {
          const members = await this.redis.smembers(tagKey);
          return members.filter(Boolean);
        } catch (error) {
          logger16.warn({ err: error, tagKey }, "Failed to read tag index");
          return [];
        }
      }
      async publishInvalidation(message) {
        if (!this.redis) return;
        try {
          await this.redis.publish(INVALIDATION_CHANNEL, JSON.stringify(message));
        } catch (error) {
          logger16.warn({ err: error, message }, "Failed to publish invalidation");
        }
      }
      setupPubSub() {
        if (!this.subscriber) return;
        this.subscriber.on("message", (channel, message) => {
          if (channel !== INVALIDATION_CHANNEL) return;
          try {
            const payload = JSON.parse(message);
            if (payload.type === "key") {
              payload.keys.forEach((key) => this.evictLocal(key));
            } else if (payload.type === "tag") {
              payload.keys.forEach((key) => this.evictLocal(key));
            }
          } catch (error) {
            logger16.warn({ err: error, message }, "Failed to process invalidation");
          }
        });
        this.subscriber.subscribe(INVALIDATION_CHANNEL).catch(
          (error) => logger16.warn({ err: error }, "Failed to subscribe to invalidation channel")
        );
      }
    };
  }
});

// src/lib/cache/index.ts
var cache;
var init_cache = __esm({
  "src/lib/cache/index.ts"() {
    "use strict";
    init_multi_tier_cache();
    cache = new MultiTierCache();
  }
});

// src/graphql/resolvers/user.ts
import { GraphQLError as GraphQLError6 } from "graphql";
import pino19 from "pino";
import argon2 from "argon2";
function formatUser(dbUser) {
  return {
    id: dbUser.id,
    email: dbUser.email,
    username: dbUser.username,
    firstName: dbUser.first_name,
    lastName: dbUser.last_name,
    fullName: dbUser.first_name && dbUser.last_name ? `${dbUser.first_name} ${dbUser.last_name}` : void 0,
    role: dbUser.role,
    isActive: dbUser.is_active,
    lastLogin: dbUser.last_login,
    preferences: dbUser.preferences,
    createdAt: dbUser.created_at,
    updatedAt: dbUser.updated_at
  };
}
var logger17, userResolvers, user_default;
var init_user = __esm({
  "src/graphql/resolvers/user.ts"() {
    "use strict";
    init_businessMetrics();
    init_cache();
    init_config();
    init_database();
    init_auth2();
    logger17 = pino19();
    userResolvers = {
      Query: {
        /**
         * Fetch a single user by ID
         */
        user: authGuard(async (_2, { id }, context4) => {
          const cacheKey = `user:${id}`;
          try {
            const cachedUser = await cache.get(cacheKey);
            if (cachedUser) {
              logger17.info({ userId: id }, "[CACHE HIT] Found user in cache");
              return cachedUser;
            }
            logger17.info({ userId: id }, "[CACHE MISS] Fetching user from database");
            const pool4 = getPostgresPool2();
            const result2 = await pool4.query(
              `SELECT id, email, username, first_name, last_name, role, is_active,
                  last_login, preferences, created_at, updated_at
           FROM users
           WHERE id = $1`,
              [id]
            );
            if (result2.rows.length === 0) {
              return null;
            }
            const user = formatUser(result2.rows[0]);
            await cache.set(cacheKey, user, cfg.CACHE_TTL_DEFAULT);
            return user;
          } catch (error) {
            logger17.error({ userId: id, error }, "Failed to fetch user");
            throw new GraphQLError6("Failed to fetch user", {
              extensions: { code: "USER_FETCH_FAILED" }
            });
          }
        }),
        /**
         * List users with pagination
         */
        users: authGuard(async (_2, { limit = 25, offset = 0 }, context4) => {
          try {
            if (context4.user && !context4.user.roles.includes("ADMIN")) {
              throw new GraphQLError6("Only administrators can list all users", {
                extensions: { code: "FORBIDDEN" }
              });
            }
            const pool4 = getPostgresPool2();
            const result2 = await pool4.query(
              `SELECT id, email, username, first_name, last_name, role, is_active,
                  last_login, preferences, created_at, updated_at
           FROM users
           WHERE is_active = true
           ORDER BY created_at DESC
           LIMIT $1 OFFSET $2`,
              [Math.min(limit, 100), offset]
              // Cap at 100 to prevent excessive queries
            );
            logger17.info(
              { limit, offset, count: result2.rows.length },
              "Fetched users list"
            );
            return result2.rows.map((row) => formatUser(row));
          } catch (error) {
            if (error instanceof GraphQLError6) {
              throw error;
            }
            logger17.error({ limit, offset, error }, "Failed to fetch users list");
            throw new GraphQLError6("Failed to fetch users", {
              extensions: { code: "USERS_FETCH_FAILED" }
            });
          }
        })
      },
      Mutation: {
        /**
         * Create a new user (admin only)
         */
        createUser: authGuard(async (_2, { input }, context4) => {
          try {
            if (!context4.user.roles.includes("ADMIN")) {
              throw new GraphQLError6("Only administrators can create users", {
                extensions: { code: "FORBIDDEN" }
              });
            }
            const pool4 = getPostgresPool2();
            const existingUser = await pool4.query(
              "SELECT id FROM users WHERE email = $1 OR username = $2",
              [input.email.toLowerCase(), input.username]
            );
            if (existingUser.rows.length > 0) {
              throw new GraphQLError6("User with this email or username already exists", {
                extensions: { code: "USER_EXISTS" }
              });
            }
            const tempPassword = __require("crypto").randomBytes(16).toString("hex");
            const passwordHash = await argon2.hash(tempPassword);
            const result2 = await pool4.query(
              `INSERT INTO users (email, username, password_hash, role, is_active)
           VALUES ($1, $2, $3, $4, true)
           RETURNING id, email, username, first_name, last_name, role, is_active, created_at`,
              [
                input.email.toLowerCase(),
                input.username || input.email.split("@")[0],
                passwordHash,
                "VIEWER"
                // Default role
              ]
            );
            const user = formatUser(result2.rows[0]);
            recordUserSignup({
              tenant: "global",
              plan: "standard",
              metadata: { email: input.email }
            });
            logger17.info(
              { userId: user.id, email: user.email },
              "User created via GraphQL"
            );
            return user;
          } catch (error) {
            if (error instanceof GraphQLError6) {
              throw error;
            }
            logger17.error({ input, error }, "Failed to create user");
            throw new GraphQLError6("Failed to create user", {
              extensions: { code: "USER_CREATE_FAILED" }
            });
          }
        }),
        /**
         * Update an existing user
         */
        updateUser: authGuard(async (_2, {
          id,
          input
        }, context4) => {
          try {
            if (context4.user.id !== id && !context4.user.roles.includes("ADMIN")) {
              throw new GraphQLError6("You can only update your own profile", {
                extensions: { code: "FORBIDDEN" }
              });
            }
            const pool4 = getPostgresPool2();
            const existingUser = await pool4.query(
              "SELECT id FROM users WHERE id = $1",
              [id]
            );
            if (existingUser.rows.length === 0) {
              throw new GraphQLError6("User not found", {
                extensions: { code: "USER_NOT_FOUND" }
              });
            }
            const updates = [];
            const values = [];
            let paramIndex = 1;
            if (input.email) {
              updates.push(`email = $${paramIndex++}`);
              values.push(input.email.toLowerCase());
            }
            if (input.username) {
              updates.push(`username = $${paramIndex++}`);
              values.push(input.username);
            }
            updates.push(`updated_at = NOW()`);
            values.push(id);
            const result2 = await pool4.query(
              `UPDATE users
           SET ${updates.join(", ")}
           WHERE id = $${paramIndex}
           RETURNING id, email, username, first_name, last_name, role, is_active, created_at, updated_at`,
              values
            );
            const user = formatUser(result2.rows[0]);
            await cache.invalidate(`user:${id}`);
            logger17.info({ userId: id }, "User updated via GraphQL");
            return user;
          } catch (error) {
            if (error instanceof GraphQLError6) {
              throw error;
            }
            logger17.error({ userId: id, input, error }, "Failed to update user");
            throw new GraphQLError6("Failed to update user", {
              extensions: { code: "USER_UPDATE_FAILED" }
            });
          }
        }),
        /**
         * Delete a user (soft delete)
         */
        deleteUser: authGuard(async (_2, { id }, context4) => {
          try {
            if (!context4.user.roles.includes("ADMIN")) {
              throw new GraphQLError6("Only administrators can delete users", {
                extensions: { code: "FORBIDDEN" }
              });
            }
            if (context4.user.id === id) {
              throw new GraphQLError6("You cannot delete your own account", {
                extensions: { code: "FORBIDDEN" }
              });
            }
            const pool4 = getPostgresPool2();
            const result2 = await pool4.query(
              `UPDATE users
           SET is_active = false, updated_at = NOW()
           WHERE id = $1
           RETURNING id`,
              [id]
            );
            if (result2.rows.length === 0) {
              throw new GraphQLError6("User not found", {
                extensions: { code: "USER_NOT_FOUND" }
              });
            }
            await cache.invalidate(`user:${id}`);
            await pool4.query(
              "UPDATE user_sessions SET is_revoked = true WHERE user_id = $1",
              [id]
            );
            logger17.info({ userId: id }, "User deleted (soft) via GraphQL");
            return true;
          } catch (error) {
            if (error instanceof GraphQLError6) {
              throw error;
            }
            logger17.error({ userId: id, error }, "Failed to delete user");
            throw new GraphQLError6("Failed to delete user", {
              extensions: { code: "USER_DELETE_FAILED" }
            });
          }
        }),
        /**
         * Update user preferences (JSON merge)
         */
        updateUserPreferences: authGuard(async (_2, {
          userId,
          preferences
        }, context4) => {
          try {
            if (context4.user.id !== userId && !context4.user.roles.includes("ADMIN")) {
              throw new GraphQLError6("You can only update your own preferences", {
                extensions: { code: "FORBIDDEN" }
              });
            }
            const pool4 = getPostgresPool2();
            const result2 = await pool4.query(
              `UPDATE users
           SET preferences = COALESCE(preferences, '{}'::jsonb) || $1::jsonb,
               updated_at = NOW()
           WHERE id = $2
           RETURNING id, email, username, first_name, last_name, role, is_active, preferences, created_at, updated_at`,
              [JSON.stringify(preferences), userId]
            );
            if (result2.rows.length === 0) {
              throw new GraphQLError6("User not found", {
                extensions: { code: "USER_NOT_FOUND" }
              });
            }
            const user = formatUser(result2.rows[0]);
            await cache.invalidate(`user:${userId}`);
            logger17.info({ userId }, "User preferences updated via GraphQL");
            return user;
          } catch (error) {
            if (error instanceof GraphQLError6) {
              throw error;
            }
            logger17.error(
              { userId, preferences, error },
              "Failed to update user preferences"
            );
            throw new GraphQLError6("Failed to update user preferences", {
              extensions: { code: "PREFERENCES_UPDATE_FAILED" }
            });
          }
        })
      }
    };
    user_default = userResolvers;
  }
});

// src/graphql/resolvers/investigation.ts
import pino20 from "pino";
var logger18, investigationResolvers, investigation_default;
var init_investigation = __esm({
  "src/graphql/resolvers/investigation.ts"() {
    "use strict";
    init_database();
    init_auth2();
    logger18 = pino20();
    investigationResolvers = {
      Query: {
        investigation: async (_2, { id }) => {
          logger18.info(`Fetching investigation with ID: ${id} (placeholder)`);
          return {
            id,
            name: `Investigation ${id}`,
            description: `Description for investigation ${id}`,
            createdAt: (/* @__PURE__ */ new Date()).toISOString()
          };
        },
        investigations: async (_2, { limit, offset }) => {
          logger18.info(
            `Fetching investigations (placeholder) limit: ${limit}, offset: ${offset}`
          );
          return [
            {
              id: "inv-1",
              name: "Project Alpha",
              description: "Initial investigation",
              createdAt: (/* @__PURE__ */ new Date()).toISOString()
            },
            {
              id: "inv-2",
              name: "Project Beta",
              description: "Follow-up investigation",
              createdAt: (/* @__PURE__ */ new Date()).toISOString()
            }
          ];
        },
        auditTrace: async (_2, {
          investigationId,
          filter
        }) => {
          logger18.info(`Fetching audit trace for investigation ${investigationId}`);
          const pool4 = getPostgresPool2();
          const params = [investigationId];
          const conditions = ["details @> $1"];
          params[0] = JSON.stringify({ investigationId: params[0] });
          if (filter?.userId) {
            params.push(filter.userId);
            conditions.push(`user_id = $${params.length}`);
          }
          if (filter?.entityType) {
            params.push(filter.entityType);
            conditions.push(`resource_type = $${params.length}`);
          }
          if (filter?.from) {
            params.push(filter.from);
            conditions.push(`created_at >= $${params.length}`);
          }
          if (filter?.to) {
            params.push(filter.to);
            conditions.push(`created_at <= $${params.length}`);
          }
          const query3 = `
        SELECT id, user_id as "userId", action, resource_type as "resourceType",
               resource_id as "resourceId", details, details->>'investigationId' as "investigationId",
               created_at as "createdAt"
        FROM audit_logs
        WHERE ${conditions.join(" AND ")}
        ORDER BY created_at ASC`;
          const { rows } = await pool4.query(query3, params);
          return rows;
        },
        investigationSnapshots: async (_2, { investigationId }) => {
          logger18.info(`Fetching snapshots for investigation ${investigationId}`);
          const pool4 = getPostgresPool2();
          const result2 = await pool4.query(
            "SELECT * FROM maestro.investigation_snapshots WHERE investigation_id = $1 ORDER BY created_at DESC",
            [investigationId]
          );
          return result2.rows.map((row) => ({
            id: row.id,
            investigationId: row.investigation_id,
            data: row.data,
            snapshotLabel: row.snapshot_label,
            createdAt: row.created_at,
            createdBy: row.created_by
          }));
        },
        investigationSnapshot: async (_2, { id }) => {
          logger18.info(`Fetching snapshot ${id}`);
          const pool4 = getPostgresPool2();
          const result2 = await pool4.query(
            "SELECT * FROM maestro.investigation_snapshots WHERE id = $1",
            [id]
          );
          if (result2.rows.length === 0) return null;
          const row = result2.rows[0];
          return {
            id: row.id,
            investigationId: row.investigation_id,
            data: row.data,
            snapshotLabel: row.snapshot_label,
            createdAt: row.created_at,
            createdBy: row.created_by
          };
        }
      },
      Mutation: {
        createInvestigationSnapshot: authGuard(async (_2, { investigationId, label }, context4) => {
          logger18.info(`Creating snapshot for investigation ${investigationId}`);
          const pool4 = getPostgresPool2();
          let investigationData = {};
          try {
            const invResult = await pool4.query("SELECT * FROM maestro.investigations WHERE id = $1", [investigationId]);
            if (invResult.rows.length > 0) {
              investigationData = invResult.rows[0];
            } else {
              investigationData = {
                id: investigationId,
                name: `Investigation ${investigationId}`,
                description: "Placeholder investigation state",
                entities: [],
                // Would fetch from Neo4j
                relationships: []
              };
            }
          } catch (e) {
            logger18.warn(`Failed to fetch investigation details for snapshot, using placeholder: ${e}`);
            investigationData = {
              id: investigationId,
              error: "Failed to fetch real state",
              timestamp: (/* @__PURE__ */ new Date()).toISOString()
            };
          }
          const userId = context4.user?.id || "system";
          const result2 = await pool4.query(
            `INSERT INTO maestro.investigation_snapshots
         (investigation_id, data, snapshot_label, created_by)
         VALUES ($1, $2, $3, $4)
         RETURNING *`,
            [investigationId, JSON.stringify(investigationData), label || "Manual Snapshot", userId]
          );
          const row = result2.rows[0];
          return {
            id: row.id,
            investigationId: row.investigation_id,
            data: row.data,
            snapshotLabel: row.snapshot_label,
            createdAt: row.created_at,
            createdBy: row.created_by
          };
        }),
        createInvestigation: authGuard(async (_2, { input }) => {
          logger18.info(`Creating investigation: ${input.name} (placeholder)`);
          return {
            id: "new-inv-id",
            name: input.name,
            description: input.description,
            createdAt: (/* @__PURE__ */ new Date()).toISOString()
          };
        }, "write:case"),
        updateInvestigation: authGuard(async (_2, {
          id,
          input
        }) => {
          logger18.info(
            `Updating investigation ${id}: ${JSON.stringify(input)} (placeholder)`
          );
          return {
            id,
            name: input.name || `Investigation ${id}`,
            description: input.description || `Description for investigation ${id}`,
            updatedAt: (/* @__PURE__ */ new Date()).toISOString()
          };
        }, "write:case"),
        deleteInvestigation: authGuard(async (_2, { id }) => {
          logger18.info(`Deleting investigation: ${id} (placeholder)`);
          return true;
        }, "write:case")
      }
    };
    investigation_default = investigationResolvers;
  }
});

// src/services/GAEnrollmentService.ts
var DEFAULT_CONFIG, GAEnrollmentService, GAEnrollmentService_default;
var init_GAEnrollmentService = __esm({
  "src/services/GAEnrollmentService.ts"() {
    "use strict";
    init_database();
    init_logger2();
    DEFAULT_CONFIG = {
      status: process.env.GA_STATUS || "public",
      maxTenants: parseInt(process.env.GA_MAX_TENANTS || "1000", 10),
      maxUsers: parseInt(process.env.GA_MAX_USERS || "10000", 10),
      allowedDomains: process.env.GA_ALLOWED_DOMAINS ? process.env.GA_ALLOWED_DOMAINS.split(",") : ["*"],
      blockedDomains: process.env.GA_BLOCKED_DOMAINS ? process.env.GA_BLOCKED_DOMAINS.split(",") : ["gmail.com", "yahoo.com", "hotmail.com"],
      allowedRegions: ["us-east-1", "eu-central-1"]
    };
    GAEnrollmentService = class _GAEnrollmentService {
      static instance;
      memoryCacheConfig = null;
      lastFetch = 0;
      CACHE_TTL = 3e4;
      // 30 seconds
      constructor() {
      }
      get pool() {
        return getPostgresPool2();
      }
      static getInstance() {
        if (!_GAEnrollmentService.instance) {
          _GAEnrollmentService.instance = new _GAEnrollmentService();
        }
        return _GAEnrollmentService.instance;
      }
      async init() {
        const client6 = await this.pool.connect();
        try {
          await client6.query(`
            CREATE TABLE IF NOT EXISTS system_kv_store (
                key VARCHAR(255) PRIMARY KEY,
                value JSONB NOT NULL,
                updated_at TIMESTAMPTZ DEFAULT NOW()
            );
        `);
          await client6.query(`
            INSERT INTO system_kv_store (key, value)
            VALUES ($1, $2)
            ON CONFLICT (key) DO NOTHING
        `, ["ga_enrollment_config", JSON.stringify(DEFAULT_CONFIG)]);
        } finally {
          client6.release();
        }
      }
      async updateConfig(newConfig) {
        const current = await this.getConfig();
        const updated = { ...current, ...newConfig };
        const client6 = await this.pool.connect();
        try {
          await client6.query(`
            INSERT INTO system_kv_store (key, value, updated_at)
            VALUES ($1, $2, NOW())
            ON CONFLICT (key) DO UPDATE SET value = $2, updated_at = NOW()
        `, ["ga_enrollment_config", JSON.stringify(updated)]);
          this.memoryCacheConfig = updated;
          this.lastFetch = Date.now();
          logger_default2.info("GA Enrollment Configuration updated", updated);
        } finally {
          client6.release();
        }
      }
      async getConfig() {
        if (this.memoryCacheConfig && Date.now() - this.lastFetch < this.CACHE_TTL) {
          return this.memoryCacheConfig;
        }
        const client6 = await this.pool.connect();
        try {
          const res = await client6.query("SELECT value FROM system_kv_store WHERE key = $1", ["ga_enrollment_config"]);
          if (res.rows.length === 0) {
            return DEFAULT_CONFIG;
          }
          this.memoryCacheConfig = res.rows[0].value;
          this.lastFetch = Date.now();
          return this.memoryCacheConfig;
        } catch (e) {
          logger_default2.error("Failed to fetch GA config, using default", e);
          return DEFAULT_CONFIG;
        } finally {
          client6.release();
        }
      }
      /**
       * Check if a new user can register.
       */
      async checkUserEnrollmentEligibility(email) {
        const config9 = await this.getConfig();
        if (config9.status === "closed") {
          return { eligible: false, reason: "Enrollment is currently closed." };
        }
        const domain = email.split("@")[1];
        if (!domain) return { eligible: false, reason: "Invalid email format" };
        if (config9.blockedDomains.includes(domain)) {
          return { eligible: false, reason: "Email domain not allowed." };
        }
        if (!config9.allowedDomains.includes("*") && !config9.allowedDomains.includes(domain)) {
          return { eligible: false, reason: "Email domain not allowed." };
        }
        const capacity = await this.checkUserCapacity(config9);
        if (!capacity.available) {
          return { eligible: false, reason: capacity.reason };
        }
        return { eligible: true };
      }
      /**
       * Check if a new tenant can be created.
       */
      async checkTenantEnrollmentEligibility(region) {
        const config9 = await this.getConfig();
        if (config9.status === "closed") {
          return { eligible: false, reason: "Enrollment is currently closed." };
        }
        if (!config9.allowedRegions.includes(region)) {
          return { eligible: false, reason: `Region ${region} is not supported in GA.` };
        }
        const capacity = await this.checkTenantCapacity(config9);
        if (!capacity.available) {
          return { eligible: false, reason: capacity.reason };
        }
        return { eligible: true };
      }
      async checkUserCapacity(config9) {
        const client6 = await this.pool.connect();
        try {
          const res = await client6.query("SELECT COUNT(*) as count FROM users");
          const count = parseInt(res.rows[0].count, 10);
          if (count >= config9.maxUsers) {
            return { available: false, reason: "Maximum user capacity reached." };
          }
          return { available: true };
        } finally {
          client6.release();
        }
      }
      async checkTenantCapacity(config9) {
        const client6 = await this.pool.connect();
        try {
          const res = await client6.query("SELECT COUNT(*) as count FROM tenants");
          const count = parseInt(res.rows[0].count, 10);
          if (count >= config9.maxTenants) {
            return { available: false, reason: "Maximum tenant capacity reached." };
          }
          return { available: true };
        } finally {
          client6.release();
        }
      }
    };
    GAEnrollmentService_default = GAEnrollmentService.getInstance();
  }
});

// src/api/scopeGuard.ts
function checkScope(userScopes, scope) {
  if (userScopes.includes(scope)) return true;
  const [resource] = scope.split(":");
  if (userScopes.includes(`${resource}:*`)) return true;
  return false;
}
var init_scopeGuard = __esm({
  "src/api/scopeGuard.ts"() {
    "use strict";
  }
});

// src/services/AuthService.ts
var AuthService_exports = {};
__export(AuthService_exports, {
  AuthService: () => AuthService,
  default: () => AuthService_default
});
import * as argon22 from "argon2";
import * as jwt3 from "jsonwebtoken";
import crypto11, { randomUUID as uuidv44 } from "node:crypto";
var ROLE_PERMISSIONS, ROLE_SCOPES, AuthService, AuthService_default;
var init_AuthService = __esm({
  "src/services/AuthService.ts"() {
    "use strict";
    init_database();
    init_config();
    init_logger2();
    init_GAEnrollmentService();
    init_metrics3();
    init_scopeGuard();
    ROLE_PERMISSIONS = {
      ADMIN: [
        "*"
        // Admin has all permissions
      ],
      ANALYST: [
        "investigation:create",
        "investigation:read",
        "investigation:update",
        "entity:create",
        "entity:read",
        "entity:update",
        "entity:delete",
        "relationship:create",
        "relationship:read",
        "relationship:update",
        "relationship:delete",
        "tag:create",
        "tag:read",
        "tag:delete",
        "graph:read",
        "graph:export",
        "ai:request",
        "support:read",
        "support:create",
        "support:update"
      ],
      VIEWER: [
        "investigation:read",
        "entity:read",
        "relationship:read",
        "tag:read",
        "graph:read",
        "graph:export"
      ]
    };
    ROLE_SCOPES = {
      ADMIN: [
        "read:graph",
        "write:case",
        "run:analytics",
        "export:*",
        "manage:keys"
      ],
      ANALYST: [
        "read:graph",
        "write:case",
        "run:analytics",
        "export:bundle"
      ],
      VIEWER: [
        "read:graph"
      ]
    };
    AuthService = class {
      /** PostgreSQL connection pool for database operations */
      /** PostgreSQL connection pool for database operations */
      get pool() {
        return getPostgresPool2();
      }
      metrics;
      /**
       * @constructor
       * @description Creates an instance of AuthService.
       * Initializes the PostgreSQL connection pool and sets up Prometheus metrics for authentication events.
       */
      constructor() {
        try {
          this.metrics = new PrometheusMetrics("summit_auth");
          this.metrics.createHistogram(
            "user_registration_duration_seconds",
            "Time taken to register a user",
            { buckets: [0.1, 0.5, 1, 2, 5] }
          );
          this.metrics.createCounter(
            "user_logins_total",
            "Total number of user login attempts",
            ["tenant_id", "result"]
          );
          this.metrics.createCounter(
            "user_logouts_total",
            "Total number of user logouts",
            ["tenant_id"]
          );
          this.metrics.createHistogram(
            "user_session_duration_seconds",
            "Duration of user sessions",
            { buckets: [60, 300, 900, 1800, 3600] }
          );
        } catch (e) {
        }
      }
      /**
       * @method register
       * @description Registers a new user account.
       * Creates a new user, hashes the password using Argon2, and generates the initial JWT and refresh tokens.
       *
       * @param {UserData} userData - The user's registration data, including email, password, and name.
       * @returns {Promise<AuthResponse>} An object containing the new user's details and authentication tokens.
       * @throws {Error} Throws an error if a user with the same email or username already exists, or if the database operation fails.
       *
       * @example
       * ```typescript
       * const authResponse = await authService.register({
       *   email: 'new.user@example.com',
       *   password: 'a-very-secure-password',
       *   firstName: 'Jane',
       *   lastName: 'Doe',
       *   role: 'ANALYST'
       * });
       * console.log(authResponse.user.id);
       * console.log(authResponse.token);
       * ```
       */
      async register(userData) {
        const start = process.hrtime();
        const client6 = await this.pool.connect();
        try {
          if (process.env.GA_ENROLLMENT_BYPASS !== "true") {
            const enrollmentCheck = await GAEnrollmentService_default.checkUserEnrollmentEligibility(userData.email);
            if (!enrollmentCheck.eligible) {
              this.metrics?.observeHistogram("user_registration_duration_seconds", { status: "rejected_enrollment" }, 0);
              throw new Error(`Registration rejected: ${enrollmentCheck.reason}`);
            }
          }
          await client6.query("BEGIN");
          const existingUser = await client6.query(
            "SELECT id FROM users WHERE email = $1 OR username = $2",
            [userData.email, userData.username]
          );
          if (existingUser.rows.length > 0) {
            throw new Error("User with this email or username already exists");
          }
          const passwordHash = await argon22.hash(userData.password);
          const userResult = await client6.query(
            `
        INSERT INTO users (email, username, password_hash, first_name, last_name, role)
        VALUES ($1, $2, $3, $4, $5, $6)
        RETURNING id, email, username, first_name, last_name, role, is_active, created_at
      `,
            [
              userData.email,
              userData.username,
              passwordHash,
              userData.firstName,
              userData.lastName,
              userData.role || "ANALYST"
            ]
          );
          const user = userResult.rows[0];
          await client6.query(
            `INSERT INTO user_tenants (user_id, tenant_id, roles)
         VALUES ($1, 'global', $2)
         ON CONFLICT DO NOTHING`,
            [user.id, [user.role]]
          );
          const { token, refreshToken } = await this.generateTokens(user, client6);
          await client6.query("COMMIT");
          const [seconds, nanoseconds] = process.hrtime(start);
          const duration = seconds + nanoseconds / 1e9;
          this.metrics?.observeHistogram("user_registration_duration_seconds", { status: "success" }, duration);
          return {
            user: this.formatUser(user),
            token,
            refreshToken,
            expiresIn: 24 * 60 * 60
          };
        } catch (error) {
          await client6.query("ROLLBACK");
          logger_default2.error("Error registering user:", error);
          throw error;
        } finally {
          client6.release();
        }
      }
      /**
       * @method externalLogin
       * @description Logs in a user via an external provider (e.g., SSO).
       * This method trusts the caller and does not verify a password. It finds the user by email
       * and generates authentication tokens.
       *
       * @param {string} email - The email of the user to log in.
       * @returns {Promise<AuthResponse>} An object containing the user's details and authentication tokens.
       * @throws {Error} Throws an error if the user is not found.
       */
      async externalLogin(email) {
        const client6 = await this.pool.connect();
        let tenantId = "unknown";
        try {
          const userResult = await client6.query(
            "SELECT * FROM users WHERE email = $1 AND is_active = true",
            [email]
          );
          if (userResult.rows.length === 0) {
            throw new Error("User not found");
          }
          const user = userResult.rows[0];
          tenantId = user.tenant_id || "unknown";
          await client6.query(
            "UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = $1",
            [user.id]
          );
          const { token, refreshToken } = await this.generateTokens(user, client6);
          this.metrics.incrementCounter("user_logins_total", { tenant_id: tenantId, result: "success_sso" });
          return {
            user: this.formatUser(user),
            token,
            refreshToken,
            expiresIn: 24 * 60 * 60
          };
        } catch (error) {
          logger_default2.error("Error logging in user via SSO:", error);
          this.metrics.incrementCounter("user_logins_total", { tenant_id: tenantId, result: "failure_sso" });
          throw error;
        } finally {
          client6.release();
        }
      }
      /**
       * @method login
       * @description Authenticates a user with their email and password.
       * It verifies the password against the stored hash, updates the last login timestamp, and generates new tokens.
       *
       * @param {string} email - The user's email address.
       * @param {string} password - The user's plain text password.
       * @param {string} [ipAddress] - Optional client IP address for audit logging.
       * @param {string} [userAgent] - Optional client user agent for audit logging.
       * @returns {Promise<AuthResponse>} An object containing the user's details and authentication tokens.
       * @throws {Error} Throws 'Invalid credentials' if the email is not found or the password does not match.
       *
       * @example
       * ```typescript
       * try {
       *   const auth = await authService.login('user@example.com', 'password123');
       *   // Store auth.token and auth.refreshToken securely
       * } catch (error: any) {
       *   console.error('Login failed:', error.message);
       * }
       * ```
       *
       * @trace REQ-AUTH-001
       */
      async login(email, password, ipAddress, userAgent) {
        const client6 = await this.pool.connect();
        let tenantId = "unknown";
        try {
          const userResult = await client6.query(
            "SELECT * FROM users WHERE email = $1 AND is_active = true",
            [email]
          );
          if (userResult.rows.length === 0) {
            throw new Error("Invalid credentials");
          }
          const user = userResult.rows[0];
          tenantId = user.tenant_id || "unknown";
          const validPassword = await argon22.verify(user.password_hash, password);
          if (!validPassword) {
            throw new Error("Invalid credentials");
          }
          await client6.query(
            "UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = $1",
            [user.id]
          );
          const { token, refreshToken } = await this.generateTokens(user, client6);
          this.metrics.incrementCounter("user_logins_total", { tenant_id: tenantId, result: "success" });
          return {
            user: this.formatUser(user),
            token,
            refreshToken,
            expiresIn: 24 * 60 * 60
          };
        } catch (error) {
          logger_default2.error("Error logging in user:", error);
          this.metrics.incrementCounter("user_logins_total", { tenant_id: tenantId, result: "failure" });
          throw error;
        } finally {
          client6.release();
        }
      }
      /**
       * @method generateTokens
       * @description Generates a new JWT and refresh token pair for a user.
       * The JWT is signed with the user's essential details, and the refresh token is stored in the database.
       *
       * @param {DatabaseUser} user - The user object from the database.
       * @param {PoolClient} client - The PostgreSQL client to use for the database transaction.
       * @returns {Promise<TokenPair>} An object containing the new JWT and refresh token.
       */
      async generateTokens(user, client6) {
        const userScopes = ROLE_SCOPES[user.role.toUpperCase()] || [];
        const tokenPayload = {
          userId: user.id,
          email: user.email,
          role: user.role,
          tenantId: user.tenant_id || user.default_tenant_id || "unknown",
          scp: userScopes
        };
        const jwtSecret = cfg.JWT_SECRET;
        const token = jwt3.sign(tokenPayload, jwtSecret, {
          expiresIn: "24h"
        });
        const refreshToken = uuidv44();
        const expiresAt = /* @__PURE__ */ new Date();
        expiresAt.setDate(expiresAt.getDate() + 7);
        await client6.query(
          `
      INSERT INTO user_sessions (user_id, refresh_token, expires_at)
      VALUES ($1, $2, $3)
    `,
          [user.id, refreshToken, expiresAt]
        );
        return { token, refreshToken };
      }
      /**
       * @method verifyToken
       * @description Verifies a JWT. It checks the token's signature, expiry, and whether it has been blacklisted.
       *
       * @param {string} token - The JWT to verify.
       * @returns {Promise<User | null>} The user object if the token is valid, otherwise null.
       *
       * @example
       * ```typescript
       * const user = await authService.verifyToken(request.headers.authorization.split(' ')[1]);
       * if (!user) {
       *   // Handle unauthorized access
       * }
       * ```
       */
      async verifyToken(token) {
        try {
          if (!token) return null;
          if (process.env.NODE_ENV === "development" && token === "dev-token") {
            return {
              id: "dev-user-1",
              email: "developer@intelgraph.com",
              username: "developer",
              role: "ADMIN",
              isActive: true,
              createdAt: /* @__PURE__ */ new Date(),
              scopes: ["*"],
              tenantId: "tenant_1"
            };
          }
          const jwtSecret = cfg.JWT_SECRET;
          const decoded = jwt3.verify(token, jwtSecret);
          const blacklistCheck = await this.pool.query(
            "SELECT 1 FROM token_blacklist WHERE token_hash = $1",
            [this.hashToken(token)]
          );
          if (blacklistCheck.rows.length > 0) {
            logger_default2.warn("Blacklisted token attempted to be used:", {
              userId: decoded.userId
            });
            return null;
          }
          const client6 = await this.pool.connect();
          const userResult = await client6.query(
            "SELECT * FROM users WHERE id = $1 AND is_active = true",
            [decoded.userId]
          );
          client6.release();
          if (userResult.rows.length === 0) {
            return null;
          }
          const user = this.formatUser(userResult.rows[0]);
          user.scopes = decoded.scp || ROLE_SCOPES[user.role.toUpperCase()] || [];
          return user;
        } catch (error) {
          logger_default2.warn("Invalid token:", error.message);
          return null;
        }
      }
      /**
       * @method refreshAccessToken
       * @description Refreshes a user's access token using a refresh token.
       * It implements token rotation by invalidating the old refresh token and issuing a new pair.
       *
       * @param {string} refreshToken - The refresh token to use.
       * @returns {Promise<TokenPair | null>} A new token pair if the refresh token is valid, otherwise null.
       */
      async refreshAccessToken(refreshToken) {
        const client6 = await this.pool.connect();
        try {
          const sessionResult = await client6.query(
            `
        SELECT user_id, expires_at, is_revoked
        FROM user_sessions
        WHERE refresh_token = $1
      `,
            [refreshToken]
          );
          if (sessionResult.rows.length === 0) {
            logger_default2.warn("Invalid refresh token used");
            return null;
          }
          const session = sessionResult.rows[0];
          if (session.is_revoked) {
            logger_default2.warn("Revoked refresh token attempted to be used");
            return null;
          }
          if (new Date(session.expires_at) < /* @__PURE__ */ new Date()) {
            logger_default2.warn("Expired refresh token used");
            await client6.query(
              "UPDATE user_sessions SET is_revoked = true WHERE refresh_token = $1",
              [refreshToken]
            );
            return null;
          }
          const userResult = await client6.query(
            "SELECT * FROM users WHERE id = $1 AND is_active = true",
            [session.user_id]
          );
          if (userResult.rows.length === 0) {
            return null;
          }
          const user = userResult.rows[0];
          await client6.query(
            "UPDATE user_sessions SET is_revoked = true WHERE refresh_token = $1",
            [refreshToken]
          );
          const newTokenPair = await this.generateTokens(user, client6);
          logger_default2.info("Token successfully refreshed with rotation", {
            userId: user.id
          });
          return newTokenPair;
        } catch (error) {
          logger_default2.error("Error refreshing token:", error);
          return null;
        } finally {
          client6.release();
        }
      }
      /**
       * @method revokeToken
       * @description Revokes an access token by adding its hash to a blacklist.
       *
       * @param {string} token - The JWT to revoke.
       * @returns {Promise<boolean>} True if the token was successfully blacklisted, false otherwise.
       */
      async revokeToken(token) {
        try {
          const tokenHash = this.hashToken(token);
          await this.pool.query(
            `
        INSERT INTO token_blacklist (token_hash, revoked_at, expires_at)
        VALUES ($1, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP + INTERVAL '24 hours')
        ON CONFLICT (token_hash) DO NOTHING
      `,
            [tokenHash]
          );
          logger_default2.info("Token successfully blacklisted");
          return true;
        } catch (error) {
          logger_default2.error("Error revoking token:", error);
          return false;
        }
      }
      /**
       * @method logout
       * @description Logs out a user by revoking all their active sessions and, optionally, the current access token.
       *
       * @param {string} userId - The ID of the user to log out.
       * @param {string} [currentToken] - The user's current JWT to blacklist.
       * @returns {Promise<boolean>} True if the logout was successful, false otherwise.
       */
      async logout(userId, currentToken) {
        const client6 = await this.pool.connect();
        try {
          await client6.query("BEGIN");
          const result2 = await client6.query(
            "UPDATE user_sessions SET is_revoked = true WHERE user_id = $1 RETURNING (SELECT tenant_id, last_login FROM users WHERE id = $1)",
            [userId]
          );
          const userData = result2.rows[0];
          const tenantId = userData?.tenant_id || "unknown";
          this.metrics.incrementCounter("user_logouts_total", { tenant_id: tenantId });
          if (userData?.last_login) {
            const sessionDuration = ((/* @__PURE__ */ new Date()).getTime() - new Date(userData.last_login).getTime()) / 1e3;
            this.metrics.observeHistogram("user_session_duration_seconds", sessionDuration, { tenant_id: tenantId });
          }
          if (currentToken) {
            await this.revokeToken(currentToken);
          }
          await client6.query("COMMIT");
          logger_default2.info("User logged out successfully", { userId });
          return true;
        } catch (error) {
          await client6.query("ROLLBACK");
          logger_default2.error("Error during logout:", error);
          return false;
        } finally {
          client6.release();
        }
      }
      /**
       * @private
       * @method hashToken
       * @description Hashes a token for storage in the blacklist, to avoid storing raw tokens.
       *
       * @param {string} token - The token to hash.
       * @returns {string} The SHA256 hash of the token.
       */
      hashToken(token) {
        return crypto11.createHash("sha256").update(token).digest("hex");
      }
      /**
       * @method hasPermission
       * @description Checks if a user has a specific permission based on their role.
       * The ADMIN role has wildcard access to all permissions.
       *
       * @param {User | null} user - The user object to check.
       * @param {string} permission - The permission string to check for (e.g., 'investigation:create').
       * @returns {boolean} True if the user has the permission, false otherwise.
       *
       * @example
       * ```typescript
       * if (authService.hasPermission(currentUser, 'investigation:create')) {
       *   // Proceed with creating investigation
       * } else {
       *   // Deny access
       * }
       * ```
       */
      hasPermission(user, permission) {
        if (!user || !user.role) return false;
        const userPermissions = ROLE_PERMISSIONS[user.role.toUpperCase()];
        if (!userPermissions) return false;
        if (userPermissions.includes("*")) return true;
        return userPermissions.includes(permission);
      }
      /**
       * Checks if a user has a specific scope.
       *
       * @param {User | null} user - The user object to check.
       * @param {string} scope - The scope string to check for (e.g., 'read:graph').
       * @returns {boolean} True if the user has the scope, false otherwise.
       */
      hasScope(user, scope) {
        if (!user || !user.scopes) return false;
        return checkScope(user.scopes, scope);
      }
      /**
       * Transform database user record to public User interface
       *
       * Converts snake_case database fields to camelCase and computes fullName.
       * Excludes sensitive fields like password_hash.
       *
       * @private
       * @param {DatabaseUser} user - Database user record
       * @returns {User} Public user representation
       */
      formatUser(user) {
        return {
          id: user.id,
          email: user.email,
          username: user.username,
          firstName: user.first_name,
          lastName: user.last_name,
          fullName: `${user.first_name} ${user.last_name}`,
          role: user.role,
          defaultTenantId: user.default_tenant_id,
          isActive: user.is_active,
          lastLogin: user.last_login,
          createdAt: user.created_at,
          updatedAt: user.updated_at,
          scopes: []
        };
      }
    };
    AuthService_default = AuthService;
  }
});

// src/email-service/providers/EmailProvider.ts
var EmailProvider;
var init_EmailProvider = __esm({
  "src/email-service/providers/EmailProvider.ts"() {
    "use strict";
    EmailProvider = class {
    };
  }
});

// src/email-service/providers/SMTPProvider.ts
import nodemailer from "nodemailer";
import { htmlToText } from "html-to-text";
var SMTPProvider;
var init_SMTPProvider = __esm({
  "src/email-service/providers/SMTPProvider.ts"() {
    "use strict";
    init_EmailProvider();
    SMTPProvider = class extends EmailProvider {
      transporter = null;
      config;
      constructor(config9) {
        super();
        this.config = config9;
      }
      async initialize() {
        if (!this.config.smtp) {
          throw new Error("SMTP configuration is required for SMTP provider");
        }
        this.transporter = nodemailer.createTransport({
          host: this.config.smtp.host,
          port: this.config.smtp.port,
          secure: this.config.smtp.secure,
          auth: this.config.smtp.auth,
          pool: this.config.smtp.pool ?? true,
          maxConnections: this.config.smtp.maxConnections ?? 5,
          maxMessages: this.config.smtp.maxMessages ?? 100,
          rateDelta: this.config.smtp.rateDelta,
          rateLimit: this.config.smtp.rateLimit
        });
        await this.transporter.verify();
      }
      async send(message) {
        if (!this.transporter) {
          throw new Error("SMTP Provider not initialized");
        }
        const recipientEmail = this.extractPrimaryRecipient(message);
        try {
          const text = message.text || htmlToText(message.html, {
            wordwrap: 130,
            preserveNewlines: true
          });
          const mailOptions = {
            from: message.from || this.config.from,
            replyTo: message.replyTo || this.config.replyTo,
            to: message.to,
            cc: message.cc,
            bcc: message.bcc,
            subject: message.subject,
            text,
            html: message.html,
            attachments: message.attachments?.map((att) => ({
              filename: att.filename,
              content: att.content,
              contentType: att.contentType,
              encoding: att.encoding,
              cid: att.cid
            })),
            headers: {
              ...message.headers,
              "X-Priority": this.getPriority(message.priority)
            },
            priority: message.priority,
            list: message.listUnsubscribe ? {
              unsubscribe: message.listUnsubscribe
            } : void 0
          };
          const info = await this.transporter.sendMail(mailOptions);
          return {
            success: true,
            messageId: info.messageId,
            recipientEmail,
            templateId: message.templateId,
            templateVersion: message.templateVersion,
            abTestVariant: message.abTestVariant,
            sentAt: /* @__PURE__ */ new Date(),
            metadata: {
              response: info.response,
              accepted: info.accepted,
              rejected: info.rejected
            }
          };
        } catch (error) {
          return {
            success: false,
            recipientEmail,
            templateId: message.templateId,
            templateVersion: message.templateVersion,
            error
          };
        }
      }
      async healthCheck() {
        if (!this.transporter) {
          return false;
        }
        try {
          await this.transporter.verify();
          return true;
        } catch {
          return false;
        }
      }
      async shutdown() {
        if (this.transporter) {
          this.transporter.close();
          this.transporter = null;
        }
      }
      extractPrimaryRecipient(message) {
        if (typeof message.to === "string") {
          return message.to;
        }
        if (Array.isArray(message.to)) {
          const first = message.to[0];
          return typeof first === "string" ? first : first.email;
        }
        return message.to.email;
      }
      getPriority(priority) {
        switch (priority) {
          case "high":
            return "1";
          case "low":
            return "5";
          default:
            return "3";
        }
      }
    };
  }
});

// src/email-service/EmailQueue.ts
import { Queue, Worker } from "bullmq";
import { Redis as Redis6 } from "ioredis";
var EmailQueue;
var init_EmailQueue = __esm({
  "src/email-service/EmailQueue.ts"() {
    "use strict";
    EmailQueue = class {
      queue;
      worker = null;
      config;
      redis;
      constructor(config9) {
        this.config = config9;
        this.redis = new Redis6({
          host: process.env.REDIS_HOST || "localhost",
          port: parseInt(process.env.REDIS_PORT || "6379"),
          maxRetriesPerRequest: null
        });
        this.queue = new Queue("email-queue", {
          connection: this.redis,
          defaultJobOptions: {
            attempts: config9?.retryAttempts || 3,
            backoff: {
              type: config9?.retryBackoff || "exponential",
              delay: config9?.retryDelay || 1e3
            },
            removeOnComplete: {
              age: 24 * 3600,
              // Keep completed jobs for 24 hours
              count: 1e3
            },
            removeOnFail: {
              age: 7 * 24 * 3600
              // Keep failed jobs for 7 days
            }
          }
        });
      }
      async initialize() {
      }
      /**
       * Start processing queue
       */
      async startWorker(processFn) {
        this.worker = new Worker(
          "email-queue",
          async (job) => {
            await processFn(job);
          },
          {
            connection: this.redis,
            concurrency: this.config?.concurrency || 5,
            limiter: {
              max: 10,
              duration: 1e3
              // 10 jobs per second
            }
          }
        );
        this.worker.on("completed", (job) => {
          console.log(`Email job ${job.id} completed`);
        });
        this.worker.on("failed", (job, err) => {
          console.error(`Email job ${job?.id} failed:`, err);
        });
      }
      /**
       * Enqueue an email for sending
       */
      async enqueue(message, options2) {
        const job = await this.queue.add(
          "send-email",
          message,
          {
            priority: options2?.priority,
            delay: options2?.delay
          }
        );
        return job.id ?? "";
      }
      /**
       * Get job status
       */
      async getJobStatus(jobId) {
        const job = await this.queue.getJob(jobId);
        if (!job) {
          return null;
        }
        const state = await job.getState();
        return {
          id: job.id ?? "",
          message: job.data,
          priority: job.opts.priority ?? 0,
          attempts: job.attemptsMade,
          maxAttempts: job.opts.attempts ?? 3,
          createdAt: new Date(job.timestamp),
          scheduledFor: job.opts.delay ? new Date(job.timestamp + job.opts.delay) : void 0,
          lastAttemptAt: job.processedOn ? new Date(job.processedOn) : void 0,
          error: job.failedReason,
          status: this.mapState(state)
        };
      }
      /**
       * Cancel a queued email
       */
      async cancel(jobId) {
        const job = await this.queue.getJob(jobId);
        if (job) {
          await job.remove();
          return true;
        }
        return false;
      }
      /**
       * Get queue metrics
       */
      async getMetrics() {
        const [waiting, active, completed, failed, delayed] = await Promise.all([
          this.queue.getWaitingCount(),
          this.queue.getActiveCount(),
          this.queue.getCompletedCount(),
          this.queue.getFailedCount(),
          this.queue.getDelayedCount()
        ]);
        return {
          waiting,
          active,
          completed,
          failed,
          delayed,
          total: waiting + active + completed + failed + delayed
        };
      }
      /**
       * Pause queue processing
       */
      async pause() {
        await this.queue.pause();
      }
      /**
       * Resume queue processing
       */
      async resume() {
        await this.queue.resume();
      }
      /**
       * Shutdown queue
       */
      async shutdown() {
        if (this.worker) {
          await this.worker.close();
        }
        await this.queue.close();
        await this.redis.quit();
      }
      mapState(state) {
        switch (state) {
          case "waiting":
          case "delayed":
            return "pending";
          case "active":
            return "processing";
          case "completed":
            return "sent";
          case "failed":
            return "failed";
          default:
            return "pending";
        }
      }
    };
  }
});

// src/email-service/TemplateRenderer.ts
import mjml2html from "mjml";
import { render as renderReactEmail } from "@react-email/render";
import { htmlToText as htmlToText2 } from "html-to-text";
import juice from "juice";
var TemplateRenderer;
var init_TemplateRenderer = __esm({
  "src/email-service/TemplateRenderer.ts"() {
    "use strict";
    TemplateRenderer = class {
      /**
       * Render an email template with variables
       */
      async render(template, variables) {
        this.validateVariables(template.variables, variables);
        const subject = this.interpolate(template.subject, variables);
        let html;
        if (template.mjmlContent) {
          html = await this.renderMJML(template.mjmlContent, variables);
        } else if (template.reactEmailComponent) {
          html = await this.renderReactEmail(template.reactEmailComponent, variables);
        } else {
          throw new Error(`Template ${template.id} has no content to render`);
        }
        html = juice(html);
        const text = htmlToText2(html, {
          wordwrap: 130,
          preserveNewlines: true,
          selectors: [
            { selector: "a", options: { hideLinkHrefIfSameAsText: true } },
            { selector: "img", format: "skip" }
          ]
        });
        return {
          subject,
          html,
          text,
          previewText: template.previewText ? this.interpolate(template.previewText, variables) : void 0
        };
      }
      /**
       * Render MJML template
       */
      async renderMJML(mjmlContent, variables) {
        const interpolatedMjml = this.interpolate(mjmlContent, variables);
        const result2 = mjml2html(interpolatedMjml, {
          keepComments: false,
          beautify: false,
          minify: true
        });
        if (result2.errors.length > 0) {
          throw new Error(
            `MJML rendering errors: ${result2.errors.map((e) => e.message).join(", ")}`
          );
        }
        return result2.html;
      }
      /**
       * Render React Email component
       */
      async renderReactEmail(componentCode, variables) {
        try {
          const html = renderReactEmail(componentCode, variables);
          return html;
        } catch (error) {
          throw new Error(
            `React Email rendering error: ${error.message}`
          );
        }
      }
      /**
       * Interpolate variables in template string
       */
      interpolate(template, variables) {
        return template.replace(/\{\{([^}]+)\}\}/g, (match, path55) => {
          const value = this.getNestedValue(variables, path55.trim());
          if (value === void 0 || value === null) {
            console.warn(`Template variable not found: ${path55}`);
            return match;
          }
          if (value instanceof Date) {
            return value.toLocaleDateString();
          }
          if (Array.isArray(value)) {
            return value.join(", ");
          }
          if (typeof value === "object") {
            return JSON.stringify(value);
          }
          return String(value);
        });
      }
      /**
       * Get nested value from object using dot notation
       */
      getNestedValue(obj, path55) {
        return path55.split(".").reduce((current, key) => current?.[key], obj);
      }
      /**
       * Validate that all required variables are provided
       */
      validateVariables(templateVars, providedVars) {
        const missing = [];
        for (const templateVar of templateVars) {
          if (templateVar.required) {
            const value = this.getNestedValue(providedVars, templateVar.name);
            if (value === void 0 || value === null) {
              if (templateVar.defaultValue === void 0) {
                missing.push(templateVar.name);
              } else {
                this.setNestedValue(providedVars, templateVar.name, templateVar.defaultValue);
              }
            }
          }
        }
        if (missing.length > 0) {
          throw new Error(
            `Missing required template variables: ${missing.join(", ")}`
          );
        }
      }
      /**
       * Set nested value in object using dot notation
       */
      setNestedValue(obj, path55, value) {
        const keys = path55.split(".");
        const lastKey = keys.pop();
        const target = keys.reduce((current, key) => {
          if (!(key in current)) {
            current[key] = {};
          }
          return current[key];
        }, obj);
        target[lastKey] = value;
      }
    };
  }
});

// src/email-service/ab-testing/ABTestManager.ts
var ABTestManager;
var init_ABTestManager = __esm({
  "src/email-service/ab-testing/ABTestManager.ts"() {
    "use strict";
    ABTestManager = class {
      config;
      activeTests = /* @__PURE__ */ new Map();
      constructor(config9) {
        this.config = config9;
      }
      /**
       * Select variant for A/B test
       */
      async selectVariant(templateId) {
        const test = Array.from(this.activeTests.values()).find(
          (t) => t.templateId === templateId && t.active
        );
        if (!test) {
          return null;
        }
        const random = Math.random() * 100;
        let cumulative = 0;
        for (const variant of test.variants) {
          cumulative += variant.weight;
          if (random <= cumulative) {
            const template = await this.loadTemplateVariant(
              templateId,
              variant.templateVariantId
            );
            return {
              template,
              variantId: variant.id
            };
          }
        }
        return null;
      }
      /**
       * Create A/B test
       */
      async createTest(config9) {
        const totalWeight = config9.variants.reduce((sum, v) => sum + v.weight, 0);
        if (Math.abs(totalWeight - 100) > 0.01) {
          throw new Error("Variant weights must sum to 100");
        }
        this.activeTests.set(config9.id, config9);
      }
      /**
       * Get test results
       */
      async getResults(testId) {
        const test = this.activeTests.get(testId);
        if (!test) {
          return null;
        }
        const variantResults = test.variants.map((variant) => {
          const openRate = variant.sent > 0 ? variant.opened / variant.sent * 100 : 0;
          const clickRate = variant.sent > 0 ? variant.clicked / variant.sent * 100 : 0;
          const conversionRate = variant.sent > 0 ? variant.converted / variant.sent * 100 : 0;
          const bounceRate = variant.sent > 0 ? variant.bounced / variant.sent * 100 : 0;
          return {
            variantId: variant.id,
            openRate,
            clickRate,
            conversionRate,
            bounceRate
          };
        });
        const bestVariant = variantResults.reduce(
          (best, current) => current.clickRate > best.clickRate ? current : best
        );
        const totalSent = test.variants.reduce((sum, v) => sum + v.sent, 0);
        return {
          winningVariantId: bestVariant.variantId,
          confidence: this.calculateConfidence(test.variants),
          // Placeholder
          startDate: test.startDate,
          endDate: test.endDate || /* @__PURE__ */ new Date(),
          totalSent,
          variantResults
        };
      }
      /**
       * Stop A/B test
       */
      async stopTest(testId) {
        const test = this.activeTests.get(testId);
        if (test) {
          test.active = false;
          test.endDate = /* @__PURE__ */ new Date();
        }
      }
      async loadTemplateVariant(templateId, variantId) {
        throw new Error("Template loading not implemented");
      }
      calculateConfidence(variants) {
        const totalSent = variants.reduce((sum, v) => sum + v.sent, 0);
        const minSampleSize = this.config?.minSampleSize || 100;
        if (totalSent < minSampleSize) {
          return 0;
        }
        return Math.min(95, totalSent / minSampleSize * 95);
      }
    };
  }
});

// src/email-service/analytics/EmailAnalyticsService.ts
import crypto12 from "crypto";
var EmailAnalyticsService;
var init_EmailAnalyticsService = __esm({
  "src/email-service/analytics/EmailAnalyticsService.ts"() {
    "use strict";
    EmailAnalyticsService = class {
      config;
      analytics = /* @__PURE__ */ new Map();
      trackingDomain;
      constructor(config9) {
        this.config = config9;
        this.trackingDomain = config9?.trackingDomain || "track.example.com";
      }
      async initialize() {
      }
      /**
       * Track email sent
       */
      async trackSent(data) {
        this.analytics.set(data.messageId, {
          messageId: data.messageId,
          templateId: data.templateId,
          recipientEmail: data.recipientEmail,
          sentAt: data.sentAt,
          opened: false,
          openCount: 0,
          clicked: false,
          clickCount: 0,
          clickedLinks: [],
          bounced: false,
          unsubscribed: false,
          metadata: data.metadata
        });
      }
      /**
       * Track email open
       */
      async trackOpen(messageId) {
        const analytics = this.analytics.get(messageId);
        if (analytics) {
          if (!analytics.opened) {
            analytics.opened = true;
            analytics.openedAt = /* @__PURE__ */ new Date();
          }
          analytics.openCount++;
        }
      }
      /**
       * Track link click
       */
      async trackClick(messageId, url) {
        const analytics = this.analytics.get(messageId);
        if (analytics) {
          if (!analytics.clicked) {
            analytics.clicked = true;
            analytics.clickedAt = /* @__PURE__ */ new Date();
          }
          analytics.clickCount++;
          if (!analytics.clickedLinks.includes(url)) {
            analytics.clickedLinks.push(url);
          }
        }
      }
      /**
       * Track bounce
       */
      async trackBounce(messageId, bounceType, reason) {
        const analytics = this.analytics.get(messageId);
        if (analytics) {
          analytics.bounced = true;
          analytics.bouncedAt = /* @__PURE__ */ new Date();
          analytics.bounceType = bounceType;
          analytics.bounceReason = reason;
        }
      }
      /**
       * Add open tracking pixel to email
       */
      async addOpenTracking(message) {
        const trackingId = this.generateTrackingId(message);
        const trackingPixel = `<img src="https://${this.trackingDomain}/open/${trackingId}" width="1" height="1" style="display:none" alt="" />`;
        message.html = message.html + trackingPixel;
        return message;
      }
      /**
       * Add click tracking to links in email
       */
      async addClickTracking(message) {
        const trackingId = this.generateTrackingId(message);
        message.html = message.html.replace(
          /<a\s+href="([^"]+)"/gi,
          (match, url) => {
            const trackedUrl = this.createTrackedUrl(trackingId, url);
            return `<a href="${trackedUrl}"`;
          }
        );
        return message;
      }
      /**
       * Get analytics for a message
       */
      async getAnalytics(messageId) {
        return this.analytics.get(messageId) || null;
      }
      /**
       * Get analytics for a template
       */
      async getTemplateAnalytics(templateId, dateRange) {
        const templateAnalytics = Array.from(this.analytics.values()).filter(
          (a) => a.templateId === templateId
        );
        if (dateRange) {
        }
        const totalSent = templateAnalytics.length;
        const totalOpened = templateAnalytics.filter((a) => a.opened).length;
        const totalClicked = templateAnalytics.filter((a) => a.clicked).length;
        const totalBounced = templateAnalytics.filter((a) => a.bounced).length;
        return {
          templateId,
          totalSent,
          totalOpened,
          totalClicked,
          totalBounced,
          openRate: totalSent > 0 ? totalOpened / totalSent * 100 : 0,
          clickRate: totalSent > 0 ? totalClicked / totalSent * 100 : 0,
          bounceRate: totalSent > 0 ? totalBounced / totalSent * 100 : 0
        };
      }
      generateTrackingId(message) {
        const data = `${message.to}-${Date.now()}-${Math.random()}`;
        return crypto12.createHash("sha256").update(data).digest("hex").substring(0, 32);
      }
      createTrackedUrl(trackingId, originalUrl) {
        const encodedUrl = encodeURIComponent(originalUrl);
        return `https://${this.trackingDomain}/click/${trackingId}?url=${encodedUrl}`;
      }
    };
  }
});

// src/email-service/unsubscribe/UnsubscribeManager.ts
var UnsubscribeManager;
var init_UnsubscribeManager = __esm({
  "src/email-service/unsubscribe/UnsubscribeManager.ts"() {
    "use strict";
    UnsubscribeManager = class {
      preferences = /* @__PURE__ */ new Map();
      emailSendHistory = /* @__PURE__ */ new Map();
      baseUrl;
      constructor(baseUrl) {
        this.baseUrl = baseUrl || process.env.APP_URL || "https://app.example.com";
      }
      async initialize() {
      }
      /**
       * Get preferences for an email
       */
      async getPreferences(email) {
        return this.preferences.get(email) || null;
      }
      /**
       * Update preferences
       */
      async updatePreferences(email, updates) {
        const existing = this.preferences.get(email) || this.createDefaultPreferences(email);
        const updated = {
          ...existing,
          ...updates,
          email
          // Ensure email doesn't change
        };
        this.preferences.set(email, updated);
        return updated;
      }
      /**
       * Unsubscribe from all emails
       */
      async unsubscribeAll(email) {
        await this.updatePreferences(email, {
          unsubscribedFromAll: true,
          unsubscribedAt: /* @__PURE__ */ new Date()
        });
      }
      /**
       * Unsubscribe from specific category
       */
      async unsubscribeCategory(email, category) {
        const prefs = await this.getPreferences(email) || this.createDefaultPreferences(email);
        prefs.categories[category] = {
          subscribed: false,
          updatedAt: /* @__PURE__ */ new Date()
        };
        this.preferences.set(email, prefs);
      }
      /**
       * Subscribe to category
       */
      async subscribeCategory(email, category) {
        const prefs = await this.getPreferences(email) || this.createDefaultPreferences(email);
        prefs.categories[category] = {
          subscribed: true,
          updatedAt: /* @__PURE__ */ new Date()
        };
        this.preferences.set(email, prefs);
      }
      /**
       * Check if frequency limit allows sending
       */
      async checkFrequencyLimit(email, frequency) {
        if (!frequency) {
          return true;
        }
        const history = this.emailSendHistory.get(email) || [];
        const now = /* @__PURE__ */ new Date();
        if (frequency.maxEmailsPerDay) {
          const today = new Date(now.getFullYear(), now.getMonth(), now.getDate());
          const todayCount = history.filter((date) => date >= today).length;
          if (todayCount >= frequency.maxEmailsPerDay) {
            return false;
          }
        }
        if (frequency.maxEmailsPerWeek) {
          const weekAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1e3);
          const weekCount = history.filter((date) => date >= weekAgo).length;
          if (weekCount >= frequency.maxEmailsPerWeek) {
            return false;
          }
        }
        return true;
      }
      /**
       * Record email sent for frequency tracking
       */
      async recordEmailSent(email) {
        const history = this.emailSendHistory.get(email) || [];
        history.push(/* @__PURE__ */ new Date());
        const thirtyDaysAgo = new Date(Date.now() - 30 * 24 * 60 * 60 * 1e3);
        const filtered = history.filter((date) => date >= thirtyDaysAgo);
        this.emailSendHistory.set(email, filtered);
      }
      /**
       * Generate unsubscribe URL
       */
      async generateUnsubscribeUrl(email, userId) {
        const token = this.generateUnsubscribeToken(email, userId);
        return `${this.baseUrl}/email/unsubscribe?token=${token}`;
      }
      /**
       * Verify and parse unsubscribe token
       */
      async verifyUnsubscribeToken(token) {
        try {
          const decoded = Buffer.from(token, "base64").toString("utf-8");
          const [email, userId, timestamp] = decoded.split("|");
          const tokenDate = new Date(parseInt(timestamp));
          const thirtyDaysAgo = new Date(Date.now() - 30 * 24 * 60 * 60 * 1e3);
          if (tokenDate < thirtyDaysAgo) {
            return null;
          }
          return { email, userId: userId || void 0 };
        } catch {
          return null;
        }
      }
      /**
       * Generate preference center URL
       */
      async generatePreferenceCenterUrl(email, userId) {
        const token = this.generateUnsubscribeToken(email, userId);
        return `${this.baseUrl}/email/preferences?token=${token}`;
      }
      createDefaultPreferences(email) {
        return {
          userId: "",
          email,
          unsubscribedFromAll: false,
          categories: {},
          frequency: {
            maxEmailsPerDay: 10,
            maxEmailsPerWeek: 50
          }
        };
      }
      generateUnsubscribeToken(email, userId) {
        const data = `${email}|${userId || ""}|${Date.now()}`;
        return Buffer.from(data).toString("base64");
      }
    };
  }
});

// src/email-service/deliverability/DeliverabilityChecker.ts
var DeliverabilityChecker;
var init_DeliverabilityChecker = __esm({
  "src/email-service/deliverability/DeliverabilityChecker.ts"() {
    "use strict";
    DeliverabilityChecker = class {
      config;
      spamScoreThreshold;
      constructor(config9) {
        this.config = config9;
        this.spamScoreThreshold = config9?.spamScoreThreshold || 5;
      }
      /**
       * Check email deliverability
       */
      async check(message) {
        const spamScore = await this.calculateSpamScore(message);
        const contentAnalysis = this.analyzeContent(message);
        const authentication = await this.checkAuthentication();
        const recommendations = this.generateRecommendations(
          spamScore,
          contentAnalysis,
          authentication
        );
        const overallScore = this.calculateOverallScore(
          spamScore,
          contentAnalysis,
          authentication
        );
        return {
          emailMessage: message,
          spamScore,
          authentication,
          contentAnalysis,
          recommendations,
          overallScore
        };
      }
      /**
       * Calculate spam score
       */
      async calculateSpamScore(message) {
        const issues = [];
        let score = 0;
        const subjectScore = this.checkSubject(message.subject, issues);
        score += subjectScore;
        const contentScore = this.checkContent(message.html, message.text, issues);
        score += contentScore;
        const linksScore = this.checkLinks(message.html, issues);
        score += linksScore;
        const imagesScore = this.checkImages(message.html, issues);
        score += imagesScore;
        const authScore = message.headers?.["DKIM-Signature"] ? 0 : 1;
        if (authScore > 0) {
          issues.push({
            severity: "warning",
            category: "authentication",
            message: "DKIM signature not found",
            fix: "Configure DKIM for your sending domain"
          });
        }
        score += authScore;
        const passed = score < this.spamScoreThreshold;
        const suggestions = issues.filter((i) => i.fix).map((i) => i.fix);
        return {
          score,
          passed,
          threshold: this.spamScoreThreshold,
          issues,
          suggestions,
          details: {
            subjectScore,
            contentScore,
            linksScore,
            imagesScore,
            authenticationScore: authScore
          }
        };
      }
      checkSubject(subject, issues) {
        let score = 0;
        const spamWords = ["free", "winner", "cash", "prize", "urgent", "act now", "!!!"];
        const lowerSubject = subject.toLowerCase();
        for (const word of spamWords) {
          if (lowerSubject.includes(word)) {
            score += 0.5;
            issues.push({
              severity: "warning",
              category: "subject",
              message: `Spam trigger word found: "${word}"`,
              fix: `Avoid using the word "${word}" in subject line`
            });
          }
        }
        const exclamationCount = (subject.match(/!/g) || []).length;
        if (exclamationCount > 1) {
          score += 0.5;
          issues.push({
            severity: "warning",
            category: "subject",
            message: "Excessive exclamation marks in subject",
            fix: "Limit exclamation marks to 1 or less"
          });
        }
        if (subject === subject.toUpperCase() && subject.length > 5) {
          score += 1;
          issues.push({
            severity: "warning",
            category: "subject",
            message: "Subject line is all uppercase",
            fix: "Use normal capitalization in subject line"
          });
        }
        return score;
      }
      checkContent(html, text, issues) {
        let score = 0;
        const htmlLength = html.replace(/<[^>]*>/g, "").length;
        const textLength = text.length;
        const ratio = textLength > 0 ? textLength / htmlLength : 0;
        if (ratio < 0.3) {
          score += 1;
          issues.push({
            severity: "warning",
            category: "content",
            message: "Low text-to-HTML ratio",
            fix: "Add more text content relative to HTML markup"
          });
        }
        const linkCount = (html.match(/<a/gi) || []).length;
        const wordCount = text.split(/\s+/).length;
        const linksPerWord = wordCount > 0 ? linkCount / wordCount : 0;
        if (linksPerWord > 0.1) {
          score += 1;
          issues.push({
            severity: "warning",
            category: "links",
            message: "Too many links relative to content",
            fix: "Reduce number of links in email"
          });
        }
        return score;
      }
      checkLinks(html, issues) {
        let score = 0;
        const shortenedUrlPatterns = ["bit.ly", "tinyurl.com", "goo.gl", "t.co"];
        for (const pattern2 of shortenedUrlPatterns) {
          if (html.includes(pattern2)) {
            score += 0.5;
            issues.push({
              severity: "warning",
              category: "links",
              message: `Shortened URL detected: ${pattern2}`,
              fix: "Use full URLs instead of URL shorteners"
            });
          }
        }
        const linkRegex = /<a[^>]*href="([^"]*)"[^>]*>([^<]*)</gi;
        let match;
        while ((match = linkRegex.exec(html)) !== null) {
          const href = match[1];
          const text = match[2];
          if (text.match(/https?:\/\//i) && text !== href) {
            score += 1;
            issues.push({
              severity: "critical",
              category: "links",
              message: "Link text and href mismatch",
              fix: "Ensure link text matches the actual destination"
            });
          }
        }
        return score;
      }
      checkImages(html, issues) {
        let score = 0;
        const imageCount = (html.match(/<img/gi) || []).length;
        const textLength = html.replace(/<[^>]*>/g, "").length;
        if (imageCount > 0 && textLength < 100) {
          score += 1;
          issues.push({
            severity: "warning",
            category: "images",
            message: "Email is mostly images with little text",
            fix: "Add more text content to balance images"
          });
        }
        const imagesWithoutAlt = (html.match(/<img(?![^>]*alt=)[^>]*>/gi) || []).length;
        if (imagesWithoutAlt > 0) {
          score += 0.5;
          issues.push({
            severity: "info",
            category: "images",
            message: `${imagesWithoutAlt} images without alt text`,
            fix: "Add alt text to all images"
          });
        }
        return score;
      }
      analyzeContent(message) {
        const html = message.html;
        const text = message.text;
        const textToHtmlRatio = text.length / html.replace(/<[^>]*>/g, "").length;
        const wordCount = text.split(/\s+/).length;
        const linkCount = (html.match(/<a/gi) || []).length;
        const imageCount = (html.match(/<img/gi) || []).length;
        const hasUnsubscribeLink = !!(message.unsubscribeUrl || message.listUnsubscribe || html.toLowerCase().includes("unsubscribe"));
        const hasPhysicalAddress = /\d+\s+[\w\s]+,\s*\w+\s+\d{5}/.test(html);
        return {
          textToHtmlRatio,
          wordCount,
          linkCount,
          imageCount,
          hasUnsubscribeLink,
          hasPhysicalAddress
        };
      }
      async checkAuthentication() {
        return {
          spfConfigured: true,
          // Placeholder
          dkimConfigured: true,
          // Placeholder
          dmarcConfigured: true
          // Placeholder
        };
      }
      generateRecommendations(spamScore, contentAnalysis, authentication) {
        const recommendations = [];
        if (spamScore.score >= this.spamScoreThreshold) {
          recommendations.push(
            `Spam score is ${spamScore.score.toFixed(1)}, which exceeds the threshold of ${this.spamScoreThreshold}. Review and fix issues.`
          );
        }
        if (!contentAnalysis.hasUnsubscribeLink) {
          recommendations.push("Add an unsubscribe link to comply with CAN-SPAM Act");
        }
        if (!contentAnalysis.hasPhysicalAddress) {
          recommendations.push("Include your physical mailing address to comply with CAN-SPAM Act");
        }
        if (contentAnalysis.textToHtmlRatio < 0.3) {
          recommendations.push("Increase text content relative to HTML markup");
        }
        if (!authentication.spfConfigured) {
          recommendations.push("Configure SPF record for your sending domain");
        }
        if (!authentication.dkimConfigured) {
          recommendations.push("Configure DKIM signature for your emails");
        }
        if (!authentication.dmarcConfigured) {
          recommendations.push("Configure DMARC policy for your domain");
        }
        return recommendations;
      }
      calculateOverallScore(spamScore, contentAnalysis, authentication) {
        let score = 100;
        score -= spamScore.score * 5;
        if (!authentication.spfConfigured) score -= 10;
        if (!authentication.dkimConfigured) score -= 10;
        if (!authentication.dmarcConfigured) score -= 5;
        if (!contentAnalysis.hasUnsubscribeLink) score -= 15;
        if (!contentAnalysis.hasPhysicalAddress) score -= 10;
        return Math.max(0, Math.min(100, score));
      }
    };
  }
});

// src/email-service/versioning/TemplateVersionManager.ts
var TemplateVersionManager;
var init_TemplateVersionManager = __esm({
  "src/email-service/versioning/TemplateVersionManager.ts"() {
    "use strict";
    TemplateVersionManager = class {
      templates = /* @__PURE__ */ new Map();
      versions = /* @__PURE__ */ new Map();
      async initialize() {
      }
      /**
       * Get active template by ID
       */
      async getActiveTemplate(templateId) {
        const template = this.templates.get(templateId);
        if (!template) {
          throw new Error(`Template not found: ${templateId}`);
        }
        if (!template.active) {
          throw new Error(`Template is not active: ${templateId}`);
        }
        return template;
      }
      /**
       * Get template variant
       */
      async getTemplateVariant(templateId, variantId) {
        const template = await this.getActiveTemplate(templateId);
        const variant = template.variants?.find((v) => v.id === variantId);
        if (!variant) {
          throw new Error(`Variant not found: ${variantId}`);
        }
        return {
          ...template,
          subject: variant.subject || template.subject,
          mjmlContent: variant.mjmlContent || template.mjmlContent,
          reactEmailComponent: variant.reactEmailComponent || template.reactEmailComponent
        };
      }
      /**
       * Save template (creates new version)
       */
      async saveTemplate(template) {
        const existingVersions = this.versions.get(template.id) || [];
        const versionNumber = this.generateVersionNumber(existingVersions);
        const version = {
          id: `${template.id}-v${versionNumber}`,
          templateId: template.id,
          version: versionNumber,
          createdAt: /* @__PURE__ */ new Date(),
          createdBy: "system",
          // Should come from auth context
          subject: template.subject,
          mjmlContent: template.mjmlContent,
          reactEmailComponent: template.reactEmailComponent,
          variables: template.variables,
          changeLog: `Version ${versionNumber} created`,
          tags: template.tags,
          active: true,
          deprecated: false
        };
        existingVersions.push(version);
        this.versions.set(template.id, existingVersions);
        template.version = versionNumber;
        template.updatedAt = /* @__PURE__ */ new Date();
        this.templates.set(template.id, template);
        return version;
      }
      /**
       * Get all versions of a template
       */
      async getVersions(templateId) {
        return this.versions.get(templateId) || [];
      }
      /**
       * Get specific version
       */
      async getVersion(templateId, version) {
        const versions = this.versions.get(templateId) || [];
        return versions.find((v) => v.version === version) || null;
      }
      /**
       * Rollback to previous version
       */
      async rollback(templateId, targetVersion) {
        const version = await this.getVersion(templateId, targetVersion);
        if (!version) {
          throw new Error(`Version not found: ${targetVersion}`);
        }
        const currentTemplate = this.templates.get(templateId);
        if (!currentTemplate) {
          throw new Error(`Template not found: ${templateId}`);
        }
        const rolledBackTemplate = {
          ...currentTemplate,
          subject: version.subject,
          mjmlContent: version.mjmlContent,
          reactEmailComponent: version.reactEmailComponent,
          variables: version.variables,
          tags: version.tags,
          version: targetVersion,
          updatedAt: /* @__PURE__ */ new Date()
        };
        await this.saveTemplate(rolledBackTemplate);
        return rolledBackTemplate;
      }
      /**
       * Deprecate a version
       */
      async deprecateVersion(templateId, version, reason) {
        const versions = this.versions.get(templateId) || [];
        const targetVersion = versions.find((v) => v.version === version);
        if (targetVersion) {
          targetVersion.deprecated = true;
          targetVersion.deprecatedAt = /* @__PURE__ */ new Date();
          targetVersion.deprecationReason = reason;
        }
      }
      /**
       * List all templates
       */
      async listTemplates(options2) {
        let templates = Array.from(this.templates.values());
        if (options2?.category) {
          templates = templates.filter((t) => t.category === options2.category);
        }
        if (options2?.active !== void 0) {
          templates = templates.filter((t) => t.active === options2.active);
        }
        return templates;
      }
      /**
       * Activate template
       */
      async activate(templateId) {
        const template = this.templates.get(templateId);
        if (template) {
          template.active = true;
          template.updatedAt = /* @__PURE__ */ new Date();
        }
      }
      /**
       * Deactivate template
       */
      async deactivate(templateId) {
        const template = this.templates.get(templateId);
        if (template) {
          template.active = false;
          template.updatedAt = /* @__PURE__ */ new Date();
        }
      }
      /**
       * Compare two versions
       */
      async compareVersions(templateId, version1, version2) {
        const v1 = await this.getVersion(templateId, version1);
        const v2 = await this.getVersion(templateId, version2);
        if (!v1 || !v2) {
          throw new Error("One or both versions not found");
        }
        const differences = [];
        if (v1.subject !== v2.subject) {
          differences.push({
            field: "subject",
            version1Value: v1.subject,
            version2Value: v2.subject
          });
        }
        if (v1.mjmlContent !== v2.mjmlContent) {
          differences.push({
            field: "mjmlContent",
            version1Value: v1.mjmlContent,
            version2Value: v2.mjmlContent
          });
        }
        return { differences };
      }
      generateVersionNumber(existingVersions) {
        if (existingVersions.length === 0) {
          return "1.0.0";
        }
        const latestVersion = existingVersions[existingVersions.length - 1].version;
        const [major, minor, patch] = latestVersion.split(".").map(Number);
        return `${major}.${minor}.${patch + 1}`;
      }
    };
  }
});

// src/email-service/EmailService.ts
var EmailService;
var init_EmailService = __esm({
  "src/email-service/EmailService.ts"() {
    "use strict";
    init_SMTPProvider();
    init_EmailQueue();
    init_TemplateRenderer();
    init_ABTestManager();
    init_EmailAnalyticsService();
    init_UnsubscribeManager();
    init_DeliverabilityChecker();
    init_TemplateVersionManager();
    EmailService = class {
      provider;
      queue;
      templateRenderer;
      abTestManager;
      analyticsService;
      unsubscribeManager;
      deliverabilityChecker;
      versionManager;
      config;
      initialized = false;
      constructor(config9) {
        this.config = config9;
        this.provider = this.createProvider(config9.provider);
        if (config9.queue?.enabled) {
          this.queue = new EmailQueue(config9.queue);
        }
        this.templateRenderer = new TemplateRenderer();
        this.abTestManager = new ABTestManager(config9.abTesting);
        this.analyticsService = new EmailAnalyticsService(config9.tracking);
        this.unsubscribeManager = new UnsubscribeManager();
        this.deliverabilityChecker = new DeliverabilityChecker(config9.deliverability);
        this.versionManager = new TemplateVersionManager();
      }
      /**
       * Initialize the email service
       */
      async initialize() {
        if (this.initialized) {
          return;
        }
        await this.provider.initialize();
        if (this.queue) {
          await this.queue.initialize();
        }
        await this.analyticsService.initialize();
        await this.unsubscribeManager.initialize();
        await this.versionManager.initialize();
        this.initialized = true;
      }
      /**
       * Send an email
       */
      async sendEmail(message, options2) {
        if (!this.initialized) {
          await this.initialize();
        }
        try {
          const recipientEmail = this.extractPrimaryRecipient(message);
          if (!options2?.skipUnsubscribeCheck) {
            const canSend = await this.checkUnsubscribePreferences(
              recipientEmail,
              message
            );
            if (!canSend) {
              return {
                success: false,
                recipientEmail,
                error: new Error("Recipient has unsubscribed"),
                metadata: { reason: "unsubscribed" }
              };
            }
          }
          if (!options2?.skipDeliverabilityCheck) {
            const deliverabilityReport = await this.deliverabilityChecker.check(
              message
            );
            if (!deliverabilityReport.spamScore.passed) {
              return {
                success: false,
                recipientEmail,
                error: new Error("Email failed deliverability check"),
                metadata: {
                  reason: "spam_score_too_high",
                  spamScore: deliverabilityReport.spamScore.score,
                  issues: deliverabilityReport.spamScore.issues
                }
              };
            }
          }
          if (this.config.tracking?.enabled && message.trackingEnabled !== false) {
            message = await this.addTracking(message);
          }
          if (!message.unsubscribeUrl) {
            message.unsubscribeUrl = await this.generateUnsubscribeUrl(
              recipientEmail,
              options2?.userId
            );
          }
          if (this.queue && !options2?.skipQueue) {
            const jobId = await this.queue.enqueue(message);
            return {
              success: true,
              recipientEmail,
              messageId: jobId,
              metadata: { queued: true }
            };
          } else {
            const result2 = await this.provider.send(message);
            if (result2.success && result2.messageId) {
              await this.analyticsService.trackSent({
                messageId: result2.messageId,
                recipientEmail,
                templateId: message.templateId,
                sentAt: /* @__PURE__ */ new Date(),
                metadata: message.metadata
              });
            }
            return result2;
          }
        } catch (error) {
          return {
            success: false,
            recipientEmail: this.extractPrimaryRecipient(message),
            error
          };
        }
      }
      /**
       * Send email from template
       */
      async sendFromTemplate(templateId, recipientEmail, variables, options2) {
        try {
          let template;
          let variantId;
          if (options2?.useABTesting && this.config.abTesting?.enabled) {
            const abResult = await this.abTestManager.selectVariant(templateId);
            if (abResult) {
              template = abResult.template;
              variantId = abResult.variantId;
            } else {
              template = await this.versionManager.getActiveTemplate(templateId);
            }
          } else if (options2?.variant) {
            template = await this.versionManager.getTemplateVariant(
              templateId,
              options2.variant
            );
            variantId = options2.variant;
          } else {
            template = await this.versionManager.getActiveTemplate(templateId);
          }
          const rendered = await this.templateRenderer.render(template, variables);
          const message = {
            to: recipientEmail,
            from: this.config.provider.from,
            subject: rendered.subject,
            text: rendered.text,
            html: rendered.html,
            templateId: template.id,
            templateVersion: template.version,
            abTestVariant: variantId,
            metadata: {
              ...variables,
              templateCategory: template.category
            }
          };
          return await this.sendEmail(message, options2);
        } catch (error) {
          return {
            success: false,
            recipientEmail: typeof recipientEmail === "string" ? recipientEmail : recipientEmail[0]?.email || "unknown",
            error
          };
        }
      }
      /**
       * Send bulk emails (e.g., for campaigns)
       */
      async sendBulk(messages, options2) {
        const results = [];
        const batchSize = options2?.batchSize || 100;
        const delay2 = options2?.delayBetweenBatches || 1e3;
        for (let i = 0; i < messages.length; i += batchSize) {
          const batch = messages.slice(i, i + batchSize);
          const batchResults = await Promise.allSettled(
            batch.map((message) => this.sendEmail(message))
          );
          for (const result2 of batchResults) {
            if (result2.status === "fulfilled") {
              results.push(result2.value);
            } else {
              results.push({
                success: false,
                recipientEmail: "unknown",
                error: result2.reason
              });
            }
          }
          if (i + batchSize < messages.length && delay2 > 0) {
            await new Promise((resolve2) => setTimeout(resolve2, delay2));
          }
        }
        return results;
      }
      /**
       * Get email analytics
       */
      async getAnalytics(messageId) {
        return await this.analyticsService.getAnalytics(messageId);
      }
      /**
       * Get template analytics
       */
      async getTemplateAnalytics(templateId, dateRange) {
        return await this.analyticsService.getTemplateAnalytics(templateId, dateRange);
      }
      /**
       * Get A/B test results
       */
      async getABTestResults(testId) {
        return await this.abTestManager.getResults(testId);
      }
      /**
       * Preview email template
       */
      async previewTemplate(templateId, variables, variant) {
        const template = variant ? await this.versionManager.getTemplateVariant(templateId, variant) : await this.versionManager.getActiveTemplate(templateId);
        return await this.templateRenderer.render(template, variables);
      }
      /**
       * Check deliverability for a message
       */
      async checkDeliverability(message) {
        return await this.deliverabilityChecker.check(message);
      }
      /**
       * Update unsubscribe preferences
       */
      async updateUnsubscribePreferences(email, preferences) {
        return await this.unsubscribeManager.updatePreferences(email, preferences);
      }
      /**
       * Create or update email template
       */
      async saveTemplate(template) {
        return await this.versionManager.saveTemplate(template);
      }
      /**
       * Rollback template to previous version
       */
      async rollbackTemplate(templateId, version) {
        return await this.versionManager.rollback(templateId, version);
      }
      /**
       * Shutdown the email service
       */
      async shutdown() {
        if (this.queue) {
          await this.queue.shutdown();
        }
        await this.provider.shutdown();
        this.initialized = false;
      }
      // Private helper methods
      createProvider(config9) {
        switch (config9.provider) {
          case "smtp":
            return new SMTPProvider(config9);
          case "sendgrid":
            throw new Error("SendGrid provider not yet implemented");
          case "aws-ses":
            throw new Error("AWS SES provider not yet implemented");
          case "mailgun":
            throw new Error("Mailgun provider not yet implemented");
          case "postmark":
            throw new Error("Postmark provider not yet implemented");
          default:
            throw new Error(`Unsupported email provider: ${config9.provider}`);
        }
      }
      extractPrimaryRecipient(message) {
        if (typeof message.to === "string") {
          return message.to;
        }
        if (Array.isArray(message.to)) {
          const first = message.to[0];
          return typeof first === "string" ? first : first.email;
        }
        return message.to.email;
      }
      async checkUnsubscribePreferences(email, message) {
        const preferences = await this.unsubscribeManager.getPreferences(email);
        if (!preferences) {
          return true;
        }
        if (preferences.unsubscribedFromAll) {
          return false;
        }
        if (message.metadata?.templateCategory) {
          const category = message.metadata.templateCategory;
          const categoryPref = preferences.categories[category];
          if (categoryPref && !categoryPref.subscribed) {
            return false;
          }
        }
        if (preferences.frequency) {
          const canSend = await this.unsubscribeManager.checkFrequencyLimit(
            email,
            preferences.frequency
          );
          if (!canSend) {
            return false;
          }
        }
        return true;
      }
      async addTracking(message) {
        if (this.config.tracking?.openTracking) {
          message = await this.analyticsService.addOpenTracking(message);
        }
        if (this.config.tracking?.clickTracking) {
          message = await this.analyticsService.addClickTracking(message);
        }
        return message;
      }
      async generateUnsubscribeUrl(email, userId) {
        return await this.unsubscribeManager.generateUnsubscribeUrl(email, userId);
      }
    };
  }
});

// src/services/PasswordResetService.ts
import crypto13 from "crypto";
import argon23 from "argon2";
var PasswordResetService;
var init_PasswordResetService = __esm({
  "src/services/PasswordResetService.ts"() {
    "use strict";
    init_database();
    init_logger2();
    init_EmailService();
    init_config();
    PasswordResetService = class {
      get pool() {
        return getPostgresPool2();
      }
      tokenExpirationMs = 60 * 60 * 1e3;
      // 1 hour
      _emailService = null;
      constructor() {
      }
      async getEmailService() {
        if (this._emailService) return this._emailService;
        this._emailService = new EmailService({
          provider: {
            provider: "smtp",
            smtp: {
              host: cfg.SMTP_HOST || "localhost",
              port: cfg.SMTP_PORT,
              secure: cfg.SMTP_PORT === 465,
              auth: cfg.SMTP_USER ? {
                user: cfg.SMTP_USER,
                pass: cfg.SMTP_PASS || ""
              } : void 0
            },
            from: {
              name: cfg.EMAIL_FROM_NAME,
              email: cfg.EMAIL_FROM_ADDRESS
            }
          },
          queue: { enabled: false, concurrency: 1, retryAttempts: 0, retryBackoff: "fixed", retryDelay: 0 }
          // Sync for now
        });
        await this._emailService.initialize();
        return this._emailService;
      }
      /**
       * Generate a secure random token
       */
      generateToken() {
        return crypto13.randomBytes(32).toString("hex");
      }
      /**
       * Hash a token for secure storage
       */
      hashToken(token) {
        return crypto13.createHash("sha256").update(token).digest("hex");
      }
      /**
       * Request a password reset for an email address
       *
       * @param email - The email address to reset password for
       * @param ipAddress - Client IP address for audit logging
       * @param userAgent - Client user agent for audit logging
       * @returns The reset token (to be sent via email) or null if user not found
       *
       * Note: This method always completes successfully to prevent email enumeration
       */
      async requestPasswordReset(email, ipAddress, userAgent) {
        const client6 = await this.pool.connect();
        try {
          const userResult = await client6.query(
            "SELECT id, email, first_name FROM users WHERE email = $1 AND is_active = true",
            [email.toLowerCase()]
          );
          if (userResult.rows.length === 0) {
            logger_default2.info({
              message: "Password reset requested for non-existent email",
              email,
              ipAddress
            });
            return null;
          }
          const user = userResult.rows[0];
          await client6.query(
            "UPDATE password_reset_tokens SET used_at = NOW() WHERE user_id = $1 AND used_at IS NULL",
            [user.id]
          );
          const token = this.generateToken();
          const tokenHash = this.hashToken(token);
          const expiresAt = new Date(Date.now() + this.tokenExpirationMs);
          await client6.query(
            `INSERT INTO password_reset_tokens
         (user_id, token_hash, expires_at, ip_address, user_agent)
         VALUES ($1, $2, $3, $4, $5)`,
            [user.id, tokenHash, expiresAt, ipAddress, userAgent]
          );
          await this.logLoginAttempt(client6, {
            userId: user.id,
            email,
            success: true,
            reason: "password_reset_requested",
            ipAddress,
            userAgent
          });
          logger_default2.info({
            message: "Password reset token generated",
            userId: user.id,
            email,
            ipAddress,
            expiresAt
          });
          await this.sendPasswordResetEmail(email, user.first_name, token);
          return token;
        } catch (error) {
          logger_default2.error({
            message: "Failed to create password reset token",
            error: error instanceof Error ? error.message : "Unknown error",
            email
          });
          throw error;
        } finally {
          client6.release();
        }
      }
      /**
       * Reset password using a reset token
       *
       * @param token - The reset token from the email
       * @param newPassword - The new password to set
       */
      async resetPassword(token, newPassword) {
        const client6 = await this.pool.connect();
        try {
          await client6.query("BEGIN");
          const tokenHash = this.hashToken(token);
          const tokenResult = await client6.query(
            `SELECT prt.id, prt.user_id, prt.expires_at, prt.used_at, u.email
         FROM password_reset_tokens prt
         JOIN users u ON u.id = prt.user_id
         WHERE prt.token_hash = $1`,
            [tokenHash]
          );
          if (tokenResult.rows.length === 0) {
            throw new Error("Invalid reset token");
          }
          const resetToken = tokenResult.rows[0];
          if (resetToken.used_at) {
            throw new Error("Reset token has already been used");
          }
          if (new Date(resetToken.expires_at) < /* @__PURE__ */ new Date()) {
            throw new Error("Reset token has expired");
          }
          const passwordHash = await argon23.hash(newPassword);
          await client6.query(
            `UPDATE users
         SET password_hash = $1, updated_at = NOW(), failed_login_attempts = 0, locked_until = NULL
         WHERE id = $2`,
            [passwordHash, resetToken.user_id]
          );
          await client6.query(
            "UPDATE password_reset_tokens SET used_at = NOW() WHERE id = $1",
            [resetToken.id]
          );
          await client6.query(
            "UPDATE user_sessions SET is_revoked = true WHERE user_id = $1",
            [resetToken.user_id]
          );
          await this.logLoginAttempt(client6, {
            userId: resetToken.user_id,
            email: resetToken.email,
            success: true,
            reason: "password_reset_completed"
          });
          await client6.query("COMMIT");
          logger_default2.info({
            message: "Password reset successful",
            userId: resetToken.user_id,
            email: resetToken.email
          });
        } catch (error) {
          await client6.query("ROLLBACK");
          logger_default2.error({
            message: "Password reset failed",
            error: error instanceof Error ? error.message : "Unknown error"
          });
          throw error;
        } finally {
          client6.release();
        }
      }
      /**
       * Change password for an authenticated user
       *
       * @param userId - The user ID
       * @param currentPassword - The current password for verification
       * @param newPassword - The new password to set
       */
      async changePassword(userId, currentPassword, newPassword) {
        const client6 = await this.pool.connect();
        try {
          await client6.query("BEGIN");
          const userResult = await client6.query(
            "SELECT id, email, password_hash FROM users WHERE id = $1 AND is_active = true",
            [userId]
          );
          if (userResult.rows.length === 0) {
            throw new Error("User not found");
          }
          const user = userResult.rows[0];
          const isValidPassword = await argon23.verify(user.password_hash, currentPassword);
          if (!isValidPassword) {
            await this.logLoginAttempt(client6, {
              userId: user.id,
              email: user.email,
              success: false,
              reason: "password_change_incorrect_current"
            });
            throw new Error("Current password is incorrect");
          }
          const newPasswordHash = await argon23.hash(newPassword);
          await client6.query(
            "UPDATE users SET password_hash = $1, updated_at = NOW() WHERE id = $2",
            [newPasswordHash, userId]
          );
          await client6.query(
            "UPDATE user_sessions SET is_revoked = true WHERE user_id = $1",
            [userId]
          );
          await this.logLoginAttempt(client6, {
            userId: user.id,
            email: user.email,
            success: true,
            reason: "password_changed"
          });
          await client6.query("COMMIT");
          logger_default2.info({
            message: "Password changed successfully",
            userId
          });
        } catch (error) {
          await client6.query("ROLLBACK");
          logger_default2.error({
            message: "Password change failed",
            userId,
            error: error instanceof Error ? error.message : "Unknown error"
          });
          throw error;
        } finally {
          client6.release();
        }
      }
      /**
       * Verify a password reset token is valid without consuming it
       *
       * @param token - The reset token to verify
       * @returns True if token is valid, false otherwise
       */
      async verifyResetToken(token) {
        try {
          const tokenHash = this.hashToken(token);
          const result2 = await this.pool.query(
            `SELECT id, expires_at, used_at
         FROM password_reset_tokens
         WHERE token_hash = $1`,
            [tokenHash]
          );
          if (result2.rows.length === 0) {
            return false;
          }
          const resetToken = result2.rows[0];
          if (resetToken.used_at) {
            return false;
          }
          if (new Date(resetToken.expires_at) < /* @__PURE__ */ new Date()) {
            return false;
          }
          return true;
        } catch (error) {
          logger_default2.error({
            message: "Failed to verify reset token",
            error: error instanceof Error ? error.message : "Unknown error"
          });
          return false;
        }
      }
      /**
       * Send password reset email
       * In production, this would integrate with an email service
       */
      async sendPasswordResetEmail(email, firstName, token) {
        const resetUrl = `${process.env.FRONTEND_URL || "http://localhost:3000"}/auth/reset-password?token=${token}`;
        logger_default2.info({
          message: "Sending password reset email",
          email,
          firstName,
          tokenLength: token.length
        });
        try {
          const emailService = await this.getEmailService();
          await emailService.sendFromTemplate(
            "password-reset",
            email,
            {
              firstName,
              resetUrl,
              expiryHours: 1
            }
          );
          logger_default2.info({
            message: "Password reset email sent successfully",
            email
          });
        } catch (error) {
          logger_default2.error({
            message: "Failed to send password reset email",
            error: error instanceof Error ? error.message : "Unknown error",
            email
          });
        }
      }
      /**
       * Log authentication-related events for audit
       */
      async logLoginAttempt(client6, data) {
        try {
          await client6.query(
            `INSERT INTO login_audit
         (user_id, email, success, failure_reason, ip_address, user_agent, provider)
         VALUES ($1, $2, $3, $4, $5, $6, 'local')`,
            [
              data.userId,
              data.email,
              data.success,
              data.reason,
              data.ipAddress,
              data.userAgent
            ]
          );
        } catch (error) {
          logger_default2.error({
            message: "Failed to log authentication event",
            error: error instanceof Error ? error.message : "Unknown error"
          });
        }
      }
    };
  }
});

// src/middleware/authRateLimit.ts
async function checkRateLimit(key, config9) {
  const redis5 = getRedisClient();
  if (!redis5) {
    logger_default2.warn("Redis unavailable for rate limiting, allowing request");
    return {
      allowed: true,
      remaining: config9.maxAttempts,
      resetTime: Date.now() + config9.windowMs,
      blocked: false
    };
  }
  const now = Date.now();
  const windowKey = `${config9.keyPrefix}:${key}:window`;
  const blockKey = `${config9.keyPrefix}:${key}:blocked`;
  try {
    const blockExpires = await redis5.get(blockKey);
    if (blockExpires) {
      const expiresAt = parseInt(blockExpires, 10);
      if (expiresAt > now) {
        return {
          allowed: false,
          remaining: 0,
          resetTime: expiresAt,
          blocked: true,
          blockExpires: expiresAt
        };
      }
    }
    const countStr = await redis5.get(windowKey);
    const count = countStr ? parseInt(countStr, 10) : 0;
    if (count >= config9.maxAttempts) {
      const blockExpireTime = now + config9.blockDuration;
      await redis5.set(blockKey, blockExpireTime.toString(), "PX", config9.blockDuration);
      await redis5.del(windowKey);
      return {
        allowed: false,
        remaining: 0,
        resetTime: blockExpireTime,
        blocked: true,
        blockExpires: blockExpireTime
      };
    }
    const newCount = await redis5.incr(windowKey);
    if (newCount === 1) {
      await redis5.pexpire(windowKey, config9.windowMs);
    }
    const ttl = await redis5.pttl(windowKey);
    return {
      allowed: true,
      remaining: Math.max(0, config9.maxAttempts - newCount),
      resetTime: now + (ttl > 0 ? ttl : config9.windowMs),
      blocked: false
    };
  } catch (error) {
    logger_default2.error("Rate limit check failed:", error);
    return {
      allowed: true,
      remaining: config9.maxAttempts,
      resetTime: Date.now() + config9.windowMs,
      blocked: false
    };
  }
}
function createRateLimiter(config9) {
  return async (req, res, next) => {
    const ip = req.ip || req.socket.remoteAddress || "unknown";
    const email = req.body?.email || "";
    const key = email ? `${ip}:${email}` : ip;
    const result2 = await checkRateLimit(key, config9);
    res.set("X-RateLimit-Limit", config9.maxAttempts.toString());
    res.set("X-RateLimit-Remaining", result2.remaining.toString());
    res.set("X-RateLimit-Reset", Math.ceil(result2.resetTime / 1e3).toString());
    if (!result2.allowed) {
      const retryAfter = Math.ceil((result2.resetTime - Date.now()) / 1e3);
      res.set("Retry-After", retryAfter.toString());
      logger_default2.warn({
        message: "Rate limit exceeded",
        ip,
        email: email || void 0,
        keyPrefix: config9.keyPrefix,
        blocked: result2.blocked,
        retryAfter
      });
      return res.status(429).json({
        error: result2.blocked ? "Too many attempts. Please try again later." : "Rate limit exceeded. Please slow down.",
        code: "RATE_LIMIT_EXCEEDED",
        retryAfter,
        blocked: result2.blocked
      });
    }
    next();
  };
}
async function recordFailedLogin(ip, email) {
  const redis5 = getRedisClient();
  if (!redis5) return;
  const key = `auth:login:${ip}:${email}`;
  try {
    const count = await redis5.incr(key);
    if (count === 1) {
      await redis5.pexpire(key, 15 * 60 * 1e3);
    }
    if (count >= 5) {
      const blockKey = `auth:login:${ip}:${email}:blocked`;
      const blockDuration = Math.min(count * 60 * 1e3, 36e5);
      await redis5.set(blockKey, (Date.now() + blockDuration).toString(), "PX", blockDuration);
    }
  } catch (error) {
    logger_default2.error("Failed to record failed login:", error);
  }
}
async function clearFailedLogins(ip, email) {
  const redis5 = getRedisClient();
  if (!redis5) return;
  try {
    const windowKey = `auth:login:${ip}:${email}`;
    const blockKey = `auth:login:${ip}:${email}:blocked`;
    await redis5.del(windowKey, blockKey);
  } catch (error) {
    logger_default2.error("Failed to clear failed logins:", error);
  }
}
var authRateLimiter, loginRateLimiter, registerRateLimiter, passwordResetRateLimiter, refreshRateLimiter;
var init_authRateLimit = __esm({
  "src/middleware/authRateLimit.ts"() {
    "use strict";
    init_database();
    init_logger2();
    authRateLimiter = createRateLimiter({
      windowMs: 15 * 60 * 1e3,
      // 15 minutes
      maxAttempts: 100,
      blockDuration: 30 * 60 * 1e3,
      // 30 minutes
      keyPrefix: "auth:general"
    });
    loginRateLimiter = createRateLimiter({
      windowMs: 15 * 60 * 1e3,
      // 15 minutes
      maxAttempts: 5,
      blockDuration: 15 * 60 * 1e3,
      // 15 minutes block
      keyPrefix: "auth:login"
    });
    registerRateLimiter = createRateLimiter({
      windowMs: 60 * 60 * 1e3,
      // 1 hour
      maxAttempts: 3,
      blockDuration: 60 * 60 * 1e3,
      // 1 hour block
      keyPrefix: "auth:register"
    });
    passwordResetRateLimiter = createRateLimiter({
      windowMs: 60 * 60 * 1e3,
      // 1 hour
      maxAttempts: 3,
      blockDuration: 60 * 60 * 1e3,
      // 1 hour block
      keyPrefix: "auth:reset"
    });
    refreshRateLimiter = createRateLimiter({
      windowMs: 15 * 60 * 1e3,
      // 15 minutes
      maxAttempts: 20,
      blockDuration: 15 * 60 * 1e3,
      // 15 minutes block
      keyPrefix: "auth:refresh"
    });
  }
});

// src/graphql/validation/auth.schema.ts
import { z as z5 } from "zod";
var passwordSchema, registerSchema, loginSchema, requestPasswordResetSchema, resetPasswordSchema, changePasswordSchema;
var init_auth_schema = __esm({
  "src/graphql/validation/auth.schema.ts"() {
    "use strict";
    passwordSchema = z5.string().min(8, "Password must be at least 8 characters").regex(/[A-Z]/, "Password must contain at least one uppercase letter").regex(/[a-z]/, "Password must contain at least one lowercase letter").regex(/[0-9]/, "Password must contain at least one number").regex(/[^A-Za-z0-9]/, "Password must contain at least one special character");
    registerSchema = z5.object({
      email: z5.string().email("Invalid email address").transform((val) => val.toLowerCase()),
      password: passwordSchema,
      username: z5.string().optional(),
      firstName: z5.string().min(1, "First name is required"),
      lastName: z5.string().min(1, "Last name is required")
    });
    loginSchema = z5.object({
      email: z5.string().email("Invalid email address").transform((val) => val.toLowerCase()),
      password: z5.string().min(1, "Password is required")
    });
    requestPasswordResetSchema = z5.object({
      email: z5.string().email("Invalid email address")
    });
    resetPasswordSchema = z5.object({
      token: z5.string().min(1, "Token is required"),
      newPassword: passwordSchema
    });
    changePasswordSchema = z5.object({
      currentPassword: z5.string().min(1, "Current password is required"),
      newPassword: passwordSchema
    });
  }
});

// src/graphql/resolvers/auth.ts
import { GraphQLError as GraphQLError7 } from "graphql";
function validateInput(schema2, input) {
  try {
    return schema2.parse(input);
  } catch (error) {
    const zodError = error;
    if (zodError.errors && Array.isArray(zodError.errors) && zodError.errors.length > 0) {
      const firstError = zodError.errors[0];
      throw new GraphQLError7(firstError.message, {
        extensions: {
          code: "GRAPHQL_VALIDATION_FAILED",
          // Use standard code we allow in production
          field: firstError.path.join("."),
          issues: zodError.errors
        }
      });
    }
    throw error;
  }
}
var authService, passwordResetService, authResolvers, auth_default;
var init_auth3 = __esm({
  "src/graphql/resolvers/auth.ts"() {
    "use strict";
    init_AuthService();
    init_PasswordResetService();
    init_authRateLimit();
    init_logger2();
    init_auth_schema();
    authService = new AuthService();
    passwordResetService = new PasswordResetService();
    authResolvers = {
      Query: {
        /**
         * Get the currently authenticated user's profile
         */
        me: async (_2, __, context4) => {
          if (!context4.isAuthenticated || !context4.user) {
            throw new GraphQLError7("Authentication required", {
              extensions: { code: "UNAUTHENTICATED" }
            });
          }
          return {
            id: context4.user.id,
            email: context4.user.email,
            username: context4.user.username,
            firstName: context4.user.firstName,
            lastName: context4.user.lastName,
            fullName: context4.user.fullName,
            role: context4.user.role,
            isActive: context4.user.isActive,
            lastLogin: context4.user.lastLogin,
            createdAt: context4.user.createdAt,
            updatedAt: context4.user.updatedAt
          };
        },
        /**
         * Verify if a token is valid
         */
        verifyToken: async (_2, { token }) => {
          try {
            const user = await authService.verifyToken(token);
            return {
              valid: !!user,
              user: user ? {
                id: user.id,
                email: user.email,
                role: user.role
              } : null
            };
          } catch (error) {
            return { valid: false, user: null };
          }
        },
        /**
         * Verify if a password reset token is valid
         */
        verifyResetToken: async (_2, { token }) => {
          const isValid = await passwordResetService.verifyResetToken(token);
          return { valid: isValid };
        }
      },
      Mutation: {
        /**
         * Register a new user account
         */
        register: async (_2, {
          input
        }, context4) => {
          const { email, password, username, firstName, lastName } = input;
          validateInput(registerSchema, input);
          if (!firstName || firstName.length < 1) {
            throw new GraphQLError7("First name is required", {
              extensions: { code: "VALIDATION_ERROR", field: "firstName" }
            });
          }
          if (!lastName || lastName.length < 1) {
            throw new GraphQLError7("Last name is required", {
              extensions: { code: "VALIDATION_ERROR", field: "lastName" }
            });
          }
          try {
            const result2 = await authService.register({
              email: email.toLowerCase(),
              password,
              username: username || email.split("@")[0],
              firstName,
              lastName,
              role: "VIEWER"
              // Default role for new registrations
            });
            logger_default2.info({
              message: "User registered via GraphQL",
              userId: result2.user.id,
              email: result2.user.email,
              ip: context4.req?.ip
            });
            return {
              success: true,
              message: "Registration successful",
              user: {
                id: result2.user.id,
                email: result2.user.email,
                username: result2.user.username,
                firstName: result2.user.firstName,
                lastName: result2.user.lastName,
                role: result2.user.role,
                createdAt: result2.user.createdAt
              },
              token: result2.token,
              refreshToken: result2.refreshToken,
              expiresIn: result2.expiresIn
            };
          } catch (error) {
            logger_default2.error({
              message: "Registration failed via GraphQL",
              error: error.message,
              email,
              ip: context4.req?.ip
            });
            if (error.message.includes("already exists")) {
              throw new GraphQLError7("User with this email or username already exists", {
                extensions: { code: "USER_EXISTS" }
              });
            }
            throw new GraphQLError7("Registration failed", {
              extensions: { code: "REGISTRATION_FAILED" }
            });
          }
        },
        /**
         * Authenticate user and return tokens
         */
        login: async (_2, { input }, context4) => {
          const { email, password } = input;
          validateInput(loginSchema, input);
          const ip = context4.req?.ip || "unknown";
          const userAgent = context4.req?.get?.("User-Agent") || "";
          try {
            const result2 = await authService.login(email.toLowerCase(), password, ip, userAgent);
            await clearFailedLogins(ip, email.toLowerCase());
            logger_default2.info({
              message: "User logged in via GraphQL",
              userId: result2.user.id,
              email: result2.user.email,
              ip
            });
            return {
              success: true,
              message: "Login successful",
              user: {
                id: result2.user.id,
                email: result2.user.email,
                username: result2.user.username,
                firstName: result2.user.firstName,
                lastName: result2.user.lastName,
                fullName: result2.user.fullName,
                role: result2.user.role,
                lastLogin: result2.user.lastLogin
              },
              token: result2.token,
              refreshToken: result2.refreshToken,
              expiresIn: result2.expiresIn
            };
          } catch (error) {
            await recordFailedLogin(ip, email.toLowerCase());
            logger_default2.warn({
              message: "Login failed via GraphQL",
              email,
              ip,
              error: error.message
            });
            throw new GraphQLError7("Invalid credentials", {
              extensions: { code: "INVALID_CREDENTIALS" }
            });
          }
        },
        /**
         * Refresh access token using refresh token
         */
        refreshToken: async (_2, { refreshToken }, context4) => {
          try {
            const result2 = await authService.refreshAccessToken(refreshToken);
            if (!result2) {
              throw new GraphQLError7("Invalid or expired refresh token", {
                extensions: { code: "INVALID_REFRESH_TOKEN" }
              });
            }
            logger_default2.info({
              message: "Token refreshed via GraphQL",
              ip: context4.req?.ip
            });
            return {
              success: true,
              token: result2.token,
              refreshToken: result2.refreshToken
            };
          } catch (error) {
            logger_default2.error({
              message: "Token refresh failed via GraphQL",
              error: error.message,
              ip: context4.req?.ip
            });
            throw new GraphQLError7("Token refresh failed", {
              extensions: { code: "REFRESH_FAILED" }
            });
          }
        },
        /**
         * Logout user and invalidate tokens
         */
        logout: async (_2, __, context4) => {
          if (!context4.isAuthenticated || !context4.user) {
            throw new GraphQLError7("Authentication required", {
              extensions: { code: "UNAUTHENTICATED" }
            });
          }
          try {
            const token = context4.req?.headers?.authorization?.replace("Bearer ", "");
            await authService.logout(context4.user.id, token);
            logger_default2.info({
              message: "User logged out via GraphQL",
              userId: context4.user.id,
              ip: context4.req?.ip
            });
            return {
              success: true,
              message: "Logout successful"
            };
          } catch (error) {
            logger_default2.error({
              message: "Logout failed via GraphQL",
              error: error.message,
              userId: context4.user?.id,
              ip: context4.req?.ip
            });
            throw new GraphQLError7("Logout failed", {
              extensions: { code: "LOGOUT_FAILED" }
            });
          }
        },
        /**
         * Request password reset email
         */
        requestPasswordReset: async (_2, { email }, context4) => {
          validateInput(requestPasswordResetSchema, { email });
          try {
            await passwordResetService.requestPasswordReset(
              email.toLowerCase(),
              context4.req?.ip,
              context4.req?.get?.("User-Agent")
            );
            return {
              success: true,
              message: "If an account with that email exists, a password reset link has been sent"
            };
          } catch (error) {
            logger_default2.error({
              message: "Password reset request failed via GraphQL",
              error: error.message,
              email,
              ip: context4.req?.ip
            });
            return {
              success: true,
              message: "If an account with that email exists, a password reset link has been sent"
            };
          }
        },
        /**
         * Reset password using reset token
         */
        resetPassword: async (_2, { token, newPassword }, context4) => {
          validateInput(resetPasswordSchema, { token, newPassword });
          try {
            await passwordResetService.resetPassword(token, newPassword);
            logger_default2.info({
              message: "Password reset successful via GraphQL",
              ip: context4.req?.ip
            });
            return {
              success: true,
              message: "Password reset successful. Please log in with your new password."
            };
          } catch (error) {
            logger_default2.warn({
              message: "Password reset failed via GraphQL",
              error: error.message,
              ip: context4.req?.ip
            });
            if (error.message.includes("expired") || error.message.includes("invalid")) {
              throw new GraphQLError7("Invalid or expired reset token", {
                extensions: { code: "INVALID_RESET_TOKEN" }
              });
            }
            throw new GraphQLError7("Password reset failed", {
              extensions: { code: "RESET_FAILED" }
            });
          }
        },
        /**
         * Change password for authenticated user
         */
        changePassword: async (_2, {
          currentPassword,
          newPassword
        }, context4) => {
          if (!context4.isAuthenticated || !context4.user) {
            throw new GraphQLError7("Authentication required", {
              extensions: { code: "UNAUTHENTICATED" }
            });
          }
          validateInput(changePasswordSchema, { currentPassword, newPassword });
          try {
            await passwordResetService.changePassword(
              context4.user.id,
              currentPassword,
              newPassword
            );
            logger_default2.info({
              message: "Password changed via GraphQL",
              userId: context4.user.id,
              ip: context4.req?.ip
            });
            return {
              success: true,
              message: "Password changed successfully"
            };
          } catch (error) {
            logger_default2.warn({
              message: "Password change failed via GraphQL",
              error: error.message,
              userId: context4.user.id,
              ip: context4.req?.ip
            });
            if (error.message.includes("incorrect")) {
              throw new GraphQLError7("Current password is incorrect", {
                extensions: { code: "INCORRECT_PASSWORD" }
              });
            }
            throw new GraphQLError7("Password change failed", {
              extensions: { code: "CHANGE_FAILED" }
            });
          }
        },
        /**
         * Revoke a specific token
         */
        revokeToken: async (_2, { token }, context4) => {
          if (!context4.isAuthenticated || !context4.user) {
            throw new GraphQLError7("Authentication required", {
              extensions: { code: "UNAUTHENTICATED" }
            });
          }
          try {
            await authService.revokeToken(token);
            logger_default2.info({
              message: "Token revoked via GraphQL",
              userId: context4.user.id,
              ip: context4.req?.ip
            });
            return {
              success: true,
              message: "Token revoked successfully"
            };
          } catch (error) {
            logger_default2.error({
              message: "Token revocation failed via GraphQL",
              error: error.message,
              ip: context4.req?.ip
            });
            throw new GraphQLError7("Token revocation failed", {
              extensions: { code: "REVOCATION_FAILED" }
            });
          }
        }
      }
    };
    auth_default = authResolvers;
  }
});

// src/resolvers/WargameResolver.ts
import { randomUUID as uuidv45 } from "node:crypto";
var PYTHON_API_URL, PYTHON_API_KEY, WargameResolver;
var init_WargameResolver = __esm({
  "src/resolvers/WargameResolver.ts"() {
    "use strict";
    init_neo4j();
    PYTHON_API_URL = process.env.PYTHON_API_URL || "http://localhost:8001";
    PYTHON_API_KEY = process.env.PYTHON_API_KEY || "default-api-key";
    WargameResolver = class {
      driver = getNeo4jDriver();
      async getCrisisTelemetry(_parent, {
        scenarioId,
        limit,
        offset
      }, _context) {
        console.log("Fetching telemetry for scenario:", scenarioId, "from Neo4j");
        const session = this.driver.session();
        try {
          const query3 = "MATCH (s:CrisisScenario {id: $scenarioId})-[:HAS_TELEMETRY]->(t:SocialMediaPost) RETURN t SKIP $offset LIMIT $limit";
          const result2 = await session.run(query3, {
            scenarioId,
            offset: offset || 0,
            limit: limit || 1e3
          });
          return result2.records.map(
            (record2) => record2.get("t").properties
          );
        } finally {
          await session.close();
        }
      }
      async getAdversaryIntentEstimates(_parent, { scenarioId }, _context) {
        console.log(
          "Fetching adversary intent estimates for scenario:",
          scenarioId,
          "from Neo4j"
        );
        const session = this.driver.session();
        try {
          const query3 = "MATCH (s:CrisisScenario {id: $scenarioId})-[:HAS_INTENT_ESTIMATE]->(i:AdversaryIntent) RETURN i";
          const result2 = await session.run(query3, { scenarioId });
          return result2.records.map(
            (record2) => record2.get("i").properties
          );
        } finally {
          await session.close();
        }
      }
      async getNarrativeHeatmapData(_parent, { scenarioId }, _context) {
        console.log(
          "Fetching narrative heatmap data for scenario:",
          scenarioId,
          "from Neo4j"
        );
        const session = this.driver.session();
        try {
          const query3 = "MATCH (s:CrisisScenario {id: $scenarioId})-[:HAS_HEATMAP_DATA]->(h:NarrativeHeatmap) RETURN h";
          const result2 = await session.run(query3, { scenarioId });
          return result2.records.map(
            (record2) => record2.get("h").properties
          );
        } finally {
          await session.close();
        }
      }
      async getStrategicResponsePlaybooks(_parent, { scenarioId }, _context) {
        console.log(
          "Fetching strategic response playbooks for scenario:",
          scenarioId,
          "from Neo4j"
        );
        const session = this.driver.session();
        try {
          const query3 = "MATCH (s:CrisisScenario {id: $scenarioId})-[:HAS_PLAYBOOK]->(p:StrategicPlaybook) RETURN p";
          const result2 = await session.run(query3, { scenarioId });
          return result2.records.map(
            (record2) => record2.get("p").properties
          );
        } finally {
          await session.close();
        }
      }
      async getCrisisScenario(_parent, { id }, _context) {
        console.log("Fetching crisis scenario:", id, "from Neo4j");
        const session = this.driver.session();
        try {
          const query3 = "MATCH (s:CrisisScenario {id: $id}) RETURN s";
          const result2 = await session.run(query3, { id });
          if (result2.records.length > 0) {
            return result2.records[0].get("s").properties;
          }
          return void 0;
        } finally {
          await session.close();
        }
      }
      async getAllCrisisScenarios(_parent, _args, _context) {
        console.log("Fetching all crisis scenarios from Neo4j");
        const session = this.driver.session();
        try {
          const query3 = "MATCH (s:CrisisScenario) RETURN s ORDER BY s.createdAt DESC";
          const result2 = await session.run(query3);
          return result2.records.map(
            (record2) => record2.get("s").properties
          );
        } finally {
          await session.close();
        }
      }
      async runWarGameSimulation(_parent, { input }, _context) {
        console.log("Running war-game simulation with input:", input);
        const session = this.driver.session();
        try {
          const scenarioId = uuidv45();
          const createdAt = (/* @__PURE__ */ new Date()).toISOString();
          const updatedAt = createdAt;
          const createScenarioResult = await session.run(
            `CREATE (s:CrisisScenario {
          id: $scenarioId,
          crisisType: $crisisType,
          createdAt: $createdAt,
          updatedAt: $updatedAt,
          simulationParameters: $simulationParameters
        }) RETURN s`,
            {
              scenarioId,
              crisisType: input.crisisType,
              createdAt,
              updatedAt,
              simulationParameters: input.simulationParameters
            }
          );
          const newScenario = createScenarioResult.records[0].get("s").properties;
          return newScenario;
        } finally {
          await session.close();
        }
      }
      async updateCrisisScenario(_parent, { id, input }, _context) {
        console.log("Updating crisis scenario:", id, "with input:", input);
        const session = this.driver.session();
        try {
          const updatedAt = (/* @__PURE__ */ new Date()).toISOString();
          const result2 = await session.run(
            `MATCH (s:CrisisScenario {id: $id})
         SET s.crisisType = $crisisType,
             s.updatedAt = $updatedAt,
             s.simulationParameters = $simulationParameters
         RETURN s`,
            {
              id,
              crisisType: input.crisisType,
              updatedAt,
              simulationParameters: input.simulationParameters
            }
          );
          if (result2.records.length > 0) {
            return result2.records[0].get("s").properties;
          }
          return void 0;
        } finally {
          await session.close();
        }
      }
      async deleteCrisisScenario(_parent, { id }, _context) {
        console.log("Deleting crisis scenario:", id, "from Neo4j");
        const session = this.driver.session();
        try {
          const result2 = await session.run(
            "MATCH (s:CrisisScenario {id: $id}) DETACH DELETE s",
            { id }
          );
          return result2.summary.counters.nodesDeleted > 0;
        } finally {
          await session.close();
        }
      }
    };
  }
});

// src/maestro/evidence/provenance-service.ts
import crypto14 from "crypto";
var EvidenceProvenanceService, evidenceProvenanceService;
var init_provenance_service = __esm({
  "src/maestro/evidence/provenance-service.ts"() {
    "use strict";
    init_postgres();
    init_otel_tracing();
    EvidenceProvenanceService = class {
      // private s3Client: S3Client;
      bucketName;
      signingKey;
      inlineThreshold;
      constructor() {
        this.bucketName = process.env.EVIDENCE_BUCKET || "maestro-evidence-worm";
        this.signingKey = process.env.EVIDENCE_SIGNING_KEY || this.generateSigningKey();
        this.inlineThreshold = Number(
          process.env.MAX_INLINE_EVIDENCE_BYTES || "1000000"
        );
      }
      generateSigningKey() {
        return crypto14.randomBytes(32).toString("hex");
      }
      /**
       * Store evidence artifact with WORM compliance
       */
      async storeEvidence(artifact) {
        const span = otelService.createSpan("evidence.store");
        try {
          const contentBuffer = Buffer.isBuffer(artifact.content) ? artifact.content : Buffer.from(artifact.content, "utf8");
          const sha256Hash = crypto14.createHash("sha256").update(contentBuffer).digest("hex");
          const artifactId = crypto14.randomUUID();
          const retentionDays = artifact.retentionDays || this.getDefaultRetentionDays(artifact.artifactType);
          const retentionUntil = /* @__PURE__ */ new Date();
          retentionUntil.setDate(retentionUntil.getDate() + retentionDays);
          const shouldInline = artifact.artifactType === "receipt" || contentBuffer.length <= this.inlineThreshold;
          const s3Key = shouldInline ? `inline://evidence_artifact_content/${artifactId}` : `evidence/${artifact.runId}/${artifact.artifactType}/${artifactId}-${sha256Hash.slice(0, 16)}`;
          const pool4 = getPostgresPool();
          await pool4.query(
            `INSERT INTO evidence_artifacts 
         (id, run_id, artifact_type, s3_key, sha256_hash, size_bytes, retention_until, created_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, now())`,
            [
              artifactId,
              artifact.runId,
              artifact.artifactType,
              s3Key,
              sha256Hash,
              contentBuffer.length,
              retentionUntil
            ]
          );
          if (shouldInline) {
            await pool4.query(
              `INSERT INTO evidence_artifact_content (artifact_id, content, content_type)
           VALUES ($1, $2, $3)`,
              [
                artifactId,
                contentBuffer,
                artifact.metadata?.contentType || this.getContentType(artifact.artifactType)
              ]
            );
          }
          await this.createProvenanceEntry(artifactId, sha256Hash, artifact.runId);
          span?.addSpanAttributes({
            "evidence.artifact_id": artifactId,
            "evidence.run_id": artifact.runId,
            "evidence.type": artifact.artifactType,
            "evidence.size_bytes": contentBuffer.length
          });
          return artifactId;
        } catch (error) {
          console.error("Failed to store evidence:", error);
          throw new Error(`Evidence storage failed: ${error.message}`);
        } finally {
          span?.end();
        }
      }
      /**
       * Create cryptographic provenance chain entry
       */
      async createProvenanceEntry(artifactId, currentHash, runId) {
        const pool4 = getPostgresPool();
        const { rows } = await pool4.query(
          `SELECT sha256_hash FROM evidence_artifacts 
       WHERE run_id = $1 AND created_at < now() 
       ORDER BY created_at DESC LIMIT 1`,
          [runId]
        );
        const previousHash = rows.length > 0 ? rows[0].sha256_hash : null;
        const chainData = JSON.stringify({
          artifactId,
          previousHash,
          currentHash,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          runId
        });
        const signature = crypto14.createHmac("sha256", this.signingKey).update(chainData).digest("hex");
        await pool4.query(
          `INSERT INTO evidence_provenance 
       (artifact_id, previous_hash, current_hash, signature, chain_data, created_at)
       VALUES ($1, $2, $3, $4, $5, now())`,
          [artifactId, previousHash, currentHash, signature, chainData]
        );
      }
      /**
       * Verify evidence integrity and provenance chain
       */
      async verifyEvidence(artifactId) {
        const span = otelService.createSpan("evidence.verify");
        try {
          const pool4 = getPostgresPool();
          const { rows: artifacts } = await pool4.query(
            "SELECT * FROM evidence_artifacts WHERE id = $1",
            [artifactId]
          );
          if (!artifacts.length) {
            return {
              valid: false,
              integrity: false,
              provenance: false,
              details: { error: "Artifact not found" }
            };
          }
          const artifact = artifacts[0];
          const integrityValid = false;
          const s3Metadata = null;
          const { rows: provenance } = await pool4.query(
            "SELECT * FROM evidence_provenance WHERE artifact_id = $1",
            [artifactId]
          );
          let provenanceValid = false;
          if (provenance.length > 0) {
            const entry = provenance[0];
            const expectedSignature = crypto14.createHmac("sha256", this.signingKey).update(entry.chain_data).digest("hex");
            provenanceValid = expectedSignature === entry.signature;
          }
          const result2 = {
            valid: integrityValid && provenanceValid,
            integrity: integrityValid,
            provenance: provenanceValid,
            details: {
              artifact: {
                id: artifact.id,
                type: artifact.artifact_type,
                runId: artifact.run_id,
                size: artifact.size_bytes,
                hash: artifact.sha256_hash,
                created: artifact.created_at
              },
              s3Metadata,
              provenance: provenance[0] || null
            }
          };
          span?.addSpanAttributes({
            "evidence.verification.valid": result2.valid,
            "evidence.verification.integrity": result2.integrity,
            "evidence.verification.provenance": result2.provenance
          });
          return result2;
        } catch (error) {
          console.error("Evidence verification failed:", error);
          return {
            valid: false,
            integrity: false,
            provenance: false,
            details: { error: error.message }
          };
        } finally {
          span?.end();
        }
      }
      /**
       * Verify receipt chain for a run (linkage + signature integrity).
       */
      async verifyReceiptChain(runId) {
        const pool4 = getPostgresPool();
        const { rows } = await pool4.query(
          `SELECT
         ea.id as artifact_id,
         ea.sha256_hash,
         ea.created_at,
         ep.previous_hash,
         ep.current_hash,
         ep.signature,
         ep.chain_data
       FROM evidence_artifacts ea
       JOIN evidence_provenance ep ON ep.artifact_id = ea.id
       WHERE ea.run_id = $1 AND ea.artifact_type = 'receipt'
       ORDER BY ea.created_at ASC`,
          [runId]
        );
        const errors = [];
        let previousHash = null;
        const chainEntries = rows.map((row) => ({
          artifactId: row.artifact_id,
          currentHash: row.current_hash,
          previousHash: row.previous_hash || null,
          createdAt: row.created_at,
          row
        }));
        for (const entry of chainEntries) {
          const currentHashMatches = entry.currentHash === entry.row.sha256_hash;
          if (!currentHashMatches) {
            errors.push(
              `Hash mismatch for receipt ${entry.artifactId}: ${entry.currentHash}`
            );
          }
          if (entry.previousHash !== previousHash) {
            errors.push(
              `Chain break for receipt ${entry.artifactId}: expected ${previousHash || "null"} got ${entry.previousHash || "null"}`
            );
          }
          const chainData = typeof entry.row.chain_data === "string" ? entry.row.chain_data : JSON.stringify(entry.row.chain_data);
          const expectedSignature = crypto14.createHmac("sha256", this.signingKey).update(chainData).digest("hex");
          if (expectedSignature !== entry.row.signature) {
            errors.push(
              `Signature mismatch for receipt ${entry.artifactId}`
            );
          }
          previousHash = entry.currentHash;
        }
        return {
          valid: errors.length === 0,
          errors,
          chain: chainEntries.map((entry) => ({
            artifactId: entry.artifactId,
            currentHash: entry.currentHash,
            previousHash: entry.previousHash,
            createdAt: entry.createdAt
          }))
        };
      }
      /**
       * Generate SBOM (Software Bill of Materials) evidence
       */
      async generateSBOMEvidence(runId, dependencies) {
        const sbom = {
          bomFormat: "CycloneDX",
          specVersion: "1.4",
          serialNumber: `urn:uuid:${crypto14.randomUUID()}`,
          version: 1,
          metadata: {
            timestamp: (/* @__PURE__ */ new Date()).toISOString(),
            tools: [
              {
                vendor: "Maestro",
                name: "Evidence Generator",
                version: "1.0.0"
              }
            ],
            component: {
              type: "application",
              name: `maestro-run-${runId}`,
              version: "1.0.0"
            }
          },
          components: dependencies.map((dep) => ({
            type: "library",
            name: dep.name,
            version: dep.version,
            licenses: dep.licenses || [],
            hashes: dep.hashes || [],
            externalReferences: dep.externalReferences || []
          }))
        };
        return await this.storeEvidence({
          runId,
          artifactType: "sbom",
          content: JSON.stringify(sbom, null, 2),
          metadata: {
            format: "cycloneDX",
            version: "1.4",
            componentCount: dependencies.length
          }
        });
      }
      /**
       * Get retention policy for artifact type
       */
      getDefaultRetentionDays(artifactType) {
        const retentionPolicies = {
          sbom: 2555,
          // ~7 years for compliance
          attestation: 2555,
          // ~7 years for compliance
          log: 365,
          // 1 year
          output: 90,
          // 3 months
          trace: 30,
          // 1 month
          receipt: 2555,
          // receipts should be retained long-term
          policy_decision: 2555
          // ~7 years for compliance
        };
        return retentionPolicies[artifactType] || 365;
      }
      /**
       * Get appropriate content type for artifact
       */
      getContentType(artifactType) {
        const contentTypes = {
          sbom: "application/json",
          attestation: "application/json",
          log: "text/plain",
          output: "application/json",
          trace: "application/json",
          receipt: "application/json",
          policy_decision: "application/json"
        };
        return contentTypes[artifactType] || "application/octet-stream";
      }
      /**
       * List evidence artifacts for a run
       */
      async listEvidenceForRun(runId) {
        const pool4 = getPostgresPool();
        const { rows } = await pool4.query(
          `SELECT 
        id, artifact_type, sha256_hash, size_bytes, 
        immutable, retention_until, created_at
       FROM evidence_artifacts 
       WHERE run_id = $1 
       ORDER BY created_at DESC`,
          [runId]
        );
        return rows;
      }
    };
    evidenceProvenanceService = new EvidenceProvenanceService();
  }
});

// src/db/repositories/evidenceRepo.ts
async function saveEvidenceBundle(ev) {
  await pg.write(
    `INSERT INTO evidence_bundles (id, tenant_id, service, release_id, artifacts, slo, cost)
     VALUES ($1,$2,$3,$4,$5,$6,$7)
     ON CONFLICT (id) DO UPDATE SET artifacts = EXCLUDED.artifacts, slo = EXCLUDED.slo, cost = EXCLUDED.cost`,
    [
      ev.id,
      ev.tenant_id || null,
      ev.service,
      ev.release_id,
      JSON.stringify(ev.artifacts || []),
      JSON.stringify(ev.slo || {}),
      JSON.stringify(ev.cost || null)
    ]
  );
}
async function getLatestEvidence(service11, releaseId) {
  return await pg.oneOrNone(
    `SELECT * FROM evidence_bundles WHERE service = $1 AND release_id = $2 ORDER BY created_at DESC LIMIT 1`,
    [service11, releaseId]
  );
}
async function listEvidence(service11, releaseId, opts = {}) {
  const clauses = ["service = $1", "release_id = $2"];
  const params = [service11, releaseId];
  let idx = params.length + 1;
  if (opts.since) {
    clauses.push(`created_at >= $${idx++}`);
    params.push(new Date(opts.since).toISOString());
  }
  if (opts.until) {
    clauses.push(`created_at <= $${idx++}`);
    params.push(new Date(opts.until).toISOString());
  }
  const limit = Math.min(Math.max(opts.limit ?? 50, 1), 200);
  const offset = Math.max(opts.offset ?? 0, 0);
  const sql = `SELECT * FROM evidence_bundles WHERE ${clauses.join(" AND ")} ORDER BY created_at DESC LIMIT ${limit} OFFSET ${offset}`;
  return await pg.readMany(sql, params);
}
var init_evidenceRepo = __esm({
  "src/db/repositories/evidenceRepo.ts"() {
    "use strict";
    init_pg();
  }
});

// src/graphql/resolvers/evidence.ts
var evidenceResolvers, evidence_default;
var init_evidence = __esm({
  "src/graphql/resolvers/evidence.ts"() {
    "use strict";
    init_provenance_service();
    init_evidenceRepo();
    evidenceResolvers = {
      Mutation: {
        async publishEvidence(_2, { input }, ctx) {
          const now = (/* @__PURE__ */ new Date()).toISOString();
          const id = `ev_${Date.now()}`;
          try {
            if (evidenceProvenanceService?.storeEvidence) {
              await evidenceProvenanceService.storeEvidence({
                type: "bundle",
                hash: input.artifacts?.[0]?.sha256 || id,
                metadata: {
                  releaseId: input.releaseId,
                  service: input.service,
                  slo: input.slo,
                  cost: input.cost
                }
              });
            }
            await saveEvidenceBundle({
              id,
              service: input.service,
              release_id: input.releaseId,
              artifacts: input.artifacts || [],
              slo: input.slo,
              cost: input.cost || null
            });
          } catch {
          }
          return {
            id,
            releaseId: input.releaseId,
            service: input.service,
            artifacts: input.artifacts || [],
            slo: input.slo,
            cost: input.cost || null,
            createdAt: now
          };
        }
      }
    };
    evidence_default = evidenceResolvers;
  }
});

// src/metrics/registry.ts
import {
  collectDefaultMetrics as collectDefaultMetrics2,
  register as defaultRegistry
} from "prom-client";
var g, registry;
var init_registry = __esm({
  "src/metrics/registry.ts"() {
    "use strict";
    g = globalThis;
    if (!g.__intelgraph_metrics_inited) {
      collectDefaultMetrics2({ register: defaultRegistry, prefix: "intelgraph_" });
      g.__intelgraph_metrics_inited = true;
    }
    registry = defaultRegistry;
  }
});

// src/crystal/slo-metrics.ts
import { Histogram as Histogram4, Gauge as Gauge4, Counter as Counter5 } from "prom-client";
var PercentileTracker, gatewayHistogram, graphHistogram, costGauge, budgetAlerts, DEFAULT_BUDGETS, SLOMetrics, sloMetrics;
var init_slo_metrics = __esm({
  "src/crystal/slo-metrics.ts"() {
    "use strict";
    init_registry();
    PercentileTracker = class {
      samples = [];
      maxSamples;
      constructor(maxSamples = 500) {
        this.maxSamples = maxSamples;
      }
      record(value) {
        if (!Number.isFinite(value)) return;
        this.samples.push(value);
        if (this.samples.length > this.maxSamples) {
          this.samples.shift();
        }
      }
      percentile(p) {
        if (this.samples.length === 0) return 0;
        const sorted = [...this.samples].sort((a, b) => a - b);
        const rank = p / 100 * (sorted.length - 1);
        const lower = Math.floor(rank);
        const upper = Math.ceil(rank);
        if (lower === upper) {
          return sorted[lower];
        }
        const weight = rank - lower;
        return sorted[lower] * (1 - weight) + sorted[upper] * weight;
      }
    };
    gatewayHistogram = new Histogram4({
      name: "crystal_gateway_latency_ms",
      help: "Crystal gateway latency observations",
      labelNames: ["operation", "method"],
      buckets: [50, 100, 200, 350, 500, 700, 900, 1200, 1500],
      registers: [registry]
    });
    graphHistogram = new Histogram4({
      name: "crystal_graph_latency_ms",
      help: "Crystal graph operation latency",
      labelNames: ["hops"],
      buckets: [50, 100, 200, 400, 800, 1200],
      registers: [registry]
    });
    costGauge = new Gauge4({
      name: "crystal_budget_spend_usd",
      help: "Budget spend per environment",
      labelNames: ["environment"],
      registers: [registry]
    });
    budgetAlerts = new Counter5({
      name: "crystal_budget_alerts_total",
      help: "Number of times the cost guardrail threshold was hit",
      labelNames: ["environment"],
      registers: [registry]
    });
    DEFAULT_BUDGETS = {
      dev: 1e3,
      staging: 3e3,
      prod: 18e3,
      llm: 5e3
    };
    SLOMetrics = class {
      gatewayRead = new PercentileTracker();
      gatewayWrite = new PercentileTracker();
      subscription = new PercentileTracker();
      graph = new PercentileTracker();
      budgets;
      constructor() {
        this.budgets = {
          dev: { limit: DEFAULT_BUDGETS.dev, spend: 0 },
          staging: { limit: DEFAULT_BUDGETS.staging, spend: 0 },
          prod: { limit: DEFAULT_BUDGETS.prod, spend: 0 },
          llm: { limit: DEFAULT_BUDGETS.llm, spend: 0 }
        };
      }
      observeGateway(operation, durationMs) {
        gatewayHistogram.observe({ operation, method: operation }, durationMs);
        if (operation === "read") this.gatewayRead.record(durationMs);
        else if (operation === "write") this.gatewayWrite.record(durationMs);
        else this.subscription.record(durationMs);
      }
      observeGraph(hops, durationMs) {
        const hopLabel = hops >= 3 ? "3" : String(hops);
        graphHistogram.observe({ hops: hopLabel }, durationMs);
        this.graph.record(durationMs);
      }
      recordCost(environment, amount) {
        const budget = this.budgets[environment];
        budget.spend = Math.max(0, budget.spend + amount);
        costGauge.set({ environment }, budget.spend);
        if (budget.spend >= budget.limit * 0.8) {
          budgetAlerts.inc({ environment });
        }
      }
      getSLOSnapshot() {
        return {
          gatewayReadP95: this.gatewayRead.percentile(95),
          gatewayReadP99: this.gatewayRead.percentile(99),
          gatewayWriteP95: this.gatewayWrite.percentile(95),
          gatewayWriteP99: this.gatewayWrite.percentile(99),
          subscriptionP95: this.subscription.percentile(95),
          graphHopP95: this.graph.percentile(95),
          graphHopP99: this.graph.percentile(99)
        };
      }
      getCostSnapshot() {
        const budgets = Object.keys(this.budgets).map((env2) => {
          const { limit, spend } = this.budgets[env2];
          return {
            environment: env2,
            monthlyLimitUsd: limit,
            monthlySpendUsd: spend,
            alertThresholdHit: spend >= limit * 0.8
          };
        });
        return { budgets };
      }
    };
    sloMetrics = new SLOMetrics();
  }
});

// src/graphql/resolvers/evidenceOk.ts
var evidenceOkResolvers, evidenceOk_default;
var init_evidenceOk = __esm({
  "src/graphql/resolvers/evidenceOk.ts"() {
    "use strict";
    init_slo_metrics();
    init_evidenceRepo();
    evidenceOkResolvers = {
      Query: {
        async evidenceOk(_root, { service: service11, releaseId }, _ctx) {
          const ev = await getLatestEvidence(service11, releaseId).catch(() => null);
          let snapshot;
          let cost;
          if (ev) {
            snapshot = {
              service: service11,
              p95Ms: ev.slo?.p95Ms ?? 0,
              p99Ms: ev.slo?.p99Ms ?? null,
              errorRate: ev.slo?.errorRate ?? 0,
              window: ev.slo?.window ?? "unknown"
            };
            cost = ev.cost || null;
          } else {
            const slo = sloMetrics.getSLOSnapshot();
            snapshot = {
              service: service11,
              p95Ms: Math.round(slo.gatewayReadP95),
              p99Ms: Math.round(slo.gatewayReadP99),
              errorRate: 0.01,
              window: "15m"
            };
            cost = { graphqlPerMillionUsd: 1.8, ingestPerThousandUsd: 0.08 };
          }
          const reasons = [];
          const READ_P95_BUDGET = 350;
          const ERROR_RATE_BUDGET = 0.02;
          const GRAPHQL_COST_BUDGET = 2;
          if (snapshot.p95Ms > READ_P95_BUDGET)
            reasons.push(`p95 ${snapshot.p95Ms}ms > ${READ_P95_BUDGET}ms`);
          if (snapshot.errorRate > ERROR_RATE_BUDGET)
            reasons.push(`errorRate ${snapshot.errorRate} > ${ERROR_RATE_BUDGET}`);
          if ((cost?.graphqlPerMillionUsd ?? 0) > GRAPHQL_COST_BUDGET)
            reasons.push(
              `graphql cost ${cost.graphqlPerMillionUsd} > ${GRAPHQL_COST_BUDGET}`
            );
          return { ok: reasons.length === 0, reasons, snapshot, cost };
        }
      }
    };
    evidenceOk_default = evidenceOkResolvers;
  }
});

// health/ci-signal.ts
var getCiSignal;
var init_ci_signal = __esm({
  "health/ci-signal.ts"() {
    "use strict";
    getCiSignal = async () => {
      return {
        status: "green",
        score: 100
      };
    };
  }
});

// health/p95-latency.ts
var getP95Latency;
var init_p95_latency = __esm({
  "health/p95-latency.ts"() {
    "use strict";
    getP95Latency = async () => {
      return {
        value: 120,
        // ms
        score: 95
      };
    };
  }
});

// health/graph-consistency.ts
var getGraphConsistency;
var init_graph_consistency = __esm({
  "health/graph-consistency.ts"() {
    "use strict";
    getGraphConsistency = async () => {
      return {
        consistency: 0.99,
        score: 99
      };
    };
  }
});

// health/error-taxonomies.ts
var getErrorTaxonomies;
var init_error_taxonomies = __esm({
  "health/error-taxonomies.ts"() {
    "use strict";
    getErrorTaxonomies = async () => {
      return {
        critical: 5,
        error: 20,
        warning: 50,
        score: 85
      };
    };
  }
});

// health/secret-drift.ts
var getSecretDrift;
var init_secret_drift = __esm({
  "health/secret-drift.ts"() {
    "use strict";
    getSecretDrift = async () => {
      return {
        drift: 0.01,
        score: 99
      };
    };
  }
});

// health/predictive-anomaly-detection.ts
var getPredictiveAnomalyDetection;
var init_predictive_anomaly_detection = __esm({
  "health/predictive-anomaly-detection.ts"() {
    "use strict";
    getPredictiveAnomalyDetection = async () => {
      return {
        anomalies: 2,
        score: 98
      };
    };
  }
});

// health/aggregator.js
var getHealthScore;
var init_aggregator = __esm({
  "health/aggregator.js"() {
    "use strict";
    init_ci_signal();
    init_p95_latency();
    init_graph_consistency();
    init_error_taxonomies();
    init_secret_drift();
    init_predictive_anomaly_detection();
    getHealthScore = async () => {
      const ciSignal = await getCiSignal();
      const p95Latency = await getP95Latency();
      const graphConsistency = await getGraphConsistency();
      const errorTaxonomies = await getErrorTaxonomies();
      const secretDrift = await getSecretDrift();
      const predictiveAnomalyDetection = await getPredictiveAnomalyDetection();
      const scores = [
        ciSignal.score,
        p95Latency.score,
        graphConsistency.score,
        errorTaxonomies.score,
        secretDrift.score,
        predictiveAnomalyDetection.score
      ];
      const totalScore = scores.reduce((acc, score) => acc + score, 0);
      const healthScore = totalScore / scores.length;
      return {
        healthScore,
        ciSignal,
        p95Latency,
        graphConsistency,
        errorTaxonomies,
        secretDrift,
        predictiveAnomalyDetection
      };
    };
  }
});

// src/graphql/resolvers/health.ts
var healthResolvers, health_default;
var init_health = __esm({
  "src/graphql/resolvers/health.ts"() {
    "use strict";
    init_aggregator();
    healthResolvers = {
      Query: {
        healthScore: () => getHealthScore()
      }
    };
    health_default = healthResolvers;
  }
});

// src/redaction/redact.ts
import { trace as trace2 } from "@opentelemetry/api";
import { Counter as Counter6 } from "prom-client";
var tracer4, redactionApplied, FIELD_METADATA, RedactionService, redactionService;
var init_redact = __esm({
  "src/redaction/redact.ts"() {
    "use strict";
    tracer4 = trace2.getTracer("redaction", "24.2.0");
    redactionApplied = new Counter6({
      name: "redaction_applied_total",
      help: "Total field redactions applied",
      labelNames: ["tenant_id", "field", "rule"]
    });
    FIELD_METADATA = {
      // Sensitive fields that may need redaction
      email: { type: "string", pii: true },
      phone: { type: "string", pii: true },
      ssn: { type: "string", pii: true },
      creditCard: { type: "string", financial: true },
      bankAccount: { type: "string", financial: true },
      ip: { type: "string", sensitive: true },
      location: { type: "object", sensitive: true },
      userId: { type: "string", sensitive: true },
      sessionId: { type: "string", sensitive: true },
      // Standard fields that are usually safe
      id: { type: "string" },
      type: { type: "string" },
      value: { type: "number" },
      timestamp: { type: "string" },
      source: { type: "string" },
      weight: { type: "number" },
      score: { type: "number" },
      status: { type: "string" },
      tenantId: { type: "string" }
    };
    RedactionService = class {
      defaultMask = "[REDACTED]";
      async redactObject(obj, policy2, tenantId, context4) {
        return tracer4.startActiveSpan(
          "redaction.redact_object",
          async (span) => {
            span.setAttributes({
              tenant_id: tenantId,
              rules: Array.isArray(policy2.rules) ? policy2.rules.join(",") : Object.keys(policy2.rules).join(","),
              has_k_anon: !!policy2.kAnonThreshold
            });
            try {
              const redacted = await this.processObject(
                obj,
                policy2,
                tenantId,
                context4
              );
              return redacted;
            } catch (error) {
              span.recordException(error);
              span.setStatus({ code: 2, message: error.message });
              throw error;
            } finally {
              span.end();
            }
          }
        );
      }
      async processObject(obj, policy2, tenantId, context4) {
        if (obj === null || obj === void 0) {
          return obj;
        }
        if (Array.isArray(obj)) {
          return Promise.all(
            obj.map((item) => this.processObject(item, policy2, tenantId, context4))
          );
        }
        if (typeof obj !== "object") {
          return obj;
        }
        const result2 = {};
        for (const [field, value] of Object.entries(obj)) {
          const shouldRedact = this.shouldRedactField(field, policy2);
          if (shouldRedact) {
            const redactionRule = this.getApplicableRule(field, policy2);
            result2[field] = this.applyRedaction(
              field,
              value,
              redactionRule,
              policy2
            );
            redactionApplied.inc({
              tenant_id: tenantId,
              field,
              rule: redactionRule
            });
          } else if (typeof value === "object" && value !== null) {
            result2[field] = await this.processObject(
              value,
              policy2,
              tenantId,
              context4
            );
          } else {
            result2[field] = value;
          }
        }
        if (policy2.kAnonThreshold && policy2.kAnonThreshold > 0) {
          return this.applyKAnonymity(result2, policy2.kAnonThreshold, tenantId);
        }
        return result2;
      }
      shouldRedactField(field, policy2) {
        if (policy2.allowedFields && policy2.allowedFields.length > 0) {
          return !policy2.allowedFields.includes(field);
        }
        if (!Array.isArray(policy2.rules)) {
          return field in policy2.rules;
        }
        const metadata = FIELD_METADATA[field];
        if (!metadata) {
          return false;
        }
        return policy2.rules.some((rule) => {
          switch (rule) {
            case "pii":
              return metadata.pii === true;
            case "financial":
              return metadata.financial === true;
            case "sensitive":
              return metadata.sensitive === true;
            default:
              return false;
          }
        });
      }
      getApplicableRule(field, policy2) {
        if (!Array.isArray(policy2.rules)) {
          return policy2.rules[field] || "default";
        }
        const metadata = FIELD_METADATA[field];
        if (!metadata) return "unknown";
        for (const rule of policy2.rules) {
          switch (rule) {
            case "pii":
              if (metadata.pii) return "pii";
              break;
            case "financial":
              if (metadata.financial) return "financial";
              break;
            case "sensitive":
              if (metadata.sensitive) return "sensitive";
              break;
          }
        }
        return "default";
      }
      applyRedaction(field, value, rule, policy2) {
        const mask = policy2.redactionMask || this.defaultMask;
        const metadata = FIELD_METADATA[field];
        if (metadata) {
          switch (metadata.type) {
            case "string":
              return this.redactString(value, rule, mask);
            case "number":
              return this.redactNumber(value, rule);
            case "object":
              return mask;
            case "array":
              return [];
            default:
              return mask;
          }
        }
        return mask;
      }
      redactString(value, rule, mask) {
        if (typeof value !== "string") return mask;
        switch (rule) {
          case "pii":
            if (value.length <= 4) return mask;
            return value.charAt(0) + mask + value.charAt(value.length - 1);
          case "financial":
            if (value.length <= 4) return mask;
            return mask + value.slice(-4);
          case "sensitive":
            return mask;
          default:
            return mask;
        }
      }
      redactNumber(value, rule) {
        switch (rule) {
          case "financial":
            return 0;
          // Zero out financial numbers
          case "sensitive":
            return -1;
          // Sentinel value for sensitive numbers
          default:
            return Math.round(value);
        }
      }
      applyKAnonymity(obj, threshold, tenantId) {
        if (!obj.id && !obj.userId) {
          return obj;
        }
        const kAnonFields = ["location", "age", "category"];
        const result2 = { ...obj };
        kAnonFields.forEach((field) => {
          if (result2[field]) {
            result2[field] = this.generalizeField(result2[field], threshold);
            redactionApplied.inc({
              tenant_id: tenantId,
              field,
              rule: "k_anon"
            });
          }
        });
        return result2;
      }
      generalizeField(value, threshold) {
        if (typeof value === "number") {
          const factor = threshold >= 10 ? 100 : 10;
          return Math.round(value / factor) * factor;
        }
        if (typeof value === "string") {
          return value.split(" ")[0] + " [REGION]";
        }
        return value;
      }
      async redactGraphQLResponse(response, policy2, tenantId) {
        if (!response || !response.data) {
          return response;
        }
        return {
          ...response,
          data: await this.redactObject(response.data, policy2, tenantId)
        };
      }
      createRedactionPolicy(rules, options2) {
        return {
          rules,
          kAnonThreshold: options2?.kAnonThreshold,
          allowedFields: options2?.allowedFields,
          redactionMask: options2?.redactionMask
        };
      }
      getRedactionStats() {
        return {
          supportedRules: ["pii", "financial", "sensitive", "k_anon"],
          defaultMask: this.defaultMask,
          fieldsWithMetadata: Object.keys(FIELD_METADATA).length,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        };
      }
    };
    redactionService = new RedactionService();
  }
});

// src/config/featureFlags.ts
var FeatureFlags;
var init_featureFlags = __esm({
  "src/config/featureFlags.ts"() {
    "use strict";
    FeatureFlags = class _FeatureFlags {
      static instance;
      features;
      constructor(features = {}) {
        this.features = features;
      }
      static getInstance() {
        if (!_FeatureFlags.instance) {
          _FeatureFlags.instance = new _FeatureFlags(_FeatureFlags.loadFromEnv());
        }
        return _FeatureFlags.instance;
      }
      // Helper to parse boolean from env string safely
      static parseBool(value) {
        if (value === void 0 || value === "") return void 0;
        const lower = value.toLowerCase();
        if (lower === "true" || lower === "1") return true;
        return false;
      }
      static loadFromEnv() {
        const features = {};
        const legacyMappings = {
          "FEATURE_RBAC_FINE_GRAINED": "rbac.fineGrained",
          "FEATURE_AUDIT_TRAIL": "audit.trail",
          "FEATURE_COPILOT_SERVICE": "copilot.service",
          "FEATURE_ANALYTICS_PANEL": "analytics.panel",
          "FEATURE_PDF_EXPORT": "pdf.export",
          "FEATURE_NARRATIVE_SIMULATION": "narrative.simulation",
          "FEATURE_AGENT_ANGLETON": "agent.angleton",
          "FEATURE_AGENT_HAREL": "agent.harel",
          "FEATURE_AGENT_SINGLAUB": "agent.singlaub",
          "FEATURE_AGENT_LEMAY": "agent.lemay",
          "FEATURE_AGENT_BUDANOV": "agent.budanov",
          "FEATURE_AGENT_WOLF": "agent.wolf",
          "FEATURE_AGENT_GEHLEN": "agent.gehlen",
          "FEATURE_AI_ENABLED": "ai.enabled",
          "FEATURE_KAFKA_ENABLED": "kafka.enabled",
          "FEATURE_OPENTELEMETRY_ENABLED": "opentelemetry.enabled"
        };
        for (const [envKey, featureKey] of Object.entries(legacyMappings)) {
          const parsed = _FeatureFlags.parseBool(process.env[envKey]);
          if (parsed !== void 0) {
            features[featureKey] = parsed;
          }
        }
        for (const key of Object.keys(process.env)) {
          if (key.startsWith("FEATURE_") && !legacyMappings[key]) {
          }
        }
        const defaults = {
          // MVP1 Features
          "mvp1.authentication": true,
          "mvp1.authorizationRbac": true,
          "mvp1.tenancyIsolation": true,
          "mvp1.auditLogging": true,
          "mvp1.dataIngestion": true,
          "mvp1.graphExploration": true,
          "mvp1.searchElastic": true,
          "mvp1.comments": true,
          "mvp1.notifications": true,
          "mvp1.workspaces": true,
          "mvp1.csvExports": true,
          // Application Features
          "rbac.fineGrained": true,
          "audit.trail": true,
          "copilot.service": true,
          "analytics.panel": true,
          "pdf.export": true,
          "opentelemetry.enabled": true,
          "narrative.simulation": true,
          "graphrag.neptuneManaged": false,
          // Agent Features
          "agent.memory": true,
          "agent.toolUse": true,
          "agent.reflection": true,
          "agent.planning": true,
          "agent.singlaub": true,
          "agent.lemay": true,
          "agent.angleton": true,
          "agent.budanov": true,
          // Agents not enabled by default in original file but listed as keys
          "agent.multiSwarm": false,
          "agent.autonomousDeployment": false
        };
        return { ...defaults, ...features };
      }
      isEnabled(key, context4) {
        return this.features[key] ?? false;
      }
      // Returns all current feature flags
      getAll() {
        return this.features;
      }
      update(features) {
        const normalized = {};
        const legacyToNew = {
          "RBAC_FINE_GRAINED": "rbac.fineGrained",
          "AUDIT_TRAIL": "audit.trail",
          "COPILOT_SERVICE": "copilot.service",
          "ANALYTICS_PANEL": "analytics.panel",
          "PDF_EXPORT": "pdf.export",
          "NARRATIVE_SIMULATION": "narrative.simulation",
          "AGENT_ANGLETON": "agent.angleton",
          "AGENT_HAREL": "agent.harel",
          "AGENT_SINGLAUB": "agent.singlaub",
          "AGENT_LEMAY": "agent.lemay",
          "AGENT_BUDANOV": "agent.budanov",
          "AGENT_WOLF": "agent.wolf",
          "AGENT_GEHLEN": "agent.gehlen",
          "AI_ENABLED": "ai.enabled",
          "KAFKA_ENABLED": "kafka.enabled",
          "OPENTELEMETRY_ENABLED": "opentelemetry.enabled"
        };
        for (const [key, value] of Object.entries(features)) {
          if (legacyToNew[key]) {
            normalized[legacyToNew[key]] = value;
          } else {
            normalized[key] = value;
          }
        }
        this.features = { ...this.features, ...normalized };
      }
      static isEnabled(key, context4) {
        return _FeatureFlags.getInstance().isEnabled(key, context4);
      }
      getVariant(key, context4) {
        return this.features[key];
      }
    };
  }
});

// src/observability/trust-risk-metrics.ts
import { Counter as Counter7, Gauge as Gauge5 } from "prom-client";
function recordTrustScore(subjectId, score) {
  if (Number.isFinite(score))
    trustScoreGauge.set({ subject: subjectId }, score);
}
function recordRiskSignal(opts) {
  riskSignalsTotal.inc({
    tenant: opts.tenantId,
    kind: opts.kind,
    severity: opts.severity,
    source: opts.source
  });
}
var trustScoreGauge, riskSignalsTotal;
var init_trust_risk_metrics = __esm({
  "src/observability/trust-risk-metrics.ts"() {
    "use strict";
    init_registry();
    trustScoreGauge = new Gauge5({
      name: "intelgraph_trust_score",
      help: "Current trust score per subject",
      labelNames: ["subject"],
      registers: [registry]
    });
    riskSignalsTotal = new Counter7({
      name: "intelgraph_risk_signals_total",
      help: "Total risk signals raised",
      labelNames: ["tenant", "kind", "severity", "source"],
      registers: [registry]
    });
  }
});

// src/db/repositories/trustRiskRepo.ts
async function upsertTrustScore(tenantId, subjectId, score, reasons, evidenceId) {
  const id = `ts_${tenantId}_${subjectId}`;
  await pg.write(
    `INSERT INTO trust_scores (id, tenant_id, subject_id, score, reasons, evidence_id)
     VALUES ($1,$2,$3,$4,$5,$6)
     ON CONFLICT (id) DO UPDATE SET score = EXCLUDED.score, reasons = EXCLUDED.reasons, evidence_id = EXCLUDED.evidence_id, updated_at = now()`,
    [
      id,
      tenantId,
      subjectId,
      score,
      JSON.stringify(reasons || []),
      evidenceId || null
    ],
    { tenantId }
  );
  return id;
}
async function getTrustScore(tenantId, subjectId) {
  return await pg.oneOrNone(
    `SELECT * FROM trust_scores WHERE tenant_id = $1 AND subject_id = $2 ORDER BY updated_at DESC LIMIT 1`,
    [tenantId, subjectId],
    { tenantId }
  );
}
async function insertRiskSignal(rec) {
  await pg.write(
    `INSERT INTO risk_signals (id, tenant_id, kind, severity, message, source, context, evidence_id)
     VALUES ($1,$2,$3,$4,$5,$6,$7,$8)`,
    [
      rec.id,
      rec.tenant_id,
      rec.kind,
      rec.severity,
      rec.message,
      rec.source,
      JSON.stringify(rec.context || null),
      rec.evidence_id || null
    ],
    { tenantId: rec.tenant_id }
  );
}
async function listRecentSignals(tenantId, subjectId, limit = 50) {
  return await pg.readMany(
    `SELECT * FROM risk_signals WHERE tenant_id = $1 ORDER BY created_at DESC LIMIT $2`,
    [tenantId, limit],
    { tenantId }
  );
}
async function listRiskSignalsPaged(tenantId, opts = {}) {
  const clauses = ["tenant_id = $1"];
  const params = [tenantId];
  let idx = 2;
  if (opts.kind) {
    clauses.push(`kind = $${idx++}`);
    params.push(opts.kind);
  }
  if (opts.severity) {
    clauses.push(`severity = $${idx++}`);
    params.push(opts.severity);
  }
  const where = clauses.join(" AND ");
  const limit = Math.min(Math.max(opts.limit ?? 50, 1), 200);
  const offset = Math.max(opts.offset ?? 0, 0);
  const items = await pg.readMany(
    `SELECT * FROM risk_signals WHERE ${where} ORDER BY created_at DESC LIMIT ${limit} OFFSET ${offset}`,
    params,
    { tenantId }
  );
  const cntRow = await pg.oneOrNone(
    `SELECT COUNT(*)::int as c FROM risk_signals WHERE ${where}`,
    params,
    { tenantId }
  );
  const total = cntRow?.c ?? 0;
  return {
    items,
    total,
    nextOffset: offset + items.length < total ? offset + items.length : null
  };
}
async function listTrustScores(tenantId, limit = 50, offset = 0) {
  const items = await pg.readMany(
    `SELECT * FROM trust_scores WHERE tenant_id = $1 ORDER BY updated_at DESC LIMIT $2 OFFSET $3`,
    [tenantId, limit, offset],
    { tenantId }
  );
  const cnt = await pg.oneOrNone(
    `SELECT COUNT(*)::int as c FROM trust_scores WHERE tenant_id = $1`,
    [tenantId],
    { tenantId }
  );
  const total = cnt?.c ?? 0;
  return {
    items,
    total,
    nextOffset: offset + items.length < total ? offset + items.length : null
  };
}
var init_trustRiskRepo = __esm({
  "src/db/repositories/trustRiskRepo.ts"() {
    "use strict";
    init_pg();
  }
});

// src/graphql/resolvers/trust-risk.ts
function nowIso() {
  return (/* @__PURE__ */ new Date()).toISOString();
}
var trustRiskResolvers, trust_risk_default;
var init_trust_risk = __esm({
  "src/graphql/resolvers/trust-risk.ts"() {
    "use strict";
    init_redact();
    init_featureFlags();
    init_trust_risk_metrics();
    init_trustRiskRepo();
    trustRiskResolvers = {
      Query: {
        async trustScore(_2, { subjectId }, ctx) {
          if (!FeatureFlags.isEnabled("agent.angleton")) {
            return {
              subjectId,
              score: 0.5,
              reasons: ["agent_disabled"],
              updatedAt: nowIso()
            };
          }
          const tenantId = ctx?.tenantId || "t0";
          const existing = await getTrustScore(tenantId, subjectId);
          const score = existing?.score ?? 0.7;
          const reasons = existing?.reasons ?? ["baseline"];
          const payload = {
            subjectId,
            score,
            reasons,
            updatedAt: existing?.updated_at ?? nowIso()
          };
          recordTrustScore(subjectId, score);
          const policy2 = { rules: { email: "pii", phone: "pii" } };
          return await redactionService.redactObject(
            payload,
            policy2,
            ctx?.tenantId ?? "t0"
          );
        },
        async riskSignals(_2, { tenantId, limit, kind, severity }) {
          const rows = await listRecentSignals(tenantId, void 0, Math.min(limit ?? 50, 100));
          return rows.filter(
            (r) => (!kind || r.kind === kind) && (!severity || r.severity === severity)
          ).map((r) => ({
            id: r.id,
            tenantId: r.tenant_id,
            kind: r.kind,
            severity: r.severity,
            message: r.message,
            source: r.source,
            createdAt: r.created_at,
            context: r.context
          }));
        },
        async riskSignalsPage(_2, { tenantId, limit, offset, kind, severity }) {
          const page = await listRiskSignalsPaged(tenantId, {
            kind,
            severity,
            limit,
            offset
          });
          return {
            items: page.items.map((r) => ({
              id: r.id,
              tenantId: r.tenant_id,
              kind: r.kind,
              severity: r.severity,
              message: r.message,
              source: r.source,
              createdAt: r.created_at,
              context: r.context
            })),
            total: page.total,
            nextOffset: page.nextOffset
          };
        },
        async trustScoresPage(_2, { tenantId, limit, offset }) {
          const page = await listTrustScores(tenantId, limit, offset);
          return {
            items: page.items.map((ts) => ({
              subjectId: ts.subject_id,
              score: Number(ts.score),
              reasons: ts.reasons || [],
              updatedAt: ts.updated_at
            })),
            total: page.total,
            nextOffset: page.nextOffset
          };
        },
        async incidentBundle(_2, { id }) {
          return {
            id,
            type: "DATA_INTEGRITY",
            status: "OPEN",
            createdAt: nowIso(),
            signals: [
              {
                id: "rs_1",
                tenantId: "t0",
                kind: "anomaly",
                severity: "HIGH",
                message: "unexpected data path",
                source: "angleton",
                createdAt: nowIso(),
                context: { path: "/ingest/v2" }
              }
            ],
            actions: ["quarantine-input", "request-corroboration", "notify-owner"],
            notes: "Auto-generated bundle for review"
          };
        }
      },
      Mutation: {
        async raiseRiskSignal(_2, { input }) {
          if (!FeatureFlags.isEnabled("agent.harel") && !FeatureFlags.isEnabled("agent.angleton")) {
            throw new Error("risk signaling disabled by feature flags");
          }
          const rec = {
            id: `rs_${Date.now()}`,
            tenantId: input.tenantId,
            kind: input.kind,
            severity: input.severity,
            message: input.message,
            source: input.source,
            createdAt: nowIso(),
            context: input.context || null
          };
          await insertRiskSignal({
            id: rec.id,
            tenant_id: rec.tenantId,
            kind: rec.kind,
            severity: rec.severity,
            message: rec.message,
            source: rec.source,
            context: rec.context
          });
          recordRiskSignal({
            tenantId: rec.tenantId,
            kind: rec.kind,
            severity: rec.severity,
            source: rec.source
          });
          return rec;
        },
        async createIncidentBundle(_2, { input }) {
          const bundle = {
            id: `ib_${Date.now()}`,
            type: input.type,
            status: "OPEN",
            createdAt: nowIso(),
            signals: (input.signalIds || []).map((id) => ({
              id,
              tenantId: "t0",
              kind: "linked",
              severity: "LOW",
              message: "linked signal",
              source: "system",
              createdAt: nowIso(),
              context: {}
            })),
            actions: ["assign-reviewer"],
            notes: input.notes || null
          };
          for (const s of bundle.signals) {
            recordRiskSignal({
              tenantId: s.tenantId,
              kind: s.kind,
              severity: s.severity,
              source: s.source
            });
          }
          return bundle;
        }
      }
    };
    trust_risk_default = trustRiskResolvers;
  }
});

// src/provenance/lineage.ts
var DataLineageGraph, lineageGraph;
var init_lineage = __esm({
  "src/provenance/lineage.ts"() {
    "use strict";
    init_ledger();
    DataLineageGraph = class {
      async buildGraph(tenantId) {
        const entries = await provenanceLedger.getEntries(tenantId, { limit: 1e4 });
        const nodes = /* @__PURE__ */ new Map();
        const edges = [];
        for (const entry of entries) {
          const nodeId = `${entry.resourceType}:${entry.resourceId}`;
          if (!nodes.has(nodeId)) {
            nodes.set(nodeId, {
              id: nodeId,
              type: entry.resourceType,
              label: entry.resourceId,
              createdAt: entry.timestamp,
              updatedAt: entry.timestamp,
              metadata: {}
            });
          } else {
            const node = nodes.get(nodeId);
            node.updatedAt = entry.timestamp;
          }
          const relations = ["derivedFrom", "dependsOn", "parent", "inputs"];
          for (const rel of relations) {
            if (entry.metadata && entry.metadata[rel]) {
              const targets = Array.isArray(entry.metadata[rel]) ? entry.metadata[rel] : [entry.metadata[rel]];
              for (const target of targets) {
                let targetId = target;
                if (!target.includes(":")) {
                  targetId = `${entry.resourceType}:${target}`;
                }
                edges.push({
                  sourceId: targetId,
                  // derivedFrom means Target -> Source dependency usually? Or Source -> Target?
                  // If A is derived from B, then B -> A (flow of data)
                  targetId: nodeId,
                  relation: rel,
                  timestamp: entry.timestamp,
                  metadata: { entryId: entry.id }
                });
              }
            }
          }
        }
        return {
          nodes: Array.from(nodes.values()),
          edges,
          generatedAt: /* @__PURE__ */ new Date(),
          tenantId
        };
      }
      serialize(graph) {
        return JSON.stringify(graph, null, 2);
      }
      deserialize(json) {
        const graph = JSON.parse(json);
        graph.generatedAt = new Date(graph.generatedAt);
        graph.nodes.forEach((n) => {
          n.createdAt = new Date(n.createdAt);
          n.updatedAt = new Date(n.updatedAt);
        });
        graph.edges.forEach((e) => {
          e.timestamp = new Date(e.timestamp);
        });
        return graph;
      }
    };
    lineageGraph = new DataLineageGraph();
  }
});

// src/provenance/provExporter.ts
function isActivity(type) {
  return ACTION_TYPES.includes(type) || type.endsWith("Run") || type.endsWith("Job");
}
function isAgent(type) {
  return AGENT_TYPES.includes(type);
}
function convertLineageToProv(graph) {
  const doc = {
    prefix: {
      prov: "http://www.w3.org/ns/prov#",
      summit: "https://summit.dev/provenance/"
    },
    entity: {},
    activity: {},
    agent: {},
    wasDerivedFrom: {},
    wasGeneratedBy: {},
    used: {},
    wasAttributedTo: {},
    wasAssociatedWith: {}
  };
  for (const node of graph.nodes) {
    const id = `summit:${node.id}`;
    if (isActivity(node.type)) {
      doc.activity[id] = {
        "prov:type": node.type,
        "prov:label": node.label,
        "prov:startTime": node.createdAt.toISOString(),
        "prov:endTime": node.updatedAt.toISOString(),
        ...node.metadata
      };
    } else if (isAgent(node.type)) {
      doc.agent[id] = {
        "prov:type": node.type,
        "prov:label": node.label,
        ...node.metadata
      };
    } else {
      doc.entity[id] = {
        "prov:type": node.type,
        "prov:label": node.label,
        "prov:generatedAt": node.createdAt.toISOString(),
        ...node.metadata
      };
    }
  }
  let relCounter = 0;
  for (const edge of graph.edges) {
    const sourceId = `summit:${edge.sourceId}`;
    const targetId = `summit:${edge.targetId}`;
    const relId = `_:rel${relCounter++}`;
    const sourceNode = graph.nodes.find((n) => n.id === edge.sourceId);
    const targetNode = graph.nodes.find((n) => n.id === edge.targetId);
    const sourceType = sourceNode ? isActivity(sourceNode.type) ? "Activity" : isAgent(sourceNode.type) ? "Agent" : "Entity" : "Entity";
    const targetType = targetNode ? isActivity(targetNode.type) ? "Activity" : isAgent(targetNode.type) ? "Agent" : "Entity" : "Entity";
    if (edge.relation === "derivedFrom") {
      if (doc.wasDerivedFrom) {
        doc.wasDerivedFrom[relId] = {
          "prov:generatedEntity": targetId,
          "prov:usedEntity": sourceId
        };
      }
    } else if (edge.relation === "generatedBy" || targetType === "Entity" && sourceType === "Activity") {
      if (doc.wasGeneratedBy) {
        doc.wasGeneratedBy[relId] = {
          "prov:entity": targetId,
          "prov:activity": sourceId
        };
      }
    } else if (edge.relation === "used" || targetType === "Activity" && sourceType === "Entity") {
      if (doc.used) {
        doc.used[relId] = {
          "prov:activity": targetId,
          "prov:entity": sourceId
        };
      }
    } else if (edge.relation === "attributedTo" || targetType === "Entity" && sourceType === "Agent") {
      if (doc.wasAttributedTo) {
        doc.wasAttributedTo[relId] = {
          "prov:entity": targetId,
          "prov:agent": sourceId
        };
      }
    } else if (targetType === "Activity" && sourceType === "Agent") {
      if (doc.wasAssociatedWith) {
        doc.wasAssociatedWith[relId] = {
          "prov:activity": targetId,
          "prov:agent": sourceId
        };
      }
    } else {
      if (doc.wasDerivedFrom) {
        doc.wasDerivedFrom[relId] = {
          "prov:generatedEntity": targetId,
          "prov:usedEntity": sourceId,
          "prov:type": edge.relation
        };
      }
    }
  }
  return doc;
}
var ACTION_TYPES, AGENT_TYPES;
var init_provExporter = __esm({
  "src/provenance/provExporter.ts"() {
    "use strict";
    ACTION_TYPES = ["Action", "Job", "Run", "Task", "Build", "Deployment", "MaestroRun"];
    AGENT_TYPES = ["User", "System", "Service", "Bot", "Agent"];
  }
});

// src/graphql/resolvers/provenance.ts
var provenanceResolvers, provenance_default;
var init_provenance = __esm({
  "src/graphql/resolvers/provenance.ts"() {
    "use strict";
    init_evidenceRepo();
    init_trustRiskRepo();
    init_lineage();
    init_provExporter();
    provenanceResolvers = {
      Query: {
        async evidenceBundles(_2, { filter }) {
          const { service: service11, releaseId, since, until, limit, offset } = filter || {};
          if (!service11 || !releaseId) return [];
          if (since || until || typeof offset === "number" || limit && limit > 1) {
            return await listEvidence(service11, releaseId, {
              since,
              until,
              limit,
              offset
            });
          }
          const latest = await getLatestEvidence(service11, releaseId);
          return latest ? [latest] : [];
        },
        async exportProvenance(_2, { tenantId, format }) {
          const graph = await lineageGraph.buildGraph(tenantId);
          let content;
          if (format === "PROV-JSON" || !format) {
            content = convertLineageToProv(graph);
          } else if (format === "JSON") {
            content = graph;
          } else {
            throw new Error(`Unsupported format: ${format}`);
          }
          return {
            format: format || "PROV-JSON",
            content,
            exportedAt: (/* @__PURE__ */ new Date()).toISOString(),
            tenantId
          };
        }
      },
      Mutation: {
        async linkTrustScoreEvidence(_2, { tenantId, subjectId, evidenceId }) {
          const cur = await getTrustScore(tenantId, subjectId);
          const score = cur?.score ?? 0.7;
          const reasons = cur?.reasons ?? ["manual_link"];
          await upsertTrustScore(tenantId, subjectId, score, reasons, evidenceId);
          const updated = await getTrustScore(tenantId, subjectId);
          return {
            subjectId,
            score: updated?.score ?? score,
            reasons: updated?.reasons ?? reasons,
            updatedAt: updated?.updated_at ?? (/* @__PURE__ */ new Date()).toISOString()
          };
        }
      }
    };
    provenance_default = provenanceResolvers;
  }
});

// src/services/support-tickets.ts
async function ensureCommentSoftDeleteSchema() {
  if (commentSchemaEnsured) return;
  const pool4 = getPostgresPool();
  await pool4.query(`
    ALTER TABLE support_ticket_comments
      ADD COLUMN IF NOT EXISTS deleted_at timestamptz,
      ADD COLUMN IF NOT EXISTS deleted_by text,
      ADD COLUMN IF NOT EXISTS delete_reason text;

    CREATE TABLE IF NOT EXISTS support_ticket_comment_audits (
      id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
      comment_id uuid NOT NULL REFERENCES support_ticket_comments(id) ON DELETE CASCADE,
      action text NOT NULL,
      actor_id text,
      reason text,
      metadata jsonb DEFAULT '{}',
      created_at timestamptz DEFAULT now()
    );

    CREATE INDEX IF NOT EXISTS idx_support_ticket_comments_deleted ON support_ticket_comments(deleted_at);
    CREATE INDEX IF NOT EXISTS idx_support_ticket_comment_audits_comment ON support_ticket_comment_audits(comment_id);
  `);
  commentSchemaEnsured = true;
}
async function recordCommentAudit(commentId, action, actorId, reason, metadata) {
  const pool4 = getPostgresPool();
  await pool4.query(
    `INSERT INTO support_ticket_comment_audits (comment_id, action, actor_id, reason, metadata)
     VALUES ($1, $2, $3, $4, $5)`,
    [commentId, action, actorId, reason || null, JSON.stringify(metadata || {})]
  );
}
async function createTicket(input) {
  const pool4 = getPostgresPool();
  const result2 = await pool4.query(
    `INSERT INTO support_tickets (title, description, priority, category, reporter_id, reporter_email, tags, metadata)
     VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
     RETURNING *`,
    [
      input.title,
      input.description,
      input.priority || "medium",
      input.category || "other",
      input.reporter_id,
      input.reporter_email || null,
      input.tags || [],
      JSON.stringify(input.metadata || {})
    ]
  );
  return safeRows2(result2)[0];
}
async function getTicketById(id) {
  const pool4 = getPostgresPool();
  const result2 = await pool4.query("SELECT * FROM support_tickets WHERE id = $1", [id]);
  const rows = safeRows2(result2);
  return rows[0] || null;
}
async function listTickets(options2 = {}) {
  const pool4 = getPostgresPool();
  const conditions = [];
  const params = [];
  let paramIdx = 1;
  if (options2.status) {
    conditions.push(`status = $${paramIdx++}`);
    params.push(options2.status);
  }
  if (options2.priority) {
    conditions.push(`priority = $${paramIdx++}`);
    params.push(options2.priority);
  }
  if (options2.category) {
    conditions.push(`category = $${paramIdx++}`);
    params.push(options2.category);
  }
  if (options2.reporter_id) {
    conditions.push(`reporter_id = $${paramIdx++}`);
    params.push(options2.reporter_id);
  }
  if (options2.assignee_id) {
    conditions.push(`assignee_id = $${paramIdx++}`);
    params.push(options2.assignee_id);
  }
  const whereClause = conditions.length > 0 ? `WHERE ${conditions.join(" AND ")}` : "";
  const limit = options2.limit || 50;
  const offset = options2.offset || 0;
  const result2 = await pool4.query(
    `SELECT * FROM support_tickets ${whereClause} ORDER BY created_at DESC LIMIT $${paramIdx++} OFFSET $${paramIdx}`,
    [...params, limit, offset]
  );
  return safeRows2(result2);
}
async function updateTicket(id, input) {
  const pool4 = getPostgresPool();
  const updates = ["updated_at = NOW()"];
  const params = [];
  let paramIdx = 1;
  if (input.title !== void 0) {
    updates.push(`title = $${paramIdx++}`);
    params.push(input.title);
  }
  if (input.description !== void 0) {
    updates.push(`description = $${paramIdx++}`);
    params.push(input.description);
  }
  if (input.status !== void 0) {
    updates.push(`status = $${paramIdx++}`);
    params.push(input.status);
    if (input.status === "resolved") {
      updates.push("resolved_at = NOW()");
    }
    if (input.status === "closed") {
      updates.push("closed_at = NOW()");
    }
  }
  if (input.priority !== void 0) {
    updates.push(`priority = $${paramIdx++}`);
    params.push(input.priority);
  }
  if (input.category !== void 0) {
    updates.push(`category = $${paramIdx++}`);
    params.push(input.category);
  }
  if (input.assignee_id !== void 0) {
    updates.push(`assignee_id = $${paramIdx++}`);
    params.push(input.assignee_id);
  }
  if (input.tags !== void 0) {
    updates.push(`tags = $${paramIdx++}`);
    params.push(input.tags);
  }
  if (input.metadata !== void 0) {
    updates.push(`metadata = $${paramIdx++}`);
    params.push(JSON.stringify(input.metadata));
  }
  params.push(id);
  const result2 = await pool4.query(
    `UPDATE support_tickets SET ${updates.join(", ")} WHERE id = $${paramIdx} RETURNING *`,
    params
  );
  const rows = safeRows2(result2);
  return rows[0] || null;
}
async function deleteTicket(id) {
  const pool4 = getPostgresPool();
  const result2 = await pool4.query("DELETE FROM support_tickets WHERE id = $1", [id]);
  return result2.rowCount === 1;
}
async function addComment(ticketId, authorId, content, options2) {
  await ensureCommentSoftDeleteSchema();
  const pool4 = getPostgresPool();
  const result2 = await pool4.query(
    `INSERT INTO support_ticket_comments (ticket_id, author_id, author_email, content, is_internal)
     VALUES ($1, $2, $3, $4, $5) RETURNING *`,
    [ticketId, authorId, options2?.authorEmail || null, content, options2?.isInternal || false]
  );
  return safeRows2(result2)[0];
}
async function getComments(ticketId, options2 = {}) {
  await ensureCommentSoftDeleteSchema();
  const pool4 = getPostgresPool();
  const conditions = ["ticket_id = $1"];
  if (isSafeDeleteEnabled() && !options2.includeDeleted) {
    conditions.push("deleted_at IS NULL");
  }
  const result2 = await pool4.query(
    `SELECT * FROM support_ticket_comments WHERE ${conditions.join(" AND ")} ORDER BY created_at ASC`,
    [ticketId]
  );
  return safeRows2(result2);
}
async function getCommentById(commentId) {
  await ensureCommentSoftDeleteSchema();
  const pool4 = getPostgresPool();
  const result2 = await pool4.query("SELECT * FROM support_ticket_comments WHERE id = $1", [commentId]);
  const rows = safeRows2(result2);
  return rows[0] || null;
}
async function softDeleteComment(commentId, actorId, reason) {
  await ensureCommentSoftDeleteSchema();
  const pool4 = getPostgresPool();
  const retentionDays = Number(process.env.SUPPORT_COMMENT_RETENTION_DAYS || "30");
  const purgeAfter = Number.isFinite(retentionDays) ? new Date(Date.now() + retentionDays * 24 * 60 * 60 * 1e3) : null;
  const result2 = await pool4.query(
    `UPDATE support_ticket_comments
     SET deleted_at = NOW(), deleted_by = $2, delete_reason = $3
     WHERE id = $1
     RETURNING *`,
    [commentId, actorId, reason || null]
  );
  const rows = safeRows2(result2);
  const comment = rows[0] || null;
  if (comment) {
    await recordCommentAudit(commentId, "delete", actorId, reason, {
      purgeAfter: purgeAfter?.toISOString?.()
    });
  }
  return comment;
}
async function restoreComment(commentId, actorId) {
  await ensureCommentSoftDeleteSchema();
  const pool4 = getPostgresPool();
  const result2 = await pool4.query(
    `UPDATE support_ticket_comments
     SET deleted_at = NULL, deleted_by = NULL, delete_reason = NULL, updated_at = NOW()
     WHERE id = $1
     RETURNING *`,
    [commentId]
  );
  const rows = safeRows2(result2);
  const comment = rows[0] || null;
  if (comment) {
    await recordCommentAudit(commentId, "restore", actorId);
  }
  return comment;
}
async function getTicketCount(options2 = {}) {
  const pool4 = getPostgresPool();
  const conditions = [];
  const params = [];
  let paramIdx = 1;
  if (options2.status) {
    conditions.push(`status = $${paramIdx++}`);
    params.push(options2.status);
  }
  if (options2.priority) {
    conditions.push(`priority = $${paramIdx++}`);
    params.push(options2.priority);
  }
  const whereClause = conditions.length > 0 ? `WHERE ${conditions.join(" AND ")}` : "";
  const result2 = await pool4.query(`SELECT COUNT(*) as count FROM support_tickets ${whereClause}`, params);
  const rows = safeRows2(result2);
  return parseInt(rows[0]?.count || "0", 10);
}
var safeRows2, isSafeDeleteEnabled, commentSchemaEnsured;
var init_support_tickets = __esm({
  "src/services/support-tickets.ts"() {
    "use strict";
    init_postgres();
    safeRows2 = (result2) => Array.isArray(result2?.rows) ? result2.rows : [];
    isSafeDeleteEnabled = () => process.env.SAFE_DELETE !== "false";
    commentSchemaEnsured = false;
  }
});

// src/graphql/resolvers/supportTicket.ts
var supportTicketResolvers, supportTicket_default;
var init_supportTicket = __esm({
  "src/graphql/resolvers/supportTicket.ts"() {
    "use strict";
    init_support_tickets();
    supportTicketResolvers = {
      Query: {
        supportTicket: async (_2, { id }) => {
          return getTicketById(id);
        },
        supportTickets: async (_2, {
          filter,
          limit,
          offset
        }) => {
          const options2 = {
            ...filter || {},
            limit: limit || 50,
            offset: offset || 0
          };
          const [data, total] = await Promise.all([
            listTickets(options2),
            getTicketCount(options2)
          ]);
          return { data, total };
        }
      },
      Mutation: {
        createSupportTicket: async (_2, { input }, context4) => {
          const user = context4?.user;
          return createTicket({
            ...input,
            reporter_id: user?.sub || user?.id || "anonymous",
            reporter_email: user?.email
          });
        },
        updateSupportTicket: async (_2, { id, input }) => {
          return updateTicket(id, input);
        },
        deleteSupportTicket: async (_2, { id }) => {
          return deleteTicket(id);
        },
        addSupportTicketComment: async (_2, {
          ticketId,
          content,
          isInternal
        }, context4) => {
          const user = context4?.user;
          return addComment(ticketId, user?.sub || user?.id || "anonymous", content, {
            authorEmail: user?.email,
            isInternal: isInternal || false
          });
        }
      },
      SupportTicket: {
        comments: async (parent, args, context4) => {
          let comments;
          if (context4?.loaders?.supportTicketLoader) {
            comments = await context4.loaders.supportTicketLoader.load(parent.id);
          } else {
            comments = await getComments(parent.id);
          }
          if (args.limit !== void 0 || args.offset !== void 0) {
            const offset = args.offset || 0;
            const limit = args.limit || 100;
            return comments.slice(offset, offset + limit);
          }
          return comments;
        }
      }
    };
    supportTicket_default = supportTicketResolvers;
  }
});

// src/graphql/resolvers/sprint28.ts
import pino21 from "pino";
var logger19, sprint28Resolvers, sprint28_default;
var init_sprint28 = __esm({
  "src/graphql/resolvers/sprint28.ts"() {
    "use strict";
    init_pg();
    logger19 = pino21({ name: "resolvers:sprint28" });
    sprint28Resolvers = {
      Query: {
        funnel: async (_2, { period }, ctx) => {
          const tenantId = ctx.user?.tenantId;
          if (!tenantId) throw new Error("Unauthorized");
          try {
            const rows = await pg.many(
              `SELECT event_name as name, count(*) as value
           FROM analytics_events
           WHERE workspace_id = $1
           AND created_at > NOW() - INTERVAL '7 days' -- simplified period handling
           GROUP BY event_name`,
              [tenantId]
            );
            return rows.map((r) => ({ ...r, period }));
          } catch (err) {
            logger19.error({ err }, "Error fetching funnel");
            return [];
          }
        },
        pilotKpis: async (_2, { workspaceId }) => {
          try {
            const [npsRow] = await pg.many(
              `SELECT AVG(score) as nps FROM nps_responses WHERE workspace_id = $1`,
              [workspaceId]
            );
            const [queriesRow] = await pg.many(
              `SELECT COUNT(*) as cnt FROM analytics_events WHERE workspace_id = $1 AND event_name = 'query'`,
              [workspaceId]
            );
            return {
              ttfwMin: 5,
              // Placeholder metric
              dau: 15,
              // Placeholder metric
              queries: parseInt(queriesRow?.cnt || "0", 10),
              cases: 12,
              exports: 3,
              nps: parseFloat(npsRow?.nps || "0")
            };
          } catch (err) {
            logger19.error({ err }, "Error fetching pilot KPIs");
            throw new Error("Failed to fetch pilot KPIs");
          }
        },
        pilotSuccess: async (_2, { workspaceId }) => {
          const kpis = await sprint28Resolvers.Query.pilotKpis(
            _2,
            { workspaceId },
            {}
          );
          return [
            {
              label: "NPS",
              value: kpis.nps.toFixed(1),
              status: kpis.nps > 8 ? "success" : "warning",
              hint: "Target: > 9.0"
            },
            {
              label: "Queries",
              value: kpis.queries.toString(),
              status: "success",
              hint: null
            }
          ];
        }
      },
      Mutation: {
        submitNps: async (_2, { score, comment }, ctx) => {
          const workspaceId = ctx.user?.tenantId;
          if (!workspaceId) throw new Error("Unauthorized");
          try {
            await pg.write(
              `INSERT INTO nps_responses (workspace_id, score, comment) VALUES ($1, $2, $3)`,
              [workspaceId, score, comment]
            );
            logger19.info({ workspaceId, score }, "NPS Submitted");
            return true;
          } catch (err) {
            logger19.error({ err }, "Error submitting NPS");
            throw new Error("Failed to submit NPS");
          }
        },
        recordEvent: async (_2, { name, props }, ctx) => {
          const workspaceId = ctx.user?.tenantId;
          if (!workspaceId) throw new Error("Unauthorized");
          try {
            await pg.write(
              `INSERT INTO analytics_events (workspace_id, event_name, properties) VALUES ($1, $2, $3)`,
              [workspaceId, name, props]
              // pg library handles JSON serialization if passed as object usually, or we stringify
            );
            return true;
          } catch (err) {
            logger19.error({ err }, "Error recording event");
            return false;
          }
        },
        startTrial: async (_2, { plan, days }, ctx) => {
          logger19.info(
            { userId: ctx.user?.id, plan, days },
            "Starting trial (Stub)"
          );
          return true;
        },
        upgradePlan: async (_2, { plan }, ctx) => {
          logger19.info({ userId: ctx.user?.id, plan }, "Upgrading plan (Stub)");
          return true;
        }
      }
    };
    sprint28_default = sprint28Resolvers;
  }
});

// src/services/ElectronicWarfareService.ts
import { EventEmitter as EventEmitter4 } from "events";
var ElectronicWarfareService, ElectronicWarfareService_default;
var init_ElectronicWarfareService = __esm({
  "src/services/ElectronicWarfareService.ts"() {
    "use strict";
    init_logger2();
    ElectronicWarfareService = class extends EventEmitter4 {
      assets = /* @__PURE__ */ new Map();
      signals = /* @__PURE__ */ new Map();
      activeJammers = /* @__PURE__ */ new Map();
      signalHistory = [];
      constructor() {
        super();
        logger_default2.info("Electronic Warfare Service initialized.");
      }
      /**
       * Registers a new friendly or neutral EW asset in the battle management system.
       */
      registerAsset(asset) {
        this.assets.set(asset.id, { ...asset, activeProtection: [] });
        logger_default2.info(`EW Asset registered: ${asset.name} (${asset.id})`);
        this.emit("assetRegistered", asset);
      }
      /**
       * Updates the location of an asset.
       */
      updateAssetLocation(assetId, location) {
        const asset = this.assets.get(assetId);
        if (asset) {
          asset.location = location;
          this.assets.set(assetId, asset);
          this.emit("assetMoved", { assetId, location });
        }
      }
      /**
       * Simulates the detection of a signal in the spectrum.
       * (Electronic Support / Signals Intelligence)
       */
      detectSignal(signal) {
        this.signals.set(signal.id, signal);
        logger_default2.info(`Signal detected: ${signal.id} at ${signal.frequency}MHz`);
        this.emit("signalDetected", signal);
        const passiveSensors = Array.from(this.assets.values()).filter(
          (a) => a.status !== "OFFLINE"
        );
        if (passiveSensors.length > 0) {
          this.analyzeSignal(signal.id);
        }
      }
      /**
       * Analyzes a specific signal to determine its characteristics.
       * (Pulse Analysis / ELINT / SIGINT)
       */
      analyzeSignal(signalId) {
        const signal = this.signals.get(signalId);
        if (!signal) return null;
        const isRadar = signal.type === "RADAR" || signal.frequency > 1e3;
        const isEncrypted = Math.random() > 0.5;
        const report = {
          id: `INT-${Date.now()}-${Math.floor(Math.random() * 1e3)}`,
          signalId: signal.id,
          interceptTime: /* @__PURE__ */ new Date(),
          analyzedType: signal.type,
          confidence: 0.85 + Math.random() * 0.1,
          content: isEncrypted ? "ENCRYPTED_DATA" : signal.content || "NO_CONTENT",
          parameters: {
            pri: isRadar ? Math.random() * 1e3 : void 0,
            pw: isRadar ? Math.random() * 50 : void 0,
            encryptionType: isEncrypted ? "AES-256" : "NONE"
          }
        };
        this.signalHistory.push(report);
        logger_default2.info(
          `Signal analyzed: ${signalId} identified as ${report.analyzedType}`
        );
        this.emit("signalAnalyzed", report);
        return report;
      }
      /**
       * Performs Direction Finding (DF) on a signal using registered sensors.
       * Uses simulated Time Difference of Arrival (TDOA) logic.
       */
      triangulateSignal(signalId) {
        const signal = this.signals.get(signalId);
        if (!signal) {
          logger_default2.warn(`Cannot triangulate unknown signal: ${signalId}`);
          return null;
        }
        const activeSensors = Array.from(this.assets.values()).filter(
          (a) => a.status === "ACTIVE" || a.status === "PASSIVE"
        );
        if (activeSensors.length < 2) {
          logger_default2.warn("Insufficient sensors for triangulation.");
          return null;
        }
        const errorRadius = 1e3 / Math.pow(activeSensors.length, 1.5);
        const result2 = {
          signalId,
          estimatedLocation: signal.location || { lat: 0, lon: 0 },
          errorRadius,
          triangulationPoints: activeSensors.length,
          timestamp: /* @__PURE__ */ new Date()
        };
        logger_default2.info(
          `Signal ${signalId} triangulated with ${result2.triangulationPoints} sensors. Error: ${errorRadius.toFixed(1)}m`
        );
        this.emit("directionFound", result2);
        return result2;
      }
      /**
       * Calculates the Jamming-to-Signal (J/S) ratio to estimate effectiveness.
       * This is a simplified physics model.
       */
      calculateJammingEffectiveness(jammer, mission) {
        if (mission.targetFrequency < jammer.frequencyRange[0] || mission.targetFrequency > jammer.frequencyRange[1]) {
          return 0;
        }
        let effectiveness = mission.powerOutput / jammer.maxPower;
        switch (mission.effect) {
          case "SPOT_JAMMING":
            effectiveness *= 1;
            break;
          case "BARRAGE_JAMMING":
            effectiveness *= 0.4;
            break;
          case "DRFM_REPEATER":
            effectiveness *= 0.9;
            break;
          case "NOISE_JAMMING":
            effectiveness *= 0.6;
            break;
          default:
            effectiveness *= 0.5;
        }
        effectiveness *= 0.8 + Math.random() * 0.4;
        return Math.min(Math.max(effectiveness, 0), 1);
      }
      /**
       * Executes an Electronic Attack (Jamming).
       */
      deployJammer(assetId, targetFrequency, bandwidth, effect, durationSeconds = 60) {
        const asset = this.assets.get(assetId);
        if (!asset) throw new Error("Asset not found");
        if (!asset.capabilities.includes(effect)) {
          throw new Error(`Asset ${asset.name} does not support effect ${effect}`);
        }
        if (asset.status === "OFFLINE" || asset.status === "DAMAGED") {
          throw new Error("Asset is not operational");
        }
        const mission = {
          id: `JAM-${Date.now()}-${Math.floor(Math.random() * 100)}`,
          assetId,
          targetFrequency,
          bandwidth,
          effect,
          startTime: /* @__PURE__ */ new Date(),
          durationSeconds,
          powerOutput: asset.maxPower * 0.9,
          // Run at 90% power by default
          status: "ACTIVE",
          effectiveness: 0
        };
        mission.effectiveness = this.calculateJammingEffectiveness(asset, mission);
        this.activeJammers.set(mission.id, mission);
        logger_default2.info(
          `Jamming mission started: ${effect} on ${targetFrequency}MHz by ${asset.name} (Eff: ${mission.effectiveness.toFixed(2)})`
        );
        this.emit("jammingStarted", mission);
        setTimeout(() => {
          this.stopJammer(mission.id);
        }, durationSeconds * 1e3);
        return mission;
      }
      /**
       * Stops an active jamming mission.
       */
      stopJammer(missionId) {
        const mission = this.activeJammers.get(missionId);
        if (mission && mission.status === "ACTIVE") {
          mission.status = "COMPLETED";
          this.activeJammers.set(missionId, mission);
          logger_default2.info(`Jamming mission completed: ${missionId}`);
          this.emit("jammingStopped", mission);
        }
      }
      /**
       * Specialized method for Communication Disruption.
       */
      disruptCommunications(assetId, targetFreq) {
        return this.deployJammer(
          assetId,
          targetFreq,
          0.025,
          // Narrowband 25kHz
          "COMM_DISRUPTION",
          30
        );
      }
      /**
       * Specialized method for Radar Jamming.
       */
      jamRadar(assetId, targetFreq) {
        return this.deployJammer(
          assetId,
          targetFreq,
          10,
          // Wideband 10MHz
          "NOISE_JAMMING",
          45
        );
      }
      /**
       * Activates Electronic Protection (EP) measures for an asset.
       * e.g., Frequency Hopping, Power Management.
       */
      activateProtection(assetId, measure) {
        const asset = this.assets.get(assetId);
        if (!asset) throw new Error("Asset not found");
        if (!asset.activeProtection.includes(measure)) {
          asset.activeProtection.push(measure);
          this.assets.set(assetId, asset);
          logger_default2.info(`Activating EP measure ${measure} for asset ${asset.name}`);
          this.emit("protectionActivated", { assetId, measure });
        }
      }
      /**
       * Deactivates an EP measure.
       */
      deactivateProtection(assetId, measure) {
        const asset = this.assets.get(assetId);
        if (!asset) return;
        asset.activeProtection = asset.activeProtection.filter((m) => m !== measure);
        this.assets.set(assetId, asset);
        this.emit("protectionDeactivated", { assetId, measure });
      }
      /**
       * Electromagnetic Battle Management (EMBM).
       * Returns a situational awareness report of the spectrum.
       */
      getBattleSpacePicture() {
        const signals = Array.from(this.signals.values());
        const utilization = signals.length > 0 ? Math.min(signals.length * 0.1, 1) : 0;
        return {
          timestamp: /* @__PURE__ */ new Date(),
          assets: Array.from(this.assets.values()),
          signals,
          activeJammers: Array.from(this.activeJammers.values()).filter(
            (j) => j.status === "ACTIVE"
          ),
          intercepts: this.signalHistory.slice(-50),
          // Last 50 intercepts
          spectrumUtilization: utilization
        };
      }
      /**
       * Simulates an EMP analysis impact assessment (Theoretical).
       */
      analyzeEMPBlast(location, yieldKt) {
        const radiusKm = Math.sqrt(yieldKt) * 4;
        const affectedAssets = Array.from(this.assets.values()).filter((asset) => {
          const latDiff = (asset.location.lat - location.lat) * 111;
          const lonDiff = (asset.location.lon - location.lon) * 111 * Math.cos(location.lat * (Math.PI / 180));
          const distKm = Math.sqrt(latDiff * latDiff + lonDiff * lonDiff);
          return distKm < radiusKm;
        });
        const report = {
          event: "EMP_ANALYSIS",
          origin: location,
          yieldKt,
          estimatedRadiusKm: radiusKm,
          assetsAtRisk: affectedAssets.map((a) => a.id),
          timestamp: /* @__PURE__ */ new Date()
        };
        logger_default2.warn(
          `EMP Analysis generated. Estimated ${affectedAssets.length} assets at risk.`
        );
        return report;
      }
    };
    ElectronicWarfareService_default = new ElectronicWarfareService();
  }
});

// src/graphql/resolvers/electronic-warfare.ts
var resolvers, electronic_warfare_default;
var init_electronic_warfare = __esm({
  "src/graphql/resolvers/electronic-warfare.ts"() {
    "use strict";
    init_ElectronicWarfareService();
    resolvers = {
      Query: {
        ewBattleSpace: () => {
          return ElectronicWarfareService_default.getBattleSpacePicture();
        },
        ewAnalyzeEMP: (_2, args) => {
          return ElectronicWarfareService_default.analyzeEMPBlast({ lat: args.lat, lon: args.lon }, args.yieldKt);
        }
      },
      Mutation: {
        ewRegisterAsset: (_2, args) => {
          const asset = {
            id: args.id,
            name: args.name,
            type: args.type,
            location: { lat: args.lat, lon: args.lon || 0 },
            capabilities: args.capabilities,
            maxPower: args.maxPower,
            frequencyRange: [args.minFreq, args.maxFreq],
            status: "ACTIVE",
            activeProtection: []
          };
          ElectronicWarfareService_default.registerAsset(asset);
          return asset;
        },
        ewDeployJammer: (_2, args) => {
          return ElectronicWarfareService_default.deployJammer(
            args.assetId,
            args.targetFrequency,
            args.bandwidth,
            args.effect,
            args.durationSeconds
          );
        },
        ewStopJammer: (_2, args) => {
          try {
            ElectronicWarfareService_default.stopJammer(args.missionId);
            return true;
          } catch (e) {
            return false;
          }
        },
        ewSimulateSignalDetection: (_2, args) => {
          const signal = {
            id: `SIG-${Date.now()}`,
            frequency: args.frequency,
            bandwidth: args.bandwidth,
            power: args.power,
            modulation: args.modulation,
            type: args.type,
            location: args.lat ? { lat: args.lat, lon: args.lon || 0 } : void 0,
            timestamp: /* @__PURE__ */ new Date()
          };
          ElectronicWarfareService_default.detectSignal(signal);
          return signal;
        },
        ewTriangulateSignal: (_2, args) => {
          return ElectronicWarfareService_default.triangulateSignal(args.signalId);
        },
        ewActivateProtection: (_2, args) => {
          try {
            ElectronicWarfareService_default.activateProtection(args.assetId, args.measure);
            return true;
          } catch (e) {
            return false;
          }
        }
      }
    };
    electronic_warfare_default = resolvers;
  }
});

// src/collaboration/warRoomService.ts
var WarRoomService, warRoomService;
var init_warRoomService = __esm({
  "src/collaboration/warRoomService.ts"() {
    "use strict";
    init_database();
    WarRoomService = class {
      get db() {
        return getPostgresPool2();
      }
      async createWarRoom(name, createdBy) {
        const { rows } = await this.db.query(
          "INSERT INTO war_rooms (name, created_by) VALUES ($1, $2) RETURNING *",
          [name, createdBy]
        );
        return rows[0];
      }
      async addParticipant(warRoomId, userId, role) {
        const { rows } = await this.db.query(
          "INSERT INTO war_room_participants (war_room_id, user_id, role) VALUES ($1, $2, $3) RETURNING *",
          [warRoomId, userId, role]
        );
        return rows[0];
      }
      async removeParticipant(warRoomId, userId) {
        await this.db.query(
          "DELETE FROM war_room_participants WHERE war_room_id = $1 AND user_id = $2",
          [warRoomId, userId]
        );
      }
      async getWarRoom(id) {
        const { rows } = await this.db.query("SELECT * FROM war_rooms WHERE id = $1", [id]);
        return rows[0];
      }
      async getWarRooms() {
        const { rows } = await this.db.query("SELECT * FROM war_rooms");
        return rows;
      }
      async getParticipants(warRoomId) {
        const { rows } = await this.db.query(
          "SELECT * FROM war_room_participants WHERE war_room_id = $1",
          [warRoomId]
        );
        return rows;
      }
    };
    warRoomService = new WarRoomService();
  }
});

// src/services/collaborationService.ts
import { EventEmitter as EventEmitter5 } from "events";
import { PubSub as PubSub2 } from "graphql-subscriptions";
var CollaborationService, collaborationService;
var init_collaborationService = __esm({
  "src/services/collaborationService.ts"() {
    "use strict";
    init_CacheService();
    CollaborationService = class extends EventEmitter5 {
      activeUsers = /* @__PURE__ */ new Map();
      pendingEdits = /* @__PURE__ */ new Map();
      comments = /* @__PURE__ */ new Map();
      notifications = [];
      maxNotifications = 100;
      pubsub;
      constructor() {
        super();
        this.pubsub = new PubSub2();
        console.log("[COLLABORATION] Real-time collaboration service initialized");
        setInterval(() => {
          this.cleanupInactiveUsers();
        }, 6e4);
      }
      /**
       * Get async iterator for GraphQL subscriptions
       */
      asyncIterator(eventNames) {
        return this.pubsub.asyncIterator(eventNames);
      }
      /**
       * User joins an investigation
       */
      async joinInvestigation(userId, investigationId, userInfo) {
        const presence = {
          userId,
          investigationId,
          currentPage: "graph",
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        };
        this.activeUsers.set(`${userId}:${investigationId}`, presence);
        await cacheService.set(
          `presence:${userId}:${investigationId}`,
          presence,
          300
        );
        const notification = {
          id: `notif-${Date.now()}`,
          type: "USER_JOINED",
          userId,
          investigationId,
          message: `${userInfo.name} joined the investigation`,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          metadata: { userInfo }
        };
        this.addNotification(notification);
        this.emit("userJoined", { userId, investigationId, userInfo, presence });
        this.pubsub.publish("userJoined", {
          userId,
          investigationId,
          userInfo,
          presence
        });
        console.log(
          `[COLLABORATION] User ${userInfo.name} joined investigation ${investigationId}`
        );
      }
      /**
       * User leaves an investigation
       */
      async leaveInvestigation(userId, investigationId) {
        const presenceKey2 = `${userId}:${investigationId}`;
        const presence = this.activeUsers.get(presenceKey2);
        if (presence) {
          this.activeUsers.delete(presenceKey2);
          await cacheService.del(`presence:${userId}:${investigationId}`);
          const notification = {
            id: `notif-${Date.now()}`,
            type: "USER_LEFT",
            userId,
            investigationId,
            message: `User left the investigation`,
            timestamp: (/* @__PURE__ */ new Date()).toISOString()
          };
          this.addNotification(notification);
          this.emit("userLeft", { userId, investigationId });
          this.pubsub.publish("userLeft", { userId, investigationId });
          console.log(
            `[COLLABORATION] User ${userId} left investigation ${investigationId}`
          );
        }
      }
      /**
       * Update user presence (cursor position, selected entity, etc.)
       */
      async updatePresence(userId, investigationId, updates) {
        const presenceKey2 = `${userId}:${investigationId}`;
        const currentPresence = this.activeUsers.get(presenceKey2);
        if (currentPresence) {
          const updatedPresence = {
            ...currentPresence,
            ...updates,
            timestamp: (/* @__PURE__ */ new Date()).toISOString()
          };
          this.activeUsers.set(presenceKey2, updatedPresence);
          await cacheService.set(
            `presence:${userId}:${investigationId}`,
            updatedPresence,
            300
          );
          this.emit("presenceUpdated", {
            userId,
            investigationId,
            presence: updatedPresence
          });
          this.pubsub.publish("presenceUpdated", {
            userId,
            investigationId,
            presence: updatedPresence
          });
        }
      }
      /**
       * Get active users for an investigation
       */
      getActiveUsers(investigationId) {
        const activeUsers = [];
        for (const [key, presence] of this.activeUsers.entries()) {
          if (presence.investigationId === investigationId) {
            const lastUpdate = new Date(presence.timestamp);
            const now = /* @__PURE__ */ new Date();
            const timeDiff = now.getTime() - lastUpdate.getTime();
            if (timeDiff < 12e4) {
              activeUsers.push(presence);
            }
          }
        }
        return activeUsers;
      }
      /**
       * Submit a collaborative edit
       */
      async submitEdit(edit) {
        const collaborativeEdit = {
          ...edit,
          id: `edit-${Date.now()}-${edit.userId}`,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          status: "PENDING"
        };
        const conflicts = this.detectEditConflicts(collaborativeEdit);
        if (conflicts.length > 0) {
          console.log(
            `[COLLABORATION] Edit conflicts detected for edit ${collaborativeEdit.id}`
          );
          const resolved = await this.resolveConflictViaConsensusML(collaborativeEdit, conflicts);
          if (resolved) {
            collaborativeEdit.status = "APPLIED";
            console.log(`[COLLABORATION] Conflict auto-resolved for ${collaborativeEdit.id}`);
          } else {
            const notification = {
              id: `notif-${Date.now()}`,
              type: "EDIT_CONFLICT",
              userId: edit.userId,
              investigationId: edit.investigationId,
              message: `Edit conflict detected for entity ${edit.entityId}`,
              timestamp: (/* @__PURE__ */ new Date()).toISOString(),
              metadata: { conflicts, editId: collaborativeEdit.id }
            };
            this.addNotification(notification);
          }
        }
        this.pendingEdits.set(collaborativeEdit.id, collaborativeEdit);
        this.emit("editSubmitted", collaborativeEdit);
        this.pubsub.publish("editSubmitted", collaborativeEdit);
        console.log(
          `[COLLABORATION] Edit submitted: ${collaborativeEdit.id} by user ${edit.userId}`
        );
        if (collaborativeEdit.status === "APPLIED") {
          this.autoArchiveEdit(collaborativeEdit);
        }
        return collaborativeEdit;
      }
      /**
       * Simulate Consensus ML for conflict resolution
       */
      async resolveConflictViaConsensusML(newEdit, conflicts) {
        return Math.random() > 0.5;
      }
      /**
       * Auto-archive edit (Wayback Machine simulation)
       */
      async autoArchiveEdit(edit) {
        console.log(`[COLLABORATION] Auto-archiving edit ${edit.id} to Wayback Machine (simulated)`);
      }
      /**
       * Apply or reject an edit
       */
      async resolveEdit(editId, status, resolvedBy) {
        const edit = this.pendingEdits.get(editId);
        if (edit) {
          edit.status = status;
          if (status === "APPLIED") {
            console.log(`[COLLABORATION] Edit ${editId} applied by ${resolvedBy}`);
            const notification = {
              id: `notif-${Date.now()}`,
              type: "ENTITY_UPDATED",
              userId: resolvedBy,
              investigationId: edit.investigationId,
              message: `Entity ${edit.entityId} was updated`,
              timestamp: (/* @__PURE__ */ new Date()).toISOString(),
              metadata: { editId, changes: edit.changes }
            };
            this.addNotification(notification);
          }
          this.emit("editResolved", { editId, status, resolvedBy, edit });
          setTimeout(() => {
            this.pendingEdits.delete(editId);
          }, 3e5);
          return edit;
        }
        return null;
      }
      /**
       * Add a comment
       */
      async addComment(comment) {
        const newComment = {
          ...comment,
          id: `comment-${Date.now()}-${comment.userId}`,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          replies: [],
          resolved: false
        };
        this.comments.set(newComment.id, newComment);
        const notification = {
          id: `notif-${Date.now()}`,
          type: "COMMENT_ADDED",
          userId: comment.userId,
          investigationId: comment.investigationId,
          message: `New comment added`,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          metadata: { commentId: newComment.id, entityId: comment.entityId }
        };
        this.addNotification(notification);
        this.emit("commentAdded", newComment);
        this.pubsub.publish("commentAdded", newComment);
        console.log(
          `[COLLABORATION] Comment added: ${newComment.id} by user ${comment.userId}`
        );
        return newComment;
      }
      /**
       * Get comments for an investigation or entity
       */
      getComments(investigationId, entityId) {
        const comments = [];
        for (const comment of this.comments.values()) {
          if (comment.investigationId === investigationId) {
            if (!entityId || comment.entityId === entityId) {
              comments.push(comment);
            }
          }
        }
        return comments.sort(
          (a, b) => new Date(b.timestamp).getTime() - new Date(a.timestamp).getTime()
        );
      }
      /**
       * Get recent notifications
       */
      getRecentNotifications(investigationId, limit = 20) {
        return this.notifications.filter((notif) => notif.investigationId === investigationId).slice(0, limit);
      }
      /**
       * Get pending edits for an investigation
       */
      getPendingEdits(investigationId) {
        const edits = [];
        for (const edit of this.pendingEdits.values()) {
          if (edit.investigationId === investigationId && edit.status === "PENDING") {
            edits.push(edit);
          }
        }
        return edits.sort(
          (a, b) => new Date(b.timestamp).getTime() - new Date(a.timestamp).getTime()
        );
      }
      /**
       * Get collaboration statistics
       */
      getCollaborationStats() {
        const activeInvestigations = /* @__PURE__ */ new Set();
        const totalUsers = this.activeUsers.size;
        for (const presence of this.activeUsers.values()) {
          activeInvestigations.add(presence.investigationId);
        }
        return {
          activeUsers: totalUsers,
          activeInvestigations: activeInvestigations.size,
          pendingEdits: this.pendingEdits.size,
          totalComments: this.comments.size,
          recentNotifications: this.notifications.length
        };
      }
      detectEditConflicts(newEdit) {
        const conflicts = [];
        for (const edit of this.pendingEdits.values()) {
          if (edit.entityId === newEdit.entityId && edit.investigationId === newEdit.investigationId && edit.userId !== newEdit.userId && edit.status === "PENDING") {
            const editTime = new Date(edit.timestamp);
            const newEditTime = new Date(newEdit.timestamp);
            const timeDiff = Math.abs(newEditTime.getTime() - editTime.getTime());
            if (timeDiff < 3e5) {
              conflicts.push(edit);
            }
          }
        }
        return conflicts;
      }
      addNotification(notification) {
        this.notifications.unshift(notification);
        if (this.notifications.length > this.maxNotifications) {
          this.notifications = this.notifications.slice(0, this.maxNotifications);
        }
        this.emit("notification", notification);
        this.pubsub.publish("notification", notification);
      }
      cleanupInactiveUsers() {
        const now = /* @__PURE__ */ new Date();
        const inactiveThreshold = 5 * 60 * 1e3;
        let cleanedUp = 0;
        for (const [key, presence] of this.activeUsers.entries()) {
          const lastUpdate = new Date(presence.timestamp);
          const timeDiff = now.getTime() - lastUpdate.getTime();
          if (timeDiff > inactiveThreshold) {
            this.activeUsers.delete(key);
            cacheService.del(
              `presence:${presence.userId}:${presence.investigationId}`
            );
            cleanedUp++;
          }
        }
        if (cleanedUp > 0) {
          console.log(
            `[COLLABORATION] Cleaned up ${cleanedUp} inactive user presences`
          );
        }
      }
    };
    collaborationService = new CollaborationService();
  }
});

// src/db.ts
var db;
var init_db = __esm({
  "src/db.ts"() {
    "use strict";
    init_pg();
    db = pool;
  }
});

// src/middleware/warRoomAuth.ts
import { GraphQLError as GraphQLError8 } from "graphql";
var checkAuth, checkWarRoomAdmin;
var init_warRoomAuth = __esm({
  "src/middleware/warRoomAuth.ts"() {
    "use strict";
    init_db();
    checkAuth = (context4) => {
      if (!context4.user || !context4.user.id) {
        throw new GraphQLError8("User is not authenticated", {
          extensions: {
            code: "UNAUTHENTICATED"
          }
        });
      }
    };
    checkWarRoomAdmin = async (context4, warRoomId) => {
      checkAuth(context4);
      const userId = context4.user.id;
      const { rows } = await db.query(
        "SELECT role FROM war_room_participants WHERE war_room_id = $1 AND user_id = $2",
        [warRoomId, userId]
      );
      const role = rows[0]?.role;
      if (role !== "ADMIN") {
        throw new GraphQLError8("User is not an admin of this War Room", {
          extensions: {
            code: "FORBIDDEN"
          }
        });
      }
    };
  }
});

// src/graphql/resolvers/collaboration.ts
var collaborationResolvers;
var init_collaboration = __esm({
  "src/graphql/resolvers/collaboration.ts"() {
    "use strict";
    init_warRoomService();
    init_collaborationService();
    init_warRoomAuth();
    collaborationResolvers = {
      Query: {
        warRoom: async (_2, { id }) => {
          return warRoomService.getWarRoom(id);
        },
        warRooms: async () => {
          return warRoomService.getWarRooms();
        }
      },
      Mutation: {
        createWarRoom: async (_2, { name }, context4) => {
          checkAuth(context4);
          const createdBy = context4.user.id;
          return warRoomService.createWarRoom(name, createdBy);
        },
        addParticipant: async (_2, { warRoomId, userId, role }, context4) => {
          await checkWarRoomAdmin(context4, warRoomId);
          await warRoomService.addParticipant(warRoomId, userId, role);
          const warRoom = await warRoomService.getWarRoom(warRoomId);
          collaborationService.pubsub.publish("PARTICIPANT_ADDED", { participantAdded: { warRoom } });
          return warRoom;
        },
        removeParticipant: async (_2, { warRoomId, userId }, context4) => {
          await checkWarRoomAdmin(context4, warRoomId);
          await warRoomService.removeParticipant(warRoomId, userId);
          const warRoom = await warRoomService.getWarRoom(warRoomId);
          collaborationService.pubsub.publish("PARTICIPANT_REMOVED", { participantRemoved: { warRoom } });
          return warRoom;
        }
      },
      WarRoom: {
        participants: async (warRoom) => {
          return warRoomService.getParticipants(warRoom.id);
        },
        createdBy: async (warRoom) => {
          return { id: warRoom.created_by, name: "Dummy User" };
        },
        createdAt: (warRoom) => new Date(warRoom.created_at).toISOString()
      },
      WarRoomParticipant: {
        user: async (participant) => {
          return { id: participant.user_id, name: "Dummy User" };
        },
        joinedAt: (participant) => new Date(participant.joined_at).toISOString()
      },
      Subscription: {
        participantAdded: {
          subscribe: () => collaborationService.pubsub.asyncIterator(["PARTICIPANT_ADDED"])
        },
        participantRemoved: {
          subscribe: () => collaborationService.pubsub.asyncIterator(["PARTICIPANT_REMOVED"])
        }
      }
    };
  }
});

// src/repos/CaseRepo.ts
import { randomUUID as uuidv46 } from "crypto";
var repoLogger, CaseRepo;
var init_CaseRepo = __esm({
  "src/repos/CaseRepo.ts"() {
    "use strict";
    init_logger();
    repoLogger = logger_default.child({ name: "CaseRepo" });
    CaseRepo = class {
      constructor(pg5) {
        this.pg = pg5;
      }
      /**
       * Create a new case
       */
      async create(input, userId) {
        const id = uuidv46();
        const { rows } = await this.pg.query(
          `INSERT INTO maestro.cases (
        id, tenant_id, title, description, status, priority, compartment,
        policy_labels, metadata, created_by
      )
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
       RETURNING *`,
          [
            id,
            input.tenantId,
            input.title,
            input.description || null,
            input.status || "open",
            input.priority || "medium",
            input.compartment || null,
            input.policyLabels || [],
            JSON.stringify(input.metadata || {}),
            userId
          ]
        );
        repoLogger.info(
          {
            caseId: id,
            tenantId: input.tenantId,
            title: input.title,
            compartment: input.compartment
          },
          "Case created"
        );
        return this.mapRow(rows[0]);
      }
      /**
       * Update an existing case
       */
      async update(input, userId) {
        const updateFields = [];
        const params = [input.id];
        let paramIndex = 2;
        if (input.title !== void 0) {
          updateFields.push(`title = $${paramIndex}`);
          params.push(input.title);
          paramIndex++;
        }
        if (input.description !== void 0) {
          updateFields.push(`description = $${paramIndex}`);
          params.push(input.description);
          paramIndex++;
        }
        if (input.status !== void 0) {
          updateFields.push(`status = $${paramIndex}`);
          params.push(input.status);
          paramIndex++;
          if (input.status === "closed" && userId) {
            updateFields.push(`closed_at = NOW()`);
            updateFields.push(`closed_by = $${paramIndex}`);
            params.push(userId);
            paramIndex++;
          }
        }
        if (input.priority !== void 0) {
          updateFields.push(`priority = $${paramIndex}`);
          params.push(input.priority);
          paramIndex++;
        }
        if (input.compartment !== void 0) {
          updateFields.push(`compartment = $${paramIndex}`);
          params.push(input.compartment);
          paramIndex++;
        }
        if (input.policyLabels !== void 0) {
          updateFields.push(`policy_labels = $${paramIndex}`);
          params.push(input.policyLabels);
          paramIndex++;
        }
        if (input.metadata !== void 0) {
          updateFields.push(`metadata = $${paramIndex}`);
          params.push(JSON.stringify(input.metadata));
          paramIndex++;
        }
        if (updateFields.length === 0) {
          return await this.findById(input.id);
        }
        updateFields.push(`updated_at = NOW()`);
        const { rows } = await this.pg.query(
          `UPDATE maestro.cases SET ${updateFields.join(", ")}
       WHERE id = $1
       RETURNING *`,
          params
        );
        if (rows[0]) {
          repoLogger.info(
            {
              caseId: input.id,
              updatedFields: Object.keys(input).filter((k) => k !== "id")
            },
            "Case updated"
          );
        }
        return rows[0] ? this.mapRow(rows[0]) : null;
      }
      /**
       * Delete a case (soft delete by archiving recommended, but hard delete supported)
       */
      async delete(id) {
        const client6 = await this.pg.connect();
        try {
          await client6.query("BEGIN");
          const { rows: auditLogs } = await client6.query(
            `SELECT id FROM maestro.audit_access_logs WHERE case_id = $1 LIMIT 1`,
            [id]
          );
          if (auditLogs.length > 0) {
            throw new Error(
              "Cannot delete case with existing audit logs. Archive the case instead."
            );
          }
          const { rowCount } = await client6.query(
            `DELETE FROM maestro.cases WHERE id = $1`,
            [id]
          );
          await client6.query("COMMIT");
          if (rowCount && rowCount > 0) {
            repoLogger.warn({ caseId: id }, "Case deleted");
          }
          return rowCount !== null && rowCount > 0;
        } catch (error) {
          await client6.query("ROLLBACK");
          throw error;
        } finally {
          client6.release();
        }
      }
      /**
       * Archive a case (soft delete - preferred method)
       */
      async archive(id, userId) {
        return this.update({ id, status: "archived" }, userId);
      }
      /**
       * Find case by ID
       */
      async findById(id, tenantId) {
        const params = [id];
        let query3 = `SELECT * FROM maestro.cases WHERE id = $1`;
        if (tenantId) {
          query3 += ` AND tenant_id = $2`;
          params.push(tenantId);
        }
        const { rows } = await this.pg.query(query3, params);
        return rows[0] ? this.mapRow(rows[0]) : null;
      }
      /**
       * List cases with filters
       */
      async list({
        tenantId,
        status,
        compartment,
        policyLabels,
        limit = 50,
        offset = 0
      }) {
        const params = [tenantId];
        let query3 = `SELECT * FROM maestro.cases WHERE tenant_id = $1`;
        let paramIndex = 2;
        if (status) {
          query3 += ` AND status = $${paramIndex}`;
          params.push(status);
          paramIndex++;
        }
        if (compartment) {
          query3 += ` AND compartment = $${paramIndex}`;
          params.push(compartment);
          paramIndex++;
        }
        if (policyLabels && policyLabels.length > 0) {
          query3 += ` AND policy_labels && $${paramIndex}`;
          params.push(policyLabels);
          paramIndex++;
        }
        query3 += ` ORDER BY created_at DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
        params.push(Math.min(limit, 1e3), offset);
        const { rows } = await this.pg.query(query3, params);
        return rows.map(this.mapRow);
      }
      /**
       * Count cases with filters
       */
      async count({
        tenantId,
        status,
        compartment,
        policyLabels
      }) {
        const params = [tenantId];
        let query3 = `SELECT COUNT(*) as count FROM maestro.cases WHERE tenant_id = $1`;
        let paramIndex = 2;
        if (status) {
          query3 += ` AND status = $${paramIndex}`;
          params.push(status);
          paramIndex++;
        }
        if (compartment) {
          query3 += ` AND compartment = $${paramIndex}`;
          params.push(compartment);
          paramIndex++;
        }
        if (policyLabels && policyLabels.length > 0) {
          query3 += ` AND policy_labels && $${paramIndex}`;
          params.push(policyLabels);
          paramIndex++;
        }
        const { rows } = await this.pg.query(query3, params);
        return parseInt(rows[0]?.count || "0", 10);
      }
      /**
       * Batch load cases by IDs (for DataLoader)
       */
      async batchByIds(ids, tenantId) {
        if (ids.length === 0) return [];
        const params = [ids];
        let query3 = `SELECT * FROM maestro.cases WHERE id = ANY($1)`;
        if (tenantId) {
          query3 += ` AND tenant_id = $2`;
          params.push(tenantId);
        }
        const { rows } = await this.pg.query(query3, params);
        const casesMap = new Map(rows.map((row) => [row.id, this.mapRow(row)]));
        return ids.map((id) => casesMap.get(id) || null);
      }
      /**
       * Get cases by policy label
       */
      async findByPolicyLabel(tenantId, policyLabel, limit = 50) {
        const { rows } = await this.pg.query(
          `SELECT * FROM maestro.cases
       WHERE tenant_id = $1 AND $2 = ANY(policy_labels)
       ORDER BY created_at DESC
       LIMIT $3`,
          [tenantId, policyLabel, limit]
        );
        return rows.map(this.mapRow);
      }
      /**
       * Map database row to domain object
       */
      /**
       * Upsert a case tab
       */
      async upsertTab(caseId, tab) {
        const { rows } = await this.pg.query(
          `INSERT INTO maestro.case_tabs (id, case_id, name, state, updated_at)
       VALUES ($1, $2, $3, $4, NOW())
       ON CONFLICT (id) DO UPDATE SET
         name = EXCLUDED.name,
         state = EXCLUDED.state,
         updated_at = NOW()
       RETURNING *`,
          [tab.id, caseId, tab.name, JSON.stringify(tab.state)]
        );
        return this.mapTabRow(rows[0]);
      }
      /**
       * Get all tabs for a case
       */
      async getTabs(caseId) {
        const { rows } = await this.pg.query(
          `SELECT * FROM maestro.case_tabs WHERE case_id = $1 ORDER BY created_at ASC`,
          [caseId]
        );
        return rows.map(this.mapTabRow);
      }
      /**
       * Delete a case tab
       */
      async deleteTab(caseId, tabId) {
        const { rowCount } = await this.pg.query(
          `DELETE FROM maestro.case_tabs WHERE case_id = $1 AND id = $2`,
          [caseId, tabId]
        );
        return (rowCount || 0) > 0;
      }
      mapTabRow(row) {
        return {
          id: row.id,
          caseId: row.case_id,
          name: row.name,
          state: row.state,
          updatedAt: row.updated_at,
          createdAt: row.created_at
        };
      }
      mapRow(row) {
        return {
          id: row.id,
          tenantId: row.tenant_id,
          title: row.title,
          description: row.description || void 0,
          status: row.status,
          priority: row.priority,
          compartment: row.compartment || void 0,
          policyLabels: row.policy_labels || [],
          metadata: row.metadata || {},
          createdAt: row.created_at,
          updatedAt: row.updated_at,
          createdBy: row.created_by,
          closedAt: row.closed_at || void 0,
          closedBy: row.closed_by || void 0
        };
      }
    };
  }
});

// src/repos/AuditAccessLogRepo.ts
import { randomUUID as uuidv47, createHash as createHash8 } from "crypto";
var repoLogger2, AuditAccessLogRepo;
var init_AuditAccessLogRepo = __esm({
  "src/repos/AuditAccessLogRepo.ts"() {
    "use strict";
    init_logger();
    repoLogger2 = logger_default.child({ name: "AuditAccessLogRepo" });
    AuditAccessLogRepo = class {
      constructor(pg5) {
        this.pg = pg5;
        this.initializeLastHash();
      }
      lastHash = "";
      /**
       * Initialize the last hash from the database
       */
      async initializeLastHash() {
        try {
          const { rows } = await this.pg.query(
            `SELECT hash FROM maestro.audit_access_logs
         ORDER BY created_at DESC LIMIT 1`
          );
          if (rows[0]?.hash) {
            this.lastHash = rows[0].hash;
          }
        } catch (error) {
          repoLogger2.warn(
            { error: error.message },
            "Failed to initialize last hash"
          );
        }
      }
      /**
       * Log an access event - PRIMARY FUNCTION
       * This is the main entry point for recording access to cases
       */
      async logAccess(input) {
        if (!input.reason || input.reason.trim() === "") {
          throw new Error(
            "Reason is required for audit logging. Access denied without proper justification."
          );
        }
        if (!input.legalBasis) {
          throw new Error(
            "Legal basis is required for audit logging. Access denied without legal justification."
          );
        }
        const id = uuidv47();
        const hash3 = this.calculateHash({
          id,
          tenantId: input.tenantId,
          caseId: input.caseId,
          userId: input.userId,
          action: input.action,
          reason: input.reason,
          legalBasis: input.legalBasis,
          timestamp: /* @__PURE__ */ new Date()
        });
        const previousHash = this.lastHash || null;
        this.lastHash = hash3;
        try {
          const { rows } = await this.pg.query(
            `INSERT INTO maestro.audit_access_logs (
          id, tenant_id, case_id, user_id, action, resource_type, resource_id,
          reason, legal_basis, warrant_id, authority_reference, approval_chain,
          ip_address, user_agent, session_id, request_id, correlation_id,
          hash, previous_hash, metadata
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20)
        RETURNING *`,
            [
              id,
              input.tenantId,
              input.caseId,
              input.userId,
              input.action,
              input.resourceType || null,
              input.resourceId || null,
              input.reason,
              input.legalBasis,
              input.warrantId || null,
              input.authorityReference || null,
              JSON.stringify(input.approvalChain || []),
              input.ipAddress || null,
              input.userAgent || null,
              input.sessionId || null,
              input.requestId || null,
              input.correlationId || null,
              hash3,
              previousHash,
              JSON.stringify(input.metadata || {})
            ]
          );
          const log6 = this.mapRow(rows[0]);
          repoLogger2.info(
            {
              logId: id,
              tenantId: input.tenantId,
              caseId: input.caseId,
              userId: input.userId,
              action: input.action,
              legalBasis: input.legalBasis
            },
            "Access logged to audit trail"
          );
          return log6;
        } catch (error) {
          repoLogger2.error(
            {
              error: error.message,
              input
            },
            "Failed to log access"
          );
          throw error;
        }
      }
      /**
       * Query audit logs with advanced filtering
       */
      async query(query3) {
        const params = [query3.tenantId];
        let sql = `SELECT * FROM maestro.audit_access_logs WHERE tenant_id = $1`;
        let paramIndex = 2;
        if (query3.caseId) {
          sql += ` AND case_id = $${paramIndex}`;
          params.push(query3.caseId);
          paramIndex++;
        }
        if (query3.userId) {
          sql += ` AND user_id = $${paramIndex}`;
          params.push(query3.userId);
          paramIndex++;
        }
        if (query3.action) {
          sql += ` AND action = $${paramIndex}`;
          params.push(query3.action);
          paramIndex++;
        }
        if (query3.legalBasis) {
          sql += ` AND legal_basis = $${paramIndex}`;
          params.push(query3.legalBasis);
          paramIndex++;
        }
        if (query3.startTime) {
          sql += ` AND created_at >= $${paramIndex}`;
          params.push(query3.startTime);
          paramIndex++;
        }
        if (query3.endTime) {
          sql += ` AND created_at <= $${paramIndex}`;
          params.push(query3.endTime);
          paramIndex++;
        }
        if (query3.warrantId) {
          sql += ` AND warrant_id = $${paramIndex}`;
          params.push(query3.warrantId);
          paramIndex++;
        }
        if (query3.correlationId) {
          sql += ` AND correlation_id = $${paramIndex}`;
          params.push(query3.correlationId);
          paramIndex++;
        }
        sql += ` ORDER BY created_at DESC`;
        if (query3.limit) {
          sql += ` LIMIT $${paramIndex}`;
          params.push(Math.min(query3.limit, 1e4));
          paramIndex++;
        }
        if (query3.offset) {
          sql += ` OFFSET $${paramIndex}`;
          params.push(query3.offset);
          paramIndex++;
        }
        const { rows } = await this.pg.query(sql, params);
        return rows.map(this.mapRow);
      }
      /**
       * Get audit logs for a specific case
       */
      async getLogsForCase(caseId, tenantId, limit = 100) {
        return this.query({ tenantId, caseId, limit });
      }
      /**
       * Get audit logs for a specific user
       */
      async getLogsForUser(userId, tenantId, limit = 100) {
        return this.query({ tenantId, userId, limit });
      }
      /**
       * Get audit logs by correlation ID (for tracking related operations)
       */
      async getLogsByCorrelationId(correlationId, tenantId) {
        return this.query({ tenantId, correlationId });
      }
      /**
       * Count audit logs with filters
       */
      async count(query3) {
        const params = [query3.tenantId];
        let sql = `SELECT COUNT(*) as count FROM maestro.audit_access_logs WHERE tenant_id = $1`;
        let paramIndex = 2;
        if (query3.caseId) {
          sql += ` AND case_id = $${paramIndex}`;
          params.push(query3.caseId);
          paramIndex++;
        }
        if (query3.userId) {
          sql += ` AND user_id = $${paramIndex}`;
          params.push(query3.userId);
          paramIndex++;
        }
        if (query3.action) {
          sql += ` AND action = $${paramIndex}`;
          params.push(query3.action);
          paramIndex++;
        }
        if (query3.legalBasis) {
          sql += ` AND legal_basis = $${paramIndex}`;
          params.push(query3.legalBasis);
          paramIndex++;
        }
        if (query3.startTime) {
          sql += ` AND created_at >= $${paramIndex}`;
          params.push(query3.startTime);
          paramIndex++;
        }
        if (query3.endTime) {
          sql += ` AND created_at <= $${paramIndex}`;
          params.push(query3.endTime);
          paramIndex++;
        }
        if (query3.warrantId) {
          sql += ` AND warrant_id = $${paramIndex}`;
          params.push(query3.warrantId);
          paramIndex++;
        }
        if (query3.correlationId) {
          sql += ` AND correlation_id = $${paramIndex}`;
          params.push(query3.correlationId);
          paramIndex++;
        }
        const { rows } = await this.pg.query(sql, params);
        return parseInt(rows[0]?.count || "0", 10);
      }
      /**
       * Verify audit trail integrity
       */
      async verifyIntegrity(tenantId, startDate, endDate) {
        const logs = await this.query({
          tenantId,
          startTime: startDate,
          endTime: endDate,
          limit: 1e4
        });
        let validLogs = 0;
        const invalidLogs = [];
        let expectedPreviousHash = void 0;
        for (const log6 of logs.reverse()) {
          const calculatedHash = this.calculateHash({
            id: log6.id,
            tenantId: log6.tenantId,
            caseId: log6.caseId,
            userId: log6.userId,
            action: log6.action,
            reason: log6.reason,
            legalBasis: log6.legalBasis,
            timestamp: log6.createdAt
          });
          if (log6.hash !== calculatedHash) {
            invalidLogs.push({
              id: log6.id,
              issue: "Hash mismatch - possible tampering"
            });
            continue;
          }
          if (expectedPreviousHash !== void 0 && log6.previousHash !== expectedPreviousHash) {
            invalidLogs.push({
              id: log6.id,
              issue: "Chain integrity violation"
            });
          }
          expectedPreviousHash = log6.hash;
          validLogs++;
        }
        const result2 = {
          valid: invalidLogs.length === 0,
          totalLogs: logs.length,
          validLogs,
          invalidLogs
        };
        repoLogger2.info(result2, "Audit trail integrity verification completed");
        return result2;
      }
      /**
       * Calculate hash for integrity verification
       */
      calculateHash(data) {
        const hashableData = {
          id: data.id,
          tenantId: data.tenantId,
          caseId: data.caseId,
          userId: data.userId,
          action: data.action,
          reason: data.reason,
          legalBasis: data.legalBasis,
          timestamp: data.timestamp.toISOString()
        };
        return createHash8("sha256").update(JSON.stringify(hashableData, Object.keys(hashableData).sort())).digest("hex");
      }
      /**
       * Map database row to domain object
       */
      mapRow(row) {
        return {
          id: row.id,
          tenantId: row.tenant_id,
          caseId: row.case_id,
          userId: row.user_id,
          action: row.action,
          resourceType: row.resource_type || void 0,
          resourceId: row.resource_id || void 0,
          reason: row.reason,
          legalBasis: row.legal_basis,
          warrantId: row.warrant_id || void 0,
          authorityReference: row.authority_reference || void 0,
          approvalChain: row.approval_chain || [],
          ipAddress: row.ip_address || void 0,
          userAgent: row.user_agent || void 0,
          sessionId: row.session_id || void 0,
          requestId: row.request_id || void 0,
          correlationId: row.correlation_id || void 0,
          createdAt: row.created_at,
          hash: row.hash || void 0,
          previousHash: row.previous_hash || void 0,
          metadata: row.metadata || {}
        };
      }
    };
  }
});

// src/cases/ReleaseCriteriaService.ts
var ReleaseCriteriaService_exports = {};
__export(ReleaseCriteriaService_exports, {
  ReleaseCriteriaService: () => ReleaseCriteriaService
});
var serviceLogger, ReleaseCriteriaService;
var init_ReleaseCriteriaService = __esm({
  "src/cases/ReleaseCriteriaService.ts"() {
    "use strict";
    init_CaseRepo();
    init_logger();
    serviceLogger = logger_default.child({ name: "ReleaseCriteriaService" });
    ReleaseCriteriaService = class {
      caseRepo;
      constructor(pg5) {
        this.caseRepo = new CaseRepo(pg5);
      }
      /**
       * Configure release criteria for a case
       */
      async configure(caseId, tenantId, userId, config9) {
        const caseRecord = await this.caseRepo.findById(caseId, tenantId);
        if (!caseRecord) {
          throw new Error("Case not found");
        }
        const metadata = caseRecord.metadata || {};
        metadata.releaseCriteria = config9;
        await this.caseRepo.update({
          id: caseId,
          metadata
        }, userId);
        serviceLogger.info({ caseId, config: config9 }, "Release criteria configured");
      }
      /**
       * Evaluate release criteria for a case
       */
      async evaluate(caseId, tenantId) {
        const caseRecord = await this.caseRepo.findById(caseId, tenantId);
        if (!caseRecord) {
          throw new Error("Case not found");
        }
        const config9 = caseRecord.metadata?.releaseCriteria || {};
        const reasons = [];
        const effectiveConfig = {
          hardBlock: false,
          // Default to informational
          ...config9
        };
        if (effectiveConfig.citationCoveragePercent !== void 0) {
          const currentCoverage = caseRecord.metadata?.citationCoverage || 0;
          if (currentCoverage < effectiveConfig.citationCoveragePercent) {
            reasons.push({
              code: "CITATION_COVERAGE_LOW",
              message: `Citation coverage ${currentCoverage}% is below required ${effectiveConfig.citationCoveragePercent}%`,
              remediation: "Add more citations to evidence."
            });
          }
        }
        if (effectiveConfig.locatorValidityPercent !== void 0) {
          const currentValidity = caseRecord.metadata?.locatorValidity || 100;
          if (currentValidity < effectiveConfig.locatorValidityPercent) {
            reasons.push({
              code: "LOCATOR_VALIDITY_LOW",
              message: `Locator validity ${currentValidity}% is below required ${effectiveConfig.locatorValidityPercent}%`,
              remediation: "Fix broken or invalid locators."
            });
          }
        }
        if (effectiveConfig.noOpenConflicts) {
          const openConflicts = caseRecord.metadata?.openConflictsCount || 0;
          if (openConflicts > 0) {
            reasons.push({
              code: "OPEN_CONFLICTS",
              message: `There are ${openConflicts} open conflicts`,
              remediation: "Resolve all conflicts before release."
            });
          }
        }
        if (effectiveConfig.minCompletenessScore !== void 0) {
          const score = caseRecord.metadata?.completenessScore || 0;
          if (score < effectiveConfig.minCompletenessScore) {
            reasons.push({
              code: "COMPLETENESS_LOW",
              message: `Completeness score ${score} is below required ${effectiveConfig.minCompletenessScore}`,
              remediation: "Complete more sections of the case."
            });
          }
        }
        if (effectiveConfig.policyVersionPinned) {
          const isPinned = caseRecord.metadata?.policyVersionPinned === true;
          if (!isPinned) {
            reasons.push({
              code: "POLICY_NOT_PINNED",
              message: "Policy version is not pinned",
              remediation: "Pin the policy version in case settings."
            });
          }
        }
        if (effectiveConfig.approvalsSatisfied) {
          const approvals = caseRecord.metadata?.approvals || [];
          const hasApproval = approvals.length > 0;
          if (!hasApproval) {
            reasons.push({
              code: "APPROVALS_MISSING",
              message: "Required approvals are missing",
              remediation: "Request and obtain necessary approvals."
            });
          }
        }
        const passed = reasons.length === 0;
        serviceLogger.info({ caseId, passed, reasonsCount: reasons.length }, "Release criteria evaluated");
        return {
          passed,
          reasons,
          config: effectiveConfig
        };
      }
    };
  }
});

// src/cases/sla/CaseSLAService.ts
import { randomUUID as randomUUID4 } from "crypto";
var CaseSLAService;
var init_CaseSLAService = __esm({
  "src/cases/sla/CaseSLAService.ts"() {
    "use strict";
    CaseSLAService = class {
      constructor(pool4) {
        this.pool = pool4;
      }
      tableName = "maestro.case_sla_timers";
      /**
       * Create and start a new SLA timer for a case
       */
      async createTimer(input) {
        const slaId = randomUUID4();
        const startTime = /* @__PURE__ */ new Date();
        const deadline = new Date(startTime.getTime() + input.targetDurationSeconds * 1e3);
        const query3 = `
      INSERT INTO ${this.tableName} (
        sla_id, case_id, tenant_id, type, name,
        start_time, deadline, status,
        target_duration_seconds, metadata
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
      RETURNING *
    `;
        const values = [
          slaId,
          input.caseId,
          input.tenantId,
          input.type,
          input.name,
          startTime.toISOString(),
          deadline.toISOString(),
          "ACTIVE",
          input.targetDurationSeconds,
          JSON.stringify(input.metadata || {})
        ];
        const result2 = await this.pool.query(query3, values);
        return this.mapRow(result2.rows[0]);
      }
      /**
       * Complete an SLA timer (mark as met)
       */
      async completeTimer(slaId) {
        const now = /* @__PURE__ */ new Date();
        const query3 = `
      UPDATE ${this.tableName}
      SET status = CASE
        WHEN status = 'BREACHED' THEN 'BREACHED' -- Sticky breach
        WHEN deadline < $2 THEN 'BREACHED' -- Late completion
        ELSE 'COMPLETED'
      END,
      completed_at = $2
      WHERE sla_id = $1
      RETURNING *
    `;
        const result2 = await this.pool.query(query3, [slaId, now.toISOString()]);
        if (result2.rows.length === 0) throw new Error(`SLA Timer ${slaId} not found`);
        return this.mapRow(result2.rows[0]);
      }
      /**
       * Check for breached timers and update their status
       * Should be run periodically (e.g. every minute)
       */
      async checkBreaches() {
        const now = (/* @__PURE__ */ new Date()).toISOString();
        const query3 = `
      UPDATE ${this.tableName}
      SET status = 'BREACHED'
      WHERE status = 'ACTIVE'
      AND deadline < $1
    `;
        const result2 = await this.pool.query(query3, [now]);
        return result2.rowCount || 0;
      }
      /**
       * Get all timers for a case
       */
      async getTimersForCase(caseId) {
        const query3 = `
      SELECT * FROM ${this.tableName}
      WHERE case_id = $1
      ORDER BY deadline ASC
    `;
        const result2 = await this.pool.query(query3, [caseId]);
        return result2.rows.map(this.mapRow);
      }
      mapRow(row) {
        return {
          slaId: row.sla_id,
          caseId: row.case_id,
          tenantId: row.tenant_id,
          type: row.type,
          name: row.name,
          startTime: row.start_time.toISOString(),
          deadline: row.deadline.toISOString(),
          completedAt: row.completed_at ? row.completed_at.toISOString() : void 0,
          status: row.status,
          targetDurationSeconds: row.target_duration_seconds,
          metadata: row.metadata || {}
        };
      }
    };
  }
});

// src/lib/featureFlags.ts
import crypto15 from "crypto";
function get(flagName, context4) {
  const definition = FLAG_CATALOG[flagName];
  if (!definition) {
    return void 0;
  }
  const override = envOverrides[flagName];
  if (definition.type === "boolean") {
    const overrideValue = coerceBoolean(override);
    const value = overrideValue ?? definition.defaultValue;
    return {
      name: flagName,
      type: definition.type,
      value,
      source: overrideValue !== void 0 ? "override" : "default"
    };
  }
  if (definition.type === "percentage") {
    const rollout = clampRollout(override, definition.rollout);
    const identifier = getIdentifier(context4);
    const bucket = getBucket(flagName, identifier);
    const enabled = rollout > 0 && bucket < rollout;
    return {
      name: flagName,
      type: definition.type,
      value: enabled,
      rollout,
      source: override !== void 0 ? "override" : "default"
    };
  }
  const variant = selectVariant(flagName, definition, override, context4);
  return {
    name: flagName,
    type: definition.type,
    value: variant,
    source: override !== void 0 ? "override" : "default"
  };
}
function isEnabled(flagName, context4) {
  const evaluation = get(flagName, context4);
  if (!evaluation) {
    return false;
  }
  if (evaluation.type === "variant") {
    return evaluation.value !== "control";
  }
  return Boolean(evaluation.value);
}
function getVariant(flagName, context4) {
  const evaluation = get(flagName, context4);
  if (!evaluation || evaluation.type !== "variant") {
    return void 0;
  }
  return evaluation.value;
}
function parseOverrideString(rawValue, nodeEnv = "development") {
  if (!rawValue || nodeEnv === "production") {
    return {};
  }
  return rawValue.split(",").reduce(
    (acc, token) => {
      const [key, value] = token.split("=").map((part) => part.trim());
      if (!key || value === void 0) {
        return acc;
      }
      if (value.endsWith("%")) {
        const percentValue = Number.parseInt(value.slice(0, -1), 10);
        if (!Number.isNaN(percentValue)) {
          acc[key] = percentValue;
        }
        return acc;
      }
      if (value.toLowerCase() === "true") {
        acc[key] = true;
        return acc;
      }
      if (value.toLowerCase() === "false") {
        acc[key] = false;
        return acc;
      }
      const numeric = Number(value);
      if (!Number.isNaN(numeric)) {
        acc[key] = numeric;
        return acc;
      }
      acc[key] = value;
      return acc;
    },
    {}
  );
}
function coerceBoolean(value) {
  if (typeof value === "boolean") {
    return value;
  }
  if (typeof value === "number") {
    return value > 0;
  }
  if (typeof value === "string") {
    const normalized = value.toLowerCase();
    if (normalized === "true") {
      return true;
    }
    if (normalized === "false") {
      return false;
    }
  }
  return void 0;
}
function clampRollout(override, fallback) {
  const base = typeof override === "number" ? override : fallback;
  if (Number.isNaN(base)) {
    return 0;
  }
  return Math.min(100, Math.max(0, base));
}
function getIdentifier(context4) {
  return context4?.userId || context4?.tenantId || context4?.sessionId || "anonymous";
}
function getBucket(flagName, identifier) {
  const hash3 = crypto15.createHash("sha1").update(`${flagName}:${identifier}`).digest("hex").slice(0, 8);
  return parseInt(hash3, 16) % 100;
}
function selectVariant(flagName, definition, override, context4) {
  if (typeof override === "string" && definition.variants.includes(override)) {
    return override;
  }
  const variants = definition.variants.length ? definition.variants : [definition.defaultValue];
  if (variants.length === 1) {
    return variants[0];
  }
  const identifier = getIdentifier(context4);
  const bucket = getBucket(flagName, identifier);
  const index = bucket % variants.length;
  return variants[index];
}
var FLAG_CATALOG, envOverrides;
var init_featureFlags2 = __esm({
  "src/lib/featureFlags.ts"() {
    "use strict";
    FLAG_CATALOG = {
      "graph-query-optimizer": {
        type: "boolean",
        defaultValue: false,
        description: "Enables the experimental IntelGraph query optimization pipeline."
      },
      "ai-orchestrator-v2": {
        type: "percentage",
        defaultValue: false,
        rollout: 0,
        description: "Progressively enables the new AI/LLM orchestration strategy."
      },
      "cache-strategy": {
        type: "variant",
        defaultValue: "control",
        variants: ["control", "aggressive-cache"],
        description: "Chooses which caching strategy to use for read-heavy paths."
      },
      "ui-insights-panel": {
        type: "variant",
        defaultValue: "control",
        variants: ["control", "insights-v2"],
        description: "Controls the rollout of the updated insights UI panel."
      },
      "release-criteria": {
        type: "boolean",
        defaultValue: false,
        description: "Enables the release criteria engine for case exports."
      },
      "support.impersonation": {
        type: "boolean",
        defaultValue: false,
        description: "Enables policy-gated support impersonation flows."
      },
      "support.healthBundle": {
        type: "boolean",
        defaultValue: false,
        description: "Enables export of tenant health bundles with redaction."
      },
      "SUSPICIOUS_DETECT_ENABLED": {
        type: "boolean",
        defaultValue: false,
        description: "Enables detection and auditing of suspicious payloads in receipt ingestion."
      },
      "release-readiness-dashboard": {
        type: "boolean",
        defaultValue: true,
        description: "Enables the Release Readiness & Evidence Explorer dashboard for GA verification."
      }
    };
    envOverrides = parseOverrideString(
      process.env.FEATURE_FLAGS,
      process.env.NODE_ENV
    );
  }
});

// src/lib/errors.ts
import { randomUUID as randomUUID5 } from "node:crypto";
import { default as pino22 } from "pino";
function mapGraphRAGError(error) {
  const traceId = randomUUID5();
  let summary = "Unknown error";
  if (error && typeof error === "object" && "issues" in error && Array.isArray(error.issues)) {
    summary = error.issues.map((i) => `${i.path.join(".")}: ${i.message}`).join("; ");
  } else if (error instanceof Error) {
    summary = error.message;
  }
  logger20.warn(
    { traceId, issues: summary },
    "GraphRAG schema validation failed"
  );
  return new UserFacingError(
    `Invalid GraphRAG response. Trace ID: ${traceId}`,
    400,
    traceId
  );
}
var logger20, UserFacingError, AppError, NotFoundError, DatabaseError;
var init_errors = __esm({
  "src/lib/errors.ts"() {
    "use strict";
    logger20 = pino22({ name: "ErrorMapper" });
    UserFacingError = class extends Error {
      statusCode;
      traceId;
      constructor(message, statusCode, traceId) {
        super(message);
        this.statusCode = statusCode;
        this.traceId = traceId;
      }
    };
    AppError = class extends Error {
      statusCode;
      code;
      constructor(message, statusCode = 500, code) {
        super(message);
        this.statusCode = statusCode;
        this.code = code;
        this.name = "AppError";
      }
    };
    NotFoundError = class extends AppError {
      constructor(message = "Resource not found") {
        super(message, 404, "NOT_FOUND");
        this.name = "NotFoundError";
      }
    };
    DatabaseError = class extends AppError {
      constructor(message = "Database error") {
        super(message, 500, "DATABASE_ERROR");
        this.name = "DatabaseError";
      }
    };
  }
});

// src/cases/CaseService.ts
import { randomUUID as randomUUID6 } from "node:crypto";
var serviceLogger2, CaseService;
var init_CaseService = __esm({
  "src/cases/CaseService.ts"() {
    "use strict";
    init_CaseRepo();
    init_AuditAccessLogRepo();
    init_ReleaseCriteriaService();
    init_CaseSLAService();
    init_featureFlags2();
    init_logger();
    init_errors();
    serviceLogger2 = logger_default.child({ name: "CaseService" });
    CaseService = class {
      caseRepo;
      auditRepo;
      releaseCriteriaService;
      slaService;
      pg;
      constructor(pg5) {
        this.pg = pg5;
        this.caseRepo = new CaseRepo(pg5);
        this.auditRepo = new AuditAccessLogRepo(pg5);
        this.releaseCriteriaService = new ReleaseCriteriaService(pg5);
        this.slaService = new CaseSLAService(pg5);
      }
      /**
       * Create a new case
       */
      async createCase(input, userId, auditContext) {
        const caseRecord = await this.caseRepo.create(input, userId);
        try {
          await this.slaService.createTimer({
            caseId: caseRecord.id,
            tenantId: input.tenantId,
            type: "RESOLUTION_TIME",
            name: "Standard Resolution SLA",
            targetDurationSeconds: 7 * 24 * 60 * 60,
            // 7 days default
            metadata: { created_by: userId }
          });
        } catch (e) {
          serviceLogger2.error({ err: e, caseId: caseRecord.id }, "Failed to create initial SLA timer");
        }
        await this.auditRepo.logAccess({
          tenantId: input.tenantId,
          caseId: caseRecord.id,
          userId,
          action: "create",
          resourceType: "case",
          resourceId: caseRecord.id,
          reason: auditContext?.reason || "Case created",
          legalBasis: auditContext?.legalBasis || "investigation",
          ...auditContext
        });
        return caseRecord;
      }
      /**
       * Get a case by ID with audit logging
       */
      async getCase(id, tenantId, userId, auditContext) {
        const caseRecord = await this.caseRepo.findById(id, tenantId);
        if (caseRecord) {
          await this.auditRepo.logAccess({
            tenantId,
            caseId: id,
            userId,
            action: "view",
            resourceType: "case",
            resourceId: id,
            ...auditContext
          });
        }
        return caseRecord;
      }
      /**
       * Update a case with audit logging
       */
      async updateCase(input, userId, tenantId, auditContext) {
        const updatedCase = await this.caseRepo.update(input, userId);
        if (updatedCase) {
          await this.auditRepo.logAccess({
            tenantId,
            caseId: input.id,
            userId,
            action: "modify",
            resourceType: "case",
            resourceId: input.id,
            ...auditContext
          });
        }
        return updatedCase;
      }
      /**
       * List cases (no audit logging for list operations by default)
       */
      async listCases(params) {
        return this.caseRepo.list(params);
      }
      /**
       * Archive a case with audit logging
       */
      async archiveCase(id, userId, tenantId, auditContext) {
        const archivedCase = await this.caseRepo.archive(id, userId);
        if (archivedCase) {
          await this.auditRepo.logAccess({
            tenantId,
            caseId: id,
            userId,
            action: "archive",
            resourceType: "case",
            resourceId: id,
            ...auditContext
          });
        }
        return archivedCase;
      }
      /**
       * Export case data with audit logging
       */
      async exportCase(id, tenantId, userId, auditContext) {
        if (isEnabled("release-criteria", { tenantId, userId })) {
          const evaluation = await this.releaseCriteriaService.evaluate(id, tenantId);
          if (!evaluation.passed) {
            if (evaluation.config.hardBlock) {
              serviceLogger2.warn({ caseId: id, reasons: evaluation.reasons }, "Export blocked by release criteria");
              throw new UserFacingError(
                `Export blocked by release criteria: ${evaluation.reasons.map((r) => r.message).join("; ")}`,
                403,
                randomUUID6()
              );
            } else {
              serviceLogger2.info({ caseId: id, reasons: evaluation.reasons }, "Export allowed despite unmet criteria (soft block)");
            }
          }
        }
        const caseRecord = await this.caseRepo.findById(id, tenantId);
        if (caseRecord) {
          await this.auditRepo.logAccess({
            tenantId,
            caseId: id,
            userId,
            action: "export",
            resourceType: "case",
            resourceId: id,
            ...auditContext
          });
        }
        return caseRecord;
      }
      /**
       * Get case repository (for advanced operations)
       */
      getCaseRepo() {
        return this.caseRepo;
      }
      /**
       * Get audit repository (for advanced operations)
       */
      getAuditRepo() {
        return this.auditRepo;
      }
    };
  }
});

// src/graphql/resolvers/cases.ts
var pg2, caseService, caseResolvers;
var init_cases = __esm({
  "src/graphql/resolvers/cases.ts"() {
    "use strict";
    init_CaseService();
    init_postgres();
    init_auth2();
    pg2 = getPostgresPool();
    caseService = new CaseService(pg2);
    caseResolvers = {
      Query: {
        case: authGuard(async (_2, { id, reason, legalBasis }, context4) => {
          const tenantId = context4.user.tenantId;
          const userId = context4.user.id;
          return caseService.getCase(id, tenantId, userId, { reason, legalBasis });
        }),
        cases: authGuard(async (_2, { status, compartment, limit, offset }, context4) => {
          const tenantId = context4.user.tenantId;
          const normalizedStatus = status;
          return caseService.listCases({ tenantId, status: normalizedStatus, compartment, limit, offset });
        })
      },
      Mutation: {
        createCase: authGuard(async (_2, { input }, context4) => {
          const tenantId = context4.user.tenantId;
          const userId = context4.user.id;
          return caseService.createCase({ ...input, tenantId }, userId, {
            reason: input.reason || "Initial creation",
            legalBasis: input.legalBasis || "investigation"
          });
        }),
        updateCase: authGuard(async (_2, { input }, context4) => {
          const tenantId = context4.user.tenantId;
          const userId = context4.user.id;
          return caseService.updateCase(input, userId, tenantId, {
            reason: input.reason || "Updated case details",
            legalBasis: input.legalBasis || "investigation"
          });
        }),
        archiveCase: authGuard(async (_2, { id, reason, legalBasis }, context4) => {
          const tenantId = context4.user.tenantId;
          const userId = context4.user.id;
          return caseService.archiveCase(id, userId, tenantId, { reason, legalBasis });
        })
      },
      Case: {
        slaTimers: async (caseRecord) => {
          const slaService = caseService.slaService;
          return slaService.getTimersForCase(caseRecord.id);
        },
        comments: async (caseRecord, { limit, offset }, context4) => {
          return [];
        }
      }
    };
  }
});

// src/cases/comments/CommentService.ts
import { randomUUID as randomUUID7 } from "node:crypto";
var CommentService;
var init_CommentService = __esm({
  "src/cases/comments/CommentService.ts"() {
    "use strict";
    CommentService = class {
      constructor(pool4) {
        this.pool = pool4;
      }
      tableName = "maestro.comments";
      /**
       * Add a new comment to a target
       */
      async addComment(input) {
        const commentId = randomUUID7();
        let rootId = commentId;
        if (input.parentId) {
          const parent = await this.getComment(input.parentId);
          if (parent) {
            rootId = parent.rootId || parent.commentId;
          }
        }
        const query3 = `
      INSERT INTO ${this.tableName} (
        comment_id, tenant_id, target_type, target_id,
        parent_id, root_id, content, author_id,
        mentions, metadata
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
      RETURNING *
    `;
        const values = [
          commentId,
          input.tenantId,
          input.targetType,
          input.targetId,
          input.parentId || null,
          rootId,
          input.content,
          input.authorId,
          input.mentions || [],
          JSON.stringify(input.metadata || {})
        ];
        const { rows } = await this.pool.query(query3, values);
        return this.mapRow(rows[0]);
      }
      /**
       * Get a single comment by ID
       */
      async getComment(commentId) {
        const { rows } = await this.pool.query(
          `SELECT * FROM ${this.tableName} WHERE comment_id = $1`,
          [commentId]
        );
        return rows[0] ? this.mapRow(rows[0]) : null;
      }
      /**
       * List comments for a target
       */
      async listComments(params) {
        const { targetType, targetId, tenantId, limit = 50, offset = 0 } = params;
        const query3 = `
      SELECT * FROM ${this.tableName}
      WHERE target_type = $1 AND target_id = $2 AND tenant_id = $3 AND is_deleted = FALSE
      ORDER BY created_at ASC
      LIMIT $4 OFFSET $5
    `;
        const { rows } = await this.pool.query(query3, [
          targetType,
          targetId,
          tenantId,
          limit,
          offset
        ]);
        return rows.map(this.mapRow);
      }
      /**
       * Update a comment
       */
      async updateComment(commentId, content, userId) {
        const query3 = `
      UPDATE ${this.tableName}
      SET content = $2, is_edited = TRUE, updated_at = NOW()
      WHERE comment_id = $1 AND author_id = $3
      RETURNING *
    `;
        const { rows } = await this.pool.query(query3, [commentId, content, userId]);
        return rows[0] ? this.mapRow(rows[0]) : null;
      }
      /**
       * Soft delete a comment
       */
      async deleteComment(commentId, userId) {
        const query3 = `
      UPDATE ${this.tableName}
      SET is_deleted = TRUE, updated_at = NOW()
      WHERE comment_id = $1 AND author_id = $2
    `;
        const { rowCount } = await this.pool.query(query3, [commentId, userId]);
        return (rowCount || 0) > 0;
      }
      mapRow(row) {
        return {
          commentId: row.comment_id,
          tenantId: row.tenant_id,
          targetType: row.target_type,
          targetId: row.target_id,
          parentId: row.parent_id,
          rootId: row.root_id,
          content: row.content,
          authorId: row.author_id,
          createdAt: row.created_at,
          updatedAt: row.updated_at,
          mentions: row.mentions || [],
          isEdited: row.is_edited,
          isDeleted: row.is_deleted,
          metadata: row.metadata || {}
        };
      }
    };
  }
});

// src/graphql/resolvers/comments.ts
var pg3, commentService, commentResolvers;
var init_comments = __esm({
  "src/graphql/resolvers/comments.ts"() {
    "use strict";
    init_CommentService();
    init_postgres();
    init_auth2();
    pg3 = getPostgresPool();
    commentService = new CommentService(pg3);
    commentResolvers = {
      Query: {
        comments: authGuard(async (_2, { targetType, targetId, limit, offset }, context4) => {
          const tenantId = context4.user.tenantId;
          return commentService.listComments({ targetType, targetId, tenantId, limit, offset });
        })
      },
      Mutation: {
        addComment: authGuard(async (_2, { input }, context4) => {
          const tenantId = context4.user.tenantId;
          const authorId = context4.user.id;
          return commentService.addComment({
            ...input,
            tenantId,
            authorId
          });
        }),
        updateComment: authGuard(async (_2, { id, content }, context4) => {
          const userId = context4.user.id;
          return commentService.updateComment(id, content, userId);
        }),
        deleteComment: authGuard(async (_2, { id }, context4) => {
          const userId = context4.user.id;
          return commentService.deleteComment(id, userId);
        })
      },
      Comment: {
        author: async (comment) => {
          return { id: comment.authorId, name: "Author Name" };
        }
      }
    };
  }
});

// src/cognitive-security/types.ts
var init_types2 = __esm({
  "src/cognitive-security/types.ts"() {
    "use strict";
  }
});

// src/cognitive-security/provenance.service.ts
import pino23 from "pino";
function getProvenanceService() {
  if (!serviceInstance) {
    throw new Error("Provenance service not initialized");
  }
  return serviceInstance;
}
var logger21, serviceInstance;
var init_provenance_service2 = __esm({
  "src/cognitive-security/provenance.service.ts"() {
    "use strict";
    logger21 = pino23({ name: "provenance-service" });
    serviceInstance = null;
  }
});

// src/cognitive-security/claims.service.ts
import pino24 from "pino";
function getClaimsService() {
  if (!serviceInstance2) {
    throw new Error("Claims service not initialized");
  }
  return serviceInstance2;
}
var logger22, serviceInstance2;
var init_claims_service = __esm({
  "src/cognitive-security/claims.service.ts"() {
    "use strict";
    init_types2();
    logger22 = pino24({ name: "claims-service" });
    serviceInstance2 = null;
  }
});

// src/cognitive-security/campaign-detection.service.ts
import pino25 from "pino";
function getCampaignDetectionService() {
  if (!serviceInstance3) {
    throw new Error("Campaign detection service not initialized");
  }
  return serviceInstance3;
}
var logger23, serviceInstance3;
var init_campaign_detection_service = __esm({
  "src/cognitive-security/campaign-detection.service.ts"() {
    "use strict";
    init_types2();
    logger23 = pino25({ name: "campaign-detection-service" });
    serviceInstance3 = null;
  }
});

// src/cognitive-security/response-ops.service.ts
import pino26 from "pino";
function getResponseOpsService() {
  if (!serviceInstance4) {
    throw new Error("Response ops service not initialized");
  }
  return serviceInstance4;
}
var logger24, serviceInstance4;
var init_response_ops_service = __esm({
  "src/cognitive-security/response-ops.service.ts"() {
    "use strict";
    logger24 = pino26({ name: "response-ops-service" });
    serviceInstance4 = null;
  }
});

// src/cognitive-security/governance.service.ts
import pino27 from "pino";
function getGovernanceService() {
  if (!serviceInstance5) {
    throw new Error("Governance service not initialized");
  }
  return serviceInstance5;
}
var logger25, serviceInstance5;
var init_governance_service = __esm({
  "src/cognitive-security/governance.service.ts"() {
    "use strict";
    logger25 = pino27({ name: "cogsec-governance-service" });
    serviceInstance5 = null;
  }
});

// src/cognitive-security/evaluation.service.ts
import pino28 from "pino";
function getEvaluationService() {
  if (!serviceInstance6) {
    throw new Error("Evaluation service not initialized");
  }
  return serviceInstance6;
}
var logger26, serviceInstance6;
var init_evaluation_service = __esm({
  "src/cognitive-security/evaluation.service.ts"() {
    "use strict";
    logger26 = pino28({ name: "cogsec-evaluation-service" });
    serviceInstance6 = null;
  }
});

// src/services/CognitiveStateService.ts
var CognitiveStateService;
var init_CognitiveStateService = __esm({
  "src/services/CognitiveStateService.ts"() {
    "use strict";
    init_database();
    init_logger2();
    CognitiveStateService = class _CognitiveStateService {
      driver;
      static instance;
      constructor() {
        this.driver = getNeo4jDriver2();
      }
      static getInstance() {
        if (!_CognitiveStateService.instance) {
          _CognitiveStateService.instance = new _CognitiveStateService();
        }
        return _CognitiveStateService.instance;
      }
      /**
       * Tracks/Updates the cognitive state of an audience segment based on new narrative ingestion.
       */
      async updateSegmentState(segmentId, narrativeId, strength, certainty, tenantId) {
        const session = this.driver.session();
        try {
          const query3 = `
        MATCH (s:AudienceSegment {id: $segmentId})
        ${tenantId ? "WHERE s.tenantId = $tenantId" : ""}
        MERGE (n:Narrative {id: $narrativeId})
        MERGE (s)-[r:ADOPTS]->(n)
        SET r.strength = $strength,
            r.certainty = $certainty,
            r.updatedAt = datetime()
        
        // Recalculate segment resilience based on belief entropy
        WITH s
        MATCH (s)-[rel:ADOPTS]->()
        WITH s, avg(rel.strength) as avgBelief, count(rel) as beliefCount
        SET s.resilienceScore = CASE 
            WHEN beliefCount > 5 THEN 1.0 - (avgBelief * 0.5) // Higher diverse beliefs = more resilient
            ELSE 0.5
          END
      `;
          await session.run(query3, { segmentId, narrativeId, strength, certainty, tenantId });
          logger_default2.info({ segmentId, narrativeId }, "CognitiveStateService: Updated segment state");
        } catch (error) {
          logger_default2.error({ segmentId, error: error.message }, "CognitiveStateService: Failed to update segment");
          throw error;
        } finally {
          await session.close();
        }
      }
      /**
       * Retrieves the current cognitive state for a segment.
       */
      async getSegmentState(segmentId, tenantId) {
        const session = this.driver.session();
        try {
          const query3 = `
        MATCH (s:AudienceSegment {id: $segmentId})
        ${tenantId ? "WHERE s.tenantId = $tenantId" : ""}
        OPTIONAL MATCH (s)-[r:ADOPTS]->(n:Narrative)
        RETURN s.id as id, s.name as name, s.description as description, s.size as size,
               s.resilienceScore as resilienceScore, s.trustInInstitutions as trustInInstitutions,
               s.polarizationIndex as polarizationIndex, s.fearSensitivity as fearSensitivity,
               s.identityClusters as identityClusters, s.createdAt as createdAt, s.updatedAt as updatedAt,
               collect(n.id) as narrativeIds
      `;
          const result2 = await session.run(query3, { segmentId, tenantId });
          if (result2.records.length === 0) return null;
          const record2 = result2.records[0];
          return {
            id: record2.get("id"),
            name: record2.get("name") || record2.get("label") || "Unknown",
            description: record2.get("description") || "",
            size: record2.get("size")?.toNumber() || 0,
            resilienceScore: record2.get("resilienceScore") || 0.5,
            regions: [],
            // Placeholder
            vulnerabilityFactors: [],
            // Placeholder
            narrativeIds: record2.get("narrativeIds").filter((id) => id !== null),
            trustInInstitutions: record2.get("trustInInstitutions") || 0.5,
            polarizationIndex: record2.get("polarizationIndex") || 0.5,
            fearSensitivity: record2.get("fearSensitivity") || 0.5,
            identityClusters: record2.get("identityClusters") || [],
            createdAt: record2.get("createdAt") || (/* @__PURE__ */ new Date()).toISOString(),
            updatedAt: record2.get("updatedAt") || (/* @__PURE__ */ new Date()).toISOString()
          };
        } finally {
          await session.close();
        }
      }
    };
  }
});

// src/services/CascadeDetectionService.ts
var CascadeDetectionService;
var init_CascadeDetectionService = __esm({
  "src/services/CascadeDetectionService.ts"() {
    "use strict";
    init_database();
    init_logger2();
    CascadeDetectionService = class _CascadeDetectionService {
      driver;
      static instance;
      constructor() {
        this.driver = getNeo4jDriver2();
      }
      static getInstance() {
        if (!_CascadeDetectionService.instance) {
          _CascadeDetectionService.instance = new _CascadeDetectionService();
        }
        return _CascadeDetectionService.instance;
      }
      /**
       * Scans the graph for narrative cascades originating from a specific source.
       */
      async detectCascades(narrativeId, tenantId) {
        const session = this.driver.session();
        try {
          const query3 = `
        MATCH (n:Narrative {id: $narrativeId})
        ${tenantId ? "WHERE n.tenantId = $tenantId" : ""}
        
        // Find the origin actor
        OPTIONAL MATCH (origin:Entity)-[r0:PROMOTES]->(n)
        WITH n, origin, r0.timestamp as originTime
        ORDER BY originTime ASC
        WITH n, head(collect(origin)) as originActor, head(collect(r0)) as originRel
        
        // Find the full propagation path
        MATCH path = (n)<-[:PROMOTES|ADOPTS|SHARES*1..5]-(actor:Entity)
        
        WITH n, originActor, originRel, actor, path, length(path) as hops
        WHERE hops > 0
        
        RETURN 
          $narrativeId as narrativeId,
          originActor.id as originActorId,
          originRel.timestamp as startTime,
          collect(actor.id) as reachNodes,
          count(distinct actor) as reachCount,
          max(hops) as maxDepth,
          avg(hops) as avgHops
      `;
          const result2 = await session.run(query3, { narrativeId, tenantId });
          const cascades = result2.records.map((record2) => ({
            id: `cascade-${narrativeId}-${Date.now()}`,
            narrativeId: record2.get("narrativeId"),
            startTime: record2.get("startTime")?.toString() || (/* @__PURE__ */ new Date()).toISOString(),
            originNodeId: record2.get("originActorId") || "unknown",
            originActorId: record2.get("originActorId"),
            totalHops: Math.floor((record2.get("avgHops") || 0) * (record2.get("reachCount")?.toNumber() || 1)),
            maxDepth: record2.get("maxDepth")?.toNumber() || 0,
            uniqueActors: record2.get("reachCount")?.toNumber() || 0,
            velocity: record2.get("reachCount")?.toNumber() / Math.max(1, record2.get("avgHops")?.toNumber() || 1),
            viralityScore: (record2.get("reachCount")?.toNumber() || 0) / 100,
            hopIds: []
          }));
          return cascades;
        } catch (error) {
          logger_default2.error({ narrativeId, error: error.message }, "CascadeDetectionService: Scans failed");
          throw error;
        } finally {
          await session.close();
        }
      }
      /**
       * Identifies "Hub" nodes that are critical to the cascade propagation.
       */
      async identifyAmplificationHubs(narrativeId, tenantId) {
        const session = this.driver.session();
        try {
          const query3 = `
        MATCH (n:Narrative {id: $narrativeId})
        ${tenantId ? "WHERE n.tenantId = $tenantId" : ""}
        MATCH (actor:Entity)-[r:PROMOTES|SHARES]->(n)
        WITH actor, count(r) as promoCount
        ORDER BY promoCount DESC
        LIMIT 10
        RETURN actor.id as id, actor.label as label, promoCount as score
      `;
          const result2 = await session.run(query3, { narrativeId, tenantId });
          return result2.records.map((r) => ({
            id: r.get("id"),
            label: r.get("label"),
            score: r.get("score").toNumber()
          }));
        } finally {
          await session.close();
        }
      }
    };
  }
});

// src/cognitive-security/index.ts
import pino29 from "pino";
var logger27;
var init_cognitive_security = __esm({
  "src/cognitive-security/index.ts"() {
    "use strict";
    init_types2();
    init_provenance_service2();
    init_claims_service();
    init_campaign_detection_service();
    init_response_ops_service();
    init_governance_service();
    init_evaluation_service();
    init_CognitiveStateService();
    init_CascadeDetectionService();
    init_provenance_service2();
    init_claims_service();
    init_campaign_detection_service();
    init_response_ops_service();
    init_governance_service();
    init_evaluation_service();
    logger27 = pino29({ name: "cognitive-security" });
  }
});

// src/graphql/resolvers/cognitive-security.ts
import { GraphQLError as GraphQLError9 } from "graphql";
var getServices, Query, Mutation, Subscription, CogSecClaim, CogSecCampaign, CogSecIncidentResolvers, VerificationAppealResolvers, AudienceSegmentResolvers, NarrativeCascadeResolvers, NarrativeConflict, cognitiveSecurityResolvers;
var init_cognitive_security2 = __esm({
  "src/graphql/resolvers/cognitive-security.ts"() {
    "use strict";
    init_cognitive_security();
    getServices = () => {
      try {
        return {
          claims: getClaimsService(),
          campaignDetection: getCampaignDetectionService(),
          responseOps: getResponseOpsService(),
          governance: getGovernanceService(),
          evaluation: getEvaluationService(),
          provenance: getProvenanceService(),
          cognitiveState: CognitiveStateService.getInstance(),
          cascadeDetection: CascadeDetectionService.getInstance()
        };
      } catch (error) {
        throw new GraphQLError9("Cognitive Security module not initialized", {
          extensions: { code: "SERVICE_UNAVAILABLE" }
        });
      }
    };
    Query = {
      // Claims
      cogSecClaim: async (_2, { id }) => {
        const { claims } = getServices();
        return claims.getClaim(id);
      },
      cogSecClaims: async (_2, {
        filter,
        limit = 20,
        offset = 0
      }) => {
        const { claims } = getServices();
        return claims.searchClaims(filter?.query || "", filter, limit);
      },
      searchCogSecClaims: async (_2, { query: query3, limit = 20 }) => {
        const { claims } = getServices();
        return claims.searchClaims(query3, void 0, limit);
      },
      similarClaims: async (_2, { claimId, threshold = 0.85 }) => {
        const { claims } = getServices();
        const claim = await claims.getClaim(claimId);
        if (!claim) return [];
        const similar = await claims.findSimilarClaims(claim, threshold);
        return similar.map((s) => s.claim);
      },
      // Evidence
      cogSecEvidence: async (_2, { id }) => {
        return null;
      },
      // Narratives
      cogSecNarrative: async (_2, { id }) => {
        return null;
      },
      narrativeGraph: async (_2, { narrativeId }) => {
        const { claims } = getServices();
        return claims.getNarrativeGraph(narrativeId);
      },
      // Campaigns
      cogSecCampaign: async (_2, { id }) => {
        const { campaignDetection } = getServices();
        return campaignDetection.getCampaign(id);
      },
      activeCampaigns: async (_2, { limit = 20 }) => {
        const { campaignDetection } = getServices();
        return campaignDetection.listActiveCampaigns(limit);
      },
      campaignSignals: async (_2, { campaignId }) => {
        const { campaignDetection } = getServices();
        return campaignDetection.getCampaignSignals(campaignId);
      },
      // Incidents
      cogSecIncident: async (_2, { id }) => {
        return null;
      },
      // Playbooks
      responsePlaybook: async (_2, { id }) => {
        return null;
      },
      // Governance
      cogSecAuditLogs: async (_2, {
        resourceType,
        resourceId,
        limit = 100
      }) => {
        const { governance } = getServices();
        return governance.queryAuditLogs(
          {
            resourceType,
            resourceId
          },
          limit
        );
      },
      pendingAppeals: async (_2, { limit = 50 }) => {
        const { governance } = getServices();
        return governance.getPendingAppeals(limit);
      },
      governancePolicies: async () => {
        const { governance } = getServices();
        return governance.getAllPolicies();
      },
      transparencyReport: async (_2, { startDate, endDate }) => {
        const { governance } = getServices();
        return governance.generateTransparencyReport(startDate, endDate);
      },
      // Metrics
      cogSecMetrics: async (_2, { startDate, endDate }) => {
        const { evaluation } = getServices();
        return evaluation.calculateAllMetrics(startDate, endDate);
      },
      benchmarkComparison: async (_2, { startDate, endDate }) => {
        const { evaluation } = getServices();
        const metrics8 = await evaluation.calculateAllMetrics(startDate, endDate);
        return evaluation.compareToBenchmarks(metrics8);
      },
      riskAssessment: async () => {
        const { evaluation } = getServices();
        return evaluation.generateRiskAssessment();
      },
      // Content Credentials
      contentCredential: async (_2, { id }) => {
        return null;
      },
      // Cognitive Operations
      audienceCognitiveProfile: async (_2, { id }) => {
        const { cognitiveState } = getServices();
        return cognitiveState.getSegmentState(id);
      },
      cognitiveRiskDashboard: async (_2, { filters }) => {
        return {
          averageResilience: 0.65,
          highRiskSegments: 3,
          topThreats: ["OVERLOAD", "POLARIZATION_WEDGE"]
        };
      },
      // Influence Pathways
      influencePathways: async (_2, { narrativeId }) => {
        const { cascadeDetection } = getServices();
        return cascadeDetection.detectCascades(narrativeId);
      },
      narrativeConflicts: async (_2, { narrativeId }) => {
        const { claims } = getServices();
        return claims.detectNarrativeConflicts(narrativeId);
      },
      narrativeEarlyWarnings: async (_2, { watchlistId }) => {
        return [];
      }
    };
    Mutation = {
      // Claims
      extractClaim: async (_2, {
        input
      }, context4) => {
        const { claims, governance } = getServices();
        const claim = await claims.extractClaim(
          input.text,
          input.sourceType,
          input.sourceUrl,
          input.actorId,
          input.channelId
        );
        await governance.logAudit("EXTRACT_CLAIM", "CLAIM", claim.id, context4.user?.id || "system", {
          tenantId: context4.tenantId,
          newState: { sourceType: input.sourceType }
        });
        return claim;
      },
      updateClaimVerdict: async (_2, {
        input
      }, context4) => {
        const { claims, governance } = getServices();
        const claim = await claims.updateVerdict(
          input.claimId,
          input.verdict,
          input.confidence,
          input.evidenceIds,
          context4.user?.id
        );
        await governance.logAudit("UPDATE_VERDICT", "CLAIM", input.claimId, context4.user?.id || "system", {
          tenantId: context4.tenantId,
          newState: { verdict: input.verdict, confidence: input.confidence }
        });
        return claim;
      },
      linkRelatedClaims: async (_2, {
        claimId1,
        claimId2,
        relationType
      }) => {
        const { claims } = getServices();
        await claims.linkRelatedClaims(claimId1, claimId2, relationType);
        return true;
      },
      // Evidence
      createEvidence: async (_2, {
        input
      }) => {
        const { claims } = getServices();
        return claims.createEvidence(input.type, input.title, input.content, {
          sourceUrl: input.sourceUrl,
          sourceCredibility: input.sourceCredibility,
          claimIds: input.claimIds,
          supportsVerdict: input.supportsVerdict
        });
      },
      verifyEvidence: async (_2, { evidenceId, notes }, context4) => {
        const { claims } = getServices();
        return claims.verifyEvidence(evidenceId, context4.user?.id || "system", notes);
      },
      linkEvidenceToClaims: async (_2, { evidenceId, claimIds }) => {
        const { claims } = getServices();
        await claims.linkEvidenceToClaims(evidenceId, claimIds);
        return true;
      },
      // Narratives
      createNarrative: async (_2, {
        input
      }) => {
        const { claims } = getServices();
        return claims.createNarrative(
          input.name,
          input.description,
          input.claimIds,
          input.keywords
        );
      },
      updateNarrativeStatus: async (_2, { narrativeId, status }) => {
        const { claims } = getServices();
        return claims.updateNarrativeStatus(narrativeId, status);
      },
      linkClaimsToNarrative: async (_2, { claimIds, narrativeId }) => {
        const { claims } = getServices();
        await claims.linkClaimsToNarrative(claimIds, narrativeId);
        return true;
      },
      // Campaigns
      runDetectionPipeline: async () => {
        const { campaignDetection } = getServices();
        return campaignDetection.runDetectionPipeline();
      },
      clusterIntoCampaigns: async () => {
        const { campaignDetection } = getServices();
        const signals = await campaignDetection.runDetectionPipeline();
        return campaignDetection.clusterIntoCampaigns(signals);
      },
      updateCampaignStatus: async (_2, { campaignId, status }) => {
        const { campaignDetection } = getServices();
        return campaignDetection.updateCampaignStatus(campaignId, status);
      },
      // Incidents
      createIncident: async (_2, {
        input
      }) => {
        const { responseOps } = getServices();
        return responseOps.createIncident(
          input.campaignId,
          input.name,
          input.description,
          input.leadAnalystId,
          input.severity
        );
      },
      updateIncidentStatus: async (_2, { incidentId, status }, context4) => {
        const { responseOps } = getServices();
        return responseOps.updateIncidentStatus(incidentId, status, context4.user?.id || "system");
      },
      addIncidentTimelineEvent: async (_2, {
        incidentId,
        type,
        description
      }, context4) => {
        const { responseOps } = getServices();
        return responseOps.addTimelineEvent(
          incidentId,
          type,
          description,
          context4.user?.id
        );
      },
      // Playbooks
      generatePlaybook: async (_2, {
        input
      }, context4) => {
        const { responseOps } = getServices();
        return responseOps.generatePlaybook(input.campaignId, context4.user?.id || "system", {
          priority: input.priority,
          assigneeId: input.assigneeId,
          dueAt: input.dueAt
        });
      },
      executePlaybookAction: async (_2, { playbookId, actionId }, context4) => {
        const { responseOps } = getServices();
        return responseOps.executeAction(playbookId, actionId, context4.user?.id || "system");
      },
      updatePlaybookStatus: async (_2, { playbookId, status }) => {
        const { responseOps } = getServices();
        return responseOps.updatePlaybookStatus(playbookId, status);
      },
      // Artifacts
      generateBriefing: async (_2, { campaignId }, context4) => {
        const { responseOps } = getServices();
        return responseOps.generateBriefing(campaignId, context4.user?.id || "system");
      },
      generateStakeholderMessage: async (_2, { campaignId, stakeholder }, context4) => {
        const { responseOps } = getServices();
        return responseOps.generateStakeholderMessage(
          campaignId,
          stakeholder,
          context4.user?.id || "system"
        );
      },
      generateTakedownPacket: async (_2, {
        input
      }, context4) => {
        const { responseOps } = getServices();
        return responseOps.generateTakedownPacket(
          input.campaignId,
          input.platform,
          input.urls,
          input.accountIds,
          input.violationType,
          context4.user?.id || "system",
          {
            legalBasis: input.legalBasis,
            contactInfo: input.contactInfo
          }
        );
      },
      // Appeals
      createAppeal: async (_2, {
        input
      }, context4) => {
        const { governance, claims } = getServices();
        const claim = await claims.getClaim(input.claimId);
        if (!claim) {
          throw new GraphQLError9("Claim not found", {
            extensions: { code: "NOT_FOUND" }
          });
        }
        return governance.createAppeal(
          input.claimId,
          claim.verdict,
          input.requestedVerdict,
          context4.user?.id || "system",
          input.reason,
          input.supportingEvidence
        );
      },
      reviewAppeal: async (_2, {
        appealId,
        decision,
        notes
      }, context4) => {
        const { governance } = getServices();
        return governance.reviewAppeal(
          appealId,
          context4.user?.id || "system",
          decision,
          notes
        );
      },
      // Content Credentials
      createContentCredential: async (_2, {
        assetId,
        mimeType,
        sourceUrl
      }) => {
        const { provenance } = getServices();
        const content = Buffer.from("");
        return provenance.createContentCredential(assetId, content, mimeType, sourceUrl);
      },
      addProvenanceLink: async (_2, {
        credentialId,
        source,
        platform
      }) => {
        return null;
      },
      // Cognitive Operations
      recordCognitiveEffect: async (_2, { exposure }) => {
        const { cognitiveState } = getServices();
        await cognitiveState.updateSegmentState(
          exposure.segmentId,
          exposure.narrativeId,
          exposure.sentimentShift || 0.1,
          // Heuristic strength
          0.8
          // Certainty placeholder
        );
        const segment = await cognitiveState.getSegmentState(exposure.segmentId);
        if (!segment) throw new GraphQLError9("Segment not found after update");
        return {
          id: `state-${exposure.segmentId}-${Date.now()}`,
          segmentId: exposure.segmentId,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          beliefVector: {},
          // In a real app, this would be computed
          resilienceScore: segment.resilienceScore,
          emotionalValence: exposure.sentimentShift || 0,
          arousalLevel: 0.5
        };
      }
    };
    Subscription = {
      campaignDetected: {
        subscribe: () => {
          throw new GraphQLError9("Not implemented");
        }
      },
      coordinationSignalDetected: {
        subscribe: () => {
          throw new GraphQLError9("Not implemented");
        }
      },
      incidentUpdated: {
        subscribe: (_2, { incidentId }) => {
          throw new GraphQLError9("Not implemented");
        }
      },
      claimVerdictUpdated: {
        subscribe: (_2, { narrativeId }) => {
          throw new GraphQLError9("Not implemented");
        }
      },
      playbookActionCompleted: {
        subscribe: (_2, { playbookId }) => {
          throw new GraphQLError9("Not implemented");
        }
      }
    };
    CogSecClaim = {
      evidence: async (claim) => {
        return [];
      },
      relatedClaims: async (claim) => {
        return [];
      },
      narratives: async (claim) => {
        return [];
      },
      actors: async (claim) => {
        return [];
      },
      channels: async (claim) => {
        return [];
      }
    };
    CogSecCampaign = {
      narratives: async (campaign) => {
        return [];
      },
      actors: async (campaign) => {
        return [];
      },
      channels: async (campaign) => {
        return [];
      },
      signals: async (campaign) => {
        const { campaignDetection } = getServices();
        return campaignDetection.getCampaignSignals(campaign.id);
      },
      claims: async (campaign) => {
        return [];
      },
      playbooks: async (campaign) => {
        return [];
      },
      incident: async (campaign) => {
        return null;
      }
    };
    CogSecIncidentResolvers = {
      campaigns: async (incident) => {
        return [];
      },
      playbooks: async (incident) => {
        return [];
      },
      leadAnalyst: async (incident) => {
        return null;
      },
      investigation: async (incident) => {
        return null;
      }
    };
    VerificationAppealResolvers = {
      claim: async (appeal) => {
        const { claims } = getServices();
        return claims.getClaim(appeal.claimId);
      },
      appellant: async (appeal) => {
        return null;
      },
      reviewer: async (appeal) => {
        return null;
      }
    };
    AudienceSegmentResolvers = {
      cognitiveStates: async (segment) => {
        return [{
          id: `current-${segment.id}`,
          segmentId: segment.id,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          beliefVector: {},
          resilienceScore: segment.resilienceScore,
          emotionalValence: 0,
          arousalLevel: 0.5
        }];
      },
      targetedByCampaigns: async (segment) => {
        const { campaignDetection } = getServices();
        const campaigns = await campaignDetection.listActiveCampaigns(10);
        return campaigns.filter((c) => c.targetAudienceIds.includes(segment.id));
      }
    };
    NarrativeCascadeResolvers = {
      narrative: async (cascade) => {
        const { claims } = getServices();
        return claims.getNarrativeGraph(cascade.narrativeId).then((g2) => g2.narrative);
      },
      originActor: async (cascade) => {
        if (!cascade.originActorId) return null;
        const { claims } = getServices();
        return claims.getActor(cascade.originActorId);
      }
    };
    NarrativeConflict = {
      competingNarrative: async (conflict) => {
        const { claims } = getServices();
        return claims.getNarrative(conflict.competingNarrativeId);
      },
      contradictingClaims: async (conflict) => {
        const { claims } = getServices();
        return conflict.contradictingClaimPairs.map(async (pair) => {
          const [c1, c2] = await Promise.all([
            claims.getClaim(pair[0]),
            claims.getClaim(pair[1])
          ]);
          return { claim1: c1, claim2: c2 };
        });
      }
    };
    cognitiveSecurityResolvers = {
      Query,
      Mutation,
      Subscription,
      CogSecClaim,
      CogSecCampaign,
      CogSecIncident: CogSecIncidentResolvers,
      VerificationAppeal: VerificationAppealResolvers,
      AudienceSegment: AudienceSegmentResolvers,
      NarrativeCascade: NarrativeCascadeResolvers,
      NarrativeConflict
    };
  }
});

// src/monitoring/opentelemetry.ts
import { NodeSDK } from "@opentelemetry/sdk-node";
import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
import * as resources from "@opentelemetry/resources";
import { SemanticResourceAttributes as SemanticResourceAttributes2 } from "@opentelemetry/semantic-conventions";
import { JaegerExporter } from "@opentelemetry/exporter-jaeger";
import { PrometheusExporter } from "@opentelemetry/exporter-prometheus";
import { trace as trace3, context, SpanStatusCode as SpanStatusCode2, SpanKind as SpanKind2 } from "@opentelemetry/api";
import pino30 from "pino";
var logger28, OpenTelemetryService, otelService2;
var init_opentelemetry = __esm({
  "src/monitoring/opentelemetry.ts"() {
    "use strict";
    logger28 = pino30({ name: "opentelemetry" });
    OpenTelemetryService = class {
      sdk = null;
      tracer = null;
      config;
      constructor(config9 = {}) {
        this.config = {
          serviceName: config9.serviceName || process.env.OTEL_SERVICE_NAME || "intelgraph-api",
          serviceVersion: config9.serviceVersion || process.env.OTEL_SERVICE_VERSION || "1.0.0",
          environment: config9.environment || process.env.NODE_ENV || "development",
          jaegerEndpoint: config9.jaegerEndpoint || process.env.JAEGER_ENDPOINT,
          enableConsoleExporter: config9.enableConsoleExporter ?? process.env.NODE_ENV === "development",
          sampleRate: config9.sampleRate ?? parseFloat(process.env.OTEL_SAMPLE_RATE || "1.0")
        };
      }
      /**
       * Initialize OpenTelemetry SDK
       */
      initialize() {
        try {
          const resource = resources.Resource.default().merge(
            new resources.Resource({
              [SemanticResourceAttributes2.SERVICE_NAME]: this.config.serviceName,
              [SemanticResourceAttributes2.SERVICE_VERSION]: this.config.serviceVersion,
              [SemanticResourceAttributes2.DEPLOYMENT_ENVIRONMENT]: this.config.environment
            })
          );
          const traceExporters = [];
          if (this.config.jaegerEndpoint) {
            traceExporters.push(
              new JaegerExporter({
                endpoint: this.config.jaegerEndpoint
              })
            );
          }
          const metricReaders = [];
          metricReaders.push(
            new PrometheusExporter({
              port: parseInt(process.env.PROMETHEUS_PORT || "9464")
            })
          );
          this.sdk = new NodeSDK({
            resource,
            traceExporter: traceExporters.length > 0 ? traceExporters[0] : void 0,
            metricReader: metricReaders.length > 0 ? metricReaders[0] : void 0,
            instrumentations: [
              getNodeAutoInstrumentations({
                // Disable instrumentation for certain modules if needed
                "@opentelemetry/instrumentation-fs": {
                  enabled: false
                }
              })
            ]
          });
          this.sdk.start();
          this.tracer = trace3.getTracer(
            this.config.serviceName,
            this.config.serviceVersion
          );
          logger28.info(
            `OpenTelemetry initialized. Service Name: ${this.config.serviceName}, Environment: ${this.config.environment}, Jaeger Enabled: ${!!this.config.jaegerEndpoint}`
          );
        } catch (error) {
          logger28.error(
            `Failed to initialize OpenTelemetry. Error: ${error instanceof Error ? error.message : "Unknown error"}`
          );
        }
      }
      /**
       * Shutdown OpenTelemetry SDK
       */
      async shutdown() {
        if (this.sdk) {
          await this.sdk.shutdown();
          logger28.info("OpenTelemetry SDK shutdown");
        }
      }
      /**
       * Start a new span with proper error handling
       */
      startSpan(name, attributes = {}, kind = SpanKind2.INTERNAL) {
        if (!this.tracer) {
          return this.createNoOpSpan();
        }
        return this.tracer.startSpan(name, {
          kind,
          attributes: {
            "service.name": this.config.serviceName,
            "service.version": this.config.serviceVersion,
            "deployment.environment": this.config.environment,
            ...attributes
          }
        });
      }
      /**
       * Wrap a GraphQL resolver with tracing
       */
      wrapResolver(operationName, resolver) {
        return async (parent, args, context4, info) => {
          const span = this.startSpan(
            `graphql.${operationName}`,
            {
              "graphql.operation.name": operationName,
              "graphql.operation.type": info.operation?.operation || "unknown",
              "graphql.field.name": info.fieldName,
              "graphql.field.path": info.path?.key || "unknown",
              "user.id": context4.user?.id || "anonymous"
            },
            SpanKind2.SERVER
          );
          try {
            const result2 = await resolver(parent, args, context4, info);
            span.setStatus({ code: SpanStatusCode2.OK });
            span.setAttributes({
              "graphql.result.success": true
            });
            return result2;
          } catch (error) {
            span.setStatus({
              code: SpanStatusCode2.ERROR,
              message: error instanceof Error ? error.message : "Unknown error"
            });
            span.setAttributes({
              "graphql.result.success": false,
              "error.name": error instanceof Error ? error.constructor.name : "Unknown",
              "error.message": error instanceof Error ? error.message : "Unknown error"
            });
            throw error;
          } finally {
            span.end();
          }
        };
      }
      /**
       * Wrap Neo4j operations with tracing
       */
      wrapNeo4jOperation(operationName, operation) {
        const span = this.startSpan(
          `neo4j.${operationName}`,
          {
            "db.system": "neo4j",
            "db.operation": operationName
          },
          SpanKind2.CLIENT
        );
        return context.with(trace3.setSpan(context.active(), span), async () => {
          try {
            const result2 = await operation();
            span.setStatus({ code: SpanStatusCode2.OK });
            return result2;
          } catch (error) {
            span.setStatus({
              code: SpanStatusCode2.ERROR,
              message: error instanceof Error ? error.message : "Unknown error"
            });
            throw error;
          } finally {
            span.end();
          }
        });
      }
      /**
       * Wrap BullMQ job processing with tracing
       */
      wrapBullMQJob(jobName, processor) {
        const span = this.startSpan(
          `bullmq.${jobName}`,
          {
            "messaging.system": "redis",
            "messaging.operation": "process",
            "job.name": jobName
          },
          SpanKind2.CONSUMER
        );
        return context.with(trace3.setSpan(context.active(), span), async () => {
          try {
            const result2 = await processor();
            span.setStatus({ code: SpanStatusCode2.OK });
            return result2;
          } catch (error) {
            span.setStatus({
              code: SpanStatusCode2.ERROR,
              message: error instanceof Error ? error.message : "Unknown error"
            });
            throw error;
          } finally {
            span.end();
          }
        });
      }
      /**
       * Add custom span attributes
       */
      addSpanAttributes(attributes) {
        const activeSpan = trace3.getActiveSpan();
        if (activeSpan) {
          activeSpan.setAttributes(attributes);
        }
      }
      /**
       * Add span event
       */
      addSpanEvent(name, attributes) {
        const activeSpan = trace3.getActiveSpan();
        if (activeSpan) {
          activeSpan.addEvent(name, attributes);
        }
      }
      /**
       * Get current trace context for propagation
       */
      getCurrentTraceContext() {
        const activeSpan = trace3.getActiveSpan();
        if (activeSpan) {
          const spanContext = activeSpan.spanContext();
          return `00-${spanContext.traceId}-${spanContext.spanId}-${spanContext.traceFlags.toString(16)}`;
        }
        return void 0;
      }
      /**
       * Create no-op span for when tracing is disabled
       */
      createNoOpSpan() {
        return {
          setStatus: () => {
          },
          setAttributes: () => {
          },
          addEvent: () => {
          },
          end: () => {
          }
        };
      }
      /**
       * Get service health and metrics
       */
      getHealth() {
        return {
          enabled: !!this.sdk,
          serviceName: this.config.serviceName,
          environment: this.config.environment,
          tracerActive: !!this.tracer
        };
      }
    };
    otelService2 = new OpenTelemetryService();
    if (process.env.NODE_ENV !== "test") {
      otelService2.initialize();
    }
    process.on("SIGTERM", async () => {
      await otelService2.shutdown();
    });
    process.on("SIGINT", async () => {
      await otelService2.shutdown();
    });
  }
});

// src/services/SimilarityService.ts
import { z as z6 } from "zod";
var serviceLogger3, SimilarityQuerySchema, BulkSimilarityQuerySchema, SimilarityService, similarityService;
var init_SimilarityService = __esm({
  "src/services/SimilarityService.ts"() {
    "use strict";
    init_database();
    init_EmbeddingService();
    init_opentelemetry();
    init_metrics2();
    init_logger2();
    serviceLogger3 = logger2.child({ name: "SimilarityService" });
    SimilarityQuerySchema = z6.object({
      investigationId: z6.string().min(1),
      entityId: z6.string().optional(),
      text: z6.string().optional(),
      topK: z6.number().int().min(1).max(100).default(10),
      threshold: z6.number().min(0).max(1).default(0.7),
      includeText: z6.boolean().default(false),
      tenantId: z6.string().optional()
    }).refine((data) => data.entityId || data.text, {
      message: "Either entityId or text must be provided"
    });
    BulkSimilarityQuerySchema = z6.object({
      investigationId: z6.string().min(1),
      entityIds: z6.array(z6.string()).min(1).max(50),
      topK: z6.number().int().min(1).max(100).default(5),
      threshold: z6.number().min(0).max(1).default(0.7)
    });
    SimilarityService = class {
      postgres;
      embeddingService;
      config;
      constructor() {
        this.postgres = null;
        this.embeddingService = new EmbeddingService_default();
        this.config = {
          defaultTopK: 10,
          defaultThreshold: 0.7,
          maxBatchSize: 50,
          cacheExpiry: 3600
          // 1 hour
        };
      }
      getPool() {
        if (!this.postgres) {
          this.postgres = getPostgresPool2();
        }
        return this.postgres;
      }
      /**
       * Find similar entities by entity ID or text
       */
      async findSimilar(query3) {
        const startTime = Date.now();
        return otelService2.wrapNeo4jOperation("similarity-search", async () => {
          try {
            const validated = SimilarityQuerySchema.parse(query3);
            serviceLogger3.debug("Similarity search requested", {
              investigationId: validated.investigationId,
              entityId: validated.entityId,
              tenantId: validated.tenantId,
              hasText: !!validated.text,
              topK: validated.topK
            });
            let targetEmbedding;
            let queryType;
            let queryValue;
            if (validated.entityId) {
              targetEmbedding = await this.getEntityEmbedding(validated.entityId);
              queryType = "entity";
              queryValue = validated.entityId;
            } else {
              targetEmbedding = await this.embeddingService.generateEmbedding({
                text: validated.text,
                model: process.env.EMBEDDING_MODEL || "text-embedding-3-small"
              });
              queryType = "text";
              queryValue = validated.text;
            }
            const results = await this.performVectorSearch(
              targetEmbedding,
              validated.investigationId,
              validated.topK,
              validated.threshold,
              validated.includeText,
              validated.entityId,
              // Exclude the query entity itself
              validated.tenantId
            );
            const executionTime = Date.now() - startTime;
            serviceLogger3.info("Similarity search completed", {
              investigationId: validated.investigationId,
              queryType,
              resultsCount: results.length,
              executionTime,
              topSimilarity: results[0]?.similarity || 0
            });
            otelService2.addSpanAttributes({
              "similarity.investigation_id": validated.investigationId,
              "similarity.query_type": queryType,
              "similarity.results_count": results.length,
              "similarity.execution_time": executionTime,
              "similarity.top_k": validated.topK
            });
            return {
              query: {
                type: queryType,
                value: queryValue
              },
              results,
              totalResults: results.length,
              executionTime
            };
          } catch (error) {
            serviceLogger3.error("Similarity search failed", {
              investigationId: query3.investigationId,
              error: error instanceof Error ? error.message : "Unknown error"
            });
            throw error;
          }
        });
      }
      /**
       * Find similar entities for multiple entities at once
       */
      async findSimilarBulk(query3) {
        return otelService2.wrapNeo4jOperation(
          "bulk-similarity-search",
          async () => {
            try {
              const validated = BulkSimilarityQuerySchema.parse(query3);
              serviceLogger3.debug("Bulk similarity search requested", {
                investigationId: validated.investigationId,
                entityCount: validated.entityIds.length,
                topK: validated.topK
              });
              const results = /* @__PURE__ */ new Map();
              const batchSize = Math.min(validated.entityIds.length, 10);
              for (let i = 0; i < validated.entityIds.length; i += batchSize) {
                const batch = validated.entityIds.slice(i, i + batchSize);
                const embeddingsMap = await this.getEntitiesEmbeddings(batch);
                const batchPromises = batch.map(async (entityId) => {
                  try {
                    const targetEmbedding = embeddingsMap.get(entityId);
                    if (!targetEmbedding) {
                      serviceLogger3.warn(
                        "No embedding found for entity in bulk search",
                        { entityId }
                      );
                      return [entityId, []];
                    }
                    const similarEntities = await this.performVectorSearch(
                      targetEmbedding,
                      validated.investigationId,
                      validated.topK,
                      validated.threshold,
                      false,
                      // includeText
                      entityId
                      // excludeEntityId
                    );
                    return [entityId, similarEntities];
                  } catch (error) {
                    serviceLogger3.warn(
                      "Failed to find similar entities for entity",
                      {
                        entityId,
                        error: error instanceof Error ? error.message : "Unknown error"
                      }
                    );
                    return [entityId, []];
                  }
                });
                const batchResults = await Promise.all(batchPromises);
                for (const [entityId, similarEntities] of batchResults) {
                  results.set(entityId, similarEntities);
                }
              }
              serviceLogger3.info("Bulk similarity search completed", {
                investigationId: validated.investigationId,
                entityCount: validated.entityIds.length,
                successfulResults: results.size
              });
              return results;
            } catch (error) {
              serviceLogger3.error("Bulk similarity search failed", {
                investigationId: query3.investigationId,
                error: error instanceof Error ? error.message : "Unknown error"
              });
              throw error;
            }
          }
        );
      }
      /**
       * Get embeddings for multiple entities in a single query
       */
      async getEntitiesEmbeddings(entityIds) {
        const client6 = await this.getPool().connect();
        try {
          const result2 = await client6.query(
            "SELECT entity_id, embedding FROM entity_embeddings WHERE entity_id = ANY($1)",
            [entityIds]
          );
          const map = /* @__PURE__ */ new Map();
          for (const row of result2.rows) {
            map.set(row.entity_id, this.parseVectorString(row.embedding));
          }
          return map;
        } finally {
          client6.release();
        }
      }
      /**
       * Get entity embedding from database
       */
      async getEntityEmbedding(entityId) {
        const client6 = await this.getPool().connect();
        try {
          const result2 = await client6.query(
            "SELECT embedding FROM entity_embeddings WHERE entity_id = $1",
            [entityId]
          );
          if (result2.rows.length === 0) {
            throw new Error(`No embedding found for entity ${entityId}`);
          }
          const embeddingString = result2.rows[0].embedding;
          return this.parseVectorString(embeddingString);
        } finally {
          client6.release();
        }
      }
      /**
       * Perform vector similarity search using pgvector
       */
      async performVectorSearch(targetEmbedding, investigationId, topK, threshold, includeText, excludeEntityId, tenantId) {
        const client6 = await this.getPool().connect();
        const tenantLabel = tenantId ?? "unknown";
        const stopTimer = vectorQueryDurationSeconds.startTimer({
          operation: "similarity-search",
          tenant_id: tenantLabel
        });
        const finishTimer = typeof stopTimer === "function" ? stopTimer : () => {
        };
        let status = "success";
        try {
          const vectorString = `[${targetEmbedding.join(",")}]`;
          let query3 = `
        SELECT
          ee.entity_id,
          1 - (ee.embedding <=> $1::vector) as similarity
          ${includeText ? ", ee.text" : ""}
        FROM entity_embeddings ee
        WHERE ee.investigation_id = $2
      `;
          const params = [vectorString, investigationId];
          let paramIndex = 3;
          if (excludeEntityId) {
            query3 += ` AND ee.entity_id != $${paramIndex}`;
            params.push(excludeEntityId);
            paramIndex++;
          }
          query3 += ` AND (1 - (ee.embedding <=> $1::vector)) >= $${paramIndex}`;
          params.push(threshold);
          paramIndex++;
          query3 += `
        ORDER BY ee.embedding <=> $1::vector
        LIMIT $${paramIndex}
      `;
          params.push(topK);
          const result2 = await client6.query(query3, params);
          return result2.rows.map((row) => ({
            entityId: row.entity_id,
            similarity: parseFloat(row.similarity),
            text: includeText ? row.text : void 0
          }));
        } catch (error) {
          status = "error";
          throw error;
        } finally {
          finishTimer();
          const counter = vectorQueriesTotal?.labels?.(
            "similarity-search",
            tenantLabel,
            status
          );
          if (counter && typeof counter.inc === "function") {
            counter.inc();
          }
          client6.release();
        }
      }
      /**
       * Parse pgvector string format back to number array
       */
      parseVectorString(vectorString) {
        const cleaned = vectorString.replace(/^\[|\]$/g, "");
        return cleaned.split(",").map((v) => parseFloat(v.trim()));
      }
      /**
       * Get similarity statistics for an investigation
       */
      async getStats(investigationId) {
        const client6 = await this.getPool().connect();
        try {
          const statsResult = await client6.query(
            `
        SELECT
          COUNT(*) as total_embeddings,
          MAX(updated_at) as last_updated
        FROM entity_embeddings
        WHERE investigation_id = $1
      `,
            [investigationId]
          );
          const indexResult = await client6.query(`
        SELECT
          schemaname,
          tablename,
          indexname,
          indexdef
        FROM pg_indexes
        WHERE tablename = 'entity_embeddings'
          AND indexname = 'entity_embeddings_hnsw_idx'
      `);
          const indexHealth = indexResult.rows.length > 0 ? "healthy" : "missing";
          return {
            totalEmbeddings: parseInt(statsResult.rows[0].total_embeddings),
            avgSimilarityThreshold: this.config.defaultThreshold,
            lastUpdated: statsResult.rows[0].last_updated,
            indexHealth
          };
        } finally {
          client6.release();
        }
      }
      /**
       * Rebuild HNSW index with configurable parameters
       */
      async rebuildIndex(config9 = {}) {
        const client6 = await this.getPool().connect();
        const m = config9.m !== void 0 ? Math.max(2, Math.min(config9.m, 100)) : 16;
        const efConstruction = config9.efConstruction !== void 0 ? Math.max(10, Math.min(config9.efConstruction, 1e3)) : 64;
        try {
          serviceLogger3.info("Rebuilding HNSW index...", { m, efConstruction });
          await client6.query("DROP INDEX IF EXISTS entity_embeddings_hnsw_idx");
          await client6.query(`
        CREATE INDEX entity_embeddings_hnsw_idx
        ON entity_embeddings
        USING hnsw (embedding vector_cosine_ops)
        WITH (m = ${m}, ef_construction = ${efConstruction})
      `);
          serviceLogger3.info("HNSW index rebuilt successfully");
        } finally {
          client6.release();
        }
      }
      async benchmarkIndexConfigurations(configs) {
        const results = [];
        for (const config9 of configs) {
          const start = Date.now();
          await this.rebuildIndex(config9);
          const buildTime = Date.now() - start;
          const searchStart = Date.now();
          const searchTime = Date.now() - searchStart;
          results.push({
            config: config9,
            buildTime,
            searchTime
          });
        }
        return results;
      }
      /**
       * Calculate topology similarity using Jaccard index
       * Measures overlap between entity neighbor sets
       */
      calculateTopologySimilarity(neighbors1, neighbors2) {
        if (!neighbors1?.length && !neighbors2?.length) {
          return 0;
        }
        const set1 = new Set(neighbors1 || []);
        const set2 = new Set(neighbors2 || []);
        const intersection = new Set([...set1].filter((x) => set2.has(x)));
        const union = /* @__PURE__ */ new Set([...set1, ...set2]);
        if (union.size === 0) {
          return 0;
        }
        return intersection.size / union.size;
      }
      /**
       * Calculate provenance similarity
       * Returns 1 if sources match, 0 otherwise
       */
      calculateProvenanceSimilarity(source1, source2) {
        if (!source1 || !source2) {
          return 0;
        }
        return source1 === source2 ? 1 : 0;
      }
      /**
       * Find potential duplicate entities using semantic similarity + topology + provenance
       * Modern pgvector-based implementation replacing O(n) legacy algorithm
       *
       * @param params Configuration for duplicate detection
       * @returns Array of duplicate candidate pairs with similarity scores and reasons
       */
      async findDuplicateCandidates(params) {
        const startTime = Date.now();
        const threshold = params.threshold ?? this.config.defaultThreshold;
        const topK = params.topK ?? 5;
        const includeReasons = params.includeReasons ?? true;
        return otelService2.wrapNeo4jOperation(
          "find-duplicate-candidates",
          async () => {
            try {
              serviceLogger3.info("Finding duplicate candidates", {
                investigationId: params.investigationId,
                threshold,
                topK
              });
              const client6 = await this.getPool().connect();
              let entities;
              try {
                const result2 = await client6.query(
                  `
              SELECT
                entity_id,
                embedding,
                text,
                metadata
              FROM entity_embeddings
              WHERE investigation_id = $1
              ORDER BY entity_id
            `,
                  [params.investigationId]
                );
                entities = result2.rows.map((row) => ({
                  entityId: row.entity_id,
                  embedding: this.parseVectorString(row.embedding),
                  text: row.text,
                  neighborIds: row.metadata?.neighbor_ids || [],
                  sourceSystem: row.metadata?.source_system
                }));
              } finally {
                client6.release();
              }
              serviceLogger3.debug("Loaded entities for deduplication", {
                entityCount: entities.length
              });
              if (entities.length === 0) {
                return [];
              }
              const candidatePairs = /* @__PURE__ */ new Map();
              for (const entity of entities) {
                const similarEntities = await this.performVectorSearch(
                  entity.embedding,
                  params.investigationId,
                  topK,
                  0,
                  // Don't filter by threshold yet, we'll apply hybrid scoring
                  false,
                  // Don't include text in results
                  entity.entityId,
                  // Exclude self
                  params.tenantId
                );
                for (const similar of similarEntities) {
                  const targetEntity = entities.find(
                    (e) => e.entityId === similar.entityId
                  );
                  if (!targetEntity) {
                    continue;
                  }
                  const semanticSimilarity = similar.similarity;
                  const topologySimilarity = this.calculateTopologySimilarity(
                    entity.neighborIds,
                    targetEntity.neighborIds
                  );
                  const provenanceSimilarity = this.calculateProvenanceSimilarity(
                    entity.sourceSystem,
                    targetEntity.sourceSystem
                  );
                  const overallSimilarity = semanticSimilarity * 0.6 + topologySimilarity * 0.3 + provenanceSimilarity * 0.1;
                  if (overallSimilarity >= threshold) {
                    const pairKey = entity.entityId < similar.entityId ? `${entity.entityId}::${similar.entityId}` : `${similar.entityId}::${entity.entityId}`;
                    if (!candidatePairs.has(pairKey)) {
                      const reasons = [];
                      if (includeReasons) {
                        if (semanticSimilarity > 0.8) {
                          reasons.push("High semantic similarity");
                        }
                        if (topologySimilarity > 0.5) {
                          reasons.push("Significant neighbor overlap");
                        }
                        if (provenanceSimilarity > 0) {
                          reasons.push("Same source");
                        }
                        if (reasons.length === 0) {
                          reasons.push("Overall similarity threshold met");
                        }
                      }
                      candidatePairs.set(pairKey, {
                        entityA: {
                          id: entity.entityId,
                          label: entity.text || entity.entityId
                        },
                        entityB: {
                          id: similar.entityId,
                          label: targetEntity.text || similar.entityId
                        },
                        similarity: overallSimilarity,
                        scores: {
                          semantic: semanticSimilarity,
                          topology: topologySimilarity,
                          provenance: provenanceSimilarity
                        },
                        reasons: includeReasons ? reasons : void 0
                      });
                    }
                  }
                }
              }
              const candidates2 = Array.from(candidatePairs.values());
              candidates2.sort((a, b) => b.similarity - a.similarity);
              const executionTime = Date.now() - startTime;
              serviceLogger3.info("Duplicate candidates found", {
                investigationId: params.investigationId,
                candidateCount: candidates2.length,
                entityCount: entities.length,
                executionTime,
                threshold
              });
              otelService2.addSpanAttributes({
                "dedup.investigation_id": params.investigationId,
                "dedup.candidate_count": candidates2.length,
                "dedup.entity_count": entities.length,
                "dedup.execution_time": executionTime,
                "dedup.threshold": threshold
              });
              return candidates2;
            } catch (error) {
              serviceLogger3.error("Failed to find duplicate candidates", {
                investigationId: params.investigationId,
                error: error instanceof Error ? error.message : "Unknown error"
              });
              throw error;
            }
          }
        );
      }
    };
    similarityService = new SimilarityService();
  }
});

// src/graphql/resolvers/deduplication.ts
var dedupLogger, deduplicationResolvers;
var init_deduplication = __esm({
  "src/graphql/resolvers/deduplication.ts"() {
    "use strict";
    init_SimilarityService();
    init_logger2();
    dedupLogger = logger2.child({ module: "deduplication-resolvers" });
    deduplicationResolvers = {
      Query: {
        /**
         * Get duplicate entity candidates for an investigation
         * Uses semantic similarity + topology + provenance for accurate detection
         */
        deduplicationCandidates: async (_parent, args, context4) => {
          try {
            const investigationId = args.investigationId;
            if (!investigationId) {
              throw new Error("investigationId is required");
            }
            dedupLogger.info("Fetching deduplication candidates", {
              investigationId,
              threshold: args.threshold,
              userId: context4.user?.id
            });
            const candidates2 = await similarityService.findDuplicateCandidates({
              investigationId,
              threshold: args.threshold,
              topK: 10,
              // Configurable if needed
              includeReasons: true,
              tenantId: context4.tenant
            });
            dedupLogger.debug("Deduplication candidates retrieved", {
              investigationId,
              count: candidates2.length
            });
            return candidates2.map((candidate) => ({
              entityA: {
                id: candidate.entityA.id,
                label: candidate.entityA.label,
                description: null
                // Can be populated from Neo4j if needed
              },
              entityB: {
                id: candidate.entityB.id,
                label: candidate.entityB.label,
                description: null
              },
              similarity: candidate.similarity,
              reasons: candidate.reasons || [],
              scores: candidate.scores
            }));
          } catch (error) {
            dedupLogger.error("Failed to fetch deduplication candidates", {
              investigationId: args.investigationId,
              error: error instanceof Error ? error.message : "Unknown error"
            });
            throw error;
          }
        }
      }
    };
  }
});

// src/graphql/resolvers/ticket-links.ts
var ticketLinksResolvers, ticket_links_default;
var init_ticket_links = __esm({
  "src/graphql/resolvers/ticket-links.ts"() {
    "use strict";
    init_postgres();
    ticketLinksResolvers = {
      Query: {
        tickets: async (_2, args) => {
          const pool4 = getPostgresPool();
          const { provider, externalId, limit = 10 } = args;
          let query3 = "SELECT * FROM tickets WHERE provider = $1";
          const queryParams = [provider];
          if (externalId) {
            query3 += " AND external_id = $2";
            queryParams.push(externalId);
          }
          query3 += ` LIMIT $${queryParams.length + 1}`;
          queryParams.push(limit);
          const res = await pool4.query(query3, queryParams);
          return res.rows.map((row) => ({
            ...row,
            externalId: row.external_id || row.externalId
          }));
        }
      },
      Ticket: {
        runs: async (parent) => {
          const pool4 = getPostgresPool();
          const res = await pool4.query("SELECT * FROM runs JOIN ticket_run_links ON runs.id = ticket_run_links.run_id WHERE ticket_run_links.ticket_external_id = $1 AND ticket_run_links.ticket_provider = $2", [parent.externalId || parent.external_id, parent.provider]);
          return res.rows;
        },
        deployments: async (parent) => {
          const pool4 = getPostgresPool();
          const res = await pool4.query("SELECT * FROM deployments JOIN ticket_deployment_links ON deployments.id = ticket_deployment_links.deployment_id WHERE ticket_deployment_links.ticket_external_id = $1 AND ticket_deployment_links.ticket_provider = $2", [parent.externalId || parent.external_id, parent.provider]);
          return res.rows;
        }
      }
    };
    ticket_links_default = ticketLinksResolvers;
  }
});

// src/modules/factgov/repo.ts
function camelCaseKeys(obj) {
  const newObj = {};
  for (const key in obj) {
    const newKey = key.replace(/_([a-z])/g, (g2) => g2[1].toUpperCase());
    newObj[newKey] = obj[key];
  }
  return newObj;
}
var pool2, factGovRepo;
var init_repo = __esm({
  "src/modules/factgov/repo.ts"() {
    "use strict";
    init_database();
    pool2 = getPostgresPool2();
    factGovRepo = {
      async createAgency(name, domain) {
        const res = await pool2.query(
          "INSERT INTO factgov_agencies (name, domain) VALUES ($1, $2) RETURNING *",
          [name, domain]
        );
        return camelCaseKeys(res.rows[0]);
      },
      async createVendor(name, tags, description) {
        const res = await pool2.query(
          "INSERT INTO factgov_vendors (name, tags, description) VALUES ($1, $2, $3) RETURNING *",
          [name, tags, description]
        );
        return camelCaseKeys(res.rows[0]);
      },
      async getVendor(id) {
        const res = await pool2.query("SELECT * FROM factgov_vendors WHERE id = $1", [id]);
        return res.rows.length ? camelCaseKeys(res.rows[0]) : null;
      },
      async findVendorsByTags(tags) {
        const res = await pool2.query("SELECT * FROM factgov_vendors WHERE tags && $1", [tags]);
        return res.rows.map(camelCaseKeys);
      },
      async createRfp(agencyId, title, content) {
        const res = await pool2.query(
          "INSERT INTO factgov_rfps (agency_id, title, content) VALUES ($1, $2, $3) RETURNING *",
          [agencyId, title, content]
        );
        return camelCaseKeys(res.rows[0]);
      },
      async getRfp(id) {
        const res = await pool2.query("SELECT * FROM factgov_rfps WHERE id = $1", [id]);
        return res.rows.length ? camelCaseKeys(res.rows[0]) : null;
      },
      async createMatch(rfpId, vendorId, score) {
        const res = await pool2.query(
          "INSERT INTO factgov_matches (rfp_id, vendor_id, score) VALUES ($1, $2, $3) RETURNING *",
          [rfpId, vendorId, score]
        );
        return camelCaseKeys(res.rows[0]);
      },
      async getMatchesForRfp(rfpId) {
        const res = await pool2.query("SELECT * FROM factgov_matches WHERE rfp_id = $1 ORDER BY score DESC", [rfpId]);
        return res.rows.map(camelCaseKeys);
      },
      async createAudit(audit) {
        const res = await pool2.query(
          `INSERT INTO factgov_audits (entity_type, entity_id, action, actor_id, details, previous_hash, hash)
           VALUES ($1, $2, $3, $4, $5, $6, $7) RETURNING *`,
          [audit.entityType, audit.entityId, audit.action, audit.actorId, JSON.stringify(audit.details), audit.previousHash, audit.hash]
        );
        return camelCaseKeys(res.rows[0]);
      },
      async getLatestAudit(entityId) {
        const res = await pool2.query(
          "SELECT * FROM factgov_audits WHERE entity_id = $1 ORDER BY created_at DESC LIMIT 1",
          [entityId]
        );
        return res.rows.length ? camelCaseKeys(res.rows[0]) : null;
      }
    };
  }
});

// src/modules/factgov/service.ts
import { createHash as createHash9 } from "crypto";
var factGovService;
var init_service = __esm({
  "src/modules/factgov/service.ts"() {
    "use strict";
    init_repo();
    factGovService = {
      async matchRfp(rfpId) {
        const rfp = await factGovRepo.getRfp(rfpId);
        if (!rfp) throw new Error("RFP not found");
        const keywords = rfp.content.split(" ").filter((w) => w.length > 4);
        const candidates2 = await factGovRepo.findVendorsByTags(keywords);
        const matches2 = [];
        for (const vendor of candidates2) {
          const score = Math.random() * 100;
          const match = await factGovRepo.createMatch(rfp.id, vendor.id, score);
          matches2.push(match);
        }
        return matches2;
      },
      async auditAction(entityType, entityId, action, actorId, details) {
        const lastAudit = await factGovRepo.getLatestAudit(entityId);
        const previousHash = lastAudit?.hash || "0000000000000000";
        const payload = JSON.stringify({ entityType, entityId, action, actorId, details, previousHash });
        const hash3 = createHash9("sha256").update(payload).digest("hex");
        return await factGovRepo.createAudit({
          entityType,
          entityId,
          action,
          actorId,
          details,
          previousHash,
          hash: hash3
        });
      }
    };
  }
});

// src/modules/factgov/resolvers.ts
var factGovResolvers;
var init_resolvers = __esm({
  "src/modules/factgov/resolvers.ts"() {
    "use strict";
    init_repo();
    init_service();
    factGovResolvers = {
      Query: {
        factgovGetRfp: async (_2, { id }) => {
          return await factGovRepo.getRfp(id);
        },
        factgovGetVendor: async (_2, { id }) => {
          return await factGovRepo.getVendor(id);
        },
        factgovGetMatches: async (_2, { rfpId }) => {
          return await factGovRepo.getMatchesForRfp(rfpId);
        }
      },
      Mutation: {
        factgovCreateAgency: async (_2, { name, domain }) => {
          return await factGovRepo.createAgency(name, domain);
        },
        factgovCreateVendor: async (_2, { name, tags, description }) => {
          return await factGovRepo.createVendor(name, tags, description);
        },
        factgovCreateRfp: async (_2, { agencyId, title, content }, context4) => {
          const rfp = await factGovRepo.createRfp(agencyId, title, content);
          const userId = context4.user?.id || "system";
          await factGovService.auditAction("RFP", rfp.id, "CREATE", userId, { title });
          return rfp;
        },
        factgovMatchRfp: async (_2, { rfpId }) => {
          return await factGovService.matchRfp(rfpId);
        }
      }
    };
  }
});

// src/services/osint-synint/SynintClient.ts
import { spawn } from "node:child_process";
import { setTimeout as delay } from "node:timers/promises";
var SynintClient;
var init_SynintClient = __esm({
  "src/services/osint-synint/SynintClient.ts"() {
    "use strict";
    SynintClient = class {
      cfg;
      constructor(cfg2) {
        this.cfg = {
          mode: cfg2.mode,
          baseUrl: cfg2.baseUrl ?? "http://localhost:8080",
          httpTimeoutMs: cfg2.httpTimeoutMs ?? 12e4,
          pythonBin: cfg2.pythonBin ?? "python3",
          cliEntrypoint: cfg2.cliEntrypoint ?? "main.py",
          cliArgsPrefix: cfg2.cliArgsPrefix ?? [],
          maxConcurrency: cfg2.maxConcurrency ?? 2,
          retry: cfg2.retry ?? { attempts: 2, backoffMs: 750 }
        };
      }
      async runSweep(target) {
        const { attempts, backoffMs } = this.cfg.retry;
        let lastErr;
        for (let i = 0; i <= attempts; i++) {
          try {
            if (this.cfg.mode === "http") return await this.runSweepHttp(target);
            return await this.runSweepCli(target);
          } catch (err) {
            lastErr = err;
            if (i < attempts) await delay(backoffMs * Math.max(1, i + 1));
          }
        }
        throw lastErr;
      }
      async runSweepHttp(target) {
        const url = `${this.cfg.baseUrl.replace(/\/$/, "")}/sweep`;
        const ctrl = new AbortController();
        const t = setTimeout(() => ctrl.abort(), this.cfg.httpTimeoutMs);
        try {
          const res = await fetch(url, {
            method: "POST",
            headers: { "content-type": "application/json" },
            body: JSON.stringify({ target }),
            signal: ctrl.signal
          });
          if (!res.ok) {
            const text = await res.text().catch(() => "");
            throw new Error(`SYNINT HTTP ${res.status}: ${text.slice(0, 300)}`);
          }
          const data = await res.json();
          return this.validateSweepResult(data, target);
        } finally {
          clearTimeout(t);
        }
      }
      async runSweepCli(target) {
        const args = [
          ...this.cfg.cliArgsPrefix,
          this.cfg.cliEntrypoint,
          "--target",
          target,
          "--json"
        ];
        const child = spawn(this.cfg.pythonBin, args, {
          stdio: ["ignore", "pipe", "pipe"],
          env: process.env
        });
        const stdout = [];
        const stderr = [];
        child.stdout.on("data", (d) => stdout.push(Buffer.from(d)));
        child.stderr.on("data", (d) => stderr.push(Buffer.from(d)));
        const exitCode = await new Promise((resolve2, reject) => {
          child.on("error", reject);
          child.on("close", resolve2);
        });
        const out = Buffer.concat(stdout).toString("utf8").trim();
        const err = Buffer.concat(stderr).toString("utf8").trim();
        if (exitCode !== 0) {
          throw new Error(`SYNINT CLI exit ${exitCode}: ${err.slice(0, 600)}`);
        }
        let parsed;
        try {
          parsed = JSON.parse(out);
        } catch {
          throw new Error(`SYNINT CLI returned non-JSON. stderr=${err.slice(0, 300)} stdout=${out.slice(0, 300)}`);
        }
        return this.validateSweepResult(parsed, target);
      }
      validateSweepResult(data, target) {
        if (!data || typeof data !== "object") throw new Error("Invalid SYNINT result: not an object");
        if (!data.target) data.target = target;
        if (!data.startedAt) data.startedAt = (/* @__PURE__ */ new Date()).toISOString();
        if (!data.completedAt) data.completedAt = (/* @__PURE__ */ new Date()).toISOString();
        if (!Array.isArray(data.agents)) data.agents = [];
        return data;
      }
    };
  }
});

// src/services/osint-synint/SynintMapper.ts
var SynintMapper;
var init_SynintMapper = __esm({
  "src/services/osint-synint/SynintMapper.ts"() {
    "use strict";
    SynintMapper = class {
      cfg;
      constructor(cfg2 = {}) {
        this.cfg = { sourceTag: cfg2.sourceTag ?? "synint" };
      }
      toMutations(sweep) {
        const muts = [];
        muts.push({
          kind: "emitEvent",
          event: {
            type: "osint.sweep.completed",
            at: sweep.completedAt,
            target: sweep.target,
            payload: {
              source: this.cfg.sourceTag,
              startedAt: sweep.startedAt,
              completedAt: sweep.completedAt,
              agentCount: sweep.agents.length,
              successCount: sweep.agents.filter((a) => a.success).length
            }
          }
        });
        for (const agent of sweep.agents) {
          const name = agent.agentName?.toLowerCase?.() ?? "";
          if (name.includes("whois")) muts.push(...this.mapWhois(sweep.target, agent.findings));
          if (name.includes("social")) muts.push(...this.mapSocial(sweep.target, agent.findings));
        }
        return muts;
      }
      mapWhois(target, findings) {
        const f = findings ?? {};
        const domain = String(f.domain ?? target);
        const domainId = `domain:${domain}`;
        const muts = [
          {
            kind: "upsertNode",
            node: {
              id: domainId,
              labels: ["Domain"],
              props: { domain, source: this.cfg.sourceTag, lastSeenAt: (/* @__PURE__ */ new Date()).toISOString() }
            }
          }
        ];
        const registrantOrg = typeof f.registrantOrg === "string" ? f.registrantOrg : void 0;
        if (registrantOrg) {
          const orgId = `org:${registrantOrg}`;
          muts.push({
            kind: "upsertNode",
            node: { id: orgId, labels: ["Organization"], props: { name: registrantOrg, source: this.cfg.sourceTag } }
          });
          muts.push({
            kind: "upsertEdge",
            edge: { id: `edge:${domainId}->registered_to->${orgId}`, type: "REGISTERED_TO", from: domainId, to: orgId, props: { source: this.cfg.sourceTag } }
          });
        }
        return muts;
      }
      mapSocial(target, findings) {
        const f = findings ?? {};
        const accounts = Array.isArray(f.accounts) ? f.accounts : [];
        const muts = [];
        const targetId = `target:${target}`;
        muts.push({
          kind: "upsertNode",
          node: { id: targetId, labels: ["Target"], props: { key: target, source: this.cfg.sourceTag } }
        });
        for (const a of accounts) {
          const platform = String(a.platform ?? "unknown");
          const handle2 = String(a.handle ?? "unknown");
          const accountId = `acct:${platform}:${handle2}`;
          muts.push({
            kind: "upsertNode",
            node: {
              id: accountId,
              labels: ["Account"],
              props: {
                platform,
                handle: handle2,
                url: a.url ?? null,
                confidence: a.confidence ?? null,
                source: this.cfg.sourceTag
              }
            }
          });
          muts.push({
            kind: "upsertEdge",
            edge: {
              id: `edge:${targetId}->associated_with->${accountId}`,
              type: "ASSOCIATED_WITH",
              from: targetId,
              to: accountId,
              props: { source: this.cfg.sourceTag }
            }
          });
        }
        return muts;
      }
    };
  }
});

// src/graphql/resolvers/osint-synint.ts
async function applyGraphMutations(mutations) {
  const driver3 = getNeo4jDriver();
  const session = driver3.session();
  try {
    for (const m of mutations) {
      if (m.kind === "upsertNode") {
        const labels2 = m.node.labels.map((l) => l.replace(/[^a-zA-Z0-9_]/g, "")).filter((l) => l.length > 0).join(":");
        const labelStr = labels2.length > 0 ? labels2 : "Entity";
        const q = `MERGE (n:${labelStr} {id: $id}) SET n += $props`;
        await session.run(q, { id: m.node.id, props: m.node.props });
      } else if (m.kind === "upsertEdge") {
        const type = m.edge.type.replace(/[^a-zA-Z0-9_]/g, "");
        if (!type) continue;
        const q = `
          MATCH (a {id: $from}), (b {id: $to})
          MERGE (a)-[r:${type}]->(b)
          SET r += $props
        `;
        await session.run(q, { from: m.edge.from, to: m.edge.to, props: m.edge.props || {} });
      } else if (m.kind === "emitEvent") {
        console.log("Synint Event:", JSON.stringify(m.event));
      }
    }
  } catch (error) {
    console.error("Error applying Synint mutations:", error);
    throw error;
  } finally {
    await session.close();
  }
}
function validateTargetOrThrow(target) {
  const t = target.trim();
  if (t.length < 3 || t.length > 255) throw new Error("Invalid target length");
  const isDomain = /^[a-z0-9.-]+\.[a-z]{2,}$/i.test(t);
  const isIPv4 = /^(?:\d{1,3}\.){3}\d{1,3}$/.test(t);
  const isHandle = /^[a-zA-Z0-9_@.-]{3,64}$/.test(t);
  if (!isDomain && !isIPv4 && !isHandle) throw new Error("Target format not allowed");
}
var osintSynintResolvers;
var init_osint_synint = __esm({
  "src/graphql/resolvers/osint-synint.ts"() {
    "use strict";
    init_SynintClient();
    init_SynintMapper();
    init_neo4j();
    osintSynintResolvers = {
      Mutation: {
        runSynintSweep: async (_2, args, ctx) => {
          validateTargetOrThrow(args.target);
          const client6 = new SynintClient({
            mode: process.env.SYNINT_MODE ?? "http",
            baseUrl: process.env.SYNINT_URL,
            pythonBin: process.env.SYNINT_PYTHON ?? "python3",
            cliEntrypoint: process.env.SYNINT_ENTRYPOINT ?? "main.py",
            httpTimeoutMs: Number(process.env.SYNINT_TIMEOUT_MS ?? "120000"),
            maxConcurrency: Number(process.env.SYNINT_CONCURRENCY ?? "2")
          });
          const sweep = await client6.runSweep(args.target);
          const mapper = new SynintMapper({ sourceTag: "synint" });
          const mutations = mapper.toMutations(sweep);
          await applyGraphMutations(mutations);
          return sweep;
        }
      }
    };
  }
});

// src/graphql/resolvers/index.ts
var resolvers_exports = {};
__export(resolvers_exports, {
  default: () => resolvers_default
});
var wargameResolver, resolvers2, resolvers_default;
var init_resolvers2 = __esm({
  "src/graphql/resolvers/index.ts"() {
    "use strict";
    init_entity();
    init_relationship();
    init_user();
    init_investigation();
    init_auth3();
    init_WargameResolver();
    init_evidence();
    init_evidenceOk();
    init_health();
    init_trust_risk();
    init_provenance();
    init_supportTicket();
    init_sprint28();
    init_electronic_warfare();
    init_collaboration();
    init_cases();
    init_comments();
    init_cognitive_security2();
    init_deduplication();
    init_ticket_links();
    init_resolvers();
    init_osint_synint();
    wargameResolver = new WargameResolver();
    resolvers2 = {
      Query: {
        ...entity_default.Query,
        ...user_default.Query,
        ...health_default.Query,
        ...investigation_default.Query,
        ...auth_default.Query || {},
        ...evidenceOk_default.Query || {},
        ...trust_risk_default.Query || {},
        ...provenance_default.Query || {},
        ...supportTicket_default.Query || {},
        ...sprint28_default.Query || {},
        ...electronic_warfare_default.Query || {},
        ...collaborationResolvers.Query || {},
        ...caseResolvers.Query || {},
        ...commentResolvers.Query || {},
        ...cognitiveSecurityResolvers.Query || {},
        ...deduplicationResolvers.Query || {},
        ...ticket_links_default.Query || {},
        ...factGovResolvers.Query || {},
        // MC Platform v0.4.0 Transcendent Intelligence (DISABLED)
        // ...(v040Resolvers.Query || {}),
        // MC Platform v0.4.1 Sovereign Safeguards (DISABLED)
        // ...(v041Resolvers.Query || {}),
        // WAR-GAMED SIMULATION - FOR DECISION SUPPORT ONLY
        getCrisisTelemetry: wargameResolver.getCrisisTelemetry.bind(wargameResolver),
        getAdversaryIntentEstimates: wargameResolver.getAdversaryIntentEstimates.bind(wargameResolver),
        getNarrativeHeatmapData: wargameResolver.getNarrativeHeatmapData.bind(wargameResolver),
        getStrategicResponsePlaybooks: wargameResolver.getStrategicResponsePlaybooks.bind(wargameResolver),
        getCrisisScenario: wargameResolver.getCrisisScenario.bind(wargameResolver),
        getAllCrisisScenarios: wargameResolver.getAllCrisisScenarios.bind(wargameResolver)
      },
      Mutation: {
        ...osintSynintResolvers.Mutation || {},
        ...entity_default.Mutation,
        ...relationship_default.Mutation,
        ...user_default.Mutation,
        ...investigation_default.Mutation,
        ...auth_default.Mutation || {},
        ...evidence_default.Mutation || {},
        ...trust_risk_default.Mutation || {},
        ...provenance_default.Mutation || {},
        ...supportTicket_default.Mutation || {},
        ...sprint28_default.Mutation || {},
        ...electronic_warfare_default.Mutation || {},
        ...collaborationResolvers.Mutation || {},
        ...caseResolvers.Mutation || {},
        ...commentResolvers.Mutation || {},
        ...cognitiveSecurityResolvers.Mutation || {},
        ...factGovResolvers.Mutation || {},
        // MC Platform v0.4.0 Transcendent Intelligence (DISABLED)
        // ...(v040Resolvers.Mutation || {}),
        // MC Platform v0.4.1 Sovereign Safeguards (DISABLED)
        // ...(v041Resolvers.Mutation || {}),
        // WAR-GAMED SIMULATION - FOR DECISION SUPPORT ONLY
        runWarGameSimulation: wargameResolver.runWarGameSimulation.bind(wargameResolver),
        updateCrisisScenario: wargameResolver.updateCrisisScenario.bind(wargameResolver),
        deleteCrisisScenario: wargameResolver.deleteCrisisScenario.bind(wargameResolver)
      },
      SupportTicket: supportTicket_default.SupportTicket,
      WarRoom: collaborationResolvers.WarRoom,
      Case: caseResolvers.Case,
      Comment: commentResolvers.Comment,
      Subscription: {
        ...collaborationResolvers.Subscription || {},
        ...cognitiveSecurityResolvers.Subscription || {}
      },
      // Cognitive Security type resolvers
      CogSecClaim: cognitiveSecurityResolvers.CogSecClaim,
      CogSecCampaign: cognitiveSecurityResolvers.CogSecCampaign,
      CogSecIncident: cognitiveSecurityResolvers.CogSecIncident,
      VerificationAppeal: cognitiveSecurityResolvers.VerificationAppeal,
      AudienceSegment: cognitiveSecurityResolvers.AudienceSegment,
      NarrativeCascade: cognitiveSecurityResolvers.NarrativeCascade,
      NarrativeConflict: cognitiveSecurityResolvers.NarrativeConflict,
      Ticket: ticket_links_default.Ticket
    };
    resolvers_default = resolvers2;
  }
});

// src/observability/tracer.ts
import * as opentelemetrySdkNode from "@opentelemetry/sdk-node";
import * as opentelemetryResources from "@opentelemetry/resources";
import * as semanticConventions from "@opentelemetry/semantic-conventions";
import { JaegerExporter as JaegerExporter2 } from "@opentelemetry/exporter-jaeger";
import * as traceExporter from "@opentelemetry/exporter-trace-otlp-http";
import * as metricsExporter from "@opentelemetry/exporter-metrics-otlp-http";
import * as sdkMetrics from "@opentelemetry/sdk-metrics";
import * as autoInstrumentations from "@opentelemetry/auto-instrumentations-node";
import {
  trace as trace4,
  context as context2,
  SpanStatusCode as SpanStatusCode3,
  SpanKind as SpanKind3
} from "@opentelemetry/api";
import * as otelApi from "@opentelemetry/api";
import pino33 from "pino";
function initializeTracing(config9) {
  if (tracerInstance) {
    return tracerInstance;
  }
  const defaultConfig = {
    serviceName: "intelgraph-server",
    serviceVersion: cfg.APP_VERSION || "1.0.0",
    environment: cfg.NODE_ENV || "development",
    jaegerEndpoint: process.env.JAEGER_ENDPOINT,
    otlpTracesEndpoint: process.env.OTEL_EXPORTER_OTLP_TRACES_ENDPOINT,
    otlpMetricsEndpoint: process.env.OTEL_EXPORTER_OTLP_METRICS_ENDPOINT,
    enableAutoInstrumentation: process.env.OTEL_AUTO_INSTRUMENT !== "false",
    sampleRate: parseFloat(process.env.OTEL_SAMPLE_RATE || "1.0")
  };
  tracerInstance = new IntelGraphTracer({ ...defaultConfig, ...config9 });
  return tracerInstance;
}
function getTracer() {
  if (!tracerInstance) {
    return initializeTracing();
  }
  return tracerInstance;
}
var NodeSDK3, Resource4, SEMRESATTRS_SERVICE_NAME2, SEMRESATTRS_SERVICE_VERSION2, SEMRESATTRS_DEPLOYMENT_ENVIRONMENT2, SEMRESATTRS_SERVICE_NAMESPACE2, OTLPTraceExporter2, OTLPMetricExporter2, PeriodicExportingMetricReader2, getNodeAutoInstrumentations3, propagation2, logger30, IntelGraphTracer, tracerInstance;
var init_tracer = __esm({
  "src/observability/tracer.ts"() {
    "use strict";
    init_config();
    NodeSDK3 = opentelemetrySdkNode.NodeSDK || opentelemetrySdkNode;
    Resource4 = opentelemetryResources.Resource || opentelemetryResources;
    SEMRESATTRS_SERVICE_NAME2 = semanticConventions.SEMRESATTRS_SERVICE_NAME || "service.name";
    SEMRESATTRS_SERVICE_VERSION2 = semanticConventions.SEMRESATTRS_SERVICE_VERSION || "service.version";
    SEMRESATTRS_DEPLOYMENT_ENVIRONMENT2 = semanticConventions.SEMRESATTRS_DEPLOYMENT_ENVIRONMENT || "deployment.environment";
    SEMRESATTRS_SERVICE_NAMESPACE2 = semanticConventions.SEMRESATTRS_SERVICE_NAMESPACE || "service.namespace";
    OTLPTraceExporter2 = traceExporter.OTLPTraceExporter || traceExporter;
    OTLPMetricExporter2 = metricsExporter.OTLPMetricExporter || metricsExporter;
    PeriodicExportingMetricReader2 = sdkMetrics.PeriodicExportingMetricReader || sdkMetrics;
    getNodeAutoInstrumentations3 = autoInstrumentations.getNodeAutoInstrumentations || (() => []);
    propagation2 = otelApi.propagation || { inject: () => {
    }, extract: () => context2.active() };
    logger30 = pino33({ name: "otel-tracer" });
    IntelGraphTracer = class {
      constructor(config9) {
        this.config = config9;
        this.tracer = trace4.getTracer(
          this.config.serviceName,
          this.config.serviceVersion
        );
      }
      sdk = null;
      tracer;
      initialized = false;
      async initialize() {
        if (this.initialized) {
          logger30.warn("Tracer already initialized");
          return;
        }
        try {
          const resource = new Resource4({
            [SEMRESATTRS_SERVICE_NAME2]: this.config.serviceName,
            [SEMRESATTRS_SERVICE_VERSION2]: this.config.serviceVersion,
            [SEMRESATTRS_DEPLOYMENT_ENVIRONMENT2]: this.config.environment,
            [SEMRESATTRS_SERVICE_NAMESPACE2]: "intelgraph"
          });
          let traceExporter2;
          if (this.config.otlpTracesEndpoint) {
            traceExporter2 = new OTLPTraceExporter2({
              url: this.config.otlpTracesEndpoint
            });
            logger30.info(`OTLP Trace exporter configured: ${this.config.otlpTracesEndpoint}`);
          } else if (this.config.jaegerEndpoint) {
            traceExporter2 = new JaegerExporter2({
              endpoint: this.config.jaegerEndpoint
            });
            logger30.info(`Jaeger exporter configured: ${this.config.jaegerEndpoint}`);
          }
          let metricReader;
          if (this.config.otlpMetricsEndpoint) {
            metricReader = new PeriodicExportingMetricReader2({
              exporter: new OTLPMetricExporter2({
                url: this.config.otlpMetricsEndpoint
              }),
              exportIntervalMillis: 15e3
            });
            logger30.info(`OTLP Metric exporter configured: ${this.config.otlpMetricsEndpoint}`);
          }
          this.sdk = new NodeSDK3({
            resource,
            traceExporter: traceExporter2,
            metricReader,
            instrumentations: this.config.enableAutoInstrumentation !== false ? [
              getNodeAutoInstrumentations3({
                "@opentelemetry/instrumentation-fs": {
                  enabled: false
                  // Disable fs instrumentation (too noisy)
                },
                "@opentelemetry/instrumentation-http": {
                  enabled: true,
                  requestHook: (span, request) => {
                    span.setAttribute("http.client_ip", request.socket?.remoteAddress || "unknown");
                  }
                },
                "@opentelemetry/instrumentation-express": {
                  enabled: true
                },
                "@opentelemetry/instrumentation-graphql": {
                  enabled: true
                }
              })
            ] : []
          });
          await this.sdk.start();
          this.initialized = true;
          logger30.info("OpenTelemetry tracing initialized successfully");
        } catch (error) {
          logger30.error("Failed to initialize tracing:", error);
        }
      }
      async shutdown() {
        if (this.sdk) {
          await this.sdk.shutdown();
          this.initialized = false;
          logger30.info("OpenTelemetry tracing shut down");
        }
      }
      // Start a new span
      startSpan(name, options2) {
        const spanOptions = {
          kind: options2?.kind || SpanKind3.INTERNAL,
          attributes: options2?.attributes || {}
        };
        if (options2?.parent) {
          return this.tracer.startSpan(
            name,
            spanOptions,
            typeof options2.parent === "object" && "spanContext" in options2.parent ? trace4.setSpan(context2.active(), options2.parent) : options2.parent
          );
        }
        return this.tracer.startSpan(name, spanOptions);
      }
      // Execute function within a span context
      async withSpan(name, fn, options2) {
        const span = this.startSpan(name, options2);
        try {
          const result2 = await context2.with(
            trace4.setSpan(context2.active(), span),
            () => fn(span)
          );
          span.setStatus({ code: SpanStatusCode3.OK });
          return result2;
        } catch (error) {
          span.recordException(error);
          span.setStatus({
            code: SpanStatusCode3.ERROR,
            message: error.message
          });
          throw error;
        } finally {
          span.end();
        }
      }
      // Get current active span
      getCurrentSpan() {
        return trace4.getActiveSpan();
      }
      // Add event to current span
      addEvent(name, attributes) {
        const span = this.getCurrentSpan();
        if (span) {
          span.addEvent(name, attributes);
        }
      }
      // Set attribute on current span
      setAttribute(key, value) {
        const span = this.getCurrentSpan();
        if (span) {
          span.setAttribute(key, value);
        }
      }
      // Record exception in current span
      recordException(error) {
        const span = this.getCurrentSpan();
        if (span) {
          span.recordException(error);
          span.setStatus({
            code: SpanStatusCode3.ERROR,
            message: error.message
          });
        }
      }
      // Extract trace context from headers
      extractContext(headers) {
        return propagation2.extract(context2.active(), headers);
      }
      // Inject trace context into headers
      injectContext(headers) {
        propagation2.inject(context2.active(), headers);
      }
      // Get trace ID for logging correlation
      getTraceId() {
        const span = this.getCurrentSpan();
        if (span) {
          return span.spanContext().traceId;
        }
        return "";
      }
      // Get span ID for logging correlation
      getSpanId() {
        const span = this.getCurrentSpan();
        if (span) {
          return span.spanContext().spanId;
        }
        return "";
      }
      // Database query tracing helper
      async traceDbQuery(database, operation, query3, fn) {
        return this.withSpan(
          `db.${database}.${operation}`,
          async (span) => {
            span.setAttributes({
              "db.system": database,
              "db.operation": operation,
              "db.statement": query3.length > 500 ? query3.substring(0, 500) + "..." : query3
            });
            return fn();
          },
          { kind: SpanKind3.CLIENT }
        );
      }
      // Cache operation tracing helper
      async traceCacheOperation(operation, key, fn) {
        return this.withSpan(
          `cache.${operation}`,
          async (span) => {
            span.setAttributes({
              "cache.operation": operation,
              "cache.key": key
            });
            const result2 = await fn();
            span.setAttribute("cache.hit", result2 !== null && result2 !== void 0);
            return result2;
          },
          { kind: SpanKind3.CLIENT }
        );
      }
      // Service method tracing helper
      async traceServiceMethod(serviceName, methodName, fn, parameters) {
        return this.withSpan(
          `${serviceName}.${methodName}`,
          async (span) => {
            span.setAttributes({
              "service.name": serviceName,
              "service.method": methodName,
              ...parameters && { "service.parameters": JSON.stringify(parameters) }
            });
            return fn();
          },
          { kind: SpanKind3.INTERNAL }
        );
      }
      isInitialized() {
        return this.initialized;
      }
    };
    tracerInstance = null;
  }
});

// src/middleware/correlation-id.ts
import { randomUUID as randomUUID9 } from "crypto";
import { trace as trace5 } from "@opentelemetry/api";
function correlationIdMiddleware(req, res, next) {
  const correlationId = req.headers[CORRELATION_ID_HEADER] || req.headers[REQUEST_ID_HEADER] || randomUUID9();
  req.correlationId = correlationId;
  const tracer11 = getTracer();
  const activeSpan = trace5.getActiveSpan();
  req.traceId = activeSpan?.spanContext().traceId || tracer11.getTraceId() || "";
  req.spanId = activeSpan?.spanContext().spanId || tracer11.getSpanId() || "";
  if (!req.traceId && typeof req.headers["traceparent"] === "string") {
    const [, inboundTraceId, inboundSpanId] = req.headers["traceparent"].split("-");
    req.traceId = inboundTraceId || req.traceId;
    req.spanId = inboundSpanId || req.spanId;
  }
  const tenantId = req.headers[TENANT_ID_HEADER] || req.user?.tenant_id || "unknown";
  if (req.traceId) {
    tracer11.setAttribute("correlation.id", correlationId);
    tracer11.setAttribute("correlation.request_id", correlationId);
    if (tenantId !== "unknown") {
      tracer11.setAttribute("tenant.id", tenantId);
    }
  }
  res.setHeader(CORRELATION_ID_HEADER, correlationId);
  res.setHeader(REQUEST_ID_HEADER, correlationId);
  if (req.traceId) {
    res.setHeader("x-trace-id", req.traceId);
  }
  const store = /* @__PURE__ */ new Map();
  store.set("correlationId", correlationId);
  store.set("requestId", correlationId);
  store.set("traceId", req.traceId);
  store.set("tenantId", tenantId);
  if (req.user) {
    store.set("principalId", req.user.sub || req.user.id);
  }
  correlationStorage.run(store, () => {
    next();
  });
}
function getCorrelationContext(req) {
  return {
    correlationId: req.correlationId,
    traceId: req.traceId,
    spanId: req.spanId,
    userId: req.user?.sub || req.user?.id,
    tenantId: req.user?.tenant_id || req.tenant_id
  };
}
var CORRELATION_ID_HEADER, REQUEST_ID_HEADER, TENANT_ID_HEADER;
var init_correlation_id = __esm({
  "src/middleware/correlation-id.ts"() {
    "use strict";
    init_tracer();
    init_logger();
    CORRELATION_ID_HEADER = "x-correlation-id";
    REQUEST_ID_HEADER = "x-request-id";
    TENANT_ID_HEADER = "x-tenant-id";
  }
});

// src/middleware/auth.ts
async function ensureAuthenticated(req, res, next) {
  try {
    const auth = req.headers.authorization || "";
    const token = auth.startsWith("Bearer ") ? auth.slice("Bearer ".length) : req.headers["x-access-token"] || null;
    if (!token) return res.status(401).json({ error: "Unauthorized" });
    const user = await authService2.verifyToken(token);
    if (!user) return res.status(401).json({ error: "Unauthorized" });
    req.user = user;
    next();
  } catch (e) {
    return res.status(401).json({ error: "Unauthorized" });
  }
}
function requirePermission(permission) {
  return (req, res, next) => {
    const user = req.user;
    if (!user) return res.status(401).json({ error: "Unauthorized" });
    if (authService2.hasPermission(user, permission)) {
      metrics3.pbacDecisionsTotal?.inc({ decision: "allow" });
      return next();
    } else {
      metrics3.pbacDecisionsTotal?.inc({ decision: "deny" });
      try {
        getAuditSystem().recordEvent({
          eventType: "policy_violation",
          action: "check_permission",
          outcome: "failure",
          userId: user.id,
          tenantId: user.tenantId || "system",
          serviceId: "api-gateway",
          resourceType: "endpoint",
          resourceId: req.originalUrl,
          message: `Permission denied: ${permission}`,
          level: "warn",
          details: { permission, role: user.role }
        });
      } catch (error) {
        if (process.env.NODE_ENV !== "test") {
          logger_default2.error("Failed to log audit event", error);
        }
      }
      return res.status(403).json({ error: "Forbidden" });
    }
  };
}
function ensureRole(requiredRole) {
  const roles = Array.isArray(requiredRole) ? requiredRole : [requiredRole];
  return (req, res, next) => {
    const user = req.user;
    if (!user || !user.role) return res.status(401).json({ error: "Unauthorized" });
    if (roles.includes(user.role)) {
      return next();
    } else {
      return res.status(403).json({ error: "Forbidden: Insufficient role" });
    }
  };
}
var authService2, authMiddleware;
var init_auth4 = __esm({
  "src/middleware/auth.ts"() {
    "use strict";
    init_AuthService();
    init_advanced_audit_system();
    init_logger2();
    init_metrics4();
    authService2 = new AuthService_default();
    authMiddleware = ensureAuthenticated;
  }
});

// src/feature-flags/PostgresProvider.ts
import Redis9 from "ioredis";
import { EventEmitter as EventEmitter8 } from "events";
import crypto19 from "crypto";
var PostgresProvider;
var init_PostgresProvider = __esm({
  "src/feature-flags/PostgresProvider.ts"() {
    "use strict";
    init_postgres();
    init_logger2();
    PostgresProvider = class extends EventEmitter8 {
      name = "postgres";
      ready = false;
      redis;
      pubsub;
      cache = /* @__PURE__ */ new Map();
      lastUpdate = 0;
      constructor() {
        super();
      }
      async initialize() {
        try {
          if (process.env.REDIS_URL) {
            this.redis = new Redis9(process.env.REDIS_URL);
            this.pubsub = new Redis9(process.env.REDIS_URL);
            this.pubsub.subscribe("feature_flag_updates", (err) => {
              if (err) {
                logger_default2.error("Failed to subscribe to feature_flag_updates", err);
              }
            });
            this.pubsub.on("message", (channel, message) => {
              if (channel === "feature_flag_updates") {
                this.handleUpdate(message);
              }
            });
          }
          await this.refreshCache();
          this.ready = true;
          logger_default2.info("PostgresFeatureFlagProvider initialized");
        } catch (error) {
          logger_default2.error("Failed to initialize PostgresFeatureFlagProvider", error);
          throw error;
        }
      }
      async close() {
        if (this.redis) await this.redis.quit();
        if (this.pubsub) await this.pubsub.quit();
        this.ready = false;
      }
      isReady() {
        return this.ready;
      }
      async handleUpdate(message) {
        try {
          await this.refreshCache();
          this.emit("update");
        } catch (error) {
          logger_default2.error("Error handling flag update", error);
        }
      }
      async refreshCache() {
        const pool4 = getPostgresPool();
        const result2 = await pool4.query("SELECT * FROM feature_flags");
        this.cache.clear();
        for (const row of result2.rows) {
          this.cache.set(row.key, this.mapRowToDefinition(row));
        }
        this.lastUpdate = Date.now();
      }
      mapRowToDefinition(row) {
        return {
          key: row.key,
          name: row.key,
          // fallback
          description: row.description,
          type: row.type,
          enabled: row.enabled,
          defaultValue: row.default_value,
          variations: row.variations || [],
          rules: row.rollout_rules || [],
          // DB column is rollout_rules, type expects rules
          rollout: void 0,
          // Simple rollout supported via rules
          metadata: { tenantId: row.tenant_id },
          createdAt: row.created_at ? new Date(row.created_at).getTime() : void 0,
          updatedAt: row.updated_at ? new Date(row.updated_at).getTime() : void 0
        };
      }
      async getBooleanFlag(key, defaultValue, context4) {
        return this.evaluate(key, defaultValue, context4);
      }
      async getStringFlag(key, defaultValue, context4) {
        return this.evaluate(key, defaultValue, context4);
      }
      async getNumberFlag(key, defaultValue, context4) {
        return this.evaluate(key, defaultValue, context4);
      }
      async getJSONFlag(key, defaultValue, context4) {
        return this.evaluate(key, defaultValue, context4);
      }
      async evaluate(key, defaultValue, context4) {
        const flag = this.cache.get(key);
        if (!flag) {
          return {
            key,
            value: defaultValue,
            exists: false,
            reason: "DEFAULT",
            timestamp: Date.now()
          };
        }
        if (!flag.enabled) {
          return {
            key,
            value: defaultValue,
            // Or strict fallback? Usually if disabled we return default
            exists: true,
            reason: "OFF",
            timestamp: Date.now()
          };
        }
        if (flag.metadata?.tenantId && context4.tenantId && flag.metadata.tenantId !== context4.tenantId) {
          return {
            key,
            value: defaultValue,
            exists: true,
            reason: "DEFAULT",
            // Or custom reason
            timestamp: Date.now()
          };
        }
        if (flag.rules && flag.rules.length > 0) {
          for (const rule of flag.rules) {
            if (this.matchesRule(rule, context4)) {
              if (rule.rollout) {
                const variationId = this.evaluateRollout(rule.rollout, context4, key);
                if (variationId) {
                  const variation = flag.variations.find((v) => v.id === variationId);
                  if (variation) {
                    return {
                      key,
                      value: variation.value,
                      variation: variation.id,
                      exists: true,
                      reason: "RULE_MATCH",
                      timestamp: Date.now()
                    };
                  }
                }
              } else if (rule.variation) {
                const variation = flag.variations.find((v) => v.id === rule.variation);
                if (variation) {
                  return {
                    key,
                    value: variation.value,
                    variation: variation.id,
                    exists: true,
                    reason: "TARGET_MATCH",
                    timestamp: Date.now()
                  };
                }
              }
            }
          }
        }
        return {
          key,
          value: flag.defaultValue,
          exists: true,
          reason: "DEFAULT",
          // Flag default
          timestamp: Date.now()
        };
      }
      matchesRule(rule, context4) {
        return rule.conditions.every((condition) => {
          const contextValue = this.getContextValue(context4, condition.attribute);
          const match = this.evaluateCondition(condition, contextValue);
          return condition.negate ? !match : match;
        });
      }
      getContextValue(context4, attribute) {
        if (attribute === "userId") return context4.userId;
        if (attribute === "email") return context4.userEmail;
        if (attribute === "role") return context4.userRole;
        if (attribute === "tenantId") return context4.tenantId;
        return context4.attributes?.[attribute];
      }
      evaluateCondition(condition, value) {
        if (value === void 0 || value === null) return false;
        switch (condition.operator) {
          case "equals":
            return value === condition.value;
          case "not_equals":
            return value !== condition.value;
          case "contains":
            return String(value).includes(condition.value);
          case "in":
            return Array.isArray(condition.value) && condition.value.includes(value);
          // Add more operators as needed
          default:
            return false;
        }
      }
      evaluateRollout(rollout, context4, flagKey) {
        const bucketBy = rollout.bucketBy || "userId";
        const bucketValue = this.getContextValue(context4, bucketBy);
        if (!bucketValue) {
          return null;
        }
        const hashInput = `${flagKey}:${String(bucketValue)}:${rollout.seed || 0}`;
        const hash3 = crypto19.createHash("sha1").update(hashInput).digest("hex");
        const bucket = parseInt(hash3.substring(0, 8), 16) % 1e4;
        let accumulated = 0;
        for (const variation of rollout.variations) {
          accumulated += variation.percentage * 100;
          if (bucket < accumulated) {
            return variation.variation;
          }
        }
        return null;
      }
      async getAllFlags(context4) {
        const result2 = {};
        for (const key of this.cache.keys()) {
          result2[key] = await this.evaluate(key, null, context4);
        }
        return result2;
      }
      async getFlagDefinition(key) {
        return this.cache.get(key) || null;
      }
      async listFlags() {
        return Array.from(this.cache.values());
      }
      async track(eventName, context4, data) {
      }
    };
  }
});

// src/feature-flags/setup.ts
var setup_exports = {};
__export(setup_exports, {
  closeFeatureFlags: () => closeFeatureFlags,
  getFeatureFlagService: () => getFeatureFlagService,
  initializeFeatureFlags: () => initializeFeatureFlags
});
import {
  FeatureFlagService,
  LaunchDarklyProvider,
  UnleashProvider,
  RedisCache,
  PrometheusMetrics as PrometheusMetrics2
} from "@intelgraph/feature-flags";
import Redis10 from "ioredis";
async function initializeFeatureFlags() {
  if (featureFlagService) {
    return featureFlagService;
  }
  try {
    logger_default2.info("Initializing feature flag service...");
    const providerType = process.env.FEATURE_FLAG_PROVIDER || "launchdarkly";
    let provider;
    if (providerType === "launchdarkly") {
      if (!process.env.LAUNCHDARKLY_SDK_KEY) {
        throw new Error("LAUNCHDARKLY_SDK_KEY is required");
      }
      provider = new LaunchDarklyProvider({
        sdkKey: process.env.LAUNCHDARKLY_SDK_KEY,
        options: {
          diagnosticOptOut: process.env.NODE_ENV === "production"
        }
      });
    } else if (providerType === "unleash") {
      if (!process.env.UNLEASH_URL || !process.env.UNLEASH_APP_NAME) {
        throw new Error("UNLEASH_URL and UNLEASH_APP_NAME are required");
      }
      provider = new UnleashProvider({
        url: process.env.UNLEASH_URL,
        appName: process.env.UNLEASH_APP_NAME,
        apiToken: process.env.UNLEASH_API_TOKEN,
        instanceId: process.env.HOSTNAME || "server"
      });
    } else if (providerType === "postgres") {
      provider = new PostgresProvider();
    } else {
      throw new Error(`Unknown feature flag provider: ${providerType}`);
    }
    let cache2;
    if (process.env.REDIS_URL && providerType !== "postgres") {
      const redis5 = new Redis10(process.env.REDIS_URL, {
        maxRetriesPerRequest: 3,
        enableReadyCheck: true
      });
      cache2 = new RedisCache({
        redis: redis5,
        keyPrefix: "ff:",
        defaultTTL: parseInt(process.env.FEATURE_FLAG_CACHE_TTL || "300"),
        enableStats: true
      });
      logger_default2.info("Feature flag Redis cache initialized");
    }
    const metrics8 = new PrometheusMetrics2({
      prefix: "summit_feature_flags_",
      enableDefaultMetrics: true
    });
    featureFlagService = new FeatureFlagService({
      provider,
      cache: cache2,
      enableCache: !!cache2,
      enableMetrics: true,
      enableAnalytics: true,
      cacheTTL: parseInt(process.env.FEATURE_FLAG_CACHE_TTL || "300"),
      defaultContext: {
        environment: process.env.NODE_ENV || "development"
      }
    });
    featureFlagService.setMetrics(metrics8);
    featureFlagService.on("ready", () => {
      logger_default2.info("Feature flag service ready", {
        provider: provider.name,
        cacheEnabled: !!cache2
      });
    });
    featureFlagService.on("error", (error) => {
      logger_default2.error("Feature flag service error", {
        error: error.message,
        stack: error.stack
      });
    });
    if (process.env.NODE_ENV !== "test") {
      await featureFlagService.initialize();
      logger_default2.info("Feature flag service initialized successfully");
    } else {
      logger_default2.info("Skipping feature flag service initialization in test environment");
      setTimeout(() => featureFlagService.emit("ready"), 0);
    }
    return featureFlagService;
  } catch (error) {
    logger_default2.error("Failed to initialize feature flag service", {
      error: error.message
    });
    throw error;
  }
}
function getFeatureFlagService() {
  if (!featureFlagService) {
    throw new Error("Feature flag service not initialized");
  }
  return featureFlagService;
}
async function closeFeatureFlags() {
  if (featureFlagService) {
    await featureFlagService.close();
    featureFlagService = null;
    logger_default2.info("Feature flag service closed");
  }
}
var featureFlagService;
var init_setup = __esm({
  "src/feature-flags/setup.ts"() {
    "use strict";
    init_PostgresProvider();
    init_logger2();
    featureFlagService = null;
  }
});

// src/config/regional-config.ts
var REGIONAL_CONFIG, getCurrentRegion;
var init_regional_config = __esm({
  "src/config/regional-config.ts"() {
    "use strict";
    REGIONAL_CONFIG = {
      "us-east-1": {
        region: "us-east-1",
        baseUrl: process.env.REGION_URL_US_EAST_1 || "https://us-east.intelgraph.com",
        isPublic: true
      },
      "us-west-2": {
        region: "us-west-2",
        baseUrl: process.env.REGION_URL_US_WEST_2 || "https://us-west.intelgraph.com",
        isPublic: true
      },
      "eu-central-1": {
        region: "eu-central-1",
        baseUrl: process.env.REGION_URL_EU_CENTRAL_1 || "https://eu-central.intelgraph.com",
        isPublic: true
      },
      "eu-west-1": {
        region: "eu-west-1",
        baseUrl: process.env.REGION_URL_EU_WEST_1 || "https://eu-west.intelgraph.com",
        isPublic: true
      }
    };
    getCurrentRegion = () => {
      return process.env.SUMMIT_REGION || process.env.REGION || "us-east-1";
    };
  }
});

// src/services/RegionalAvailabilityService.ts
var RegionalAvailabilityService;
var init_RegionalAvailabilityService = __esm({
  "src/services/RegionalAvailabilityService.ts"() {
    "use strict";
    init_logger2();
    RegionalAvailabilityService = class _RegionalAvailabilityService {
      static instance;
      state = {
        regions: {
          "us-east-1": { status: "HEALTHY", lastUpdated: /* @__PURE__ */ new Date() },
          "us-west-2": { status: "HEALTHY", lastUpdated: /* @__PURE__ */ new Date() },
          // DR for us-east-1
          "eu-central-1": { status: "HEALTHY", lastUpdated: /* @__PURE__ */ new Date() },
          "eu-west-1": { status: "HEALTHY", lastUpdated: /* @__PURE__ */ new Date() }
          // DR for eu-central-1
        },
        failoverMode: "AUTOMATIC"
      };
      constructor() {
      }
      static getInstance() {
        if (!_RegionalAvailabilityService.instance) {
          _RegionalAvailabilityService.instance = new _RegionalAvailabilityService();
        }
        return _RegionalAvailabilityService.instance;
      }
      getStatus() {
        return this.state;
      }
      setRegionStatus(region, status) {
        if (!this.state.regions[region]) {
          throw new Error(`Unknown region: ${region}`);
        }
        this.state.regions[region] = { status, lastUpdated: /* @__PURE__ */ new Date() };
        logger_default2.info(`Region ${region} status set to ${status}`);
      }
      setFailoverMode(mode) {
        this.state.failoverMode = mode;
        logger_default2.info(`Failover mode set to ${mode}`);
      }
      reset() {
        this.state = {
          regions: {
            "us-east-1": { status: "HEALTHY", lastUpdated: /* @__PURE__ */ new Date() },
            "us-west-2": { status: "HEALTHY", lastUpdated: /* @__PURE__ */ new Date() },
            "eu-central-1": { status: "HEALTHY", lastUpdated: /* @__PURE__ */ new Date() },
            "eu-west-1": { status: "HEALTHY", lastUpdated: /* @__PURE__ */ new Date() }
          },
          failoverMode: "AUTOMATIC"
        };
        logger_default2.info("Regional availability state reset");
      }
    };
  }
});

// src/services/RegionalFailoverService.ts
var RegionalFailoverService, regionalFailoverService;
var init_RegionalFailoverService = __esm({
  "src/services/RegionalFailoverService.ts"() {
    "use strict";
    init_RegionalAvailabilityService();
    init_logger2();
    RegionalFailoverService = class _RegionalFailoverService {
      static instance;
      drPairs = {
        "us-east-1": "us-west-2",
        "us-west-2": "us-east-1",
        "eu-central-1": "eu-west-1",
        "eu-west-1": "eu-central-1"
      };
      constructor() {
      }
      static getInstance() {
        if (!_RegionalFailoverService.instance) {
          _RegionalFailoverService.instance = new _RegionalFailoverService();
        }
        return _RegionalFailoverService.instance;
      }
      /**
       * Resolves the target region for a given original region.
       * If the original region is DOWN, it returns the DR pair if healthy.
       */
      resolveTargetRegion(region) {
        const availability = RegionalAvailabilityService.getInstance();
        const status = availability.getStatus();
        const regionState = status.regions[region];
        if (!regionState || regionState.status !== "DOWN") {
          return region;
        }
        const drRegion = this.drPairs[region];
        if (!drRegion) {
          logger_default2.warn(`Region ${region} is DOWN but no DR pair is defined.`);
          return region;
        }
        const drState = status.regions[drRegion];
        if (drState && drState.status === "HEALTHY") {
          logger_default2.info(`Failover: Shifting traffic from ${region} (DOWN) to ${drRegion} (HEALTHY)`);
          return drRegion;
        }
        logger_default2.error(`Disaster: Both ${region} and its DR pair ${drRegion} are unavailable!`);
        return region;
      }
      getDRPair(region) {
        return this.drPairs[region];
      }
    };
    regionalFailoverService = RegionalFailoverService.getInstance();
  }
});

// src/runtime/global/GlobalTrafficSteering.ts
var GlobalTrafficSteering_exports = {};
__export(GlobalTrafficSteering_exports, {
  GlobalTrafficSteering: () => GlobalTrafficSteering,
  globalTrafficSteering: () => globalTrafficSteering
});
var GlobalTrafficSteering, globalTrafficSteering;
var init_GlobalTrafficSteering = __esm({
  "src/runtime/global/GlobalTrafficSteering.ts"() {
    "use strict";
    init_residency_guard();
    init_RegionalFailoverService();
    init_regional_config();
    GlobalTrafficSteering = class _GlobalTrafficSteering {
      static instance;
      currentRegion;
      constructor() {
        this.currentRegion = getCurrentRegion();
      }
      static getInstance() {
        if (!_GlobalTrafficSteering.instance) {
          _GlobalTrafficSteering.instance = new _GlobalTrafficSteering();
        }
        return _GlobalTrafficSteering.instance;
      }
      /**
       * Resolves the steering action based on the routing decision.
       */
      async resolveSteeringAction(tenantId) {
        const decision = await this.resolveRegion(tenantId);
        if (decision.targetRegion === this.currentRegion) {
          return { action: "ALLOW", reason: decision.reason };
        }
        const targetUrl = REGIONAL_CONFIG[decision.targetRegion]?.baseUrl;
        if (!targetUrl) {
          return { action: "ALLOW", reason: `${decision.reason} (No URL mapping for ${decision.targetRegion})` };
        }
        return {
          action: "REDIRECT",
          targetUrl,
          reason: decision.reason
        };
      }
      /**
       * Resolves the optimal region for a given tenant.
       * Uses residency configurations to determine if the local region is appropriate.
       */
      async resolveRegion(tenantId) {
        const guard = ResidencyGuard.getInstance();
        const config9 = await guard.getResidencyConfig(tenantId);
        if (!config9) {
          return {
            targetRegion: this.currentRegion,
            isOptimal: true,
            reason: "No residency configuration found, defaulting to local region."
          };
        }
        const primaryRegion = config9.primaryRegion;
        const failoverService = RegionalFailoverService.getInstance();
        const effectiveTarget = failoverService.resolveTargetRegion(primaryRegion);
        if (effectiveTarget !== primaryRegion) {
          return {
            targetRegion: effectiveTarget,
            isOptimal: false,
            reason: `Primary region ${primaryRegion} is DOWN. Failing over to ${effectiveTarget}.`
          };
        }
        if (primaryRegion === this.currentRegion) {
          return {
            targetRegion: this.currentRegion,
            isOptimal: true,
            reason: "Current region matches tenant primary residency and is healthy."
          };
        }
        const isAllowed = config9.allowedRegions.includes(this.currentRegion);
        if (isAllowed && config9.residencyMode !== "strict") {
          return {
            targetRegion: this.currentRegion,
            isOptimal: true,
            reason: "Current region is an allowed secondary region (Preferred mode) and is healthy."
          };
        }
        return {
          targetRegion: primaryRegion,
          isOptimal: false,
          reason: `Routing to tenant primary region: ${primaryRegion} (Current: ${this.currentRegion})`
        };
      }
    };
    globalTrafficSteering = GlobalTrafficSteering.getInstance();
  }
});

// src/observability/context.ts
import { AsyncLocalStorage as AsyncLocalStorage2 } from "async_hooks";
function getContext2() {
  return contextStorage.getStore();
}
var contextStorage;
var init_context = __esm({
  "src/observability/context.ts"() {
    "use strict";
    contextStorage = new AsyncLocalStorage2();
  }
});

// src/observability/logging/logger.ts
var ContextAwareLogger, logger33;
var init_logger3 = __esm({
  "src/observability/logging/logger.ts"() {
    "use strict";
    init_logger();
    init_context();
    ContextAwareLogger = class _ContextAwareLogger {
      constructor(baseLogger2 = logger) {
        this.baseLogger = baseLogger2;
      }
      getMeta(userMeta) {
        const ctx = getContext2();
        const meta = { ...userMeta };
        if (ctx) {
          if (ctx.correlationId) meta.correlationId = ctx.correlationId;
          if (ctx.tenantId) meta.tenantId = ctx.tenantId;
          if (ctx.requestId) meta.requestId = ctx.requestId;
        }
        return meta;
      }
      debug(msg, meta) {
        this.baseLogger.debug(this.getMeta(meta), msg);
      }
      info(msg, meta) {
        this.baseLogger.info(this.getMeta(meta), msg);
      }
      warn(msg, meta) {
        this.baseLogger.warn(this.getMeta(meta), msg);
      }
      error(msg, meta) {
        this.baseLogger.error(this.getMeta(meta), msg);
      }
      child(bindings) {
        return new _ContextAwareLogger(this.baseLogger.child(bindings));
      }
    };
    logger33 = new ContextAwareLogger();
  }
});

// src/observability/metrics/metrics.ts
import { Counter as Counter9, Histogram as Histogram6, Gauge as Gauge7 } from "prom-client";
var METRIC_DEFINITIONS, PrometheusMetricsService, metrics4;
var init_metrics5 = __esm({
  "src/observability/metrics/metrics.ts"() {
    "use strict";
    init_metrics4();
    METRIC_DEFINITIONS = {
      "summit_api_requests_total": {
        type: "counter",
        help: "Total HTTP requests",
        labelNames: ["method", "route", "status", "tenantId"]
      },
      "summit_api_latency_seconds": {
        type: "histogram",
        help: "API Latency distribution",
        labelNames: ["method", "route"]
      },
      "summit_errors_total": {
        type: "counter",
        help: "Global error counter",
        labelNames: ["code", "component", "tenantId"]
      },
      "summit_maestro_runs_total": {
        type: "counter",
        help: "Maestro orchestration runs",
        labelNames: ["status", "tenantId"]
      },
      "summit_maestro_run_duration_seconds": {
        type: "histogram",
        help: "Time to complete a run",
        labelNames: ["status", "tenantId"]
      },
      "summit_maestro_task_duration_seconds": {
        type: "histogram",
        help: "Time to complete a task",
        labelNames: ["status", "agent", "tenantId"]
      },
      "summit_llm_requests_total": {
        type: "counter",
        help: "LLM calls",
        labelNames: ["provider", "model", "status", "tenantId"]
      },
      "summit_llm_latency_seconds": {
        type: "histogram",
        help: "LLM latency",
        labelNames: ["provider", "model"]
      },
      "summit_llm_tokens_total": {
        type: "counter",
        help: "LLM token usage",
        labelNames: ["provider", "model", "kind"]
      },
      "summit_webhook_deliveries_total": {
        type: "counter",
        help: "Webhook deliveries",
        labelNames: ["status", "provider"]
      }
    };
    PrometheusMetricsService = class {
      metrics = /* @__PURE__ */ new Map();
      constructor() {
        this.initializeMetrics();
      }
      initializeMetrics() {
        for (const [name, config9] of Object.entries(METRIC_DEFINITIONS)) {
          if (typeof register4.getSingleMetric === "function" && register4.getSingleMetric(name)) {
            this.metrics.set(name, register4.getSingleMetric(name));
            continue;
          }
          let metric;
          switch (config9.type) {
            case "counter":
              metric = new Counter9({
                name,
                help: config9.help,
                labelNames: config9.labelNames,
                registers: [register4]
              });
              break;
            case "histogram":
              metric = new Histogram6({
                name,
                help: config9.help,
                labelNames: config9.labelNames,
                registers: [register4],
                buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 30, 60]
              });
              break;
            case "gauge":
              metric = new Gauge7({
                name,
                help: config9.help,
                labelNames: config9.labelNames,
                registers: [register4]
              });
              break;
          }
          this.metrics.set(name, metric);
        }
      }
      incrementCounter(name, labels2 = {}, value = 1) {
        const metric = this.metrics.get(name);
        if (metric && metric instanceof Counter9) {
          metric.inc(labels2, value);
        } else {
        }
      }
      observeHistogram(name, value, labels2 = {}) {
        const metric = this.metrics.get(name);
        if (metric && metric instanceof Histogram6) {
          metric.observe(labels2, value);
        }
      }
      setGauge(name, value, labels2 = {}) {
        const metric = this.metrics.get(name);
        if (metric && metric instanceof Gauge7) {
          metric.set(labels2, value);
        }
      }
    };
    metrics4 = new PrometheusMetricsService();
  }
});

// src/observability/tracing.ts
import { trace as trace7, context as context3, SpanStatusCode as SpanStatusCode4, SpanKind as SpanKind4 } from "@opentelemetry/api";
import { NodeSDK as NodeSDK4 } from "@opentelemetry/sdk-node";
import { getNodeAutoInstrumentations as getNodeAutoInstrumentations4 } from "@opentelemetry/auto-instrumentations-node";
import { Resource as Resource5 } from "@opentelemetry/resources";
import { SEMRESATTRS_SERVICE_NAME as SEMRESATTRS_SERVICE_NAME3, SEMRESATTRS_SERVICE_VERSION as SEMRESATTRS_SERVICE_VERSION3, SEMRESATTRS_DEPLOYMENT_ENVIRONMENT as SEMRESATTRS_DEPLOYMENT_ENVIRONMENT3 } from "@opentelemetry/semantic-conventions";
import { JaegerExporter as JaegerExporter3 } from "@opentelemetry/exporter-jaeger";
import { PrometheusExporter as PrometheusExporter2 } from "@opentelemetry/exporter-prometheus";
import pino36 from "pino";
var logger34, TracingService, tracer6;
var init_tracing = __esm({
  "src/observability/tracing.ts"() {
    "use strict";
    logger34 = pino36({ name: "observability:tracing" });
    TracingService = class _TracingService {
      static instance;
      sdk = null;
      tracer = null;
      config;
      constructor(config9) {
        this.config = {
          serviceName: config9?.serviceName || process.env.OTEL_SERVICE_NAME || "intelgraph-api",
          serviceVersion: config9?.serviceVersion || process.env.OTEL_SERVICE_VERSION || "2.5.0",
          environment: config9?.environment || process.env.NODE_ENV || "development",
          jaegerEndpoint: config9?.jaegerEndpoint || process.env.JAEGER_ENDPOINT,
          prometheusPort: config9?.prometheusPort || parseInt(process.env.PROMETHEUS_PORT || "9464"),
          sampleRate: config9?.sampleRate ?? parseFloat(process.env.OTEL_SAMPLE_RATE || "0.1"),
          enabled: config9?.enabled ?? process.env.OTEL_ENABLED !== "false"
        };
        if (this.config.enabled && process.env.NODE_ENV !== "test") {
          this.initialize();
        }
      }
      static getInstance(config9) {
        if (!_TracingService.instance) {
          _TracingService.instance = new _TracingService(config9);
        }
        return _TracingService.instance;
      }
      /**
       * Initialize OpenTelemetry SDK with exporters and instrumentation
       */
      initialize() {
        try {
          const resource = Resource5.default().merge(
            new Resource5({
              [SEMRESATTRS_SERVICE_NAME3]: this.config.serviceName,
              [SEMRESATTRS_SERVICE_VERSION3]: this.config.serviceVersion,
              [SEMRESATTRS_DEPLOYMENT_ENVIRONMENT3]: this.config.environment
            })
          );
          const traceExporter2 = this.config.jaegerEndpoint ? new JaegerExporter3({ endpoint: this.config.jaegerEndpoint }) : void 0;
          const metricReader = new PrometheusExporter2({
            port: this.config.prometheusPort
          });
          this.sdk = new NodeSDK4({
            resource,
            traceExporter: traceExporter2,
            metricReader,
            instrumentations: [
              getNodeAutoInstrumentations4({
                "@opentelemetry/instrumentation-fs": { enabled: false }
              })
            ]
          });
          this.sdk.start();
          this.tracer = trace7.getTracer(this.config.serviceName, this.config.serviceVersion);
          logger34.info({
            serviceName: this.config.serviceName,
            environment: this.config.environment,
            jaegerEnabled: !!this.config.jaegerEndpoint,
            prometheusPort: this.config.prometheusPort
          }, "OpenTelemetry tracing initialized");
        } catch (error) {
          logger34.error({ error }, "Failed to initialize OpenTelemetry");
          this.config.enabled = false;
        }
      }
      /**
       * Generic trace wrapper - wraps any async operation with tracing
       *
       * @example
       * const result = await tracer.trace('database.query', async (span: any) => {
       *   span.setAttribute('db.statement', query);
       *   return await db.query(query);
       * });
       */
      async trace(operationName, operation, options2 = {}) {
        if (!this.config.enabled || !this.tracer) {
          const noopSpan = this.createNoOpSpan();
          return await operation(noopSpan);
        }
        const span = this.startSpan(operationName, options2);
        return context3.with(trace7.setSpan(context3.active(), span), async () => {
          try {
            const result2 = await operation(span);
            span.setStatus({ code: SpanStatusCode4.OK });
            return result2;
          } catch (error) {
            span.setStatus({
              code: SpanStatusCode4.ERROR,
              message: error instanceof Error ? error.message : "Unknown error"
            });
            span.recordException(error instanceof Error ? error : new Error(String(error)));
            throw error;
          } finally {
            span.end();
          }
        });
      }
      /**
       * Trace database operations with standardized attributes
       */
      async traceDatabase(operation, dbType, dbOperation, query3) {
        return this.trace(
          `db.${dbType}.${operation}`,
          async (span) => {
            span.setAttribute("db.system", dbType);
            span.setAttribute("db.operation", operation);
            if (query3) {
              span.setAttribute("db.statement", query3.substring(0, 500));
            }
            return await dbOperation();
          },
          { kind: SpanKind4.CLIENT }
        );
      }
      /**
       * Trace GraphQL resolver operations
       */
      async traceGraphQL(operationName, fieldName, resolver, contextData) {
        return this.trace(
          `graphql.${operationName}`,
          async (span) => {
            span.setAttribute("graphql.operation.name", operationName);
            span.setAttribute("graphql.field.name", fieldName);
            if (contextData?.user?.id) {
              span.setAttribute("user.id", contextData.user.id);
            }
            return await resolver();
          },
          { kind: SpanKind4.SERVER }
        );
      }
      /**
       * Trace message queue operations (BullMQ, Kafka, etc.)
       */
      async traceQueue(queueName, jobName, processor) {
        return this.trace(
          `queue.${queueName}.${jobName}`,
          async (span) => {
            span.setAttribute("messaging.system", "redis");
            span.setAttribute("messaging.destination", queueName);
            span.setAttribute("messaging.operation", "process");
            span.setAttribute("job.name", jobName);
            return await processor();
          },
          { kind: SpanKind4.CONSUMER }
        );
      }
      /**
       * Trace HTTP requests
       */
      async traceHTTP(method, url, httpOperation) {
        return this.trace(
          `http.${method.toLowerCase()}`,
          async (span) => {
            span.setAttribute("http.method", method);
            span.setAttribute("http.url", url);
            return await httpOperation();
          },
          { kind: SpanKind4.CLIENT }
        );
      }
      /**
       * Start a new span manually
       */
      startSpan(name, options2 = {}) {
        if (!this.config.enabled || !this.tracer) {
          return this.createNoOpSpan();
        }
        return this.tracer.startSpan(name, {
          kind: options2.kind || SpanKind4.INTERNAL,
          attributes: {
            "service.name": this.config.serviceName,
            "service.version": this.config.serviceVersion,
            "deployment.environment": this.config.environment,
            ...options2.attributes
          }
        });
      }
      /**
       * Get the currently active span
       */
      getActiveSpan() {
        return trace7.getActiveSpan();
      }
      /**
       * Add attributes to the active span
       */
      addAttributes(attributes) {
        const span = this.getActiveSpan();
        if (span) {
          span.setAttributes(attributes);
        }
      }
      /**
       * Record an exception in the active span
       */
      recordException(error, attributes) {
        const span = this.getActiveSpan();
        if (span) {
          const errorObj = typeof error === "string" ? new Error(error) : error;
          span.recordException(errorObj);
          if (attributes) {
            span.setAttributes(attributes);
          }
        }
      }
      /**
       * Get current trace context for propagation (W3C format)
       */
      getTraceContext() {
        const span = this.getActiveSpan();
        if (span) {
          const spanContext = span.spanContext();
          return `00-${spanContext.traceId}-${spanContext.spanId}-${spanContext.traceFlags.toString(16).padStart(2, "0")}`;
        }
        return void 0;
      }
      /**
       * Get configuration
       */
      getConfig() {
        return { ...this.config };
      }
      /**
       * Shutdown SDK gracefully
       */
      async shutdown() {
        if (this.sdk) {
          await this.sdk.shutdown();
          logger34.info("OpenTelemetry SDK shutdown");
        }
      }
      /**
       * Create no-op span for disabled tracing
       */
      createNoOpSpan() {
        const noopSpan = {
          setStatus: () => noopSpan,
          setAttributes: () => noopSpan,
          setAttribute: () => noopSpan,
          addEvent: () => noopSpan,
          recordException: () => {
          },
          end: () => {
          },
          updateName: () => noopSpan,
          isRecording: () => false,
          spanContext: () => ({
            traceId: "00000000000000000000000000000000",
            spanId: "0000000000000000",
            traceFlags: 0
          })
        };
        return noopSpan;
      }
    };
    tracer6 = TracingService.getInstance();
    if (process.env.NODE_ENV !== "test") {
      process.on("SIGTERM", async () => {
        await tracer6.shutdown();
      });
      process.on("SIGINT", async () => {
        await tracer6.shutdown();
      });
    }
  }
});

// src/observability/operational-intelligence/fabric.ts
var DEFAULT_WINDOW_MS;
var init_fabric = __esm({
  "src/observability/operational-intelligence/fabric.ts"() {
    "use strict";
    DEFAULT_WINDOW_MS = 5 * 60 * 1e3;
  }
});

// src/observability/operational-intelligence/root-cause.ts
var init_root_cause = __esm({
  "src/observability/operational-intelligence/root-cause.ts"() {
    "use strict";
  }
});

// src/observability/operational-intelligence/predictive.ts
var init_predictive = __esm({
  "src/observability/operational-intelligence/predictive.ts"() {
    "use strict";
  }
});

// src/observability/operational-intelligence/failure-simulator.ts
var init_failure_simulator = __esm({
  "src/observability/operational-intelligence/failure-simulator.ts"() {
    "use strict";
  }
});

// src/observability/operational-intelligence/types.ts
var init_types3 = __esm({
  "src/observability/operational-intelligence/types.ts"() {
    "use strict";
  }
});

// src/observability/operational-intelligence/index.ts
var init_operational_intelligence = __esm({
  "src/observability/operational-intelligence/index.ts"() {
    "use strict";
    init_fabric();
    init_root_cause();
    init_predictive();
    init_failure_simulator();
    init_types3();
  }
});

// src/observability/index.ts
var init_observability = __esm({
  "src/observability/index.ts"() {
    "use strict";
    init_context();
    init_logger3();
    init_metrics5();
    init_tracing();
    init_operational_intelligence();
  }
});

// src/lib/resources/quota-manager.ts
var QuotaManager, quota_manager_default;
var init_quota_manager = __esm({
  "src/lib/resources/quota-manager.ts"() {
    "use strict";
    QuotaManager = class _QuotaManager {
      static instance;
      tenantTiers = /* @__PURE__ */ new Map();
      constructor() {
      }
      static getInstance() {
        if (!_QuotaManager.instance) {
          _QuotaManager.instance = new _QuotaManager();
        }
        return _QuotaManager.instance;
      }
      getQuotaForTenant(tenantId) {
        const tier = this.tenantTiers.get(tenantId) || "FREE";
        return this.getQuotaForTier(tier);
      }
      getQuotaForTier(tier) {
        const baseQuota = this.getBaseQuota(tier);
        const workloadLimits = {};
        switch (tier) {
          case "ENTERPRISE":
            workloadLimits.PLANNING = { limit: 1e4, period: "minute" };
            workloadLimits.EVALUATION = { limit: 5e3, period: "minute" };
            workloadLimits.READ_ONLY = { limit: 5e4, period: "minute" };
            break;
          case "PRO":
            workloadLimits.PLANNING = { limit: 1e3, period: "minute" };
            workloadLimits.EVALUATION = { limit: 500, period: "minute" };
            workloadLimits.READ_ONLY = { limit: 5e3, period: "minute" };
            break;
          case "STARTER":
            workloadLimits.PLANNING = { limit: 100, period: "minute" };
            workloadLimits.EVALUATION = { limit: 50, period: "minute" };
            workloadLimits.READ_ONLY = { limit: 500, period: "minute" };
            break;
          case "FREE":
          default:
            workloadLimits.PLANNING = { limit: 10, period: "minute" };
            workloadLimits.EVALUATION = { limit: 5, period: "minute" };
            workloadLimits.READ_ONLY = { limit: 100, period: "minute" };
            break;
        }
        return { ...baseQuota, workloadLimits };
      }
      getBaseQuota(tier) {
        switch (tier) {
          case "ENTERPRISE":
            return {
              tier: "ENTERPRISE",
              requestsPerMinute: 1e4,
              requestsPerDay: 1e7,
              ingestEventsPerMinute: 5e4,
              maxTokensPerRequest: 32e3,
              storageLimitBytes: 1e12,
              // 1 TB
              seatCap: 500,
              burstAllowance: 1.5
              // Generous burst
            };
          case "PRO":
            return {
              tier: "PRO",
              requestsPerMinute: 1e3,
              requestsPerDay: 1e6,
              ingestEventsPerMinute: 5e3,
              maxTokensPerRequest: 16e3,
              storageLimitBytes: 25e10,
              // 250 GB
              seatCap: 150,
              burstAllowance: 1.2
            };
          case "STARTER":
            return {
              tier: "STARTER",
              requestsPerMinute: 100,
              requestsPerDay: 1e5,
              ingestEventsPerMinute: 500,
              maxTokensPerRequest: 4e3,
              storageLimitBytes: 5e10,
              // 50 GB
              seatCap: 50,
              burstAllowance: 1.1
            };
          case "FREE":
          default:
            return {
              tier: "FREE",
              requestsPerMinute: 20,
              requestsPerDay: 1e3,
              ingestEventsPerMinute: 100,
              maxTokensPerRequest: 1e3,
              storageLimitBytes: 5e9,
              // 5 GB
              seatCap: 5,
              burstAllowance: 1
              // No burst
            };
        }
      }
      setTenantTier(tenantId, tier) {
        this.tenantTiers.set(tenantId, tier);
      }
    };
    quota_manager_default = QuotaManager.getInstance();
  }
});

// src/services/RateLimiter.ts
import pino37 from "pino";
var logger35, RateLimiter, rateLimiter;
var init_RateLimiter = __esm({
  "src/services/RateLimiter.ts"() {
    "use strict";
    init_database();
    init_metrics3();
    logger35 = pino37();
    RateLimiter = class {
      metrics;
      namespace = "rate_limit";
      constructor() {
        this.metrics = new PrometheusMetrics("rate_limiter");
        this.metrics.createCounter("hits_total", "Total rate limit checks", ["status"]);
        this.metrics.createCounter("blocked_total", "Total blocked requests", ["key_prefix"]);
      }
      /**
       * Check if a key has exceeded the rate limit (consumes 1 point).
       */
      async checkLimit(key, limit, windowMs) {
        return this.consume(key, 1, limit, windowMs);
      }
      /**
       * Consume points from the rate limit bucket.
       * Uses a sliding window counter (fixed window with Redis expiration).
       *
       * @param key Unique key for the limit
       * @param points Number of points to consume
       * @param limit Max points allowed in window
       * @param windowMs Window size in milliseconds
       * @returns RateLimitResult
       */
      async consume(key, points, limit, windowMs) {
        const redisKey = `${this.namespace}:${key}`;
        const now = Date.now();
        const redisClient4 = getRedisClient();
        if (!redisClient4) {
          logger35.warn("Redis client not available for rate limiting, allowing request");
          return {
            allowed: true,
            total: limit,
            remaining: limit,
            reset: now + windowMs
          };
        }
        try {
          const script = `
        local current = redis.call("INCRBY", KEYS[1], ARGV[1])
        local ttl = redis.call("PTTL", KEYS[1])
        if tonumber(current) == tonumber(ARGV[1]) then
          redis.call("PEXPIRE", KEYS[1], ARGV[2])
          ttl = ARGV[2]
        end
        return {current, ttl}
      `;
          const result2 = await redisClient4.eval(script, 1, redisKey, points, windowMs);
          const current = result2[0];
          const ttl = result2[1];
          const allowed = current <= limit;
          const remaining = Math.max(0, limit - current);
          const reset = now + (ttl > 0 ? ttl : windowMs);
          this.metrics.incrementCounter("hits_total", { status: allowed ? "allowed" : "blocked" });
          if (!allowed) {
            const prefix = key.split(":")[0] || "unknown";
            this.metrics.incrementCounter("blocked_total", { key_prefix: prefix });
          }
          return {
            allowed,
            total: limit,
            remaining,
            reset
          };
        } catch (error) {
          logger35.error({ err: error }, "Rate limiting error");
          return {
            allowed: true,
            total: limit,
            remaining: limit,
            reset: now + windowMs
          };
        }
      }
    };
    rateLimiter = new RateLimiter();
  }
});

// src/services/GACoremetricsService.ts
var GACoremetricsService_exports = {};
__export(GACoremetricsService_exports, {
  GACoreMetricsService: () => GACoreMetricsService,
  gaCoreMetrics: () => gaCoreMetrics
});
import {
  register as register6,
  Gauge as Gauge9,
  Histogram as Histogram8,
  collectDefaultMetrics as collectDefaultMetrics4
} from "prom-client";
var log2, GACoreMetricsService, gaCoreMetrics;
var init_GACoremetricsService = __esm({
  "src/services/GACoremetricsService.ts"() {
    "use strict";
    init_database();
    init_database();
    init_neo4j();
    init_logger();
    log2 = logger_default.child({ name: "GACoreMetrics" });
    collectDefaultMetrics4();
    GACoreMetricsService = class {
      // GA Core Release Status (0=NO GO, 0.5=CONDITIONAL GO, 1=GO)
      gaCoreOverallStatus = new Gauge9({
        name: "ga_core_overall_status",
        help: "Overall GA Core release status (0=NO GO, 0.5=CONDITIONAL GO, 1=GO)"
      });
      // Entity Resolution Precision Metrics
      erPrecisionPersonCurrent = new Gauge9({
        name: "er_precision_person_current",
        help: "Current Entity Resolution precision for PERSON entities"
      });
      erPrecisionOrgCurrent = new Gauge9({
        name: "er_precision_org_current",
        help: "Current Entity Resolution precision for ORG entities"
      });
      erPrecisionPersonDaily = new Gauge9({
        name: "er_precision_person_daily",
        help: "Daily Entity Resolution precision for PERSON entities",
        labelNames: ["date"]
      });
      erPrecisionOrgDaily = new Gauge9({
        name: "er_precision_org_daily",
        help: "Daily Entity Resolution precision for ORG entities",
        labelNames: ["date"]
      });
      // Merge Decision Performance
      mergeDecisionDuration = new Histogram8({
        name: "merge_decision_duration_seconds",
        help: "Time taken to make merge decisions",
        labelNames: ["entity_type", "method"],
        buckets: [0.1, 0.5, 1, 2, 5, 10, 30]
      });
      // Policy & Appeals Metrics
      policyDenialsWithAppealsRate = new Gauge9({
        name: "policy_denials_with_appeals_rate",
        help: "Percentage of policy denials that have appeal paths available"
      });
      policyAppealSlaComplianceRate = new Gauge9({
        name: "policy_appeal_sla_compliance_rate",
        help: "Percentage of policy appeals responded to within SLA"
      });
      // Export Integrity Metrics
      exportManifestIntegrityRate = new Gauge9({
        name: "export_manifest_integrity_rate",
        help: "Percentage of exports with valid manifest integrity"
      });
      exportBundleVerificationRate = new Gauge9({
        name: "export_bundle_verification_rate",
        help: "Percentage of export bundles that pass verification"
      });
      // Copilot NLCypher Metrics
      copilotNlSuccessRate = new Gauge9({
        name: "copilot_nl_success_rate",
        help: "Success rate of NL to Cypher translations"
      });
      // Hypothesis Rigor Score
      hypothesisRigorScoreAvg = new Gauge9({
        name: "hypothesis_rigor_score_avg",
        help: "Average hypothesis rigor score (0-10)"
      });
      // Data Quality Score
      dataQualityScoreOverall = new Gauge9({
        name: "data_quality_score_overall",
        help: "Overall data quality score (0-1)"
      });
      // GA Core Gate Status (detailed)
      gaCoreGateStatus = new Gauge9({
        name: "ga_core_gate_status",
        help: "Individual GA Core gate status",
        labelNames: ["gate", "status", "requirement", "current_value", "threshold"]
      });
      // @ts-ignore - HybridEntityResolutionService class doesn't exist
      // private erService = new HybridEntityResolutionService();
      constructor() {
      }
      start() {
        if (this.isStarted) return;
        this.isStarted = true;
        this.startMetricsCollection();
      }
      isStarted = false;
      startMetricsCollection() {
        setInterval(async () => {
          try {
            await this.collectAllMetrics();
          } catch (error) {
            log2.error(
              { error: error.message },
              "Failed to collect GA Core metrics"
            );
          }
        }, 3e4);
        this.collectAllMetrics().catch((error) => {
          log2.error({ error: error.message }, "Initial metrics collection failed");
        });
      }
      async collectAllMetrics() {
        log2.debug("Collecting GA Core metrics");
        await Promise.allSettled([
          this.collectERPrecisionMetrics(),
          this.collectPolicyAppealMetrics(),
          this.collectExportIntegrityMetrics(),
          this.collectCopilotMetrics(),
          this.collectHypothesisRigorMetrics(),
          this.collectDataQualityMetrics()
        ]);
        await this.calculateOverallStatus();
      }
      async collectERPrecisionMetrics() {
        try {
          const pool4 = getPostgresPool2();
          const currentMetrics = await pool4.query(`
        SELECT
          entity_type,
          precision,
          total_decisions,
          avg_merge_confidence
        FROM er_precision_metrics
        WHERE last_updated >= NOW() - INTERVAL '1 day'
        ORDER BY last_updated DESC
      `);
          for (const row of currentMetrics.rows) {
            const precision = parseFloat(row.precision);
            if (row.entity_type === "PERSON") {
              this.erPrecisionPersonCurrent.set(precision);
            } else if (row.entity_type === "ORG") {
              this.erPrecisionOrgCurrent.set(precision);
            }
          }
          const dailyMetrics = await pool4.query(`
        SELECT
          DATE(created_at) as metric_date,
          entity_type,
          precision
        FROM er_ci_metrics
        WHERE created_at >= NOW() - INTERVAL '7 days'
        ORDER BY metric_date DESC
      `);
          for (const row of dailyMetrics.rows) {
            const precision = parseFloat(row.precision);
            const date = row.metric_date;
            if (row.entity_type === "PERSON") {
              this.erPrecisionPersonDaily.set({ date }, precision);
            } else if (row.entity_type === "ORG") {
              this.erPrecisionOrgDaily.set({ date }, precision);
            }
          }
          log2.debug("Collected ER precision metrics");
        } catch (error) {
          log2.error(
            { error: error.message },
            "Failed to collect ER precision metrics"
          );
        }
      }
      async collectPolicyAppealMetrics() {
        try {
          const pool4 = getPostgresPool2();
          const appealMetrics = await pool4.query(`
        SELECT get_ga_appeal_metrics(7) as metrics
      `);
          const metrics8 = appealMetrics.rows[0]?.metrics;
          if (metrics8) {
            this.policyAppealSlaComplianceRate.set(
              parseFloat(metrics8.sla_compliance_rate)
            );
          }
          const denialMetrics = await pool4.query(`
        SELECT
          COUNT(CASE WHEN appeal_available = true THEN 1 END)::DECIMAL /
          NULLIF(COUNT(*), 0) as appeals_rate
        FROM policy_decisions_log
        WHERE created_at >= NOW() - INTERVAL '7 days'
          AND decision = 'DENY'
      `);
          const appealsRate = parseFloat(
            denialMetrics.rows[0]?.appeals_rate || "0"
          );
          this.policyDenialsWithAppealsRate.set(appealsRate);
          log2.debug("Collected policy appeal metrics");
        } catch (error) {
          log2.error(
            { error: error.message },
            "Failed to collect policy appeal metrics"
          );
        }
      }
      async collectExportIntegrityMetrics() {
        try {
          const pool4 = getPostgresPool2();
          const exportMetrics = await pool4.query(`
        SELECT get_ga_export_metrics(7) as metrics
      `);
          const metrics8 = exportMetrics.rows[0]?.metrics;
          if (metrics8) {
            this.exportManifestIntegrityRate.set(
              parseFloat(metrics8.integrity_rate)
            );
            this.exportBundleVerificationRate.set(
              parseFloat(metrics8.integrity_rate)
            );
          }
          log2.debug("Collected export integrity metrics");
        } catch (error) {
          log2.error(
            { error: error.message },
            "Failed to collect export integrity metrics"
          );
        }
      }
      async collectCopilotMetrics() {
        try {
          const pool4 = getPostgresPool2();
          const copilotMetrics = await pool4.query(`
        SELECT
          COUNT(CASE WHEN e.status = 'EXECUTED' THEN 1 END)::DECIMAL /
          NULLIF(COUNT(*), 0) as success_rate
        FROM nl_cypher_translations t
        LEFT JOIN nl_cypher_executions e ON t.id = e.translation_id
        WHERE t.created_at >= NOW() - INTERVAL '7 days'
      `);
          const successRate = parseFloat(
            copilotMetrics.rows[0]?.success_rate || "0"
          );
          this.copilotNlSuccessRate.set(successRate);
          log2.debug("Collected Copilot metrics");
        } catch (error) {
          log2.error({ error: error.message }, "Failed to collect Copilot metrics");
        }
      }
      async collectHypothesisRigorMetrics() {
        try {
          const pool4 = getPostgresPool2();
          const rigorMetrics = await pool4.query(`
        SELECT
          AVG(confidence * 10) as avg_rigor_score
        FROM merge_decisions
        WHERE created_at >= NOW() - INTERVAL '7 days'
          AND confidence IS NOT NULL
      `);
          const rigorScore = parseFloat(
            rigorMetrics.rows[0]?.avg_rigor_score || "7"
          );
          this.hypothesisRigorScoreAvg.set(rigorScore);
          log2.debug("Collected hypothesis rigor metrics");
        } catch (error) {
          log2.error(
            { error: error.message },
            "Failed to collect hypothesis rigor metrics"
          );
        }
      }
      async collectDataQualityMetrics() {
        try {
          if (isNeo4jMockMode()) {
            this.dataQualityScoreOverall.set(0.8);
            return;
          }
          const session = getNeo4jDriver2().session();
          const result2 = await session.run(`
        MATCH (e:Entity)
        WITH COUNT(e) as total,
             COUNT(CASE WHEN e.name IS NOT NULL AND e.name <> '' THEN 1 END) as with_names,
             COUNT(CASE WHEN e.created_at IS NOT NULL THEN 1 END) as with_dates
        RETURN CASE WHEN total = 0 THEN 0.8 ELSE toFloat(with_names + with_dates) / (total * 2) END as quality_score
      `);
          const scoreVal = result2.records[0]?.get("quality_score");
          const qualityScore = typeof scoreVal === "number" ? scoreVal : scoreVal?.toNumber?.() || 0.8;
          this.dataQualityScoreOverall.set(qualityScore);
          await session.close();
          log2.debug("Collected data quality metrics");
        } catch (error) {
          log2.error(
            { error: error.message },
            "Failed to collect data quality metrics"
          );
        }
      }
      async calculateOverallStatus() {
        try {
          const personPrecision = await this.erPrecisionPersonCurrent.get();
          const orgPrecision = await this.erPrecisionOrgCurrent.get();
          const appealsSla = await this.policyAppealSlaComplianceRate.get();
          const exportIntegrity = await this.exportManifestIntegrityRate.get();
          const copilotSuccess = await this.copilotNlSuccessRate.get();
          const gates = [
            {
              name: "ER_PRECISION_PERSON",
              current: personPrecision?.values?.[0]?.value || 0,
              threshold: 0.9,
              requirement: "Entity Resolution PERSON precision >= 90%"
            },
            {
              name: "ER_PRECISION_ORG",
              current: orgPrecision?.values?.[0]?.value || 0,
              threshold: 0.88,
              requirement: "Entity Resolution ORG precision >= 88%"
            },
            {
              name: "APPEALS_SLA",
              current: appealsSla?.values?.[0]?.value || 0,
              threshold: 0.9,
              requirement: "Policy appeals SLA compliance >= 90%"
            },
            {
              name: "EXPORT_INTEGRITY",
              current: exportIntegrity?.values?.[0]?.value || 0,
              threshold: 0.95,
              requirement: "Export manifest integrity >= 95%"
            },
            {
              name: "COPILOT_SUCCESS",
              current: copilotSuccess?.values?.[0]?.value || 0,
              threshold: 0.8,
              requirement: "Copilot NL\u2192Cypher success rate >= 80%"
            }
          ];
          for (const gate2 of gates) {
            const status = gate2.current >= gate2.threshold ? "PASS" : "FAIL";
            this.gaCoreGateStatus.set(
              {
                gate: gate2.name,
                status,
                requirement: gate2.requirement,
                current_value: gate2.current.toFixed(4),
                threshold: gate2.threshold.toString()
              },
              gate2.current >= gate2.threshold ? 1 : 0
            );
          }
          const passingGates = gates.filter((g2) => g2.current >= g2.threshold).length;
          const totalGates = gates.length;
          let overallStatus;
          if (passingGates === totalGates) {
            overallStatus = 1;
          } else if (passingGates >= totalGates * 0.8) {
            overallStatus = 0.5;
          } else {
            overallStatus = 0;
          }
          this.gaCoreOverallStatus.set(overallStatus);
          const statusText = overallStatus === 1 ? "GO" : overallStatus === 0.5 ? "CONDITIONAL GO" : "NO GO";
          log2.info(
            {
              status: statusText,
              passingGates,
              totalGates,
              gates: gates.map((g2) => ({
                name: g2.name,
                pass: g2.current >= g2.threshold,
                current: g2.current,
                threshold: g2.threshold
              }))
            },
            "GA Core overall status calculated"
          );
        } catch (error) {
          log2.error(
            { error: error.message },
            "Failed to calculate overall GA status"
          );
        }
      }
      /**
       * Record merge decision timing for performance metrics
       */
      recordMergeDecisionTime(entityType, method, durationSeconds) {
        this.mergeDecisionDuration.labels(entityType, method).observe(durationSeconds);
      }
      /**
       * Get current GA Core status for API endpoints
       */
      async getCurrentStatus() {
        const overallMetric = await this.gaCoreOverallStatus.get();
        const overallValue = overallMetric?.values?.[0]?.value || 0;
        const overall = overallValue === 1 ? "GO" : overallValue === 0.5 ? "CONDITIONAL_GO" : "NO_GO";
        return {
          overall,
          score: overallValue,
          gates: [],
          // Would populate from gate metrics
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      /**
       * Get Prometheus metrics registry for scraping
       */
      getMetricsRegistry() {
        return register6;
      }
    };
    gaCoreMetrics = new GACoreMetricsService();
  }
});

// src/narrative/generators.ts
var MOMENTUM_LABELS, RuleBasedNarrativeGenerator, LLMDrivenNarrativeGenerator;
var init_generators = __esm({
  "src/narrative/generators.ts"() {
    "use strict";
    MOMENTUM_LABELS = {
      improving: "rising",
      degrading: "sliding",
      steady: "steady"
    };
    RuleBasedNarrativeGenerator = class {
      mode = "rule-based";
      async generate(state, recentEvents) {
        const highlightPieces = state.arcs.map((arc) => {
          const label = MOMENTUM_LABELS[arc.outlook];
          const entities = arc.keyEntities.length ? `Actors: ${arc.keyEntities.join(", ")}.` : "";
          return {
            theme: arc.theme,
            text: `${arc.theme} is ${label} with momentum ${(arc.momentum * 100).toFixed(0)}% and confidence ${(arc.confidence * 100).toFixed(0)}%. ${entities}`.trim()
          };
        });
        const riskSignals = highlightPieces.filter((h) => h.text.includes("sliding") || h.text.includes("degrading")).map((h) => `Watch ${h.theme} \u2014 outlook deteriorating.`);
        const opportunitySignals = highlightPieces.filter((h) => h.text.includes("rising") || h.text.includes("improving")).map((h) => `Capitalize on ${h.theme} momentum.`);
        const primaryArc = state.arcs.sort((a, b) => b.momentum - a.momentum)[0];
        const summary = primaryArc ? `Tick ${state.tick}: ${primaryArc.theme} dominates the narrative with ${(primaryArc.momentum * 100).toFixed(1)}% momentum.` : `Tick ${state.tick}: Narrative remains in equilibrium.`;
        const recent = recentEvents.slice(-3).map((event) => `${event.type} event: ${event.description}`);
        const summaryWithEvents = recent.length ? `${summary} Recent drivers: ${recent.join("; ")}.` : summary;
        return {
          mode: this.mode,
          summary: summaryWithEvents,
          highlights: highlightPieces,
          risks: riskSignals,
          opportunities: opportunitySignals
        };
      }
    };
    LLMDrivenNarrativeGenerator = class {
      mode = "llm";
      llmClient;
      fallback;
      constructor(llmClient) {
        this.llmClient = llmClient;
        this.fallback = new RuleBasedNarrativeGenerator();
      }
      async generate(state, recentEvents) {
        const request = { state, recentEvents };
        try {
          const response = await this.llmClient.generateNarrative(request);
          const highlights = state.arcs.map((arc) => ({
            theme: arc.theme,
            text: arc.narrative
          }));
          return {
            mode: this.mode,
            summary: response,
            highlights,
            risks: this.extractLines(response, /risk:/i),
            opportunities: this.extractLines(response, /opportunity:/i)
          };
        } catch (error) {
          const fallbackResult = await this.fallback.generate(state, recentEvents);
          return {
            ...fallbackResult,
            mode: this.mode,
            summary: `${fallbackResult.summary} (LLM unavailable, fallback engaged)`
          };
        }
      }
      extractLines(text, pattern2) {
        return text.split(/\n|\.\s/).map((line) => line.trim()).filter((line) => pattern2.test(line)).map((line) => line.replace(pattern2, "").trim()).filter(Boolean);
      }
    };
  }
});

// src/narrative/agents.ts
import { randomUUID as randomUUID14 } from "node:crypto";
var SimulationAgent, RuleBasedAgent, LLMAgent;
var init_agents = __esm({
  "src/narrative/agents.ts"() {
    "use strict";
    SimulationAgent = class {
      constructor(config9, entity) {
        this.config = config9;
        this.entity = entity;
      }
      get id() {
        return this.config.entityId;
      }
      get role() {
        return this.config.role;
      }
      createEvent(type, description, state, overrides) {
        const theme = overrides?.theme ?? state.themes[0];
        return {
          id: randomUUID14(),
          type,
          actorId: this.id,
          theme,
          intensity: 1,
          description,
          scheduledTick: state.tick + 1,
          ...overrides
        };
      }
    };
    RuleBasedAgent = class extends SimulationAgent {
      constructor(config9, entity) {
        super(config9, entity);
      }
      async decideAction(state) {
        const entityState = state.entities[this.id];
        if (!entityState) return null;
        if (entityState.pressure > 0.7) {
          return this.createEvent(
            "intervention",
            `${this.entity.name} takes measures to reduce pressure.`,
            state,
            {
              sentimentShift: 0.1,
              influenceShift: 0,
              intensity: 0.8
            }
          );
        }
        if (entityState.sentiment < -0.5) {
          return this.createEvent(
            "social",
            `${this.entity.name} issues a statement to clarify their position.`,
            state,
            {
              sentimentShift: 0.2,
              intensity: 0.5
            }
          );
        }
        if (Math.random() < entityState.influence * 0.3) {
          return this.createEvent(
            "political",
            `${this.entity.name} exercises influence to advance their goal: ${this.config.goal}.`,
            state,
            {
              influenceShift: 0.05
            }
          );
        }
        return null;
      }
    };
    LLMAgent = class extends SimulationAgent {
      constructor(config9, entity, llmClient) {
        super(config9, entity);
        this.llmClient = llmClient;
      }
      async decideAction(state) {
        const entityState = state.entities[this.id];
        if (!entityState) return null;
        if (Math.random() > 0.2 + entityState.pressure * 0.2) {
          return null;
        }
        const prompt = this.constructPrompt(state);
        try {
          const response = await this.llmClient.generateNarrative({
            state,
            recentEvents: state.recentEvents
          });
          if (response.includes("Action:")) {
            const description = response.split("Action:")[1].trim();
            return this.createEvent("intervention", description, state);
          }
          return this.createEvent("political", `LLM Action based on: ${response.substring(0, 50)}...`, state);
        } catch (error) {
          console.error("LLM Agent failed to decide action", error);
          return null;
        }
      }
      constructPrompt(state) {
        return `You are ${this.config.role} (${this.entity.name}).
      Your goal is: ${this.config.goal}.
      Current situation: ...
      Decide on an action.`;
      }
    };
  }
});

// src/narrative/engine.ts
import { randomUUID as randomUUID15 } from "node:crypto";
var HISTORY_LIMIT, MOMENTUM_SENSITIVITY, NarrativeSimulationEngine;
var init_engine = __esm({
  "src/narrative/engine.ts"() {
    "use strict";
    init_generators();
    init_agents();
    HISTORY_LIMIT = 64;
    MOMENTUM_SENSITIVITY = 0.05;
    NarrativeSimulationEngine = class {
      constructor(config9) {
        this.config = config9;
        const start = /* @__PURE__ */ new Date();
        const entities = Object.fromEntries(
          config9.initialEntities.map((entity) => [
            entity.id,
            this.bootstrapEntityState(entity)
          ])
        );
        if (config9.agents) {
          config9.agents.forEach((agentConfig) => {
            const entity = config9.initialEntities.find(
              (e) => e.id === agentConfig.entityId
            );
            if (entity) {
              if (agentConfig.type === "llm" && config9.llmClient) {
                this.agents.push(
                  new LLMAgent(agentConfig, entity, config9.llmClient)
                );
              } else {
                this.agents.push(new RuleBasedAgent(agentConfig, entity));
              }
            }
          });
        }
        const parameters = Object.fromEntries(
          (config9.initialParameters ?? []).map((parameter) => [
            parameter.name,
            this.bootstrapParameter(parameter.name, parameter.value)
          ])
        );
        this.state = {
          id: config9.id,
          name: config9.name,
          tick: 0,
          startedAt: start,
          timestamp: start,
          tickIntervalMinutes: config9.tickIntervalMinutes,
          themes: config9.themes,
          entities,
          parameters,
          arcs: [],
          recentEvents: [],
          narrative: {
            mode: "rule-based",
            summary: "Simulation initialized.",
            highlights: [],
            risks: [],
            opportunities: []
          },
          metadata: config9.metadata
        };
        this.generator = this.createGenerator(config9.generatorMode, config9);
        this.state.arcs = this.computeArcs();
      }
      state;
      generator;
      eventQueue = [];
      agents = [];
      getState() {
        return this.state;
      }
      getSummary() {
        const { id, name, tick, themes } = this.state;
        return {
          id,
          name,
          tick,
          themes,
          activeEntities: Object.keys(this.state.entities).length,
          activeEvents: this.eventQueue.length
        };
      }
      setGeneratorMode(mode, llmClientConfig) {
        this.generator = this.createGenerator(mode, {
          ...this.config,
          llmClient: llmClientConfig
        });
      }
      queueEvent(event) {
        const scheduledTick = event.scheduledTick ?? this.state.tick + 1;
        this.eventQueue.push({ ...event, scheduledTick });
      }
      async tick(steps = 1) {
        for (let index = 0; index < steps; index += 1) {
          await this.resolveAgentActions();
          this.advanceClock();
          const ready = this.dequeueReadyEvents();
          ready.forEach((event) => this.applyEvent(event));
          this.state.recentEvents = [...this.state.recentEvents, ...ready].slice(
            -HISTORY_LIMIT
          );
          this.state.arcs = this.computeArcs();
          await this.refreshNarrative(ready);
          this.applyNaturalDynamics();
        }
        return this.state;
      }
      async resolveAgentActions() {
        for (const agent of this.agents) {
          try {
            const event = await agent.decideAction(this.state);
            if (event) {
              this.queueEvent(event);
            }
          } catch (error) {
            console.error(
              `Agent ${agent.id} failed to decide action:`,
              error instanceof Error ? error.message : error
            );
          }
        }
      }
      injectActorAction(actorId, description, overrides) {
        const action = {
          id: randomUUID15(),
          type: "intervention",
          actorId,
          targetIds: overrides?.targetIds,
          theme: overrides?.theme ?? this.state.themes[0],
          intensity: overrides?.intensity ?? 0.6,
          sentimentShift: overrides?.sentimentShift ?? 0.1,
          influenceShift: overrides?.influenceShift ?? 0.05,
          description,
          parameterAdjustments: overrides?.parameterAdjustments,
          scheduledTick: overrides?.scheduledTick ?? this.state.tick + 1,
          metadata: overrides?.metadata
        };
        this.queueEvent(action);
      }
      updateEntityProfile(entity) {
        this.state.entities[entity.id] = {
          ...this.bootstrapEntityState(entity),
          history: this.state.entities[entity.id]?.history ?? []
        };
      }
      async refreshNarrative(recent) {
        const narration = await this.generator.generate(
          this.state,
          recent
        );
        this.state.narrative = narration;
      }
      bootstrapEntityState(entity) {
        return {
          ...entity,
          pressure: 0.2,
          trend: "stable",
          lastUpdatedTick: 0,
          history: [
            {
              tick: 0,
              sentiment: entity.sentiment,
              influence: entity.influence
            }
          ]
        };
      }
      bootstrapParameter(name, value) {
        return {
          name,
          value,
          trend: "stable",
          history: [
            {
              tick: 0,
              value
            }
          ]
        };
      }
      createGenerator(mode, config9) {
        if (mode === "llm" && config9.llmClient) {
          return new LLMDrivenNarrativeGenerator(config9.llmClient);
        }
        return new RuleBasedNarrativeGenerator();
      }
      advanceClock() {
        this.state.tick += 1;
        this.state.timestamp = new Date(
          this.state.startedAt.getTime() + this.state.tick * this.state.tickIntervalMinutes * 6e4
        );
      }
      dequeueReadyEvents() {
        const ready = [];
        for (let index = this.eventQueue.length - 1; index >= 0; index -= 1) {
          const event = this.eventQueue[index];
          if ((event.scheduledTick ?? this.state.tick) <= this.state.tick) {
            ready.push(event);
            this.eventQueue.splice(index, 1);
          }
        }
        return ready.reverse();
      }
      applyEvent(event) {
        if (event.actorId && this.state.entities[event.actorId]) {
          this.adjustEntityState(this.state.entities[event.actorId], event, 1);
        }
        (event.targetIds ?? []).forEach((targetId) => {
          const target = this.state.entities[targetId];
          if (target) {
            this.adjustEntityState(target, event, 0.8);
          }
        });
        if (event.parameterAdjustments?.length) {
          event.parameterAdjustments.forEach((param) => {
            const existing = this.state.parameters[param.name] ?? this.bootstrapParameter(param.name, 0);
            existing.value += param.delta;
            existing.history.push({ tick: this.state.tick, value: existing.value });
            existing.trend = this.calculateTrend(existing.history);
            this.state.parameters[param.name] = existing;
          });
        }
        if (event.actorId && event.intensity > 0.05) {
          const actor = this.state.entities[event.actorId];
          if (actor) {
            actor.relationships.forEach((edge) => {
              const related = this.state.entities[edge.targetId];
              if (!related) return;
              const newIntensity = event.intensity * edge.strength * 0.5;
              if (newIntensity < 0.01) return;
              const propagatedEvent = {
                ...event,
                id: `${event.id}:${edge.targetId}`,
                actorId: related.id,
                // Neighbor becomes the "actor" of the propagated event (re-transmission)
                targetIds: [],
                // Clear specific targets
                intensity: newIntensity,
                sentimentShift: (event.sentimentShift ?? 0) * edge.strength * related.resilience,
                influenceShift: (event.influenceShift ?? 0) * edge.strength * 0.5,
                scheduledTick: this.state.tick + 1
                // Queue for NEXT tick (simulating travel time)
              };
              this.queueEvent(propagatedEvent);
            });
          }
        }
      }
      adjustEntityState(entity, event, weight) {
        let sentimentDelta = (event.sentimentShift ?? 0) * event.intensity * weight * (1 - entity.resilience * 0.5);
        let influenceDelta = (event.influenceShift ?? 0) * weight * (1 - entity.volatility * 0.5);
        if (event.type === "suppression") {
          if (event.targetIds?.includes(entity.id)) {
            influenceDelta = -Math.abs(event.intensity * weight * 0.5);
            sentimentDelta = 0;
          }
        }
        entity.sentiment = this.clamp(entity.sentiment + sentimentDelta, -1, 1);
        entity.influence = this.clamp(entity.influence + influenceDelta, 0, 1.5);
        entity.pressure = this.clamp(
          entity.pressure + Math.abs(sentimentDelta) * 0.5,
          0,
          1
        );
        entity.trend = sentimentDelta > MOMENTUM_SENSITIVITY ? "rising" : sentimentDelta < -MOMENTUM_SENSITIVITY ? "falling" : "stable";
        entity.lastEventId = event.id;
        entity.lastUpdatedTick = this.state.tick;
        entity.history.push({
          tick: this.state.tick,
          sentiment: entity.sentiment,
          influence: entity.influence
        });
        if (entity.history.length > HISTORY_LIMIT) {
          entity.history.splice(0, entity.history.length - HISTORY_LIMIT);
        }
      }
      computeArcs() {
        return this.state.themes.map((theme) => {
          const entityScores = Object.values(this.state.entities).map((entity) => {
            const themeAffinity = entity.themes[theme] ?? 0;
            const normalizedSentiment = (entity.sentiment + 1) / 2;
            return {
              id: entity.id,
              name: entity.name,
              score: normalizedSentiment * entity.influence * themeAffinity
            };
          });
          entityScores.sort((a, b) => b.score - a.score);
          const momentum = this.clamp(
            entityScores.reduce((total, current) => total + current.score, 0),
            0,
            1
          );
          const previous = this.state.arcs.find((arc) => arc.theme === theme)?.momentum ?? momentum;
          const delta = momentum - previous;
          const outlook = delta > MOMENTUM_SENSITIVITY ? "improving" : delta < -MOMENTUM_SENSITIVITY ? "degrading" : "steady";
          const confidence = this.clamp(
            entityScores.slice(0, 3).reduce((total, current) => total + current.score, 0),
            0,
            1
          );
          return {
            theme,
            momentum,
            outlook,
            confidence,
            keyEntities: entityScores.slice(0, 3).map((entry) => entry.name),
            narrative: this.renderArcNarrative(
              theme,
              outlook,
              entityScores.slice(0, 2)
            )
          };
        });
      }
      renderArcNarrative(theme, outlook, leaders) {
        const leadNames = leaders.map((leader) => leader.name).join(", ");
        const outlookText = outlook === "improving" ? "Narrative sentiment trending upward." : outlook === "degrading" ? "Narrative momentum deteriorating." : "Narrative pressure stable.";
        return `${theme}: ${outlookText}${leadNames ? ` Key drivers: ${leadNames}.` : ""}`;
      }
      applyNaturalDynamics() {
        Object.values(this.state.entities).forEach((entity) => {
          const decay = (entity.pressure - entity.resilience * 0.3) * 0.05;
          entity.pressure = this.clamp(entity.pressure - decay, 0, 1);
          if (entity.trend === "stable") {
            const regression = (entity.sentiment - 0) * 0.02;
            entity.sentiment = this.clamp(entity.sentiment - regression, -1, 1);
          }
        });
        Object.values(this.state.parameters).forEach((parameter) => {
          const regression = parameter.value * 0.01;
          parameter.value -= regression;
          parameter.history.push({ tick: this.state.tick, value: parameter.value });
          if (parameter.history.length > HISTORY_LIMIT) {
            parameter.history.splice(0, parameter.history.length - HISTORY_LIMIT);
          }
          parameter.trend = this.calculateTrend(parameter.history);
        });
      }
      calculateTrend(history) {
        if (history.length < 2) return "stable";
        const recent = history.slice(-3);
        const deltas = recent.slice(1).map((point, index) => point.value - recent[index].value);
        const avgDelta = deltas.reduce((total, value) => total + value, 0) / (deltas.length || 1);
        if (avgDelta > MOMENTUM_SENSITIVITY / 2) return "rising";
        if (avgDelta < -MOMENTUM_SENSITIVITY / 2) return "falling";
        return "stable";
      }
      clamp(value, min, max) {
        return Math.max(min, Math.min(max, value));
      }
    };
  }
});

// src/narrative/manager.ts
var manager_exports = {};
__export(manager_exports, {
  NarrativeSimulationManager: () => NarrativeSimulationManager,
  narrativeSimulationManager: () => narrativeSimulationManager
});
import { randomUUID as randomUUID16 } from "node:crypto";
var NarrativeSimulationManager, narrativeSimulationManager;
var init_manager = __esm({
  "src/narrative/manager.ts"() {
    "use strict";
    init_engine();
    init_metrics4();
    NarrativeSimulationManager = class _NarrativeSimulationManager {
      static instance;
      simulations = /* @__PURE__ */ new Map();
      static getInstance() {
        if (!_NarrativeSimulationManager.instance) {
          _NarrativeSimulationManager.instance = new _NarrativeSimulationManager();
        }
        return _NarrativeSimulationManager.instance;
      }
      createSimulation(input) {
        const id = randomUUID16();
        const config9 = {
          id,
          name: input.name,
          themes: input.themes,
          tickIntervalMinutes: input.tickIntervalMinutes ?? 60,
          initialEntities: input.initialEntities,
          initialParameters: input.initialParameters,
          agents: input.agents,
          generatorMode: input.generatorMode,
          llmClient: input.llmClient,
          metadata: input.metadata
        };
        const engine2 = new NarrativeSimulationEngine(config9);
        this.simulations.set(id, engine2);
        metrics3.narrativeSimulationActiveSimulations.inc();
        return engine2.getState();
      }
      getState(id) {
        return this.simulations.get(id)?.getState();
      }
      getEngine(id) {
        return this.simulations.get(id);
      }
      list() {
        return Array.from(this.simulations.values()).map(
          (engine2) => engine2.getSummary()
        );
      }
      remove(id) {
        const deleted = this.simulations.delete(id);
        if (deleted) {
          metrics3.narrativeSimulationActiveSimulations.dec();
        }
        return deleted;
      }
      queueEvent(id, event) {
        const engine2 = this.getEngine(id);
        if (!engine2) {
          throw new Error(`Simulation ${id} not found`);
        }
        metrics3.narrativeSimulationEventsTotal.inc({ simulation_id: id, event_type: event.type });
        engine2.queueEvent(event);
      }
      injectActorAction(id, actorId, description, overrides) {
        const engine2 = this.getEngine(id);
        if (!engine2) {
          throw new Error(`Simulation ${id} not found`);
        }
        engine2.injectActorAction(actorId, description, overrides);
      }
      async tick(id, steps = 1) {
        const engine2 = this.getEngine(id);
        if (!engine2) {
          throw new Error(`Simulation ${id} not found`);
        }
        const end = metrics3.narrativeSimulationDurationSeconds.startTimer({ simulation_id: id });
        try {
          metrics3.narrativeSimulationTicksTotal.inc({ simulation_id: id }, steps);
          return await engine2.tick(steps);
        } finally {
          end();
        }
      }
    };
    narrativeSimulationManager = NarrativeSimulationManager.getInstance();
  }
});

// src/policy/tenantBundle.ts
import * as z15 from "zod";
import cloneDeep from "lodash/cloneDeep.js";
import get2 from "lodash/get.js";
import mergeWith from "lodash/mergeWith.js";
import set from "lodash/set.js";
import unset from "lodash/unset.js";
var overlayPatchSchema, overlaySelectorSchema, overlayContextSchema, policyRuleSchema, crossTenantSchema, guardrailSchema, tenantIsolationSchema, quotaLimitSchema, quotasSchema, rampRuleSchema, rampsSchema, freezeWindowSchema, dualControlSchema, baseProfileSchema, overlaySchema, tenantPolicyBundleSchema, policySimulationInputSchema;
var init_tenantBundle = __esm({
  "src/policy/tenantBundle.ts"() {
    "use strict";
    overlayPatchSchema = z15.object({
      op: z15.enum(["set", "remove", "append", "merge"]),
      path: z15.string(),
      value: z15.any().optional()
    });
    overlaySelectorSchema = z15.object({
      environments: z15.array(z15.string()).optional(),
      regions: z15.array(z15.string()).optional(),
      labels: z15.array(z15.string()).optional()
    });
    overlayContextSchema = overlaySelectorSchema.extend({
      environment: z15.string().optional()
    });
    policyRuleSchema = z15.object({
      id: z15.string(),
      description: z15.string().optional(),
      effect: z15.enum(["allow", "deny"]),
      priority: z15.number().int().default(0),
      conditions: z15.object({
        actions: z15.array(z15.string()).optional(),
        resourceTenants: z15.array(z15.string()).optional(),
        subjectTenants: z15.array(z15.string()).optional(),
        purposes: z15.array(z15.string()).optional(),
        environments: z15.array(z15.string()).optional()
      }).default({})
    });
    crossTenantSchema = z15.object({
      mode: z15.enum(["deny", "allowlist", "delegated"]).default("deny"),
      allow: z15.array(z15.string()).default([]),
      requireAgreements: z15.boolean().default(true)
    });
    guardrailSchema = z15.object({
      defaultDeny: z15.boolean().default(true),
      requirePurpose: z15.boolean().default(false),
      requireJustification: z15.boolean().default(false)
    });
    tenantIsolationSchema = z15.object({
      enabled: z15.boolean().default(true),
      allowCrossTenant: z15.boolean().default(false),
      actions: z15.array(z15.string()).default([])
    });
    quotaLimitSchema = z15.object({
      limit: z15.number().int().positive(),
      period: z15.enum(["hour", "day", "week", "month"]).default("day")
    });
    quotasSchema = z15.object({
      actions: z15.record(quotaLimitSchema).default({})
    });
    rampRuleSchema = z15.object({
      maxPercent: z15.number().min(0).max(100)
    });
    rampsSchema = z15.object({
      actions: z15.record(rampRuleSchema).default({})
    });
    freezeWindowSchema = z15.object({
      id: z15.string(),
      start: z15.string(),
      end: z15.string(),
      actions: z15.array(z15.string()).optional(),
      description: z15.string().optional()
    });
    dualControlSchema = z15.object({
      actions: z15.array(z15.string()).default(["delete"]),
      minApprovals: z15.number().int().min(2).default(2)
    });
    baseProfileSchema = z15.object({
      id: z15.string(),
      version: z15.string(),
      regoPackage: z15.string(),
      entrypoints: z15.array(z15.string()).min(1),
      guardrails: guardrailSchema.default({}),
      tenantIsolation: tenantIsolationSchema.default({}),
      crossTenant: crossTenantSchema.default({
        mode: "deny",
        allow: [],
        requireAgreements: true
      }),
      quotas: quotasSchema.default({}),
      ramps: rampsSchema.default({}),
      freezeWindows: z15.array(freezeWindowSchema).default([]),
      dualControl: dualControlSchema.default({}),
      rules: z15.array(policyRuleSchema).min(1)
    });
    overlaySchema = z15.object({
      id: z15.string(),
      description: z15.string().optional(),
      precedence: z15.number().int().default(0),
      selectors: overlaySelectorSchema.default({}),
      patches: z15.array(overlayPatchSchema).min(1)
    });
    tenantPolicyBundleSchema = z15.object({
      tenantId: z15.string(),
      bundleId: z15.string().optional(),
      metadata: z15.object({
        issuedAt: z15.string().optional(),
        expiresAt: z15.string().optional(),
        source: z15.string().optional()
      }).default({}),
      baseProfile: baseProfileSchema,
      overlays: z15.array(overlaySchema).default([])
    });
    policySimulationInputSchema = z15.object({
      subjectTenantId: z15.string(),
      resourceTenantId: z15.string(),
      action: z15.string(),
      purpose: z15.string().optional(),
      justification: z15.string().optional(),
      requestTime: z15.string().optional(),
      approvals: z15.number().int().nonnegative().optional(),
      rampPercent: z15.number().min(0).max(100).optional(),
      quotaUsage: z15.object({
        actions: z15.record(z15.number().int().nonnegative()).default({})
      }).optional()
    });
  }
});

// src/utils/input-sanitization.ts
import validator from "validator";
import { escape as htmlEscape } from "html-escaper";
import DOMPurify2 from "isomorphic-dompurify";
function sanitizeFilePath(pathInput, allowedBasePath) {
  if (typeof pathInput !== "string") {
    throw new Error("Path must be a string");
  }
  let sanitized = pathInput.replace(/\0/g, "");
  if (sanitized.includes("..") || sanitized.includes("~")) {
    throw new Error("Path traversal detected");
  }
  sanitized = sanitized.replace(/^\/+/, "");
  if (allowedBasePath) {
    const pathModule = __require("path");
    const { resolve: resolve2, normalize: normalize3 } = pathModule;
    const normalizedBase = normalize3(resolve2(allowedBasePath));
    const normalizedPath = normalize3(resolve2(allowedBasePath, sanitized));
    if (!normalizedPath.startsWith(normalizedBase)) {
      throw new Error("Path is outside allowed directory");
    }
    return normalizedPath;
  }
  return sanitized;
}
var init_input_sanitization = __esm({
  "src/utils/input-sanitization.ts"() {
    "use strict";
  }
});

// src/policy/loader.ts
import { execFile } from "node:child_process";
import { createHash as createHash15 } from "node:crypto";
import { promises as fs11 } from "fs";
import path11 from "path";
function assertNonEmptyString(value, label) {
  if (typeof value !== "string" || value.trim().length === 0) {
    throw new Error(`${label} must be a non-empty string`);
  }
}
async function ensureFileReadable(filePath, label) {
  const stat = await fs11.stat(filePath).catch(() => {
    throw new Error(`${label} not found at ${filePath}`);
  });
  if (!stat.isFile()) {
    throw new Error(`${label} at ${filePath} is not a regular file`);
  }
  if (stat.size === 0) {
    throw new Error(`${label} at ${filePath} is empty`);
  }
  return stat;
}
function validateExtension(filePath) {
  const ext = DEFAULT_ALLOWED_EXTENSIONS.find(
    (candidate) => filePath.toLowerCase().endsWith(candidate)
  );
  if (!ext) {
    throw new Error(
      `policy bundle must use one of the allowed extensions: ${DEFAULT_ALLOWED_EXTENSIONS.join(", ")}`
    );
  }
}
function digestFileBuffer(buf) {
  return createHash15("sha256").update(buf).digest("hex");
}
async function loadSignedPolicy(bundlePath, sigPath) {
  assertNonEmptyString(bundlePath, "bundlePath");
  if (bundlePath.startsWith("-")) {
    throw new Error("bundlePath cannot start with a hyphen");
  }
  const safeBundlePath = sanitizeFilePath(bundlePath);
  validateExtension(safeBundlePath);
  let safeSigPath;
  if (sigPath) {
    assertNonEmptyString(sigPath, "sigPath");
    if (sigPath.startsWith("-")) {
      throw new Error("sigPath cannot start with a hyphen");
    }
    safeSigPath = sanitizeFilePath(sigPath);
  }
  const allowUnsigned = (process.env.ALLOW_UNSIGNED_POLICY || "false").toLowerCase() === "true";
  if (!sigPath && !allowUnsigned) {
    throw new Error(
      "unsigned policy not allowed (set ALLOW_UNSIGNED_POLICY=true to override)"
    );
  }
  const stat = await ensureFileReadable(safeBundlePath, "policy bundle");
  const buf = await fs11.readFile(safeBundlePath);
  const digest = digestFileBuffer(buf);
  let signatureVerified = false;
  if (safeSigPath) {
    await ensureFileReadable(safeSigPath, "policy signature");
    await new Promise(
      (res, rej) => execFile(
        "cosign",
        ["verify-blob", "--signature", safeSigPath, safeBundlePath],
        (e) => e ? rej(e) : res()
      )
    );
    signatureVerified = true;
  }
  return {
    ok: true,
    path: path11.resolve(safeBundlePath),
    size: stat.size,
    modified: stat.mtime,
    signatureVerified,
    digest,
    buf
  };
}
var DEFAULT_ALLOWED_EXTENSIONS;
var init_loader = __esm({
  "src/policy/loader.ts"() {
    "use strict";
    init_input_sanitization();
    DEFAULT_ALLOWED_EXTENSIONS = [".tar", ".tgz", ".tar.gz", ".bundle"];
  }
});

// src/policy/bundleStore.ts
function deriveVersionId(bundle, digest) {
  return bundle.bundleId || bundle.baseProfile?.version || `${bundle.tenantId}-${bundle.baseProfile?.regoPackage || "policy"}-${digest.slice(0, 12)}`;
}
async function loadPolicyBundleFromDisk(bundlePath, signaturePath) {
  const verification2 = await loadSignedPolicy(bundlePath, signaturePath);
  const content = verification2.buf.toString("utf8");
  const parsed = tenantPolicyBundleSchema.parse(JSON.parse(content));
  const versionId = deriveVersionId(parsed, verification2.digest);
  return {
    versionId,
    path: verification2.path,
    digest: verification2.digest,
    loadedAt: /* @__PURE__ */ new Date(),
    signatureVerified: verification2.signatureVerified,
    bundle: parsed
  };
}
var PolicyBundleStore, policyBundleStore;
var init_bundleStore = __esm({
  "src/policy/bundleStore.ts"() {
    "use strict";
    init_tenantBundle();
    init_loader();
    PolicyBundleStore = class {
      versions = /* @__PURE__ */ new Map();
      currentPolicyVersionId;
      reset() {
        this.versions.clear();
        this.currentPolicyVersionId = void 0;
      }
      addVersion(version, makeCurrent = true) {
        this.versions.set(version.versionId, version);
        if (makeCurrent) this.currentPolicyVersionId = version.versionId;
        return version;
      }
      get(versionId) {
        return this.versions.get(versionId);
      }
      getCurrent() {
        return this.currentPolicyVersionId ? this.versions.get(this.currentPolicyVersionId) : void 0;
      }
      resolveForTenant(tenantId, versionId) {
        if (versionId) {
          const byVersion = this.versions.get(versionId);
          if (byVersion && byVersion.bundle.tenantId === tenantId) return byVersion;
        }
        const current = this.getCurrent();
        if (current && current.bundle.tenantId === tenantId) return current;
        const candidates2 = Array.from(this.versions.values()).filter((version) => version.bundle.tenantId === tenantId).sort((a, b) => a.loadedAt.getTime() - b.loadedAt.getTime());
        return candidates2.at(-1);
      }
      list() {
        return Array.from(this.versions.values()).sort((a, b) => a.loadedAt.getTime() - b.loadedAt.getTime());
      }
      resolve(versionId) {
        const candidate = versionId && this.versions.get(versionId);
        if (candidate) return candidate;
        const current = this.getCurrent();
        if (!current) {
          throw new Error("no policy bundles loaded");
        }
        return current;
      }
      rollback(toVersion) {
        const target = this.versions.get(toVersion);
        if (!target) {
          throw new Error(`policy version ${toVersion} not found`);
        }
        this.currentPolicyVersionId = target.versionId;
        return target;
      }
    };
    policyBundleStore = new PolicyBundleStore();
  }
});

// src/services/ticket-links.ts
async function linkTicketToRun({
  provider,
  externalId,
  runId
}) {
  const pool4 = getPostgresPool();
  await pool4.query(
    `INSERT INTO ticket_runs (provider, external_id, run_id)
     VALUES ($1,$2,$3) ON CONFLICT DO NOTHING`,
    [provider, externalId, runId]
  );
}
async function addTicketRunLink(ticket, runId, metadata) {
  const pool4 = getPostgresPool();
  const runResult = await pool4.query("SELECT id FROM runs WHERE id = $1", [
    runId
  ]);
  if (safeRows3(runResult).length === 0) {
    if (process.env.NODE_ENV === "test") {
      console.warn(`Run ${runId} not found; skipping ticket link in test mode`);
      return null;
    }
    throw new Error(`Run ${runId} not found`);
  }
  const ticketResult = await pool4.query(
    "SELECT id FROM tickets WHERE provider = $1 AND external_id = $2",
    [ticket.provider, ticket.externalId]
  );
  if (safeRows3(ticketResult).length === 0) {
    console.warn(
      `Ticket ${ticket.provider}:${ticket.externalId} not found, skipping link creation`
    );
    return null;
  }
  await pool4.query(
    `INSERT INTO ticket_runs (provider, external_id, run_id, metadata, created_at)
     VALUES ($1, $2, $3, $4, NOW()) ON CONFLICT (provider, external_id, run_id) DO UPDATE SET 
     metadata = EXCLUDED.metadata, created_at = NOW()`,
    [ticket.provider, ticket.externalId, runId, JSON.stringify(metadata || {})]
  );
  console.log(
    `Linked ticket ${ticket.provider}:${ticket.externalId} to run ${runId}`
  );
}
async function linkTicketToDeployment({
  provider,
  externalId,
  deploymentId
}) {
  const pool4 = getPostgresPool();
  await pool4.query(
    `INSERT INTO ticket_deployments (provider, external_id, deployment_id)
     VALUES ($1,$2,$3) ON CONFLICT DO NOTHING`,
    [provider, externalId, deploymentId]
  );
}
async function addTicketDeploymentLink(ticket, deploymentId, metadata) {
  const pool4 = getPostgresPool();
  const deploymentResult = await pool4.query(
    "SELECT id FROM deployments WHERE id = $1",
    [deploymentId]
  );
  if (safeRows3(deploymentResult).length === 0) {
    throw new Error(`Deployment ${deploymentId} not found`);
  }
  const ticketResult = await pool4.query(
    "SELECT id FROM tickets WHERE provider = $1 AND external_id = $2",
    [ticket.provider, ticket.externalId]
  );
  if (safeRows3(ticketResult).length === 0) {
    console.warn(
      `Ticket ${ticket.provider}:${ticket.externalId} not found, skipping link creation`
    );
    return null;
  }
  await pool4.query(
    `INSERT INTO ticket_deployments (provider, external_id, deployment_id, metadata, created_at)
     VALUES ($1, $2, $3, $4, NOW()) ON CONFLICT (provider, external_id, deployment_id) DO UPDATE SET 
     metadata = EXCLUDED.metadata, created_at = NOW()`,
    [
      ticket.provider,
      ticket.externalId,
      deploymentId,
      JSON.stringify(metadata || {})
    ]
  );
  console.log(
    `Linked ticket ${ticket.provider}:${ticket.externalId} to deployment ${deploymentId}`
  );
}
async function getTicketLinks({
  provider,
  externalId
}) {
  const pool4 = getPostgresPool();
  const runsResult = await pool4.query(
    `SELECT run_id AS id FROM ticket_runs WHERE provider=$1 AND external_id=$2 ORDER BY created_at DESC`,
    [provider, externalId]
  );
  const deploymentsResult = await pool4.query(
    `SELECT deployment_id AS id FROM ticket_deployments WHERE provider=$1 AND external_id=$2 ORDER BY created_at DESC`,
    [provider, externalId]
  );
  return {
    runs: safeRows3(runsResult),
    deployments: safeRows3(deploymentsResult)
  };
}
function extractTicketFromPR(prUrl, body4) {
  if (prUrl.includes("github.com")) {
    const issuePattern = /#(\d+)/g;
    const match = body4?.match(issuePattern) || prUrl.match(issuePattern);
    if (match) {
      return {
        provider: "github",
        externalId: match[0].replace("#", "")
      };
    }
  }
  const jiraPattern = /([A-Z]+-\d+)/g;
  const jiraMatch = body4?.match(jiraPattern) || prUrl.match(jiraPattern);
  if (jiraMatch) {
    return {
      provider: "jira",
      externalId: jiraMatch[0]
    };
  }
  return null;
}
function extractTicketFromMetadata(metadata) {
  if (metadata.ticket?.provider && metadata.ticket?.external_id) {
    return {
      provider: metadata.ticket.provider,
      externalId: metadata.ticket.external_id
    };
  }
  if (metadata.pr_url || metadata.pull_request) {
    const prUrl = metadata.pr_url || metadata.pull_request;
    return extractTicketFromPR(prUrl, metadata.pr_body);
  }
  if (metadata.commit_message) {
    return extractTicketFromPR("", metadata.commit_message);
  }
  return null;
}
var safeRows3;
var init_ticket_links2 = __esm({
  "src/services/ticket-links.ts"() {
    "use strict";
    init_postgres();
    safeRows3 = (result2) => Array.isArray(result2?.rows) ? result2.rows : [];
  }
});

// src/middleware/async-handler.ts
var asyncHandler;
var init_async_handler = __esm({
  "src/middleware/async-handler.ts"() {
    "use strict";
    asyncHandler = (fn) => (req, res, next) => {
      Promise.resolve(fn(req, res, next)).catch(next);
    };
  }
});

// src/lib/resources/overrides/QuotaOverrideService.ts
import pino49 from "pino";
var logger46, QuotaOverrideService, quotaOverrideService;
var init_QuotaOverrideService = __esm({
  "src/lib/resources/overrides/QuotaOverrideService.ts"() {
    "use strict";
    init_database();
    logger46 = pino49({ name: "QuotaOverrideService" });
    QuotaOverrideService = class _QuotaOverrideService {
      static instance;
      constructor() {
      }
      static getInstance() {
        if (!_QuotaOverrideService.instance) {
          _QuotaOverrideService.instance = new _QuotaOverrideService();
        }
        return _QuotaOverrideService.instance;
      }
      /**
       * Set a temporary override for a tenant's quota.
       * @param tenantId The tenant ID.
       * @param meter The meter to override (e.g. 'requests_day', 'api_rpm').
       * @param ttlSeconds How long the override should last.
       * @param reason Why the override was granted.
       */
      async setOverride(tenantId, meter3, ttlSeconds, reason) {
        const redis5 = getRedisClient();
        if (!redis5) throw new Error("Redis unavailable");
        const key = `tenant:${tenantId}:override:${meter3}`;
        await redis5.set(key, "true", "EX", ttlSeconds);
        logger46.info({ tenantId, meter: meter3, ttlSeconds, reason }, "Quota override set");
      }
      /**
       * Check if a valid override exists for a tenant's meter.
       */
      async hasOverride(tenantId, meter3) {
        const redis5 = getRedisClient();
        if (!redis5) return false;
        const key = `tenant:${tenantId}:override:${meter3}`;
        const exists = await redis5.exists(key);
        return exists === 1;
      }
      /**
       * Remove an override manually.
       */
      async removeOverride(tenantId, meter3) {
        const redis5 = getRedisClient();
        if (!redis5) return;
        const key = `tenant:${tenantId}:override:${meter3}`;
        await redis5.del(key);
        logger46.info({ tenantId, meter: meter3 }, "Quota override removed");
      }
    };
    quotaOverrideService = QuotaOverrideService.getInstance();
  }
});

// src/config/rateLimit.ts
function parseBool(value, fallback) {
  if (value === void 0) return fallback;
  const normalized = value.trim().toLowerCase();
  if (normalized === "true" || normalized === "1") return true;
  if (normalized === "false" || normalized === "0") return false;
  return fallback;
}
function parseNumber(value, fallback) {
  const parsed = Number(value);
  return Number.isFinite(parsed) && parsed > 0 ? parsed : fallback;
}
function loadRateLimitConfig(env2 = process.env) {
  const enabled = parseBool(env2.RATE_LIMIT_ENABLED, false);
  const store = env2.RATE_LIMIT_STORE === "redis" ? "redis" : "memory";
  const defaultWindowMs = parseNumber(env2.RATE_LIMIT_DEFAULT_WINDOW_MS, 6e4);
  const defaultLimit = parseNumber(env2.RATE_LIMIT_DEFAULT_LIMIT, 100);
  const webhookWindowMs = parseNumber(env2.RATE_LIMIT_WEBHOOK_WINDOW_MS, defaultWindowMs);
  const webhookLimit = parseNumber(env2.RATE_LIMIT_WEBHOOK_LIMIT, 30);
  const governanceWindowMs = parseNumber(env2.RATE_LIMIT_GOVERNANCE_WINDOW_MS, defaultWindowMs);
  const governanceLimit = parseNumber(env2.RATE_LIMIT_GOVERNANCE_LIMIT, 30);
  const caseWorkflowWindowMs = parseNumber(env2.RATE_LIMIT_CASE_WORKFLOW_WINDOW_MS, defaultWindowMs);
  const caseWorkflowLimit = parseNumber(env2.RATE_LIMIT_CASE_WORKFLOW_LIMIT, 60);
  return {
    enabled,
    store,
    groups: {
      default: { limit: defaultLimit, windowMs: defaultWindowMs },
      webhookIngest: { limit: webhookLimit, windowMs: webhookWindowMs },
      governance: { limit: governanceLimit, windowMs: governanceWindowMs },
      caseWorkflow: { limit: caseWorkflowLimit, windowMs: caseWorkflowWindowMs }
    }
  };
}
var currentConfig;
var init_rateLimit = __esm({
  "src/config/rateLimit.ts"() {
    "use strict";
    currentConfig = loadRateLimitConfig();
  }
});

// src/middleware/rateLimit.ts
var ONE_DAY_MS2, CLASS_LIMITS, createRateLimiter2, rateLimitMiddleware;
var init_rateLimit2 = __esm({
  "src/middleware/rateLimit.ts"() {
    "use strict";
    init_RateLimiter();
    init_quota_manager();
    init_QuotaOverrideService();
    init_ledger();
    init_metrics2();
    init_rateLimit();
    ONE_DAY_MS2 = 24 * 60 * 60 * 1e3;
    CLASS_LIMITS = {
      ["AUTH" /* AUTH */]: { limit: 10, windowMs: 60 * 1e3 },
      // Strict: 10/min
      ["EXPORT" /* EXPORT */]: { limit: 5, windowMs: 60 * 1e3 },
      // Expensive: 5/min
      ["INGEST" /* INGEST */]: { limit: 100, windowMs: 60 * 1e3 },
      // High throughput
      ["QUERY" /* QUERY */]: { limit: 300, windowMs: 60 * 1e3 },
      // Standard API
      ["AI" /* AI */]: { limit: 60, windowMs: 60 * 1e3 },
      // AI Endpoints (Base limit, can be overridden)
      ["DEFAULT" /* DEFAULT */]: { limit: 60, windowMs: 60 * 1e3 }
    };
    createRateLimiter2 = (endpointClass = "DEFAULT" /* DEFAULT */) => {
      return async (req, res, next) => {
        if (req.path === "/health" || req.path === "/ping") {
          return next();
        }
        const tenantContext = req.tenant;
        const user = req.user;
        let key;
        const policy2 = CLASS_LIMITS[endpointClass];
        let limit = policy2.limit;
        const windowMs = policy2.windowMs;
        if (tenantContext?.tenantId) {
          key = `tenant:${tenantContext.tenantId}:class:${endpointClass}`;
          try {
            const quota = quota_manager_default.getQuotaForTenant(tenantContext.tenantId);
            if (quota) {
              if (endpointClass === "INGEST" /* INGEST */) limit = quota.ingestEventsPerMinute;
              else if (endpointClass === "EXPORT" /* EXPORT */) limit = Math.max(5, Math.floor(quota.requestsPerMinute / 20));
              else limit = quota.requestsPerMinute;
            }
          } catch (e) {
          }
        } else if (user) {
          key = `user:${user.id}:class:${endpointClass}`;
        } else {
          key = `ip:${req.ip}:class:${endpointClass}`;
          limit = Math.floor(limit / 2);
        }
        if (endpointClass === "AUTH" /* AUTH */ && !user) {
          key = `ip:${req.ip}:auth`;
          limit = 5;
        }
        try {
          if (tenantContext?.tenantId && await quotaOverrideService.hasOverride(tenantContext.tenantId, "api_all")) {
            return next();
          }
          const result2 = await rateLimiter.checkLimit(key, limit, windowMs);
          if (tenantContext?.tenantId) {
            const hasDailyOverride = await quotaOverrideService.hasOverride(tenantContext.tenantId, "requests_day");
            const quota = quota_manager_default.getQuotaForTenant(tenantContext.tenantId);
            const dailyKey = `tenant:${tenantContext.tenantId}:requests_day`;
            const baseDailyLimit = quota.requestsPerDay;
            const burstMultiplier = quota.burstAllowance || 1;
            const hardDailyLimit = Math.floor(baseDailyLimit * burstMultiplier);
            const dailyResult = await rateLimiter.checkLimit(dailyKey, hardDailyLimit, ONE_DAY_MS2);
            if (!hasDailyOverride && dailyResult.allowed && dailyResult.total > baseDailyLimit) {
              if (Math.random() < 0.01) {
                metrics2.rateLimitExceededTotal.labels(tenantContext.tenantId, "DAILY_BURST").inc();
              }
              res.set("X-Quota-Burst-Active", "true");
            }
            if (!dailyResult.allowed && !hasDailyOverride) {
              if (tenantContext) {
                metrics2.rateLimitExceededTotal.labels(tenantContext.tenantId, "DAILY_QUOTA").inc();
                provenanceLedger.appendEntry({
                  tenantId: tenantContext.tenantId,
                  actionType: "DAILY_QUOTA_EXCEEDED",
                  resourceType: "tenant_quota",
                  resourceId: "requests_day",
                  actorId: user?.id || "system",
                  actorType: "user",
                  payload: { dailyKey, hardDailyLimit, baseDailyLimit, remaining: dailyResult.remaining },
                  metadata: { ip: req.ip }
                }).catch(() => {
                });
              }
              return res.status(429).json({
                error: "Daily quota exceeded",
                code: "QUO_DAILY_LIMIT_EXCEEDED",
                retryAfter: Math.ceil((dailyResult.reset - Date.now()) / 1e3)
              });
            }
            res.set("X-RateLimit-Daily-Limit", String(baseDailyLimit));
            res.set("X-RateLimit-Daily-Burst-Limit", String(hardDailyLimit));
            res.set("X-RateLimit-Daily-Remaining", String(dailyResult.remaining));
          }
          res.set("X-RateLimit-Limit", String(result2.total));
          res.set("X-RateLimit-Remaining", String(result2.remaining));
          res.set("X-RateLimit-Reset", String(Math.ceil(result2.reset / 1e3)));
          if (tenantContext) {
            res.set("X-RateLimit-Tenant", tenantContext.tenantId);
          }
          if (!result2.allowed) {
            if (tenantContext) {
              metrics2.rateLimitExceededTotal.labels(tenantContext.tenantId, endpointClass).inc();
              provenanceLedger.appendEntry({
                tenantId: tenantContext.tenantId,
                actionType: "RATE_LIMIT_EXCEEDED",
                resourceType: "endpoint_class",
                resourceId: endpointClass,
                actorId: user?.id || "system",
                actorType: "user",
                payload: { key, limit, remaining: result2.remaining },
                metadata: { ip: req.ip }
              }).catch(() => {
              });
            } else {
              metrics2.rateLimitExceededTotal.labels("unknown", endpointClass).inc();
            }
            return res.status(429).json({
              error: "Too many requests",
              class: endpointClass,
              retryAfter: Math.ceil((result2.reset - Date.now()) / 1e3)
            });
          }
          next();
        } catch (e) {
          console.error("Rate limit error", e);
          next();
        }
      };
    };
    rateLimitMiddleware = createRateLimiter2("DEFAULT" /* DEFAULT */);
  }
});

// src/maestro/cost_meter.ts
var cost_meter_exports = {};
__export(cost_meter_exports, {
  CostMeter: () => CostMeter
});
import crypto27 from "node:crypto";
import fs14 from "node:fs";
import path14 from "node:path";
import { metrics as metrics6 } from "@opentelemetry/api";
var meter2, costCounter, tokenCounter, CostMeter;
var init_cost_meter = __esm({
  "src/maestro/cost_meter.ts"() {
    "use strict";
    meter2 = metrics6.getMeter("maestro-llm-observability");
    costCounter = meter2.createCounter("llm_cost_usd_total", {
      description: "Estimated LLM spend in USD"
    });
    tokenCounter = meter2.createCounter("llm_tokens_total", {
      description: "Total LLM tokens consumed by segment"
    });
    CostMeter = class {
      constructor(ig, pricingTable2, options2 = {}) {
        this.ig = ig;
        this.pricingTable = pricingTable2;
        this.usageLogPath = options2.usageLogPath || process.env.LLM_USAGE_LOG_PATH || "logs/llm-usage.ndjson";
        this.defaultEnvironment = options2.defaultEnvironment || process.env.LLM_ENVIRONMENT || process.env.NODE_ENV || "unknown";
      }
      usageLogPath;
      defaultEnvironment;
      estimateCost(usage) {
        const key = `${usage.vendor}:${usage.model}`;
        const pricing = this.pricingTable[key];
        if (!pricing) return 0;
        const inCost = usage.inputTokens / 1e3 * pricing.inputPer1K;
        const outCost = usage.outputTokens / 1e3 * pricing.outputPer1K;
        return inCost + outCost;
      }
      async record(runId, taskId, usage, metadata = {}) {
        const cost = this.estimateCost(usage);
        const sample = {
          id: crypto27.randomUUID(),
          runId,
          taskId,
          model: usage.model,
          vendor: usage.vendor,
          inputTokens: usage.inputTokens,
          outputTokens: usage.outputTokens,
          currency: "USD",
          cost,
          createdAt: (/* @__PURE__ */ new Date()).toISOString(),
          feature: metadata.feature,
          tenantId: metadata.tenantId,
          environment: metadata.environment || this.defaultEnvironment
        };
        await this.ig.recordCostSample(sample);
        this.emitMetrics(sample);
        this.appendUsageLog(sample, metadata.traceId);
        return sample;
      }
      async summarize(runId) {
        return this.ig.getRunCostSummary(runId);
      }
      emitMetrics(sample) {
        const attributes = {
          vendor: sample.vendor,
          model: sample.model,
          feature: sample.feature || "unspecified",
          tenant: sample.tenantId || "unspecified",
          environment: sample.environment || this.defaultEnvironment
        };
        costCounter.add(sample.cost, attributes);
        tokenCounter.add(sample.inputTokens, { ...attributes, segment: "prompt" });
        tokenCounter.add(sample.outputTokens, { ...attributes, segment: "completion" });
      }
      appendUsageLog(sample, traceId) {
        const logEntry = {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          traceId,
          vendor: sample.vendor,
          model: sample.model,
          inputTokens: sample.inputTokens,
          outputTokens: sample.outputTokens,
          cost: sample.cost,
          currency: sample.currency,
          feature: sample.feature,
          tenantId: sample.tenantId,
          environment: sample.environment || this.defaultEnvironment
        };
        const logDir = path14.dirname(this.usageLogPath);
        fs14.mkdirSync(logDir, { recursive: true });
        fs14.appendFileSync(this.usageLogPath, `${JSON.stringify(logEntry)}
`);
      }
    };
  }
});

// src/maestro/policy-client.ts
var PolicyClient, policyClient;
var init_policy_client = __esm({
  "src/maestro/policy-client.ts"() {
    "use strict";
    PolicyClient = class _PolicyClient {
      static instance;
      constructor() {
      }
      static getInstance() {
        if (!_PolicyClient.instance) {
          _PolicyClient.instance = new _PolicyClient();
        }
        return _PolicyClient.instance;
      }
      async evaluate(input) {
        console.log("[PolicyClient] Evaluating:", JSON.stringify(input));
        if (!input.user || !input.user.tenantId) {
          return { allowed: false, reason: "No user context" };
        }
        if (input.user.roles && input.user.roles.length > 0) {
          return { allowed: true };
        }
        return { allowed: false, reason: "No roles found" };
      }
    };
    policyClient = PolicyClient.getInstance();
  }
});

// src/cases/workflow/repos/TaskRepo.ts
var TaskRepo_exports = {};
__export(TaskRepo_exports, {
  TaskRepo: () => TaskRepo
});
import { randomUUID as uuidv412 } from "crypto";
var repoLogger3, TaskRepo;
var init_TaskRepo = __esm({
  "src/cases/workflow/repos/TaskRepo.ts"() {
    "use strict";
    init_logger();
    repoLogger3 = logger_default.child({ name: "TaskRepo" });
    TaskRepo = class {
      constructor(pg5) {
        this.pg = pg5;
      }
      /**
       * Create a new task
       */
      async create(input) {
        const id = uuidv412();
        const { rows } = await this.pg.query(
          `INSERT INTO maestro.case_tasks (
        id, case_id, title, description, task_type, priority,
        assigned_to, assigned_by, due_date, required_role_id,
        depends_on_task_ids, metadata, created_by
      )
      VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
      RETURNING *`,
          [
            id,
            input.caseId,
            input.title,
            input.description || null,
            input.taskType || "standard",
            input.priority || "medium",
            input.assignedTo || null,
            input.assignedBy || null,
            input.dueDate || null,
            input.requiredRoleId || null,
            input.dependsOnTaskIds || [],
            JSON.stringify(input.metadata || {}),
            input.createdBy
          ]
        );
        repoLogger3.info(
          { taskId: id, caseId: input.caseId, title: input.title },
          "Task created"
        );
        return this.mapRow(rows[0]);
      }
      /**
       * Update a task
       */
      async update(input) {
        const updateFields = [];
        const params = [input.id];
        let paramIndex = 2;
        if (input.title !== void 0) {
          updateFields.push(`title = $${paramIndex}`);
          params.push(input.title);
          paramIndex++;
        }
        if (input.description !== void 0) {
          updateFields.push(`description = $${paramIndex}`);
          params.push(input.description);
          paramIndex++;
        }
        if (input.status !== void 0) {
          updateFields.push(`status = $${paramIndex}`);
          params.push(input.status);
          paramIndex++;
          if (input.status === "completed") {
            updateFields.push(`completed_at = NOW()`);
          }
        }
        if (input.priority !== void 0) {
          updateFields.push(`priority = $${paramIndex}`);
          params.push(input.priority);
          paramIndex++;
        }
        if (input.assignedTo !== void 0) {
          updateFields.push(`assigned_to = $${paramIndex}`);
          params.push(input.assignedTo);
          paramIndex++;
          updateFields.push(`assigned_at = COALESCE(assigned_at, NOW())`);
          if (input.assignedBy) {
            updateFields.push(`assigned_by = $${paramIndex}`);
            params.push(input.assignedBy);
            paramIndex++;
          }
        }
        if (input.dueDate !== void 0) {
          updateFields.push(`due_date = $${paramIndex}`);
          params.push(input.dueDate);
          paramIndex++;
        }
        if (input.dependsOnTaskIds !== void 0) {
          updateFields.push(`depends_on_task_ids = $${paramIndex}`);
          params.push(input.dependsOnTaskIds);
          paramIndex++;
        }
        if (input.resultData !== void 0) {
          updateFields.push(`result_data = $${paramIndex}`);
          params.push(JSON.stringify(input.resultData));
          paramIndex++;
        }
        if (input.metadata !== void 0) {
          updateFields.push(`metadata = $${paramIndex}`);
          params.push(JSON.stringify(input.metadata));
          paramIndex++;
        }
        if (updateFields.length === 0) {
          return await this.findById(input.id);
        }
        updateFields.push(`updated_at = NOW()`);
        const { rows } = await this.pg.query(
          `UPDATE maestro.case_tasks
       SET ${updateFields.join(", ")}
       WHERE id = $1
       RETURNING *`,
          params
        );
        if (rows[0]) {
          repoLogger3.info({ taskId: input.id }, "Task updated");
        }
        return rows[0] ? this.mapRow(rows[0]) : null;
      }
      /**
       * Find task by ID
       */
      async findById(id) {
        const { rows } = await this.pg.query(
          `SELECT * FROM maestro.case_tasks WHERE id = $1`,
          [id]
        );
        return rows[0] ? this.mapRow(rows[0]) : null;
      }
      /**
       * List tasks with filters
       */
      async list(filters) {
        const params = [];
        const conditions = [];
        let paramIndex = 1;
        if (filters.caseId) {
          conditions.push(`case_id = $${paramIndex}`);
          params.push(filters.caseId);
          paramIndex++;
        }
        if (filters.assignedTo) {
          conditions.push(`assigned_to = $${paramIndex}`);
          params.push(filters.assignedTo);
          paramIndex++;
        }
        if (filters.status) {
          if (Array.isArray(filters.status)) {
            conditions.push(`status = ANY($${paramIndex})`);
            params.push(filters.status);
          } else {
            conditions.push(`status = $${paramIndex}`);
            params.push(filters.status);
          }
          paramIndex++;
        }
        if (filters.taskType) {
          conditions.push(`task_type = $${paramIndex}`);
          params.push(filters.taskType);
          paramIndex++;
        }
        if (filters.priority) {
          conditions.push(`priority = $${paramIndex}`);
          params.push(filters.priority);
          paramIndex++;
        }
        if (filters.isOverdue) {
          conditions.push(`due_date < NOW()`);
          conditions.push(`status NOT IN ('completed', 'cancelled')`);
        }
        if (filters.requiredRoleId) {
          conditions.push(`required_role_id = $${paramIndex}`);
          params.push(filters.requiredRoleId);
          paramIndex++;
        }
        const whereClause = conditions.length > 0 ? `WHERE ${conditions.join(" AND ")}` : "";
        const sortBy = filters.sortBy || "createdAt";
        const sortOrder = filters.sortOrder || "desc";
        const orderByMap = {
          createdAt: "created_at",
          dueDate: "due_date",
          priority: "priority"
        };
        const orderBy = orderByMap[sortBy] || "created_at";
        const limit = Math.min(filters.limit || 50, 1e3);
        const offset = filters.offset || 0;
        const { rows } = await this.pg.query(
          `SELECT * FROM maestro.case_tasks
       ${whereClause}
       ORDER BY ${orderBy} ${sortOrder.toUpperCase()}
       LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`,
          [...params, limit, offset]
        );
        return rows.map(this.mapRow);
      }
      /**
       * Get tasks for a case
       */
      async getCaseTasks(caseId, status) {
        return this.list({ caseId, status });
      }
      /**
       * Assign task to user
       */
      async assignTask(taskId, userId, assignedBy) {
        return this.update({
          id: taskId,
          assignedTo: userId,
          assignedBy,
          status: "assigned"
        });
      }
      /**
       * Complete task
       */
      async completeTask(taskId, userId, resultData) {
        const { rows } = await this.pg.query(
          `UPDATE maestro.case_tasks
       SET status = 'completed',
           completed_at = NOW(),
           completed_by = $2,
           result_data = COALESCE($3, result_data),
           updated_at = NOW()
       WHERE id = $1
       RETURNING *`,
          [taskId, userId, resultData ? JSON.stringify(resultData) : null]
        );
        if (rows[0]) {
          repoLogger3.info({ taskId, userId }, "Task completed");
        }
        return rows[0] ? this.mapRow(rows[0]) : null;
      }
      /**
       * Get overdue tasks for a case
       */
      async getOverdueTasks(caseId) {
        const { rows } = await this.pg.query(
          `SELECT * FROM maestro.get_overdue_tasks($1)`,
          [caseId]
        );
        return rows.map((row) => ({
          taskId: row.task_id,
          title: row.title,
          assignedTo: row.assigned_to,
          dueDate: row.due_date,
          daysOverdue: parseFloat(row.days_overdue)
        }));
      }
      /**
       * Check if task dependencies are met
       */
      async areDependenciesMet(taskId) {
        const task = await this.findById(taskId);
        if (!task || task.dependsOnTaskIds.length === 0) {
          return true;
        }
        const { rows } = await this.pg.query(
          `SELECT COUNT(*) as count
       FROM maestro.case_tasks
       WHERE id = ANY($1)
       AND status != 'completed'`,
          [task.dependsOnTaskIds]
        );
        return parseInt(rows[0].count, 10) === 0;
      }
      /**
       * Delete task
       */
      async delete(id) {
        const { rowCount } = await this.pg.query(
          `DELETE FROM maestro.case_tasks WHERE id = $1`,
          [id]
        );
        if (rowCount && rowCount > 0) {
          repoLogger3.info({ taskId: id }, "Task deleted");
        }
        return rowCount !== null && rowCount > 0;
      }
      /**
       * Map database row to domain object
       */
      mapRow(row) {
        return {
          id: row.id,
          caseId: row.case_id,
          title: row.title,
          description: row.description,
          taskType: row.task_type,
          status: row.status,
          priority: row.priority,
          assignedTo: row.assigned_to,
          assignedBy: row.assigned_by,
          assignedAt: row.assigned_at,
          dueDate: row.due_date,
          completedAt: row.completed_at,
          completedBy: row.completed_by,
          requiredRoleId: row.required_role_id,
          dependsOnTaskIds: row.depends_on_task_ids || [],
          resultData: row.result_data || {},
          metadata: row.metadata || {},
          createdAt: row.created_at,
          updatedAt: row.updated_at,
          createdBy: row.created_by
        };
      }
    };
  }
});

// src/services/opa-client.ts
import axios5 from "axios";
import pino50 from "pino";
var logger47, OPAClient, opaClient;
var init_opa_client = __esm({
  "src/services/opa-client.ts"() {
    "use strict";
    logger47 = pino50({ name: "opa-client" });
    OPAClient = class {
      // 5 seconds
      constructor(baseURL = process.env.OPA_URL || "http://localhost:8181", timeout = 3e3) {
        this.baseURL = baseURL;
        this.timeout = timeout;
        this.client = axios5.create({
          baseURL,
          timeout,
          headers: {
            "Content-Type": "application/json"
          }
        });
        this.client.interceptors.request.use(
          (config9) => {
            logger47.debug({ url: config9.url, method: config9.method }, "OPA request");
            return config9;
          },
          (error) => {
            logger47.error({ error }, "OPA request error");
            return Promise.reject(error);
          }
        );
        this.client.interceptors.response.use(
          (response) => {
            logger47.debug(
              {
                url: response.config.url,
                status: response.status,
                duration: response.headers["x-response-time"]
              },
              "OPA response"
            );
            return response;
          },
          (error) => {
            logger47.error(
              {
                url: error.config?.url,
                status: error.response?.status,
                message: error.message
              },
              "OPA response error"
            );
            return Promise.reject(error);
          }
        );
      }
      client;
      decisionCache = /* @__PURE__ */ new Map();
      cacheTimeout = 5e3;
      async evaluateExportPolicy(input) {
        const startTime = Date.now();
        const cacheKey = this.generateCacheKey(input);
        const cached = this.decisionCache.get(cacheKey);
        if (cached && Date.now() - cached.timestamp < this.cacheTimeout) {
          logger47.debug(
            { cacheKey, age: Date.now() - cached.timestamp },
            "OPA cache hit"
          );
          return cached.decision;
        }
        try {
          const response = await this.client.post(
            "/v1/data/intelgraph/policy/export/enhanced/decision",
            { input }
          );
          const decisionId = response.data.decision_id || response.headers["x-opa-decision-id"];
          const decision = {
            ...response.data.result.decision,
            decision_id: decisionId
          };
          this.decisionCache.set(cacheKey, {
            decision,
            timestamp: Date.now()
          });
          const duration = Date.now() - startTime;
          logger47.info(
            {
              action: decision.action,
              allow: decision.allow,
              violationsCount: decision.violations.length,
              riskLevel: decision.risk_assessment.level,
              durationMs: duration,
              cached: false
            },
            "OPA export policy evaluation completed"
          );
          return decision;
        } catch (error) {
          const duration = Date.now() - startTime;
          logger47.error(
            {
              error: error instanceof Error ? error.message : "Unknown error",
              durationMs: duration,
              input: this.sanitizeInputForLogging(input)
            },
            "OPA export policy evaluation failed"
          );
          return this.failSafeDecision(error);
        }
      }
      async evaluateQuery(query3, input) {
        try {
          const response = await this.client.post(`/v1/data/${query3}`, { input });
          return response.data.result;
        } catch (error) {
          logger47.error({ query: query3, error }, "OPA query evaluation failed");
          throw error;
        }
      }
      async checkDataAccess(userId, tenantId, resource, action) {
        const input = {
          user: { id: userId, tenant: tenantId },
          resource,
          action
        };
        try {
          const result2 = await this.evaluateQuery(
            "intelgraph/policy/authz/allow",
            input
          );
          return result2 === true;
        } catch (error) {
          logger47.error(
            { userId, tenantId, resource, action, error },
            "Data access check failed"
          );
          return false;
        }
      }
      async simulatePolicy(input, policyChanges) {
        const current = await this.evaluateExportPolicy(input);
        const simulatedInput = this.applyPolicySimulation(input, policyChanges);
        const simulated = await this.evaluateExportPolicy(simulatedInput);
        const impact = {
          decision_changed: current.action !== simulated.action,
          violations_added: simulated.violations.filter(
            (v) => !current.violations.some((cv) => cv.code === v.code)
          ),
          violations_removed: current.violations.filter(
            (v) => !simulated.violations.some((sv) => sv.code === v.code)
          )
        };
        logger47.info(
          {
            decisionChanged: impact.decision_changed,
            violationsAdded: impact.violations_added.length,
            violationsRemoved: impact.violations_removed.length
          },
          "Policy simulation completed"
        );
        return { current, simulated, impact };
      }
      applyPolicySimulation(input, changes) {
        const simulated = JSON.parse(JSON.stringify(input));
        if (changes.stricterLicenseCheck) {
          simulated.context.purpose = "commercial";
        }
        if (changes.requireStepUpForAllExports) {
          simulated.context.step_up_verified = false;
        }
        return simulated;
      }
      generateCacheKey(input) {
        const key = {
          action: input.action,
          licenses: input.dataset.sources.map((s) => s.license).sort(),
          userId: input.context.user_id,
          purpose: input.context.purpose,
          exportType: input.context.export_type,
          stepUpVerified: input.context.step_up_verified
        };
        return Buffer.from(JSON.stringify(key)).toString("base64");
      }
      sanitizeInputForLogging(input) {
        return {
          action: input.action,
          sourceCount: input.dataset.sources.length,
          licenses: [...new Set(input.dataset.sources.map((s) => s.license))],
          userRole: input.context.user_role,
          purpose: input.context.purpose,
          exportType: input.context.export_type
        };
      }
      failSafeDecision(error) {
        logger47.warn("Returning fail-safe decision due to OPA error");
        return {
          action: "deny",
          allow: false,
          violations: [
            {
              code: "OPA_UNAVAILABLE",
              message: "Policy evaluation service unavailable - export denied for safety",
              source: null,
              appeal_code: "SYS001",
              appeal_url: "https://compliance.intelgraph.io/appeal/SYS001",
              severity: "blocking"
            }
          ],
          risk_assessment: {
            level: "high",
            factors: ["Policy evaluation service unavailable"],
            requires_approval: true,
            requires_dual_control: true,
            requires_step_up: true
          },
          required_approvals: ["system-admin", "compliance-officer"],
          appeal_available: true,
          next_steps: [
            "Contact system administrator about policy service availability",
            "Submit appeal for manual review"
          ]
        };
      }
      // Clear cache (useful for testing or manual cache invalidation)
      clearCache() {
        this.decisionCache.clear();
        logger47.info("OPA decision cache cleared");
      }
      // Health check
      async healthCheck() {
        const startTime = Date.now();
        try {
          await this.client.get("/health");
          return {
            healthy: true,
            response_time_ms: Date.now() - startTime
          };
        } catch (error) {
          return {
            healthy: false,
            response_time_ms: Date.now() - startTime
          };
        }
      }
    };
    opaClient = new OPAClient();
  }
});

// src/services/LLMService.ts
import { OpenAI } from "openai";
var LLMService, LLMService_default;
var init_LLMService = __esm({
  "src/services/LLMService.ts"() {
    "use strict";
    init_logger2();
    init_tracing();
    init_metrics4();
    LLMService = class {
      config;
      metrics;
      _openai;
      constructor(config9 = {}) {
        this.config = {
          defaultProvider: process.env.LLM_PROVIDER || "openai",
          defaultModel: process.env.LLM_MODEL || "gpt-4-turbo-preview",
          maxRetries: parseInt(process.env.LLM_MAX_RETRIES || "3"),
          timeout: parseInt(process.env.LLM_TIMEOUT || "60000"),
          temperature: parseFloat(process.env.LLM_TEMPERATURE || "0.7"),
          apiKey: process.env.OPENAI_API_KEY,
          ...config9
        };
        this.metrics = {
          totalRequests: 0,
          totalTokens: 0,
          errorCount: 0,
          averageLatency: 0
        };
      }
      get openai() {
        if (!this._openai) {
          this._openai = new OpenAI({
            apiKey: this.config.apiKey
          });
        }
        return this._openai;
      }
      /**
       * Execute a completion request
       */
      async complete(prompt, options2 = {}) {
        return tracer6.trace("llm.complete", async (span) => {
          const startTime = Date.now();
          const provider = options2.provider || this.config.defaultProvider;
          const model = options2.model || this.config.defaultModel;
          span.setAttributes({
            "llm.provider": provider,
            "llm.model": model,
            "llm.temperature": options2.temperature || this.config.temperature
          });
          try {
            let response;
            switch (provider) {
              case "openai":
                response = await this.callOpenAI(prompt, options2);
                break;
              case "anthropic":
                response = await this.callAnthropic(prompt, options2);
                break;
              case "google":
                response = await this.callGoogle(prompt, options2);
                break;
              case "ollama":
                response = await this.callOllama(prompt, options2);
                break;
              default:
                throw new Error(`Unsupported LLM provider: ${provider}`);
            }
            this.updateMetrics(Date.now() - startTime, response.usage, provider, model);
            if (response.usage) {
              span.setAttributes({
                "llm.usage.prompt_tokens": response.usage.prompt_tokens,
                "llm.usage.completion_tokens": response.usage.completion_tokens,
                "llm.usage.total_tokens": response.usage.total_tokens
              });
            }
            return response.text;
          } catch (error) {
            this.metrics.errorCount++;
            if (metrics3.llmRequestDuration) {
              metrics3.llmRequestDuration.observe({
                provider,
                model,
                status: "error"
              }, (Date.now() - startTime) / 1e3);
            }
            logger_default2.error("LLM request failed", {
              provider,
              model,
              error: error instanceof Error ? error.message : String(error)
            });
            throw error;
          }
        });
      }
      /**
       * Chat completion (multi-turn)
       */
      async chat(messages, options2 = {}) {
        return tracer6.trace("llm.chat", async (span) => {
          const startTime = Date.now();
          const provider = options2.provider || this.config.defaultProvider;
          const model = options2.model || this.config.defaultModel;
          span.setAttributes({
            "llm.provider": provider,
            "llm.model": model,
            "llm.message_count": messages.length
          });
          try {
            if (provider !== "openai") {
              throw new Error(`Provider ${provider} not fully implemented for chat`);
            }
            const response = await this.openai.chat.completions.create({
              messages,
              model,
              temperature: options2.temperature || this.config.temperature,
              max_tokens: options2.maxTokens
            });
            const text = response.choices[0].message.content || "";
            this.updateMetrics(Date.now() - startTime, response.usage, provider, model);
            if (response.usage) {
              span.setAttributes({
                "llm.usage.prompt_tokens": response.usage.prompt_tokens,
                "llm.usage.completion_tokens": response.usage.completion_tokens,
                "llm.usage.total_tokens": response.usage.total_tokens
              });
            }
            return text;
          } catch (error) {
            this.metrics.errorCount++;
            if (metrics3.llmRequestDuration) {
              metrics3.llmRequestDuration.observe({
                provider,
                model,
                status: "error"
              }, (Date.now() - startTime) / 1e3);
            }
            logger_default2.error("LLM chat request failed", {
              provider,
              model,
              error: error instanceof Error ? error.message : String(error)
            });
            throw error;
          }
        });
      }
      /**
       * OpenAI implementation
       */
      async callOpenAI(prompt, options2) {
        const body4 = {
          model: options2.model || this.config.defaultModel,
          messages: [{ role: "user", content: prompt }],
          temperature: options2.temperature || this.config.temperature,
          max_tokens: options2.maxTokens
        };
        if (options2.responseFormat === "json") {
          body4.response_format = { type: "json_object" };
        }
        const response = await this.openai.chat.completions.create(body4);
        return {
          text: response.choices[0].message.content || "",
          usage: response.usage,
          provider: "openai"
        };
      }
      /**
       * Anthropic implementation (placeholder)
       */
      async callAnthropic(_prompt, _options) {
        throw new Error("Anthropic provider not implemented");
      }
      /**
       * Google implementation (placeholder)
       */
      async callGoogle(_prompt, _options) {
        throw new Error("Google provider not implemented");
      }
      /**
       * Ollama implementation (placeholder)
       */
      async callOllama(_prompt, _options) {
        throw new Error("Ollama provider not implemented");
      }
      updateMetrics(latency, usage, provider = "openai", model = "unknown") {
        this.metrics.totalRequests++;
        this.metrics.totalTokens += usage?.total_tokens || 0;
        this.metrics.averageLatency = (this.metrics.averageLatency * (this.metrics.totalRequests - 1) + latency) / this.metrics.totalRequests;
        if (metrics3.llmTokensTotal) {
          metrics3.llmTokensTotal.inc({
            provider,
            model,
            type: "total"
          }, usage?.total_tokens || 0);
        }
        if (metrics3.llmRequestDuration) {
          metrics3.llmRequestDuration.observe({
            provider,
            model,
            status: "success"
          }, latency / 1e3);
        }
      }
    };
    LLMService_default = LLMService;
  }
});

// src/maestro/runs/runs-repo.ts
import { randomUUID as uuidv416 } from "crypto";
function getPool() {
  if (!pool3) {
    pool3 = getPostgresPool2();
  }
  return pool3;
}
var pool3, RunsRepo, _runsRepo, runsRepo;
var init_runs_repo = __esm({
  "src/maestro/runs/runs-repo.ts"() {
    "use strict";
    init_database();
    pool3 = null;
    RunsRepo = class {
      async list(tenantId, limit = 50, offset = 0) {
        const query3 = `
      SELECT id, pipeline_id, pipeline_name as pipeline, status, started_at, 
             completed_at, duration_ms, cost, input_params, output_data, 
             error_message, executor_id, created_at, updated_at, tenant_id
      FROM runs 
      WHERE tenant_id = $1
      ORDER BY created_at DESC 
      LIMIT $2 OFFSET $3
    `;
        const result2 = await getPool().query(query3, [tenantId, limit, offset]);
        return result2.rows;
      }
      async get(id, tenantId) {
        const query3 = `
      SELECT id, pipeline_id, pipeline_name as pipeline, status, started_at, 
             completed_at, duration_ms, cost, input_params, output_data, 
             error_message, executor_id, created_at, updated_at, tenant_id
      FROM runs 
      WHERE id = $1 AND tenant_id = $2
    `;
        const result2 = await getPool().query(query3, [id, tenantId]);
        return result2.rows[0] || null;
      }
      async create(data) {
        const id = uuidv416();
        const query3 = `
      INSERT INTO runs (id, pipeline_id, pipeline_name, input_params, executor_id, tenant_id)
      VALUES ($1, $2, $3, $4, $5, $6)
      RETURNING id, pipeline_id, pipeline_name as pipeline, status, started_at, 
                completed_at, duration_ms, cost, input_params, output_data, 
                error_message, executor_id, created_at, updated_at, tenant_id
    `;
        const result2 = await getPool().query(query3, [
          id,
          data.pipeline_id,
          data.pipeline_name,
          JSON.stringify(data.input_params || {}),
          data.executor_id,
          data.tenant_id
        ]);
        return result2.rows[0];
      }
      async update(id, data, tenantId) {
        const sets = [];
        const values = [];
        let paramCount = 1;
        if (data.status !== void 0) {
          sets.push(`status = $${paramCount++}`);
          values.push(data.status);
        }
        if (data.started_at !== void 0) {
          sets.push(`started_at = $${paramCount++}`);
          values.push(data.started_at);
        }
        if (data.completed_at !== void 0) {
          sets.push(`completed_at = $${paramCount++}`);
          values.push(data.completed_at);
        }
        if (data.duration_ms !== void 0) {
          sets.push(`duration_ms = $${paramCount++}`);
          values.push(data.duration_ms);
        }
        if (data.cost !== void 0) {
          sets.push(`cost = $${paramCount++}`);
          values.push(data.cost);
        }
        if (data.output_data !== void 0) {
          sets.push(`output_data = $${paramCount++}`);
          values.push(JSON.stringify(data.output_data));
        }
        if (data.error_message !== void 0) {
          sets.push(`error_message = $${paramCount++}`);
          values.push(data.error_message);
        }
        if (sets.length === 0) return this.get(id, tenantId);
        const query3 = `
      UPDATE runs 
      SET ${sets.join(", ")}, updated_at = NOW()
      WHERE id = $${paramCount} AND tenant_id = $${paramCount + 1}
      RETURNING id, pipeline_id, pipeline_name as pipeline, status, started_at, 
                completed_at, duration_ms, cost, input_params, output_data, 
                error_message, executor_id, created_at, updated_at, tenant_id
    `;
        values.push(id);
        values.push(tenantId);
        const result2 = await getPool().query(query3, values);
        return result2.rows[0] || null;
      }
      async delete(id, tenantId) {
        const query3 = "DELETE FROM runs WHERE id = $1 AND tenant_id = $2";
        const result2 = await getPool().query(query3, [id, tenantId]);
        return result2.rowCount > 0;
      }
      async getByPipeline(pipelineId, tenantId, limit = 20) {
        const query3 = `
      SELECT id, pipeline_id, pipeline_name as pipeline, status, started_at, 
             completed_at, duration_ms, cost, input_params, output_data, 
             error_message, executor_id, created_at, updated_at, tenant_id
      FROM runs 
      WHERE pipeline_id = $1 AND tenant_id = $2
      ORDER BY created_at DESC 
      LIMIT $3
    `;
        const result2 = await getPool().query(query3, [pipelineId, tenantId, limit]);
        return result2.rows;
      }
    };
    _runsRepo = null;
    runsRepo = {
      get instance() {
        if (!_runsRepo) {
          _runsRepo = new RunsRepo();
        }
        return _runsRepo;
      },
      // Proxy methods for backward compatibility
      async list(tenantId, limit = 50, offset = 0) {
        return this.instance.list(tenantId, limit, offset);
      },
      async get(id, tenantId) {
        return this.instance.get(id, tenantId);
      },
      async create(data) {
        return this.instance.create(data);
      },
      async update(id, data, tenantId) {
        return this.instance.update(id, data, tenantId);
      },
      async delete(id, tenantId) {
        return this.instance.delete(id, tenantId);
      },
      async getByPipeline(pipelineId, tenantId, limit = 20) {
        return this.instance.getByPipeline(pipelineId, tenantId, limit);
      },
      /**
       * Helper to strictly get a run for a tenant, throwing if not found or unauthorized (implicit).
       */
      async getRunForTenant(id, tenantId) {
        return this.instance.get(id, tenantId);
      }
    };
  }
});

// src/security/quantum-identity-manager.ts
import * as crypto30 from "crypto";
var QuantumIdentityManager, quantumIdentityManager;
var init_quantum_identity_manager = __esm({
  "src/security/quantum-identity-manager.ts"() {
    "use strict";
    init_logger();
    QuantumIdentityManager = class _QuantumIdentityManager {
      static instance;
      rootKey;
      // Simulated Root CA Key
      constructor() {
        this.rootKey = process.env.PQC_ROOT_KEY || crypto30.randomBytes(32).toString("hex");
      }
      static getInstance() {
        if (!_QuantumIdentityManager.instance) {
          _QuantumIdentityManager.instance = new _QuantumIdentityManager();
        }
        return _QuantumIdentityManager.instance;
      }
      /**
       * For drills/testing: Force re-initialization to pick up environment changes.
       */
      reinitialize() {
        this.rootKey = process.env.PQC_ROOT_KEY || crypto30.randomBytes(32).toString("hex");
        logger.info("QuantumIdentityManager: Root key reinitialized");
      }
      /**
       * Issues a new Quantum Identity for a service.
       */
      issueIdentity(serviceId) {
        logger.info({ serviceId }, "QuantumIdentity: Issuing PQC Identity");
        const publicKey = `pqc-kyber-v1:${crypto30.randomBytes(32).toString("base64")}`;
        const identity = {
          serviceId,
          publicKey,
          algorithm: "KYBER-768",
          issuedAt: (/* @__PURE__ */ new Date()).toISOString(),
          expiresAt: new Date(Date.now() + 864e5).toISOString()
          // 24 hours
        };
        const signature = this.sign(serviceId);
        return { ...identity, signature };
      }
      /**
       * Verifies a Quantum Identity.
       */
      verifyIdentity(identity) {
        const isValid = this.verify(identity.serviceId, identity.signature);
        if (!isValid) {
          logger.warn({ serviceId: identity.serviceId }, "QuantumIdentity: Invalid signature");
          return false;
        }
        if (new Date(identity.expiresAt) < /* @__PURE__ */ new Date()) {
          logger.warn({ serviceId: identity.serviceId }, "QuantumIdentity: Identity expired");
          return false;
        }
        return true;
      }
      /**
       * Simulates PQC Key Encapsulation Mechanism (KEM) Handshake.
       * Alice (Initiator) uses Bob's Public Key to encapsulate a shared secret.
       */
      encapsulate(peerPublicKey) {
        if (!peerPublicKey.startsWith("pqc-kyber-v1:")) {
          throw new Error("Invalid PQC Public Key format");
        }
        const sharedSecret = crypto30.randomBytes(32).toString("hex");
        const ciphertext = `kem-enc:${crypto30.randomBytes(16).toString("hex")}`;
        logger.debug("QuantumIdentity: KEM Encapsulation complete");
        return { sharedSecret, ciphertext };
      }
      /**
       * Simulates PQC Key Decapsulation.
       * Bob (Receiver) uses his Private Key (implicit here) to recover the shared secret.
       */
      decapsulate(ciphertext) {
        if (!ciphertext.startsWith("kem-enc:")) {
          throw new Error("Invalid KEM Ciphertext");
        }
        return "simulated-decapsulated-secret";
      }
      // --- Private Helpers ---
      sign(data) {
        const hash3 = crypto30.createHmac("sha256", this.rootKey).update(data).digest("hex");
        return `pqc-sig:${hash3}`;
      }
      verify(data, signature) {
        if (!signature.startsWith("pqc-sig:")) return false;
        const hash3 = crypto30.createHmac("sha256", this.rootKey).update(data).digest("hex");
        return signature === `pqc-sig:${hash3}`;
      }
    };
    quantumIdentityManager = QuantumIdentityManager.getInstance();
  }
});

// src/analytics/telemetry/allowlist.ts
var ALLOWLIST;
var init_allowlist = __esm({
  "src/analytics/telemetry/allowlist.ts"() {
    "use strict";
    ALLOWLIST = {
      "page_view": ["path", "referrer", "userAgent", "duration"],
      "feature_usage": ["featureName", "action", "status", "latency"],
      "api_call": ["endpoint", "method", "statusCode", "durationMs"],
      "system_alert": ["alertId", "severity", "component"],
      "user_action": ["actionName", "targetIdHash", "context"]
    };
  }
});

// src/analytics/telemetry/scrubber.ts
import crypto31 from "crypto";
var PII_PATTERNS, TelemetryScrubber;
var init_scrubber = __esm({
  "src/analytics/telemetry/scrubber.ts"() {
    "use strict";
    init_allowlist();
    PII_PATTERNS = [
      /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/,
      // Email
      /\b\d{3}-\d{2}-\d{4}\b/,
      // SSN (US)
      /\b(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|3[47][0-9]{13}|3(?:0[0-5]|[68][0-9])[0-9]{11}|6(?:011|5[0-9]{2})[0-9]{12}|(?:2131|1800|35\d{3})\d{11})\b/,
      // Credit Card
      /eyJ[a-zA-Z0-9_-]{10,}\.[a-zA-Z0-9_-]{10,}\.[a-zA-Z0-9_-]{10,}/,
      // JWT
      /(?:Bearer\s+)?[a-zA-Z0-9\-\._~\+\/]{20,}/
      // Generic API tokens
    ];
    TelemetryScrubber = class {
      salt;
      constructor(salt) {
        this.salt = salt;
      }
      /**
       * Hashes a value deterministically with the environment salt.
       */
      hash(value) {
        if (!value) return "";
        return crypto31.createHmac("sha256", this.salt).update(value).digest("hex");
      }
      /**
       * Sanitizes the event properties:
       * 1. Filters out properties not in the allowlist.
       * 2. Scrubs/Redacts PII from allowed string properties.
       */
      scrubProps(eventType, props) {
        const cleanProps = {};
        for (const [key, value] of Object.entries(props)) {
          if (!ALLOWLIST[eventType]?.includes(key)) {
            continue;
          }
          cleanProps[key] = this.scrubValue(value);
        }
        return cleanProps;
      }
      scrubValue(value) {
        if (typeof value === "string") {
          for (const pattern2 of PII_PATTERNS) {
            if (pattern2.test(value)) {
              return "[REDACTED]";
            }
          }
          return value;
        } else if (Array.isArray(value)) {
          return value.map((v) => this.scrubValue(v));
        } else if (typeof value === "object" && value !== null) {
          const result2 = {};
          for (const k in value) {
            result2[k] = this.scrubValue(value[k]);
          }
          return result2;
        }
        return value;
      }
    };
  }
});

// src/analytics/telemetry/TelemetryService.ts
import fs21 from "fs";
import path21 from "path";
import crypto32 from "crypto";
var TelemetryService, config8, telemetryService;
var init_TelemetryService = __esm({
  "src/analytics/telemetry/TelemetryService.ts"() {
    "use strict";
    init_scrubber();
    TelemetryService = class {
      config;
      scrubber;
      logStream = null;
      currentLogFile = "";
      enabled;
      constructor(config9) {
        this.config = config9;
        this.enabled = config9.enabled ?? true;
        this.scrubber = new TelemetryScrubber(config9.salt);
        if (this.enabled) {
          this.ensureLogDir();
          this.rotateLog();
        }
      }
      ensureLogDir() {
        if (!fs21.existsSync(this.config.logDir)) {
          console.log(`[TelemetryService] Creating log dir: ${this.config.logDir}`);
          fs21.mkdirSync(this.config.logDir, { recursive: true });
        } else {
          console.log(`[TelemetryService] Log dir exists: ${this.config.logDir}`);
        }
      }
      rotateLog() {
        const date = (/* @__PURE__ */ new Date()).toISOString().split("T")[0];
        const filename = `telemetry-${date}.jsonl`;
        const filepath = path21.join(this.config.logDir, filename);
        if (this.currentLogFile !== filepath) {
          console.log(`[TelemetryService] Rotating log to: ${filepath}`);
          if (this.logStream) {
            this.logStream.end();
          }
          this.currentLogFile = filepath;
          this.logStream = fs21.createWriteStream(filepath, { flags: "a" });
          this.logStream.on("error", (err) => {
            console.error("[TelemetryService] Stream error:", err);
          });
          this.logStream.on("open", (fd) => {
            console.log(`[TelemetryService] Stream opened for ${filepath}, fd: ${fd}`);
          });
        }
      }
      track(eventType, rawTenantId, rawUserId, actorRole, props) {
        const tenantIdHash = this.scrubber.hash(rawTenantId);
        const scopeHash = this.scrubber.hash(rawUserId);
        const cleanProps = this.scrubber.scrubProps(eventType, props);
        const event = {
          eventId: crypto32.randomUUID(),
          tenantIdHash,
          scopeHash,
          actorRole,
          eventType,
          ts: (/* @__PURE__ */ new Date()).toISOString(),
          props: cleanProps
        };
        this.writeEvent(event);
      }
      writeEvent(event) {
        if (!this.enabled) {
          return;
        }
        this.rotateLog();
        if (this.logStream) {
          const line = JSON.stringify(event) + "\n";
          this.logStream.write(line);
        } else {
          console.error("[TelemetryService] No log stream available");
        }
      }
    };
    config8 = {
      salt: process.env.TELEMETRY_SALT || "development_salt",
      logDir: process.env.TELEMETRY_LOG_DIR || path21.join(process.cwd(), "logs", "telemetry"),
      enabled: process.env.DISABLE_TELEMETRY !== "true"
    };
    telemetryService = new TelemetryService(config8);
  }
});

// ../schemas/audit_event_v1.json
var audit_event_v1_default;
var init_audit_event_v1 = __esm({
  "../schemas/audit_event_v1.json"() {
    audit_event_v1_default = {
      $schema: "https://json-schema.org/draft/2020-12/schema",
      $id: "https://summit.intelgraph/schemas/audit_event_v1.json",
      title: "Audit Event v1",
      description: "Versioned audit event schema capturing policy enforcement decisions with tamper-evident support",
      type: "object",
      additionalProperties: false,
      required: [
        "version",
        "actor",
        "action",
        "resource",
        "classification",
        "policy_version",
        "decision_id",
        "trace_id",
        "timestamp"
      ],
      properties: {
        version: {
          const: "audit_event_v1"
        },
        actor: {
          type: "object",
          description: "Principal responsible for the action",
          required: ["type"],
          additionalProperties: false,
          properties: {
            type: {
              type: "string",
              enum: ["user", "service", "system", "api", "automation"]
            },
            id: {
              type: "string",
              description: "Stable identifier for the actor"
            },
            name: {
              type: "string"
            },
            ip_address: {
              type: "string",
              format: "ipv4"
            }
          }
        },
        action: {
          type: "string",
          description: "Action verb performed (e.g., policy_decision, export, approve)"
        },
        resource: {
          type: "object",
          description: "Resource affected by the action",
          required: ["type"],
          additionalProperties: false,
          properties: {
            type: { type: "string" },
            id: { type: "string" },
            name: { type: "string" },
            owner: { type: "string" }
          }
        },
        classification: {
          type: "string",
          enum: ["public", "internal", "confidential", "restricted"]
        },
        policy_version: {
          type: "string",
          description: "Policy version that produced the decision"
        },
        decision_id: {
          type: "string",
          description: "Opaque decision identifier",
          minLength: 8
        },
        trace_id: {
          type: "string",
          description: "Trace identifier linking audit event to request chain"
        },
        timestamp: {
          type: "string",
          format: "date-time"
        },
        customer: {
          type: "string",
          description: "Customer or tenant id for export filtering"
        },
        metadata: {
          type: "object",
          description: "Optional structured context",
          additionalProperties: true
        }
      }
    };
  }
});

// src/audit/appendOnlyAuditStore.ts
import crypto34 from "crypto";
import fs24 from "fs";
import path27 from "path";
import Ajv2 from "ajv/dist/2020";
import addFormats2 from "ajv-formats";
import pino54 from "pino";
var ajv2, validateEvent, canonicalizeEvent, hashPayload, hashRecord, defaultStorePath, readLastRecord, AppendOnlyAuditStore;
var init_appendOnlyAuditStore = __esm({
  "src/audit/appendOnlyAuditStore.ts"() {
    "use strict";
    init_audit_event_v1();
    ajv2 = new Ajv2({ allErrors: true, removeAdditional: true });
    addFormats2(ajv2);
    validateEvent = ajv2.compile(audit_event_v1_default);
    canonicalizeEvent = (event) => ({
      version: event.version,
      actor: { ...event.actor },
      action: event.action,
      resource: { ...event.resource },
      classification: event.classification,
      policy_version: event.policy_version,
      decision_id: event.decision_id,
      trace_id: event.trace_id,
      timestamp: event.timestamp,
      customer: event.customer,
      metadata: event.metadata ? { ...event.metadata } : void 0
    });
    hashPayload = (event) => crypto34.createHash("sha256").update(JSON.stringify(canonicalizeEvent(event))).digest("hex");
    hashRecord = (record2) => crypto34.createHash("sha256").update(
      JSON.stringify({
        sequence: record2.sequence,
        recorded_at: record2.recorded_at,
        prev_hash: record2.prev_hash,
        payload_hash: record2.payload_hash
      })
    ).digest("hex");
    defaultStorePath = () => process.env.AUDIT_EVENT_STORE ?? path27.join(process.cwd(), "logs", "audit", "audit-events.jsonl");
    readLastRecord = (filePath) => {
      if (!fs24.existsSync(filePath)) return null;
      const content = fs24.readFileSync(filePath, "utf8");
      const lines = content.split("\n").filter(Boolean);
      if (!lines.length) return null;
      try {
        return JSON.parse(lines[lines.length - 1]);
      } catch {
        return null;
      }
    };
    AppendOnlyAuditStore = class {
      filePath;
      logger;
      sequence = 0;
      lastHash = "GENESIS";
      constructor(options2 = {}) {
        this.filePath = options2.filePath ?? defaultStorePath();
        this.logger = options2.logger ?? pino54({
          name: "append-only-audit-store",
          level: process.env.LOG_LEVEL || "info"
        });
        fs24.mkdirSync(path27.dirname(this.filePath), { recursive: true });
        const tail = readLastRecord(this.filePath);
        if (tail) {
          this.sequence = tail.sequence;
          this.lastHash = tail.hash;
        }
      }
      async append(event) {
        const candidate = {
          ...event,
          trace_id: event.trace_id || crypto34.randomUUID(),
          timestamp: event.timestamp || (/* @__PURE__ */ new Date()).toISOString()
        };
        const valid = validateEvent(candidate);
        if (!valid) {
          const errors = validateEvent.errors?.map((e) => `${e.instancePath} ${e.message}`).join("; ");
          throw new Error(`Invalid audit event: ${errors ?? "unknown error"}`);
        }
        const payload_hash = hashPayload(candidate);
        const recordBase = {
          sequence: this.sequence + 1,
          recorded_at: (/* @__PURE__ */ new Date()).toISOString(),
          prev_hash: this.lastHash,
          payload_hash,
          event: canonicalizeEvent(candidate)
        };
        const hash3 = hashRecord(recordBase);
        const record2 = { ...recordBase, hash: hash3 };
        await fs24.promises.appendFile(this.filePath, `${JSON.stringify(record2)}
`, "utf8");
        this.sequence = record2.sequence;
        this.lastHash = record2.hash;
        this.logger.debug({ sequence: record2.sequence, hash: record2.hash }, "Appended audit record");
        return record2;
      }
      async verify() {
        if (!fs24.existsSync(this.filePath)) {
          return { ok: true, checked: 0, errors: [] };
        }
        const contents = await fs24.promises.readFile(this.filePath, "utf8");
        const lines = contents.split("\n").filter(Boolean);
        let expectedPrev = "GENESIS";
        const errors = [];
        let lastHash;
        lines.forEach((line, index) => {
          let record2;
          try {
            record2 = JSON.parse(line);
          } catch {
            errors.push(`line ${index + 1}: invalid JSON`);
            return;
          }
          const computed = hashRecord({
            sequence: record2.sequence,
            recorded_at: record2.recorded_at,
            prev_hash: record2.prev_hash,
            payload_hash: record2.payload_hash,
            event: record2.event
          });
          if (record2.prev_hash !== expectedPrev) {
            errors.push(
              `line ${index + 1}: prev_hash mismatch (expected ${expectedPrev}, got ${record2.prev_hash})`
            );
          }
          if (computed !== record2.hash) {
            errors.push(
              `line ${index + 1}: hash mismatch (expected ${computed}, got ${record2.hash})`
            );
          }
          expectedPrev = record2.hash;
          lastHash = record2.hash;
        });
        return {
          ok: errors.length === 0,
          checked: lines.length,
          errors,
          last_hash: lastHash
        };
      }
      async readRange(options2 = {}) {
        if (!fs24.existsSync(this.filePath)) return [];
        const contents = await fs24.promises.readFile(this.filePath, "utf8");
        const lines = contents.split("\n").filter(Boolean);
        const fromTime = options2.from ? new Date(options2.from).getTime() : null;
        const toTime = options2.to ? new Date(options2.to).getTime() : null;
        return lines.map((line) => JSON.parse(line)).filter((record2) => {
          const eventTime = new Date(record2.event.timestamp).getTime();
          const matchesCustomer = options2.customer ? record2.event.customer === options2.customer : true;
          const afterFrom = fromTime !== null ? eventTime >= fromTime : true;
          const beforeTo = toTime !== null ? eventTime <= toTime : true;
          return matchesCustomer && afterFrom && beforeTo;
        });
      }
    };
  }
});

// src/conductor/observability/prometheus.ts
import * as promClient3 from "prom-client";
function createHistogram4(config9) {
  try {
    return new client5.Histogram(config9);
  } catch (e) {
    return { observe: () => {
    }, startTimer: () => () => {
    }, labels: () => ({ observe: () => {
    } }), reset: () => {
    } };
  }
}
function createCounter4(config9) {
  try {
    return new client5.Counter(config9);
  } catch (e) {
    return { inc: () => {
    }, labels: () => ({ inc: () => {
    } }), reset: () => {
    } };
  }
}
function createGauge3(config9) {
  try {
    return new client5.Gauge(config9);
  } catch (e) {
    return { inc: () => {
    }, dec: () => {
    }, set: () => {
    }, labels: () => ({ inc: () => {
    }, dec: () => {
    }, set: () => {
    } }), reset: () => {
    } };
  }
}
function getConfidenceBucket(confidence) {
  if (confidence >= 0.9) return "high";
  if (confidence >= 0.7) return "medium";
  if (confidence >= 0.5) return "low";
  return "very_low";
}
function getCostBucket(cost) {
  if (cost >= 5) return "expensive";
  if (cost >= 1) return "moderate";
  if (cost >= 0.1) return "cheap";
  return "free";
}
var client5, conductorRouterDecisionsTotal, conductorExpertExecutionsTotal, conductorExpertLatencySeconds, conductorExpertCostUsd, conductorMcpOperationsTotal, conductorMcpLatencySeconds, conductorActiveTasksGauge, conductorSecurityEventsTotal, conductorMcpServerStatus, conductorSystemHealthStatus, conductorQuotaRemainingGauge, conductorRoutingConfidenceHistogram, conductorConcurrencyLimitHitsTotal, conductorTaskTimeoutTotal, pricingReadRequestsTotal, pricingReadLatencyMs, capacityReservationsCounter, capacityReserveLatencyMs, capacityActiveReservationsGauge, PrometheusConductorMetrics, prometheusConductorMetrics;
var init_prometheus = __esm({
  "src/conductor/observability/prometheus.ts"() {
    "use strict";
    init_metrics2();
    client5 = promClient3.default || promClient3;
    conductorRouterDecisionsTotal = createCounter4({
      name: "conductor_router_decisions_total",
      help: "Total number of routing decisions made by the Conductor",
      labelNames: ["expert", "result", "confidence_bucket"]
    });
    conductorExpertExecutionsTotal = createCounter4({
      name: "conductor_expert_executions_total",
      help: "Total number of expert executions",
      labelNames: ["expert", "result", "cost_bucket"]
    });
    conductorExpertLatencySeconds = createHistogram4({
      name: "conductor_expert_latency_seconds",
      help: "Duration of expert executions in seconds",
      labelNames: ["expert"],
      buckets: [0.1, 0.5, 1, 2, 5, 10, 30, 60]
    });
    conductorExpertCostUsd = createHistogram4({
      name: "conductor_expert_cost_usd",
      help: "Cost of expert executions in USD",
      labelNames: ["expert"],
      buckets: [1e-3, 0.01, 0.1, 0.5, 1, 2, 5, 10]
    });
    conductorMcpOperationsTotal = createCounter4({
      name: "conductor_mcp_operations_total",
      help: "Total number of MCP operations",
      labelNames: ["server", "tool", "result"]
    });
    conductorMcpLatencySeconds = createHistogram4({
      name: "conductor_mcp_latency_seconds",
      help: "Duration of MCP operations in seconds",
      labelNames: ["server", "tool"],
      buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5]
    });
    conductorActiveTasksGauge = createGauge3({
      name: "conductor_active_tasks",
      help: "Number of currently active Conductor tasks"
    });
    conductorSecurityEventsTotal = createCounter4({
      name: "conductor_security_events_total",
      help: "Total number of security events in Conductor",
      labelNames: ["type", "result"]
    });
    conductorMcpServerStatus = createGauge3({
      name: "conductor_mcp_server_status",
      help: "Status of MCP servers (1=healthy, 0=unhealthy)",
      labelNames: ["server", "url"]
    });
    conductorSystemHealthStatus = createGauge3({
      name: "conductor_system_health_status",
      help: "Overall Conductor system health (1=healthy, 0.5=degraded, 0=unhealthy)"
    });
    conductorQuotaRemainingGauge = createGauge3({
      name: "conductor_quota_remaining",
      help: "Remaining quota for Conductor operations",
      labelNames: ["expert", "quota_type", "user_id"]
    });
    conductorRoutingConfidenceHistogram = createHistogram4({
      name: "conductor_routing_confidence",
      help: "Confidence scores for routing decisions",
      labelNames: ["expert"],
      buckets: [0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 0.99, 1]
    });
    conductorConcurrencyLimitHitsTotal = createCounter4({
      name: "conductor_concurrency_limit_hits_total",
      help: "Total number of times concurrency limits were hit",
      labelNames: ["expert"]
    });
    conductorTaskTimeoutTotal = createCounter4({
      name: "conductor_task_timeout_total",
      help: "Total number of tasks that timed out",
      labelNames: ["expert", "timeout_type"]
    });
    pricingReadRequestsTotal = createCounter4({
      name: "pricing_read_requests_total",
      help: "Total number of pricing read/debug API requests",
      labelNames: ["route", "status"]
    });
    pricingReadLatencyMs = createHistogram4({
      name: "pricing_read_latency_ms",
      help: "Latency of pricing read/debug API requests in milliseconds",
      labelNames: ["route"],
      buckets: [5, 10, 25, 50, 100, 250, 500, 1e3, 2e3, 5e3]
    });
    capacityReservationsCounter = createCounter4({
      name: "capacity_reservations_total",
      help: "Total capacity reservation actions",
      labelNames: ["action", "status"]
    });
    capacityReserveLatencyMs = createHistogram4({
      name: "capacity_reserve_latency_ms",
      help: "Latency of capacity reservation requests in milliseconds",
      buckets: [5, 10, 25, 50, 100, 250, 500, 1e3, 2e3, 5e3]
    });
    capacityActiveReservationsGauge = createGauge3({
      name: "capacity_active_reservations",
      help: "Number of active capacity reservations"
    });
    try {
      [
        conductorRouterDecisionsTotal,
        conductorExpertExecutionsTotal,
        conductorExpertLatencySeconds,
        conductorExpertCostUsd,
        conductorMcpOperationsTotal,
        conductorMcpLatencySeconds,
        conductorActiveTasksGauge,
        conductorSecurityEventsTotal,
        conductorMcpServerStatus,
        conductorSystemHealthStatus,
        conductorQuotaRemainingGauge,
        conductorRoutingConfidenceHistogram,
        conductorConcurrencyLimitHitsTotal,
        conductorTaskTimeoutTotal,
        pricingReadRequestsTotal,
        pricingReadLatencyMs,
        capacityReservationsCounter,
        capacityReserveLatencyMs,
        capacityActiveReservationsGauge
      ].forEach((metric) => register4.registerMetric(metric));
    } catch (e) {
    }
    PrometheusConductorMetrics = class {
      /**
       * Record a routing decision
       */
      recordRoutingDecision(expert, latencyMs, confidence, success) {
        const confidenceBucket = getConfidenceBucket(confidence);
        conductorRouterDecisionsTotal.inc({
          expert,
          result: success ? "success" : "error",
          confidence_bucket: confidenceBucket
        });
        conductorRoutingConfidenceHistogram.observe({ expert }, confidence);
      }
      /**
       * Record expert execution
       */
      recordExpertExecution(expert, latencyMs, cost, success) {
        const costBucket = getCostBucket(cost);
        conductorExpertExecutionsTotal.inc({
          expert,
          result: success ? "success" : "error",
          cost_bucket: costBucket
        });
        conductorExpertLatencySeconds.observe({ expert }, latencyMs / 1e3);
        conductorExpertCostUsd.observe({ expert }, cost);
      }
      /**
       * Record MCP operation
       */
      recordMCPOperation(serverName, toolName, latencyMs, success) {
        conductorMcpOperationsTotal.inc({
          server: serverName,
          tool: toolName,
          result: success ? "success" : "error"
        });
        conductorMcpLatencySeconds.observe(
          {
            server: serverName,
            tool: toolName
          },
          latencyMs / 1e3
        );
      }
      /**
       * Update active task count
       */
      updateActiveTaskCount(count) {
        conductorActiveTasksGauge.set(count);
      }
      /**
       * Record security event
       */
      recordSecurityEvent(eventType, success) {
        conductorSecurityEventsTotal.inc({
          type: eventType,
          result: success ? "allowed" : "denied"
        });
      }
      /**
       * Update MCP server status
       */
      updateMCPServerStatus(serverName, url, healthy) {
        conductorMcpServerStatus.set({ server: serverName, url }, healthy ? 1 : 0);
      }
      /**
       * Update system health status
       */
      updateSystemHealthStatus(status) {
        const statusValue = status === "healthy" ? 1 : status === "degraded" ? 0.5 : 0;
        conductorSystemHealthStatus.set(statusValue);
      }
      /**
       * Update quota remaining
       */
      updateQuotaRemaining(expert, quotaType, userId, remaining) {
        conductorQuotaRemainingGauge.set(
          { expert, quota_type: quotaType, user_id: userId },
          remaining
        );
      }
      /**
       * Record concurrency limit hit
       */
      recordConcurrencyLimitHit(expert) {
        conductorConcurrencyLimitHitsTotal.inc({ expert });
      }
      /**
       * Record task timeout
       */
      recordTaskTimeout(expert, timeoutType) {
        conductorTaskTimeoutTotal.inc({ expert, timeout_type: timeoutType });
      }
      /**
       * Record operational event (for general operational tracking)
       */
      recordOperationalEvent(eventType, metadata) {
        conductorSecurityEventsTotal.inc({
          type: eventType,
          result: metadata?.success !== false ? "allowed" : "denied"
        });
      }
      /**
       * Record operational metric (for general metric tracking)
       */
      recordOperationalMetric(metricName, value, labels2) {
        if (metricName.includes("active") || metricName.includes("count")) {
          conductorActiveTasksGauge.set(value);
        }
      }
    };
    prometheusConductorMetrics = new PrometheusConductorMetrics();
  }
});

// src/conductor/governance/opa-integration.ts
import axios6 from "axios";
import crypto35 from "crypto";
var OpaPolicyEngine, TagPropagationSystem, TenantIsolationMiddleware, opaPolicyEngine, tagPropagationSystem, tenantIsolationMiddleware;
var init_opa_integration = __esm({
  "src/conductor/governance/opa-integration.ts"() {
    "use strict";
    init_prometheus();
    init_logger2();
    init_bundleStore();
    init_audit2();
    OpaPolicyEngine = class {
      opaBaseUrl;
      policyCache;
      CACHE_TTL = 3e5;
      // 5 minutes
      constructor() {
        this.opaBaseUrl = process.env.OPA_BASE_URL || "http://localhost:8181";
        this.policyCache = /* @__PURE__ */ new Map();
      }
      /**
       * Evaluate policy decision
       */
      async evaluatePolicy(policyName, context4) {
        const startTime = Date.now();
        const policyBundleVersion = this.resolvePolicyBundleVersion(context4);
        let cacheHit = false;
        try {
          const cacheKey = this.generateCacheKey(policyName, context4);
          const cached = this.policyCache.get(cacheKey);
          if (cached && cached.expiry > Date.now()) {
            prometheusConductorMetrics.recordOperationalEvent(
              "opa_cache_hit"
            );
            cacheHit = true;
            const decision2 = {
              ...cached.decision,
              policyBundleVersion
            };
            await this.recordPolicyDecision(
              policyName,
              context4,
              decision2,
              policyBundleVersion,
              cacheHit
            );
            return decision2;
          }
          const opaInput = {
            input: {
              ...context4,
              timestamp: Date.now(),
              policyVersion: process.env.OPA_POLICY_VERSION || "1.0"
            }
          };
          const response = await axios6.post(
            `${this.opaBaseUrl}/v1/data/${policyName}`,
            opaInput,
            {
              timeout: 5e3,
              headers: {
                "Content-Type": "application/json",
                "X-Request-ID": crypto35.randomUUID()
              }
            }
          );
          const decision = this.parseOpaResponse(response.data);
          const enrichedDecision = {
            ...decision,
            policyBundleVersion
          };
          const ttl = decision.allow ? 6e4 : 3e5;
          this.policyCache.set(cacheKey, {
            decision: enrichedDecision,
            expiry: Date.now() + ttl
          });
          prometheusConductorMetrics.recordOperationalEvent(
            "opa_policy_evaluation"
          );
          prometheusConductorMetrics.recordOperationalMetric(
            "opa_evaluation_time",
            Date.now() - startTime
          );
          await this.recordPolicyDecision(
            policyName,
            context4,
            enrichedDecision,
            policyBundleVersion,
            cacheHit
          );
          return enrichedDecision;
        } catch (error) {
          console.error("OPA policy evaluation failed:", error);
          prometheusConductorMetrics.recordOperationalEvent(
            "opa_evaluation_error"
          );
          const decision = {
            allow: false,
            reason: `Policy evaluation failed: ${error instanceof Error ? error.message : "Unknown error"}`,
            reasons: ["opa_error"],
            attrsUsed: this.defaultAttrsUsed(context4),
            policyBundleVersion,
            auditLog: {
              logLevel: "error",
              message: "OPA policy evaluation failure",
              metadata: {
                error: error instanceof Error ? error.message : "Unknown error"
              }
            }
          };
          await this.recordPolicyDecision(
            policyName,
            context4,
            decision,
            policyBundleVersion,
            cacheHit
          );
          return decision;
        }
      }
      /**
       * Evaluate tenant isolation policy
       */
      async evaluateTenantIsolation(context4) {
        return this.evaluatePolicy("conductor/tenant_isolation", context4);
      }
      /**
       * Evaluate data access policy
       */
      async evaluateDataAccess(context4) {
        return this.evaluatePolicy("conductor/data_access", context4);
      }
      /**
       * Evaluate cross-tenant operation policy
       */
      async evaluateCrossTenantOperation(context4) {
        return this.evaluatePolicy("conductor/cross_tenant", context4);
      }
      /**
       * Load tenant isolation configuration
       */
      async loadTenantConfig(tenantId) {
        try {
          const response = await axios6.get(
            `${this.opaBaseUrl}/v1/data/conductor/tenant_config/${tenantId}`,
            {
              timeout: 3e3
            }
          );
          return response.data.result || null;
        } catch (error) {
          console.error(`Failed to load tenant config for ${tenantId}:`, error);
          return null;
        }
      }
      /**
       * Parse OPA response
       */
      parseOpaResponse(opaResponse) {
        const result2 = opaResponse.result;
        if (typeof result2 === "boolean") {
          return {
            allow: result2,
            reason: result2 ? "allowed" : "denied",
            reasons: [result2 ? "allowed" : "denied"],
            attrsUsed: [],
            conditions: [],
            tags: []
          };
        }
        const resolved = result2 || {};
        return {
          allow: resolved.allow || false,
          reason: resolved.reason || "No reason provided",
          reasons: resolved.reasons || (resolved.reason ? [resolved.reason] : []),
          attrsUsed: resolved.attrs_used || resolved.attrsUsed || [],
          conditions: resolved.conditions || [],
          tags: resolved.tags || [],
          auditLog: resolved.audit_log ? {
            logLevel: resolved.audit_log.level || "info",
            message: resolved.audit_log.message || "Policy evaluation",
            metadata: resolved.audit_log.metadata || {}
          } : void 0,
          dataFilters: resolved.data_filters ? {
            tenantScope: resolved.data_filters.tenant_scope || [],
            fieldMask: resolved.data_filters.field_mask || [],
            rowLevelFilters: resolved.data_filters.row_level_filters || {}
          } : void 0
        };
      }
      resolvePolicyBundleVersion(context4) {
        try {
          const resolved = policyBundleStore.resolve(context4.policyVersion);
          return resolved.versionId;
        } catch (_error) {
          return process.env.OPA_POLICY_VERSION || "unknown";
        }
      }
      defaultAttrsUsed(context4) {
        const attributes = /* @__PURE__ */ new Set(["tenantId", "role", "action", "resource"]);
        if (context4.resourceAttributes) attributes.add("resourceAttributes");
        if (context4.subjectAttributes) attributes.add("subjectAttributes");
        return Array.from(attributes);
      }
      async recordPolicyDecision(policyName, context4, decision, policyBundleVersion, cacheHit) {
        const reasons = decision.reasons && decision.reasons.length > 0 ? decision.reasons : decision.reason ? [decision.reason] : [];
        const attrsUsed = decision.attrsUsed && decision.attrsUsed.length > 0 ? decision.attrsUsed : this.defaultAttrsUsed(context4);
        const correlationId = context4.sessionContext?.traceId || context4.sessionContext?.sessionId || crypto35.randomUUID();
        logger_default2.info(
          {
            policy_bundle_version: policyBundleVersion,
            decision: decision.allow ? "allow" : "deny",
            reasons,
            attrs_used: attrsUsed,
            policy: policyName,
            tenantId: context4.tenantId,
            userId: context4.userId,
            action: context4.action,
            resource: context4.resource,
            cacheHit
          },
          "OPA policy decision recorded"
        );
        try {
          await advancedAuditSystem.recordEvent({
            eventType: "policy_decision",
            level: decision.allow ? "info" : "warn",
            correlationId,
            tenantId: context4.tenantId,
            serviceId: "opa-policy-engine",
            action: context4.action,
            outcome: decision.allow ? "success" : "failure",
            message: "OPA policy decision recorded",
            details: {
              policy: policyName,
              policy_bundle_version: policyBundleVersion,
              decision: decision.allow ? "allow" : "deny",
              reasons,
              attrs_used: attrsUsed,
              cacheHit,
              input: {
                tenantId: context4.tenantId,
                userId: context4.userId,
                role: context4.role,
                action: context4.action,
                resource: context4.resource,
                resourceAttributes: context4.resourceAttributes,
                subjectAttributes: context4.subjectAttributes
              }
            },
            complianceRelevant: true,
            complianceFrameworks: ["SOC2"],
            userId: context4.userId,
            resourceType: context4.resource,
            resourceId: context4.resourceAttributes?.id || context4.resourceAttributes?.runId
          });
        } catch (error) {
          logger_default2.error(
            {
              error: error instanceof Error ? error.message : String(error),
              policy: policyName,
              tenantId: context4.tenantId
            },
            "Failed to write policy decision audit event"
          );
        }
      }
      /**
       * Generate cache key for policy decision
       */
      generateCacheKey(policyName, context4) {
        const keyData = {
          policy: policyName,
          tenant: context4.tenantId,
          user: context4.userId,
          role: context4.role,
          action: context4.action,
          resource: context4.resource
        };
        return crypto35.createHash("sha256").update(JSON.stringify(keyData)).digest("hex");
      }
      /**
       * Clear policy cache
       */
      clearCache() {
        this.policyCache.clear();
        prometheusConductorMetrics.recordOperationalEvent(
          "opa_cache_cleared"
        );
      }
    };
    TagPropagationSystem = class {
      tagStore;
      constructor() {
        this.tagStore = /* @__PURE__ */ new Map();
      }
      /**
       * Propagate tags from source to target resource
       */
      async propagateTags(sourceResourceId, targetResourceId, context4) {
        try {
          const sourceTags = this.tagStore.get(sourceResourceId) || /* @__PURE__ */ new Set();
          const targetTags = this.tagStore.get(targetResourceId) || /* @__PURE__ */ new Set();
          const tenantTag = `tenant:${context4.tenantId}`;
          targetTags.add(tenantTag);
          for (const tag of sourceTags) {
            if (tag.startsWith("classification:") || tag.startsWith("sensitivity:")) {
              targetTags.add(tag);
            }
          }
          targetTags.add(`created_by:${context4.userId || "system"}`);
          targetTags.add(`action:${context4.action}`);
          targetTags.add(`timestamp:${Date.now()}`);
          this.tagStore.set(targetResourceId, targetTags);
          console.log(
            `Tags propagated from ${sourceResourceId} to ${targetResourceId}:`,
            Array.from(targetTags)
          );
        } catch (error) {
          console.error("Tag propagation failed:", error);
          prometheusConductorMetrics.recordOperationalEvent(
            "tag_propagation_error"
          );
        }
      }
      /**
       * Get resource tags
       */
      getResourceTags(resourceId) {
        const tags = this.tagStore.get(resourceId);
        return tags ? Array.from(tags) : [];
      }
      /**
       * Add tags to resource
       */
      addTags(resourceId, tags) {
        const existingTags = this.tagStore.get(resourceId) || /* @__PURE__ */ new Set();
        tags.forEach((tag) => existingTags.add(tag));
        this.tagStore.set(resourceId, existingTags);
      }
      /**
       * Filter resources by tenant isolation
       */
      filterResourcesByTenant(resourceIds, tenantId) {
        return resourceIds.filter((id) => {
          const tags = this.getResourceTags(id);
          return tags.includes(`tenant:${tenantId}`);
        });
      }
      /**
       * Validate cross-tenant access
       */
      validateCrossTenantAccess(resourceId, requestingTenantId) {
        const tags = this.getResourceTags(resourceId);
        if (tags.includes(`tenant:${requestingTenantId}`)) {
          return true;
        }
        return tags.some(
          (tag) => tag.startsWith("shared_with:") && tag.includes(requestingTenantId)
        );
      }
    };
    TenantIsolationMiddleware = class {
      opaPolicyEngine;
      tagPropagationSystem;
      constructor() {
        this.opaPolicyEngine = new OpaPolicyEngine();
        this.tagPropagationSystem = new TagPropagationSystem();
      }
      /**
       * Express middleware for tenant isolation
       */
      middleware() {
        return async (req, res, next) => {
          try {
            const tenantId = this.extractTenantId(req);
            const userId = req.user?.sub;
            const role = req.user?.role || "user";
            if (!tenantId) {
              return res.status(400).json({
                success: false,
                message: "Tenant ID is required"
              });
            }
            const context4 = {
              tenantId,
              userId,
              role,
              action: this.mapHttpMethodToAction(req.method),
              resource: this.extractResourceFromPath(req.path),
              sessionContext: {
                ipAddress: req.ip,
                userAgent: req.get("User-Agent"),
                timestamp: Date.now(),
                sessionId: req.sessionID
              }
            };
            const decision = await this.opaPolicyEngine.evaluateTenantIsolation(context4);
            if (!decision.allow) {
              console.warn("Tenant isolation policy violation:", {
                tenantId,
                userId,
                action: context4.action,
                resource: context4.resource,
                reason: decision.reason
              });
              prometheusConductorMetrics.recordOperationalEvent(
                "tenant_isolation_violation"
              );
              return res.status(403).json({
                success: false,
                message: "Access denied by tenant isolation policy",
                reason: decision.reason
              });
            }
            req.policyContext = context4;
            req.policyDecision = decision;
            if (decision.dataFilters) {
              req.dataFilters = decision.dataFilters;
            }
            next();
          } catch (error) {
            console.error("Tenant isolation middleware error:", error);
            return res.status(500).json({
              success: false,
              message: "Tenant isolation check failed"
            });
          }
        };
      }
      /**
       * GraphQL middleware for field-level isolation
       */
      graphqlMiddleware() {
        return {
          requestDidStart() {
            return {
              willSendResponse(requestContext) {
                if (requestContext.request.http?.policyDecision?.dataFilters?.fieldMask) {
                  const fieldMask = requestContext.request.http.policyDecision.dataFilters.fieldMask;
                  const self = this;
                  requestContext.response.body = self.applyFieldMask(
                    requestContext.response.body,
                    fieldMask
                  );
                }
              }
            };
          },
          applyFieldMask(responseBody, fieldMask) {
            if (!responseBody || !fieldMask.length) return responseBody;
            const maskFields = (obj) => {
              if (Array.isArray(obj)) {
                return obj.map(maskFields);
              }
              if (obj && typeof obj === "object") {
                const masked = { ...obj };
                fieldMask.forEach((field) => {
                  delete masked[field];
                });
                Object.keys(masked).forEach((key) => {
                  masked[key] = maskFields(masked[key]);
                });
                return masked;
              }
              return obj;
            };
            return maskFields(responseBody);
          }
        };
      }
      extractTenantId(req) {
        return req.headers["x-tenant-id"] || req.user?.tenantId || req.query.tenantId || null;
      }
      mapHttpMethodToAction(method) {
        const methodMap = {
          GET: "read",
          POST: "create",
          PUT: "update",
          PATCH: "update",
          DELETE: "delete"
        };
        return methodMap[method] || "unknown";
      }
      extractResourceFromPath(path55) {
        const pathParts = path55.split("/").filter(Boolean);
        return pathParts.length > 1 ? pathParts[1] : "unknown";
      }
    };
    opaPolicyEngine = new OpaPolicyEngine();
    tagPropagationSystem = new TagPropagationSystem();
    tenantIsolationMiddleware = new TenantIsolationMiddleware();
  }
});

// src/temporal/lib/activities.ts
var activities_exports = {};
__export(activities_exports, {
  activities: () => activities
});
var activities;
var init_activities = __esm({
  "src/temporal/lib/activities.ts"() {
    "use strict";
    init_otel_tracing();
    activities = {
      async heartbeat(payload) {
        const span = otelService.createSpan("temporal.activity.heartbeat", {
          payload_len: JSON.stringify(payload || {}).length
        });
        if (span) span.end();
        return { ok: true, received: payload, ts: Date.now() };
      },
      async planRun(input) {
        const span = otelService.createSpan("temporal.activity.planRun", {
          "run.id": input.runId
        });
        const steps = Array.isArray(input?.parameters?.steps) ? input.parameters.steps : ["prepare", "execute", "finalize"];
        const result2 = { plan: steps, runId: input.runId, createdAt: Date.now() };
        if (span) span.end();
        return result2;
      },
      async executeStep(input) {
        const span = otelService.createSpan("temporal.activity.executeStep", {
          "run.id": input.runId,
          step: input.step,
          idx: input.idx
        });
        await new Promise((r) => setTimeout(r, 200));
        const res = {
          runId: input.runId,
          step: input.step,
          idx: input.idx,
          status: "OK",
          ts: Date.now()
        };
        if (span) span.end();
        return res;
      },
      async finalizeRun(input) {
        const span = otelService.createSpan("temporal.activity.finalizeRun", {
          "run.id": input.runId
        });
        const out = {
          ok: true,
          runId: input.runId,
          summary: input.result,
          finishedAt: Date.now()
        };
        if (span) span.end();
        return out;
      }
    };
  }
});

// src/temporal/lib/workflows-path.ts
var workflows_path_exports = {};
__export(workflows_path_exports, {
  default: () => workflows_path_default
});
var workflows_path_default;
var init_workflows_path = __esm({
  "src/temporal/lib/workflows-path.ts"() {
    "use strict";
    workflows_path_default = new URL("./workflows.js", import.meta.url).pathname;
  }
});

// src/maestro/evidence/receipt.ts
import crypto41 from "crypto";
function canonicalStringify(value) {
  const seen = /* @__PURE__ */ new WeakSet();
  const order = (input) => {
    if (input === null || typeof input !== "object") {
      return input;
    }
    if (seen.has(input)) {
      return null;
    }
    seen.add(input);
    if (Array.isArray(input)) {
      return input.map((item) => order(item));
    }
    const orderedEntries = Object.keys(input).sort().map((key) => [key, order(input[key])]);
    return Object.fromEntries(orderedEntries);
  };
  return JSON.stringify(order(value));
}
function hashCanonical(value) {
  return crypto41.createHash("sha256").update(canonicalStringify(value)).digest("hex");
}
function getCodeDigest() {
  return process.env.GIT_SHA || process.env.SOURCE_COMMIT || process.env.BUILD_SHA || "unknown";
}
function resolveSigningSecret() {
  const secret = process.env.EVIDENCE_SIGNING_SECRET || (process.env.NODE_ENV !== "production" ? "dev-secret" : void 0);
  if (!secret) {
    throw new Error("EVIDENCE_SIGNING_SECRET is required to sign receipts");
  }
  return {
    secret,
    kid: process.env.EVIDENCE_SIGNER_KID || "dev",
    alg: "HS256"
  };
}
function signReceiptPayload(payload, secret) {
  return crypto41.createHmac("sha256", secret).update(canonicalStringify(payload)).digest("base64url");
}
var init_receipt = __esm({
  "src/maestro/evidence/receipt.ts"() {
    "use strict";
  }
});

// src/services/approvals.ts
var approvals_exports = {};
__export(approvals_exports, {
  approveApproval: () => approveApproval,
  canApprove: () => canApprove,
  createApproval: () => createApproval,
  getApprovalById: () => getApprovalById,
  listApprovals: () => listApprovals,
  rejectApproval: () => rejectApproval
});
async function createApproval(input) {
  const pool4 = getPostgresPool();
  const result2 = await pool4.query(
    `INSERT INTO approvals (requester_id, status, action, payload, reason, run_id)
     VALUES ($1, $2, $3, $4, $5, $6)
     RETURNING *`,
    [
      input.requesterId,
      "pending",
      input.action || null,
      JSON.stringify(input.payload || {}),
      input.reason || null,
      input.runId || null
    ]
  );
  approvalsPending.inc();
  const approval = safeRows4(result2)[0];
  approvalsLogger.info(
    {
      approval_id: approval?.id,
      action: approval?.action,
      requester: input.requesterId,
      run_id: input.runId
    },
    "Approval requested"
  );
  return approval;
}
async function listApprovals(options2 = {}) {
  const pool4 = getPostgresPool();
  const conditions = [];
  const params = [];
  let paramIdx = 1;
  if (options2.status) {
    conditions.push(`status = $${paramIdx++}`);
    params.push(options2.status);
  }
  const whereClause = conditions.length ? `WHERE ${conditions.join(" AND ")}` : "";
  const result2 = await pool4.query(
    `SELECT * FROM approvals ${whereClause} ORDER BY created_at DESC`,
    params
  );
  return safeRows4(result2);
}
async function getApprovalById(id) {
  const pool4 = getPostgresPool();
  const result2 = await pool4.query("SELECT * FROM approvals WHERE id = $1", [id]);
  const approvals = safeRows4(result2);
  return approvals[0] || null;
}
async function approveApproval(id, approverId, decisionReason) {
  const pool4 = getPostgresPool();
  const result2 = await pool4.query(
    `UPDATE approvals
       SET status = 'approved',
           approver_id = $2,
           decision_reason = $3,
           resolved_at = NOW(),
           updated_at = NOW()
     WHERE id = $1 AND status = 'pending'
     RETURNING *`,
    [id, approverId, decisionReason || null]
  );
  const approval = safeRows4(result2)[0];
  if (!approval) return null;
  approvalsPending.dec();
  approvalsApprovedTotal.inc();
  approvalsLogger.info(
    {
      approval_id: approval.id,
      action: approval.action,
      approver: approverId,
      run_id: approval.run_id,
      decision_reason: decisionReason
    },
    "Approval granted"
  );
  return approval;
}
async function rejectApproval(id, approverId, decisionReason) {
  const pool4 = getPostgresPool();
  const result2 = await pool4.query(
    `UPDATE approvals
       SET status = 'rejected',
           approver_id = $2,
           decision_reason = $3,
           resolved_at = NOW(),
           updated_at = NOW()
     WHERE id = $1 AND status = 'pending'
     RETURNING *`,
    [id, approverId, decisionReason || null]
  );
  const approval = safeRows4(result2)[0];
  if (!approval) return null;
  approvalsPending.dec();
  approvalsRejectedTotal.inc();
  approvalsLogger.info(
    {
      approval_id: approval.id,
      action: approval.action,
      approver: approverId,
      run_id: approval.run_id,
      decision_reason: decisionReason
    },
    "Approval rejected"
  );
  return approval;
}
var APPROVER_ROLES, approvalsLogger, safeRows4, canApprove;
var init_approvals = __esm({
  "src/services/approvals.ts"() {
    "use strict";
    init_postgres();
    init_logger();
    init_metrics2();
    APPROVER_ROLES = /* @__PURE__ */ new Set([
      "ADMIN",
      "SECURITY_ADMIN",
      "OPERATIONS",
      "SAFETY"
    ]);
    approvalsLogger = logger_default.child({ name: "ApprovalsService" });
    safeRows4 = (result2) => Array.isArray(result2?.rows) ? result2.rows : [];
    canApprove = (role) => {
      if (!role) return false;
      return APPROVER_ROLES.has(role.toUpperCase());
    };
  }
});

// src/config/production-security.ts
var production_security_exports = {};
__export(production_security_exports, {
  applyProductionSecurity: () => applyProductionSecurity,
  productionAuthMiddleware: () => productionAuthMiddleware
});
import helmet2 from "helmet";
var productionAuthMiddleware, applyProductionSecurity;
var init_production_security = __esm({
  "src/config/production-security.ts"() {
    "use strict";
    init_logger();
    init_AuthService();
    productionAuthMiddleware = async (req, res, next) => {
      const authHeader = req.headers["authorization"];
      const token = authHeader && authHeader.split(" ")[1];
      if (!token) {
        return res.status(401).json({ error: "Unauthorized: No token provided" });
      }
      try {
        const authService5 = new AuthService_default();
        const user = await authService5.verifyToken(token);
        if (!user) {
          return res.status(403).json({ error: "Forbidden: Invalid token" });
        }
        req.user = user;
        next();
      } catch (error) {
        logger.error("Authentication error:", error);
        return res.status(403).json({ error: "Forbidden: Token verification failed" });
      }
    };
    applyProductionSecurity = (app) => {
      logger.info("Applying additional production security configurations...");
      app.use((req, res, next) => {
        res.setHeader(
          "Permissions-Policy",
          "geolocation=(), microphone=(), camera=(), payment=(), usb=(), vr=()"
        );
        next();
      });
      app.use(
        helmet2.permittedCrossDomainPolicies({
          permittedPolicies: "none"
        })
      );
      app.disable("x-powered-by");
    };
  }
});

// src/services/audit/AuditTrailService.ts
import crypto45 from "crypto";
var AuditTrailService, auditTrailService;
var init_AuditTrailService = __esm({
  "src/services/audit/AuditTrailService.ts"() {
    "use strict";
    init_appendOnlyAuditStore();
    AuditTrailService = class {
      constructor(store = new AppendOnlyAuditStore()) {
        this.store = store;
      }
      async recordPolicyDecision(input) {
        await this.store.append({
          version: "audit_event_v1",
          actor: { type: "service", id: input.actorId },
          action: input.action,
          resource: { type: input.resourceType ?? "policy_target", id: input.resourceId },
          classification: input.classification,
          policy_version: input.policyVersion,
          decision_id: input.decisionId ?? crypto45.randomUUID(),
          trace_id: input.traceId ?? crypto45.randomUUID(),
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          customer: input.customer,
          metadata: input.metadata
        });
      }
    };
    auditTrailService = new AuditTrailService();
  }
});

// src/routes/health.ts
var health_exports = {};
__export(health_exports, {
  checkHealth: () => checkHealth,
  default: () => health_default2
});
import { Router as Router52 } from "express";
import { randomUUID as randomUUID72 } from "crypto";
var router102, healthEndpointsEnabled, baseStatus, startedAt, buildDisabledResponse, checkHealth, health_default2;
var init_health2 = __esm({
  "src/routes/health.ts"() {
    "use strict";
    init_logger2();
    init_featureFlags2();
    init_TelemetryService();
    init_AuditTrailService();
    init_async_handler();
    router102 = Router52();
    healthEndpointsEnabled = () => (process.env.HEALTH_ENDPOINTS_ENABLED ?? "true").toLowerCase() === "true";
    baseStatus = () => ({
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      uptime: process.uptime(),
      environment: process.env.NODE_ENV || "development"
    });
    startedAt = () => new Date(Date.now() - Math.floor(process.uptime() * 1e3)).toISOString();
    buildDisabledResponse = (res) => res.status(404).json({ status: "disabled", reason: "HEALTH_ENDPOINTS_ENABLED is false" });
    router102.get("/healthz", (_req, res) => {
      if (!healthEndpointsEnabled()) {
        return buildDisabledResponse(res);
      }
      res.status(200).json({
        status: "ok",
        ...baseStatus()
      });
    });
    router102.get("/readyz", (_req, res) => {
      if (!healthEndpointsEnabled()) {
        return buildDisabledResponse(res);
      }
      const readiness = {
        database: "skipped",
        cache: "skipped",
        messaging: "skipped"
      };
      res.status(200).json({
        status: "ready",
        checks: readiness,
        message: "Shallow readiness probe; deep checks remain on /health/ready",
        ...baseStatus()
      });
    });
    router102.get("/status", (_req, res) => {
      if (!healthEndpointsEnabled()) {
        return buildDisabledResponse(res);
      }
      const version = process.env.APP_VERSION || process.env.npm_package_version || "unknown";
      const commit = process.env.GIT_COMMIT || process.env.COMMIT_SHA || "unknown";
      res.status(200).json({
        status: "ok",
        version,
        commit,
        startedAt: startedAt(),
        ...baseStatus()
      });
    });
    router102.get("/health", asyncHandler(async (_req, res) => {
      res.status(200).json({
        status: "ok",
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        uptime: process.uptime(),
        environment: process.env.NODE_ENV || "development"
      });
    }));
    router102.get("/health/detailed", async (req, res) => {
      telemetryService.track("system_alert", "system", "detailed_health_check", "system", {
        component: "health_detailed",
        severity: "info",
        alertId: "health_check_deep"
      });
      const errors = [];
      const health = {
        status: "ok",
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        uptime: process.uptime(),
        environment: process.env.NODE_ENV || "development",
        services: {
          neo4j: "unknown",
          postgres: "unknown",
          redis: "unknown"
        },
        memory: {
          used: Math.round(process.memoryUsage().heapUsed / 1024 / 1024),
          total: Math.round(process.memoryUsage().heapTotal / 1024 / 1024),
          unit: "MB"
        },
        errors: []
      };
      if (process.env.DISABLE_NEO4J === "true" || process.env.SKIP_DB_CHECKS === "true") {
        health.services.neo4j = "skipped";
      } else {
        try {
          const { getNeo4jDriver: getNeo4jDriver3 } = await Promise.resolve().then(() => (init_neo4j(), neo4j_exports));
          await getNeo4jDriver3().verifyConnectivity();
          health.services.neo4j = "healthy";
        } catch (error) {
          const errorMsg = error instanceof Error ? error.message : "Connection failed";
          health.services.neo4j = "unhealthy";
          health.status = "degraded";
          errors.push({
            service: "neo4j",
            error: errorMsg,
            timestamp: (/* @__PURE__ */ new Date()).toISOString()
          });
          logger2.error({ error, service: "neo4j" }, "Neo4j health check failed");
        }
      }
      try {
        const { getPostgresPool: getPostgresPool3 } = await Promise.resolve().then(() => (init_postgres(), postgres_exports));
        const pool4 = getPostgresPool3();
        await pool4.query("SELECT 1");
        health.services.postgres = "healthy";
      } catch (error) {
        const errorMsg = error instanceof Error ? error.message : "Connection failed";
        health.services.postgres = "unhealthy";
        health.status = "degraded";
        errors.push({
          service: "postgres",
          error: errorMsg,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        });
        logger2.error({ error, service: "postgres" }, "PostgreSQL health check failed");
      }
      try {
        const { getRedisClient: getRedisClient3 } = await Promise.resolve().then(() => (init_redis(), redis_exports));
        const redis5 = getRedisClient3();
        await redis5.ping();
        health.services.redis = "healthy";
      } catch (error) {
        const errorMsg = error instanceof Error ? error.message : "Connection failed";
        health.services.redis = "unhealthy";
        health.status = "degraded";
        errors.push({
          service: "redis",
          error: errorMsg,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        });
        logger2.error({ error, service: "redis" }, "Redis health check failed");
      }
      health.errors = errors;
      const graphQueryOptimizer = isEnabled("graph-query-optimizer", {
        userId: "health-check"
      });
      if (graphQueryOptimizer) {
        health.services["graph-query-optimizer"] = "enabled";
      }
      const cacheStrategy = getVariant("cache-strategy", {
        userId: "health-check"
      });
      if (cacheStrategy && cacheStrategy !== "control") {
        health.services["cache-strategy"] = cacheStrategy;
      }
      const traceIdHeader = req.headers["x-request-id"] || req.headers["x-trace-id"] || randomUUID72();
      const customerId = req.headers["x-customer-id"] || req.headers["x-tenant-id"] || "platform";
      try {
        await auditTrailService.recordPolicyDecision({
          customer: customerId,
          actorId: "health-monitor",
          action: "health_detailed_check",
          resourceId: "service-health",
          resourceType: "system",
          classification: "internal",
          policyVersion: "health-monitoring-v1",
          decisionId: randomUUID72(),
          traceId: traceIdHeader,
          metadata: {
            status: health.status,
            services: health.services,
            errorCount: errors.length
          }
        });
      } catch (error) {
        logger2.warn({ error }, "Failed to append audit trail event for health check");
      }
      const statusCode = health.status === "ok" ? 200 : 503;
      res.status(statusCode).json(health);
    });
    router102.get("/health/ready", async (_req, res) => {
      const failures = [];
      if (process.env.DISABLE_NEO4J !== "true" && process.env.SKIP_DB_CHECKS !== "true") {
        try {
          const { getNeo4jDriver: getNeo4jDriver3 } = await Promise.resolve().then(() => (init_neo4j(), neo4j_exports));
          await getNeo4jDriver3().verifyConnectivity();
        } catch (error) {
          const msg = error instanceof Error ? error.message : "Unknown error";
          failures.push(`Neo4j: ${msg}`);
          logger2.warn({ error }, "Readiness check failed: Neo4j unavailable");
        }
      }
      try {
        const { getPostgresPool: getPostgresPool3 } = await Promise.resolve().then(() => (init_postgres(), postgres_exports));
        const pool4 = getPostgresPool3();
        await pool4.query("SELECT 1");
      } catch (error) {
        const msg = error instanceof Error ? error.message : "Unknown error";
        failures.push(`PostgreSQL: ${msg}`);
        logger2.warn({ error }, "Readiness check failed: PostgreSQL unavailable");
      }
      try {
        const { getRedisClient: getRedisClient3 } = await Promise.resolve().then(() => (init_redis(), redis_exports));
        const redis5 = getRedisClient3();
        await redis5.ping();
      } catch (error) {
        const msg = error instanceof Error ? error.message : "Unknown error";
        failures.push(`Redis: ${msg}`);
        logger2.warn({ error }, "Readiness check failed: Redis unavailable");
      }
      if (failures.length > 0) {
        res.status(503).json({
          status: "not ready",
          failures,
          message: "Critical services are unavailable. Check database connections."
        });
      } else {
        res.status(200).json({ status: "ready" });
      }
    });
    router102.get("/health/live", (_req, res) => {
      res.status(200).json({ status: "alive" });
    });
    router102.get("/health/deployment", async (_req, res) => {
      const checks = {
        connectivity: true,
        migrations: true,
        // In real app, query schema_migrations table
        config: true
      };
      if (checks.connectivity && checks.migrations && checks.config) {
        res.status(200).json({ status: "ready_for_traffic", checks });
      } else {
        res.status(503).json({ status: "deployment_failed", checks });
      }
    });
    checkHealth = async () => {
    };
    health_default2 = router102;
  }
});

// src/services/EntityLinkingService.ts
var EntityLinkingService;
var init_EntityLinkingService = __esm({
  "src/services/EntityLinkingService.ts"() {
    "use strict";
    EntityLinkingService = class {
      async linkEntity(entity, context4) {
        return [];
      }
      async findLinks(text) {
        return [];
      }
      static async suggestLinksForEntity(entityId, context4) {
        return { success: true, links: [] };
      }
    };
  }
});

// src/ai/engines/OCREngine.ts
import { spawn as spawn2 } from "child_process";
import path43 from "path";
import pino57 from "pino";
import sharp from "sharp";
var logger53, OCREngine;
var init_OCREngine = __esm({
  "src/ai/engines/OCREngine.ts"() {
    "use strict";
    logger53 = pino57({ name: "OCREngine" });
    OCREngine = class {
      config;
      isInitialized = false;
      constructor(config9) {
        this.config = config9;
      }
      /**
       * Initialize OCR engine and verify dependencies
       */
      async initialize() {
        try {
          await this.verifyTesseractInstallation();
          await this.verifyPaddleOCRInstallation();
          this.isInitialized = true;
          logger53.info("OCR Engine initialized successfully");
        } catch (error) {
          logger53.error("Failed to initialize OCR Engine:", error);
          throw error;
        }
      }
      /**
       * Extract text from image using multiple OCR engines
       */
      async extractText(imagePath, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          language = "eng",
          enhanceImage = true,
          confidenceThreshold = 0.6,
          preserveWhitespace = false,
          enableStructureAnalysis = true
        } = options2;
        logger53.info(`Starting OCR extraction for: ${imagePath}`);
        try {
          let processedImagePath = imagePath;
          if (enhanceImage) {
            processedImagePath = await this.enhanceImageForOCR(imagePath);
          }
          const [tesseractResults, paddleResults] = await Promise.allSettled([
            this.runTesseractOCR(processedImagePath, language, confidenceThreshold),
            this.runPaddleOCR(processedImagePath, language, confidenceThreshold)
          ]);
          const allResults = [];
          if (tesseractResults.status === "fulfilled") {
            allResults.push(...tesseractResults.value);
          } else {
            logger53.warn("Tesseract OCR failed:", tesseractResults.reason);
          }
          if (paddleResults.status === "fulfilled") {
            allResults.push(...paddleResults.value);
          } else {
            logger53.warn("PaddleOCR failed:", paddleResults.reason);
          }
          const mergedResults = this.mergeOCRResults(
            allResults,
            confidenceThreshold
          );
          if (enableStructureAnalysis && mergedResults.length > 0) {
            await this.analyzeTextStructure(mergedResults);
          }
          logger53.info(
            `OCR extraction completed: ${mergedResults.length} text regions found`
          );
          return mergedResults;
        } catch (error) {
          logger53.error("OCR extraction failed:", error);
          throw error;
        }
      }
      /**
       * Run Tesseract OCR
       */
      async runTesseractOCR(imagePath, language, confidenceThreshold) {
        return new Promise((resolve2, reject) => {
          const args = [
            imagePath,
            "stdout",
            "-l",
            language,
            "--psm",
            "6",
            // Uniform block of text
            "--oem",
            "3",
            // Default OCR Engine Mode
            "-c",
            "preserve_interword_spaces=1",
            "tsv"
          ];
          const tesseract = spawn2("tesseract", args);
          let output = "";
          let errorOutput = "";
          tesseract.stdout.on("data", (data) => {
            output += data.toString();
          });
          tesseract.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          tesseract.on("close", (code) => {
            if (code !== 0) {
              reject(
                new Error(`Tesseract failed with code ${code}: ${errorOutput}`)
              );
              return;
            }
            try {
              const results = this.parseTesseractTSV(output, confidenceThreshold);
              resolve2(results);
            } catch (parseError) {
              reject(parseError);
            }
          });
          tesseract.on("error", (error) => {
            reject(new Error(`Failed to spawn tesseract: ${error.message}`));
          });
        });
      }
      /**
       * Run PaddleOCR
       */
      async runPaddleOCR(imagePath, language, confidenceThreshold) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path43.join(this.config.modelsPath, "paddle_ocr.py");
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--lang",
            this.mapLanguageForPaddle(language),
            "--confidence",
            confidenceThreshold.toString()
          ];
          const python = spawn2(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code !== 0) {
              reject(
                new Error(`PaddleOCR failed with code ${code}: ${errorOutput}`)
              );
              return;
            }
            try {
              const results = JSON.parse(output);
              const ocrResults = this.parsePaddleOCRResults(
                results,
                confidenceThreshold
              );
              resolve2(ocrResults);
            } catch (parseError) {
              reject(parseError);
            }
          });
          python.on("error", (error) => {
            reject(new Error(`Failed to spawn PaddleOCR: ${error.message}`));
          });
        });
      }
      /**
       * Enhance image for better OCR results
       */
      async enhanceImageForOCR(imagePath) {
        const enhancedPath = path43.join(
          this.config.tempPath,
          `enhanced_${Date.now()}_${path43.basename(imagePath)}`
        );
        try {
          await sharp(imagePath).resize(null, 2e3, {
            // Upscale to minimum 2000px height
            withoutEnlargement: false,
            kernel: sharp.kernel.lanczos3
          }).sharpen().normalize().threshold(128).png().toFile(enhancedPath);
          return enhancedPath;
        } catch (error) {
          logger53.warn("Image enhancement failed, using original:", error);
          return imagePath;
        }
      }
      /**
       * Parse Tesseract TSV output
       */
      parseTesseractTSV(tsvOutput, confidenceThreshold) {
        const lines = tsvOutput.trim().split("\n");
        const results = [];
        for (let i = 1; i < lines.length; i++) {
          const columns = lines[i].split("	");
          if (columns.length >= 12) {
            const confidence = parseFloat(columns[10]);
            const text = columns[11];
            if (confidence >= confidenceThreshold * 100 && text.trim()) {
              results.push({
                text: text.trim(),
                confidence: confidence / 100,
                boundingBox: {
                  x: parseInt(columns[6]),
                  y: parseInt(columns[7]),
                  width: parseInt(columns[8]),
                  height: parseInt(columns[9]),
                  confidence: confidence / 100
                },
                language: "detected",
                engine: "tesseract"
              });
            }
          }
        }
        return results;
      }
      /**
       * Parse PaddleOCR results
       */
      parsePaddleOCRResults(paddleResults, confidenceThreshold) {
        const results = [];
        for (const result2 of paddleResults) {
          const [bbox, [text, confidence]] = result2;
          if (confidence >= confidenceThreshold && text.trim()) {
            const xs = bbox.map((point) => point[0]);
            const ys = bbox.map((point) => point[1]);
            const x = Math.min(...xs);
            const y = Math.min(...ys);
            const width = Math.max(...xs) - x;
            const height = Math.max(...ys) - y;
            results.push({
              text: text.trim(),
              confidence,
              boundingBox: {
                x: Math.round(x),
                y: Math.round(y),
                width: Math.round(width),
                height: Math.round(height),
                confidence
              },
              language: "detected",
              engine: "paddleocr"
            });
          }
        }
        return results;
      }
      /**
       * Merge results from multiple OCR engines
       */
      mergeOCRResults(allResults, confidenceThreshold) {
        if (allResults.length === 0) return [];
        const groups = [];
        for (const result2 of allResults) {
          let merged = false;
          for (const group of groups) {
            const representative = group[0];
            const overlap = this.calculateBoundingBoxOverlap(
              result2.boundingBox,
              representative.boundingBox
            );
            if (overlap > 0.5) {
              group.push(result2);
              merged = true;
              break;
            }
          }
          if (!merged) {
            groups.push([result2]);
          }
        }
        const mergedResults = [];
        for (const group of groups) {
          if (group.length === 1) {
            mergedResults.push(group[0]);
          } else {
            const bestResult = group.reduce(
              (best, current) => current.confidence > best.confidence ? current : best
            );
            if (group.length > 1) {
              const textSimilarity = this.calculateTextSimilarity(
                group.map((r) => r.text)
              );
              if (textSimilarity > 0.8) {
                bestResult.confidence = Math.min(0.95, bestResult.confidence * 1.2);
              }
            }
            mergedResults.push(bestResult);
          }
        }
        return mergedResults.filter((r) => r.confidence >= confidenceThreshold);
      }
      /**
       * Calculate bounding box overlap (IoU)
       */
      calculateBoundingBoxOverlap(box1, box2) {
        const x1 = Math.max(box1.x, box2.x);
        const y1 = Math.max(box1.y, box2.y);
        const x2 = Math.min(box1.x + box1.width, box2.x + box2.width);
        const y2 = Math.min(box1.y + box1.height, box2.y + box2.height);
        if (x2 <= x1 || y2 <= y1) return 0;
        const intersectionArea = (x2 - x1) * (y2 - y1);
        const box1Area = box1.width * box1.height;
        const box2Area = box2.width * box2.height;
        const unionArea = box1Area + box2Area - intersectionArea;
        return intersectionArea / unionArea;
      }
      /**
       * Calculate text similarity between multiple texts
       */
      calculateTextSimilarity(texts) {
        if (texts.length < 2) return 1;
        const normalized = texts.map((t) => t.toLowerCase().trim());
        let totalSimilarity = 0;
        let comparisons = 0;
        for (let i = 0; i < normalized.length; i++) {
          for (let j = i + 1; j < normalized.length; j++) {
            const similarity = this.levenshteinSimilarity(
              normalized[i],
              normalized[j]
            );
            totalSimilarity += similarity;
            comparisons++;
          }
        }
        return comparisons > 0 ? totalSimilarity / comparisons : 0;
      }
      /**
       * Calculate Levenshtein similarity
       */
      levenshteinSimilarity(str1, str2) {
        const maxLength = Math.max(str1.length, str2.length);
        if (maxLength === 0) return 1;
        const distance = this.levenshteinDistance(str1, str2);
        return (maxLength - distance) / maxLength;
      }
      /**
       * Calculate Levenshtein distance
       */
      levenshteinDistance(str1, str2) {
        const matrix = [];
        for (let i = 0; i <= str2.length; i++) {
          matrix[i] = [i];
        }
        for (let j = 0; j <= str1.length; j++) {
          matrix[0][j] = j;
        }
        for (let i = 1; i <= str2.length; i++) {
          for (let j = 1; j <= str1.length; j++) {
            if (str2.charAt(i - 1) === str1.charAt(j - 1)) {
              matrix[i][j] = matrix[i - 1][j - 1];
            } else {
              matrix[i][j] = Math.min(
                matrix[i - 1][j - 1] + 1,
                matrix[i][j - 1] + 1,
                matrix[i - 1][j] + 1
              );
            }
          }
        }
        return matrix[str2.length][str1.length];
      }
      /**
       * Analyze text structure for layout understanding
       */
      async analyzeTextStructure(results) {
        results.sort((a, b) => {
          const yDiff = a.boundingBox.y - b.boundingBox.y;
          if (Math.abs(yDiff) < 20) {
            return a.boundingBox.x - b.boundingBox.x;
          }
          return yDiff;
        });
        for (let i = 0; i < results.length; i++) {
          const result2 = results[i];
          const isHeading = this.detectHeading(result2, results);
          const isTableCell = this.detectTableCell(result2, results);
          result2.metadata = {
            ...result2.metadata,
            readingOrder: i,
            isHeading,
            isTableCell,
            structureType: isHeading ? "heading" : isTableCell ? "table" : "paragraph"
          };
        }
      }
      /**
       * Detect if text is likely a heading
       */
      detectHeading(result2, allResults) {
        const height = result2.boundingBox.height;
        const avgHeight = allResults.reduce((sum, r) => sum + r.boundingBox.height, 0) / allResults.length;
        const isLargerText = height > avgHeight * 1.3;
        const isShortText = result2.text.length < 50;
        const isIsolated = !this.hasNearbyText(result2, allResults, 30);
        return isLargerText && isShortText && isIsolated;
      }
      /**
       * Detect if text is part of a table
       */
      detectTableCell(result2, allResults) {
        const threshold = 10;
        const alignedElements = allResults.filter((other) => {
          if (other === result2) return false;
          const sameRow = Math.abs(result2.boundingBox.y - other.boundingBox.y) < threshold;
          const sameColumn = Math.abs(result2.boundingBox.x - other.boundingBox.x) < threshold;
          return sameRow || sameColumn;
        });
        return alignedElements.length >= 2;
      }
      /**
       * Check if text has nearby text elements
       */
      hasNearbyText(result2, allResults, threshold) {
        return allResults.some((other) => {
          if (other === result2) return false;
          const distance = Math.sqrt(
            Math.pow(result2.boundingBox.x - other.boundingBox.x, 2) + Math.pow(result2.boundingBox.y - other.boundingBox.y, 2)
          );
          return distance < threshold;
        });
      }
      /**
       * Map language codes for PaddleOCR
       */
      mapLanguageForPaddle(tesseractLang) {
        const languageMap = {
          eng: "en",
          chi_sim: "ch",
          chi_tra: "chinese_cht",
          jpn: "japan",
          kor: "korean",
          fra: "french",
          deu: "german",
          spa: "spanish",
          rus: "russian",
          ara: "arabic"
        };
        return languageMap[tesseractLang] || "en";
      }
      /**
       * Verify Tesseract installation
       */
      async verifyTesseractInstallation() {
        return new Promise((resolve2, reject) => {
          const tesseract = spawn2("tesseract", ["--version"]);
          tesseract.on("close", (code) => {
            if (code === 0) {
              resolve2();
            } else {
              reject(
                new Error("Tesseract not found. Please install Tesseract OCR.")
              );
            }
          });
          tesseract.on("error", () => {
            reject(new Error("Tesseract not found. Please install Tesseract OCR."));
          });
        });
      }
      /**
       * Verify PaddleOCR installation
       */
      async verifyPaddleOCRInstallation() {
        return new Promise((resolve2, reject) => {
          const python = spawn2(this.config.pythonPath, [
            "-c",
            'import paddleocr; print("OK")'
          ]);
          python.on("close", (code) => {
            if (code === 0) {
              resolve2();
            } else {
              reject(
                new Error(
                  "PaddleOCR not installed. Please install PaddleOCR Python package."
                )
              );
            }
          });
          python.on("error", () => {
            reject(new Error("Python not found or PaddleOCR not installed."));
          });
        });
      }
      /**
       * Check if OCR engine is ready
       */
      isReady() {
        return this.isInitialized;
      }
      /**
       * Cleanup resources
       */
      async shutdown() {
        logger53.info("Shutting down OCR Engine...");
        this.isInitialized = false;
        logger53.info("OCR Engine shutdown complete");
      }
    };
  }
});

// src/ai/engines/ObjectDetectionEngine.ts
import { spawn as spawn3 } from "child_process";
import path44 from "path";
import pino58 from "pino";
var logger54, ObjectDetectionEngine;
var init_ObjectDetectionEngine = __esm({
  "src/ai/engines/ObjectDetectionEngine.ts"() {
    "use strict";
    logger54 = pino58({ name: "ObjectDetectionEngine" });
    ObjectDetectionEngine = class {
      config;
      isInitialized = false;
      availableModels = [];
      constructor(config9) {
        this.config = config9;
      }
      /**
       * Initialize object detection engine
       */
      async initialize() {
        try {
          await this.verifyDependencies();
          await this.loadAvailableModels();
          this.isInitialized = true;
          logger54.info("Object Detection Engine initialized successfully");
        } catch (error) {
          logger54.error("Failed to initialize Object Detection Engine:", error);
          throw error;
        }
      }
      /**
       * Detect objects in image
       */
      async detectObjects(imagePath, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          model = "yolov8n",
          confidenceThreshold = 0.5,
          nmsThreshold = 0.4,
          maxDetections = 100,
          enableTracking = false,
          extractFeatures = false,
          customClasses = []
        } = options2;
        logger54.info(
          `Starting object detection for: ${imagePath} with model: ${model}`
        );
        try {
          if (!this.availableModels.includes(model)) {
            throw new Error(
              `Model ${model} not available. Available models: ${this.availableModels.join(", ")}`
            );
          }
          const detections = await this.runObjectDetection(
            imagePath,
            model,
            confidenceThreshold,
            nmsThreshold,
            maxDetections,
            customClasses
          );
          if (extractFeatures) {
            await this.extractObjectFeatures(detections, imagePath);
          }
          if (enableTracking) {
            this.applyObjectTracking(detections);
          }
          logger54.info(
            `Object detection completed: ${detections.length} objects detected`
          );
          return detections;
        } catch (error) {
          logger54.error("Object detection failed:", error);
          throw error;
        }
      }
      /**
       * Detect objects in video
       */
      async detectObjectsInVideo(videoPath, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          model = "yolov8n",
          confidenceThreshold = 0.5,
          nmsThreshold = 0.4,
          maxDetections = 100,
          frameRate = 1,
          // Process 1 frame per second
          startTime = 0,
          endTime,
          enableTemporalSmoothing = true,
          enableTracking = true
        } = options2;
        logger54.info(`Starting video object detection for: ${videoPath}`);
        try {
          const results = await this.runVideoObjectDetection(
            videoPath,
            model,
            confidenceThreshold,
            nmsThreshold,
            maxDetections,
            frameRate,
            startTime,
            endTime
          );
          if (enableTemporalSmoothing) {
            this.applyTemporalSmoothing(results);
          }
          if (enableTracking) {
            this.applyVideoTracking(results);
          }
          logger54.info(
            `Video object detection completed: ${results.length} frames processed`
          );
          return results;
        } catch (error) {
          logger54.error("Video object detection failed:", error);
          throw error;
        }
      }
      /**
       * Run object detection using YOLO models
       */
      async runObjectDetection(imagePath, model, confidenceThreshold, nmsThreshold, maxDetections, customClasses) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path44.join(
            this.config.modelsPath,
            "yolo_detection.py"
          );
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--model",
            model,
            "--confidence",
            confidenceThreshold.toString(),
            "--nms",
            nmsThreshold.toString(),
            "--max-detections",
            maxDetections.toString()
          ];
          if (customClasses.length > 0) {
            args.push("--classes", customClasses.join(","));
          }
          if (this.config.enableGPU) {
            args.push("--device", "cuda");
          }
          const python = spawn3(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code !== 0) {
              reject(
                new Error(
                  `Object detection failed with code ${code}: ${errorOutput}`
                )
              );
              return;
            }
            try {
              const results = JSON.parse(output);
              const detections = this.parseDetectionResults(results, model);
              resolve2(detections);
            } catch (parseError) {
              reject(parseError);
            }
          });
          python.on("error", (error) => {
            reject(new Error(`Failed to spawn object detection: ${error.message}`));
          });
        });
      }
      /**
       * Run video object detection
       */
      async runVideoObjectDetection(videoPath, model, confidenceThreshold, nmsThreshold, maxDetections, frameRate, startTime, endTime) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path44.join(
            this.config.modelsPath,
            "yolo_video_detection.py"
          );
          const args = [
            pythonScript,
            "--video",
            videoPath,
            "--model",
            model,
            "--confidence",
            confidenceThreshold.toString(),
            "--nms",
            nmsThreshold.toString(),
            "--max-detections",
            maxDetections.toString(),
            "--frame-rate",
            frameRate.toString()
          ];
          if (startTime !== void 0) {
            args.push("--start-time", startTime.toString());
          }
          if (endTime !== void 0) {
            args.push("--end-time", endTime.toString());
          }
          if (this.config.enableGPU) {
            args.push("--device", "cuda");
          }
          const python = spawn3(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code !== 0) {
              reject(
                new Error(
                  `Video object detection failed with code ${code}: ${errorOutput}`
                )
              );
              return;
            }
            try {
              const results = JSON.parse(output);
              const frameResults = this.parseVideoDetectionResults(results, model);
              resolve2(frameResults);
            } catch (parseError) {
              reject(parseError);
            }
          });
          python.on("error", (error) => {
            reject(
              new Error(`Failed to spawn video object detection: ${error.message}`)
            );
          });
        });
      }
      /**
       * Parse detection results from Python script
       */
      parseDetectionResults(results, model) {
        const detections = [];
        for (const detection of results.detections || []) {
          detections.push({
            className: detection.class_name,
            classId: detection.class_id,
            confidence: detection.confidence,
            boundingBox: {
              x: Math.round(detection.bbox[0]),
              y: Math.round(detection.bbox[1]),
              width: Math.round(detection.bbox[2]),
              height: Math.round(detection.bbox[3]),
              confidence: detection.confidence
            },
            model,
            features: detection.features || void 0
          });
        }
        return detections;
      }
      /**
       * Parse video detection results
       */
      parseVideoDetectionResults(results, model) {
        const frameResults = [];
        for (const frameResult of results.frames || []) {
          const detections = this.parseDetectionResults(frameResult, model);
          frameResults.push({
            frame: frameResult.frame_number,
            timestamp: frameResult.timestamp,
            detections
          });
        }
        return frameResults;
      }
      /**
       * Extract visual features from detected objects
       */
      async extractObjectFeatures(detections, imagePath) {
        for (const detection of detections) {
          try {
            const features = await this.runFeatureExtraction(
              imagePath,
              detection.boundingBox
            );
            detection.features = features;
          } catch (error) {
            logger54.warn(
              `Failed to extract features for object ${detection.className}:`,
              error
            );
          }
        }
      }
      /**
       * Run feature extraction for a specific bounding box
       */
      async runFeatureExtraction(imagePath, boundingBox) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path44.join(
            this.config.modelsPath,
            "feature_extraction.py"
          );
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--bbox",
            `${boundingBox.x},${boundingBox.y},${boundingBox.width},${boundingBox.height}`
          ];
          const python = spawn3(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code !== 0) {
              reject(new Error(`Feature extraction failed: ${errorOutput}`));
              return;
            }
            try {
              const result2 = JSON.parse(output);
              resolve2(result2.features || []);
            } catch (parseError) {
              reject(parseError);
            }
          });
          python.on("error", (error) => {
            reject(
              new Error(`Failed to spawn feature extraction: ${error.message}`)
            );
          });
        });
      }
      /**
       * Apply object tracking for single image (placeholder)
       */
      applyObjectTracking(detections) {
        detections.forEach((detection, index) => {
          detection.trackingId = `obj_${index}_${Date.now()}`;
        });
      }
      /**
       * Apply tracking across video frames using DeepSORT or similar
       */
      applyVideoTracking(frameResults) {
        const tracks = /* @__PURE__ */ new Map();
        let nextTrackId = 0;
        for (const frameResult of frameResults) {
          const unmatchedDetections = [...frameResult.detections];
          const activeTrackIds = /* @__PURE__ */ new Set();
          for (const [trackId, trackHistory] of tracks.entries()) {
            const lastDetection = trackHistory[trackHistory.length - 1];
            let bestMatch = null;
            let bestIoU = 0;
            let bestIndex = -1;
            for (let i = 0; i < unmatchedDetections.length; i++) {
              const detection = unmatchedDetections[i];
              if (detection.className === lastDetection.className) {
                const iou = this.calculateIoU(
                  detection.boundingBox,
                  lastDetection.boundingBox
                );
                if (iou > bestIoU && iou > 0.3) {
                  bestMatch = detection;
                  bestIoU = iou;
                  bestIndex = i;
                }
              }
            }
            if (bestMatch) {
              bestMatch.trackingId = trackId;
              trackHistory.push(bestMatch);
              unmatchedDetections.splice(bestIndex, 1);
              activeTrackIds.add(trackId);
            }
          }
          for (const detection of unmatchedDetections) {
            const trackId = `track_${nextTrackId++}`;
            detection.trackingId = trackId;
            tracks.set(trackId, [detection]);
            activeTrackIds.add(trackId);
          }
          for (const [trackId, trackHistory] of tracks.entries()) {
            if (!activeTrackIds.has(trackId)) {
              const lastFrame = trackHistory[trackHistory.length - 1];
              const frameGap = frameResult.frame - this.getFrameNumber(lastFrame);
              if (frameGap > 10) {
                tracks.delete(trackId);
              }
            }
          }
        }
      }
      /**
       * Apply temporal smoothing to reduce noise in detections
       */
      applyTemporalSmoothing(frameResults) {
        const trackGroups = /* @__PURE__ */ new Map();
        for (const frameResult of frameResults) {
          for (const detection of frameResult.detections) {
            if (detection.trackingId) {
              if (!trackGroups.has(detection.trackingId)) {
                trackGroups.set(detection.trackingId, []);
              }
              trackGroups.get(detection.trackingId).push(detection);
            }
          }
        }
        for (const track of trackGroups.values()) {
          if (track.length >= 3) {
            this.smoothTrack(track);
          }
        }
      }
      /**
       * Smooth detection track using moving average
       */
      smoothTrack(track) {
        const windowSize = 3;
        for (let i = windowSize - 1; i < track.length; i++) {
          const window = track.slice(i - windowSize + 1, i + 1);
          const avgConfidence = window.reduce((sum, d) => sum + d.confidence, 0) / window.length;
          track[i].confidence = avgConfidence;
          const avgX = window.reduce((sum, d) => sum + d.boundingBox.x, 0) / window.length;
          const avgY = window.reduce((sum, d) => sum + d.boundingBox.y, 0) / window.length;
          const avgWidth = window.reduce((sum, d) => sum + d.boundingBox.width, 0) / window.length;
          const avgHeight = window.reduce((sum, d) => sum + d.boundingBox.height, 0) / window.length;
          track[i].boundingBox = {
            ...track[i].boundingBox,
            x: Math.round(avgX),
            y: Math.round(avgY),
            width: Math.round(avgWidth),
            height: Math.round(avgHeight)
          };
        }
      }
      /**
       * Calculate Intersection over Union (IoU) for bounding boxes
       */
      calculateIoU(box1, box2) {
        const x1 = Math.max(box1.x, box2.x);
        const y1 = Math.max(box1.y, box2.y);
        const x2 = Math.min(box1.x + box1.width, box2.x + box2.width);
        const y2 = Math.min(box1.y + box1.height, box2.y + box2.height);
        if (x2 <= x1 || y2 <= y1) return 0;
        const intersectionArea = (x2 - x1) * (y2 - y1);
        const box1Area = box1.width * box1.height;
        const box2Area = box2.width * box2.height;
        const unionArea = box1Area + box2Area - intersectionArea;
        return intersectionArea / unionArea;
      }
      /**
       * Extract frame number from detection (helper method)
       */
      getFrameNumber(detection) {
        return 0;
      }
      /**
       * Verify dependencies
       */
      async verifyDependencies() {
        return new Promise((resolve2, reject) => {
          const python = spawn3(this.config.pythonPath, [
            "-c",
            'import ultralytics, cv2, numpy; print("Dependencies OK")'
          ]);
          python.on("close", (code) => {
            if (code === 0) {
              resolve2();
            } else {
              reject(
                new Error(
                  "Required dependencies not found. Please install ultralytics, opencv-python, numpy."
                )
              );
            }
          });
          python.on("error", () => {
            reject(new Error("Python not found or dependencies missing."));
          });
        });
      }
      /**
       * Load available models
       */
      async loadAvailableModels() {
        try {
          const models = [
            "yolov8n",
            "yolov8s",
            "yolov8m",
            "yolov8l",
            "yolov8x",
            "yolov9n",
            "yolov9s",
            "yolov9m",
            "yolov9l",
            "yolov9x",
            "yolo11n",
            "yolo11s",
            "yolo11m",
            "yolo11l",
            "yolo11x"
          ];
          this.availableModels = models;
          logger54.info(`Available models: ${this.availableModels.join(", ")}`);
        } catch (error) {
          logger54.error("Failed to load available models:", error);
          throw error;
        }
      }
      /**
       * Check if object detection engine is ready
       */
      isReady() {
        return this.isInitialized;
      }
      /**
       * Get available models
       */
      getAvailableModels() {
        return [...this.availableModels];
      }
      /**
       * Cleanup resources
       */
      async shutdown() {
        logger54.info("Shutting down Object Detection Engine...");
        this.isInitialized = false;
        logger54.info("Object Detection Engine shutdown complete");
      }
    };
  }
});

// src/ai/engines/SpeechToTextEngine.ts
import { spawn as spawn4 } from "child_process";
import path45 from "path";
import pino59 from "pino";
var logger55, SpeechToTextEngine;
var init_SpeechToTextEngine = __esm({
  "src/ai/engines/SpeechToTextEngine.ts"() {
    "use strict";
    logger55 = pino59({ name: "SpeechToTextEngine" });
    SpeechToTextEngine = class {
      config;
      isInitialized = false;
      availableModels = [];
      constructor(config9) {
        this.config = config9;
      }
      /**
       * Initialize speech-to-text engine
       */
      async initialize() {
        try {
          await this.verifyDependencies();
          await this.loadAvailableModels();
          this.isInitialized = true;
          logger55.info("Speech-to-Text Engine initialized successfully");
        } catch (error) {
          logger55.error("Failed to initialize Speech-to-Text Engine:", error);
          throw error;
        }
      }
      /**
       * Transcribe audio file
       */
      async transcribe(audioPath, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          language = "auto",
          model = "whisper-base",
          enableDiarization = false,
          enhanceAudio = true,
          timestamping = true,
          maxSpeakers = 5,
          chunkDuration = 30,
          enablePunctuation = true,
          filterProfanity = false
        } = options2;
        logger55.info(
          `Starting transcription for: ${audioPath} with model: ${model}`
        );
        try {
          let processedAudioPath = audioPath;
          if (enhanceAudio) {
            processedAudioPath = await this.enhanceAudio(audioPath);
          }
          const segments = await this.runWhisperTranscription(
            processedAudioPath,
            model,
            language,
            timestamping,
            enablePunctuation
          );
          if (enableDiarization && segments.length > 0) {
            await this.applySpeakerDiarization(
              segments,
              processedAudioPath,
              maxSpeakers
            );
          }
          await this.analyzeAudioQuality(segments, processedAudioPath);
          if (filterProfanity) {
            this.filterProfanity(segments);
          }
          this.postProcessTranscription(segments);
          logger55.info(
            `Transcription completed: ${segments.length} segments, ${this.getTotalDuration(segments).toFixed(2)}s`
          );
          return segments;
        } catch (error) {
          logger55.error("Transcription failed:", error);
          throw error;
        }
      }
      /**
       * Transcribe video file (extract audio and transcribe)
       */
      async transcribeVideo(videoPath, options2 = {}) {
        try {
          const audioPath = await this.extractAudioFromVideo(videoPath);
          return await this.transcribe(audioPath, options2);
        } catch (error) {
          logger55.error("Video transcription failed:", error);
          throw error;
        }
      }
      /**
       * Get speaker analysis from transcription
       */
      getSpeakerAnalysis(segments) {
        const speakers = /* @__PURE__ */ new Map();
        for (const segment of segments) {
          const speakerId = segment.speaker || "unknown";
          if (!speakers.has(speakerId)) {
            speakers.set(speakerId, []);
          }
          speakers.get(speakerId).push(segment);
        }
        const speakerInfos = [];
        for (const [speakerId, speakerSegments] of speakers.entries()) {
          const totalDuration = speakerSegments.reduce(
            (sum, seg) => sum + (seg.endTime - seg.startTime),
            0
          );
          const characterCount = speakerSegments.reduce(
            (sum, seg) => sum + seg.text.length,
            0
          );
          const avgConfidence = speakerSegments.reduce((sum, seg) => sum + seg.confidence, 0) / speakerSegments.length;
          speakerInfos.push({
            speakerId,
            segments: speakerSegments,
            totalDuration,
            characterCount,
            confidence: avgConfidence
          });
        }
        return speakerInfos.sort((a, b) => b.totalDuration - a.totalDuration);
      }
      /**
       * Run Whisper transcription
       */
      async runWhisperTranscription(audioPath, model, language, timestamping, enablePunctuation) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path45.join(
            this.config.modelsPath,
            "whisper_transcription.py"
          );
          const args = [
            pythonScript,
            "--audio",
            audioPath,
            "--model",
            model,
            "--output-format",
            "json"
          ];
          if (language !== "auto") {
            args.push("--language", language);
          }
          if (timestamping) {
            args.push("--word-timestamps");
          }
          if (enablePunctuation) {
            args.push("--enable-punctuation");
          }
          if (this.config.enableGPU) {
            args.push("--device", "cuda");
          }
          const python = spawn4(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code !== 0) {
              reject(
                new Error(
                  `Whisper transcription failed with code ${code}: ${errorOutput}`
                )
              );
              return;
            }
            try {
              const result2 = JSON.parse(output);
              const segments = this.parseWhisperOutput(result2);
              resolve2(segments);
            } catch (parseError) {
              reject(parseError);
            }
          });
          python.on("error", (error) => {
            reject(new Error(`Failed to spawn Whisper: ${error.message}`));
          });
        });
      }
      /**
       * Parse Whisper output into transcription segments
       */
      parseWhisperOutput(whisperResult) {
        const segments = [];
        for (const segment of whisperResult.segments || []) {
          const words = [];
          if (segment.words) {
            for (const word of segment.words) {
              words.push({
                word: word.word,
                startTime: word.start,
                endTime: word.end,
                confidence: word.confidence || 0.8
              });
            }
          }
          segments.push({
            text: segment.text.trim(),
            startTime: segment.start,
            endTime: segment.end,
            confidence: segment.confidence || 0.8,
            detectedLanguage: whisperResult.language || "unknown",
            words
          });
        }
        return segments;
      }
      /**
       * Apply speaker diarization using pyannote-audio
       */
      async applySpeakerDiarization(segments, audioPath, maxSpeakers) {
        try {
          const diarizationResult = await this.runSpeakerDiarization(
            audioPath,
            maxSpeakers
          );
          for (const segment of segments) {
            const segmentMidpoint = (segment.startTime + segment.endTime) / 2;
            for (const speakerSegment of diarizationResult) {
              if (segmentMidpoint >= speakerSegment.start && segmentMidpoint <= speakerSegment.end) {
                segment.speaker = speakerSegment.speaker;
                break;
              }
            }
            if (!segment.speaker) {
              let bestOverlap = 0;
              let bestSpeaker = "unknown";
              for (const speakerSegment of diarizationResult) {
                const overlap = this.calculateTemporalOverlap(
                  { startTime: segment.startTime, endTime: segment.endTime },
                  { startTime: speakerSegment.start, endTime: speakerSegment.end }
                );
                if (overlap > bestOverlap) {
                  bestOverlap = overlap;
                  bestSpeaker = speakerSegment.speaker;
                }
              }
              segment.speaker = bestSpeaker;
            }
          }
        } catch (error) {
          logger55.warn(
            "Speaker diarization failed, continuing without speaker labels:",
            error
          );
        }
      }
      /**
       * Run speaker diarization
       */
      async runSpeakerDiarization(audioPath, maxSpeakers) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path45.join(
            this.config.modelsPath,
            "speaker_diarization.py"
          );
          const args = [
            pythonScript,
            "--audio",
            audioPath,
            "--max-speakers",
            maxSpeakers.toString()
          ];
          const python = spawn4(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code !== 0) {
              reject(new Error(`Speaker diarization failed: ${errorOutput}`));
              return;
            }
            try {
              const result2 = JSON.parse(output);
              resolve2(result2.segments || []);
            } catch (parseError) {
              reject(parseError);
            }
          });
          python.on("error", (error) => {
            reject(
              new Error(`Failed to spawn speaker diarization: ${error.message}`)
            );
          });
        });
      }
      /**
       * Enhance audio quality for better transcription
       */
      async enhanceAudio(audioPath) {
        const enhancedPath = path45.join(
          this.config.tempPath,
          `enhanced_${Date.now()}_${path45.basename(audioPath)}`
        );
        return new Promise((resolve2, reject) => {
          const ffmpeg3 = spawn4("ffmpeg", [
            "-i",
            audioPath,
            "-af",
            "highpass=f=80,lowpass=f=8000,volume=1.5,dynaudnorm",
            // Audio filters
            "-ar",
            "16000",
            // Sample rate
            "-ac",
            "1",
            // Mono
            "-y",
            // Overwrite output
            enhancedPath
          ]);
          ffmpeg3.on("close", (code) => {
            if (code === 0) {
              resolve2(enhancedPath);
            } else {
              logger55.warn("Audio enhancement failed, using original audio");
              resolve2(audioPath);
            }
          });
          ffmpeg3.on("error", () => {
            logger55.warn("FFmpeg not available, using original audio");
            resolve2(audioPath);
          });
        });
      }
      /**
       * Extract audio from video file
       */
      async extractAudioFromVideo(videoPath) {
        const audioPath = path45.join(
          this.config.tempPath,
          `extracted_${Date.now()}.wav`
        );
        return new Promise((resolve2, reject) => {
          const ffmpeg3 = spawn4("ffmpeg", [
            "-i",
            videoPath,
            "-vn",
            // No video
            "-acodec",
            "pcm_s16le",
            // Audio codec
            "-ar",
            "16000",
            // Sample rate
            "-ac",
            "1",
            // Mono
            "-y",
            // Overwrite output
            audioPath
          ]);
          ffmpeg3.on("close", (code) => {
            if (code === 0) {
              resolve2(audioPath);
            } else {
              reject(new Error("Failed to extract audio from video"));
            }
          });
          ffmpeg3.on("error", (error) => {
            reject(new Error(`FFmpeg error: ${error.message}`));
          });
        });
      }
      /**
       * Analyze audio quality metrics
       */
      async analyzeAudioQuality(segments, audioPath) {
        try {
          const qualityMetrics = await this.runAudioQualityAnalysis(audioPath);
          for (const segment of segments) {
            segment.audioQuality = qualityMetrics.snr;
            segment.noiseLevel = qualityMetrics.noiseLevel;
          }
        } catch (error) {
          logger55.warn("Audio quality analysis failed:", error);
        }
      }
      /**
       * Run audio quality analysis
       */
      async runAudioQualityAnalysis(audioPath) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path45.join(
            this.config.modelsPath,
            "audio_quality.py"
          );
          const args = [pythonScript, "--audio", audioPath];
          const python = spawn4(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Audio quality analysis failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Filter profanity from transcription
       */
      filterProfanity(segments) {
        const profanityWords = [
          // Add profanity words to filter - using placeholders here
          "profanity1",
          "profanity2",
          "profanity3"
        ];
        for (const segment of segments) {
          let filteredText = segment.text;
          for (const word of profanityWords) {
            const regex = new RegExp(`\\b${word}\\b`, "gi");
            filteredText = filteredText.replace(regex, "*".repeat(word.length));
          }
          segment.text = filteredText;
          if (segment.words) {
            for (const wordTimestamp of segment.words) {
              for (const profanityWord of profanityWords) {
                if (wordTimestamp.word.toLowerCase() === profanityWord.toLowerCase()) {
                  wordTimestamp.word = "*".repeat(profanityWord.length);
                }
              }
            }
          }
        }
      }
      /**
       * Post-process transcription for better readability
       */
      postProcessTranscription(segments) {
        for (const segment of segments) {
          segment.text = segment.text.replace(/\s+/g, " ").replace(/([.!?])\s*([a-z])/g, "$1 $2").trim();
          segment.text = segment.text.replace(
            /(^|\. )([a-z])/g,
            (match, prefix, letter) => prefix + letter.toUpperCase()
          );
        }
      }
      /**
       * Calculate temporal overlap between two time ranges
       */
      calculateTemporalOverlap(range1, range2) {
        const start = Math.max(range1.startTime, range2.startTime);
        const end = Math.min(range1.endTime, range2.endTime);
        if (end <= start) return 0;
        const intersectionDuration = end - start;
        const range1Duration = range1.endTime - range1.startTime;
        const range2Duration = range2.endTime - range2.startTime;
        const unionDuration = range1Duration + range2Duration - intersectionDuration;
        return intersectionDuration / unionDuration;
      }
      /**
       * Get total duration of all segments
       */
      getTotalDuration(segments) {
        if (segments.length === 0) return 0;
        const firstStart = Math.min(...segments.map((s) => s.startTime));
        const lastEnd = Math.max(...segments.map((s) => s.endTime));
        return lastEnd - firstStart;
      }
      /**
       * Verify dependencies
       */
      async verifyDependencies() {
        return new Promise((resolve2, reject) => {
          const python = spawn4(this.config.pythonPath, [
            "-c",
            'import whisper, pyannote.audio; print("Dependencies OK")'
          ]);
          python.on("close", (code) => {
            if (code === 0) {
              resolve2();
            } else {
              reject(
                new Error(
                  "Required dependencies not found. Please install whisper and pyannote.audio."
                )
              );
            }
          });
          python.on("error", () => {
            reject(new Error("Python not found or dependencies missing."));
          });
        });
      }
      /**
       * Load available models
       */
      async loadAvailableModels() {
        try {
          const models = [
            "whisper-tiny",
            "whisper-base",
            "whisper-small",
            "whisper-medium",
            "whisper-large",
            "whisper-large-v2",
            "whisper-large-v3"
          ];
          this.availableModels = models;
          logger55.info(`Available models: ${this.availableModels.join(", ")}`);
        } catch (error) {
          logger55.error("Failed to load available models:", error);
          throw error;
        }
      }
      /**
       * Check if speech-to-text engine is ready
       */
      isReady() {
        return this.isInitialized;
      }
      /**
       * Get available models
       */
      getAvailableModels() {
        return [...this.availableModels];
      }
      /**
       * Cleanup resources
       */
      async shutdown() {
        logger55.info("Shutting down Speech-to-Text Engine...");
        this.isInitialized = false;
        logger55.info("Speech-to-Text Engine shutdown complete");
      }
    };
  }
});

// src/ai/engines/FaceDetectionEngine.ts
import { spawn as spawn5 } from "child_process";
import path46 from "path";
import pino60 from "pino";
var logger56, FaceDetectionEngine;
var init_FaceDetectionEngine = __esm({
  "src/ai/engines/FaceDetectionEngine.ts"() {
    "use strict";
    logger56 = pino60({ name: "FaceDetectionEngine" });
    FaceDetectionEngine = class {
      config;
      isInitialized = false;
      identityDatabase = /* @__PURE__ */ new Map();
      constructor(config9) {
        this.config = config9;
      }
      /**
       * Initialize face detection engine
       */
      async initialize() {
        try {
          await this.verifyDependencies();
          await this.loadModels();
          await this.initializeIdentityDatabase();
          this.isInitialized = true;
          logger56.info("Face Detection Engine initialized successfully");
        } catch (error) {
          logger56.error("Failed to initialize Face Detection Engine:", error);
          throw error;
        }
      }
      /**
       * Detect faces in image
       */
      async detectFaces(imagePath, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          minFaceSize = 20,
          confidenceThreshold = 0.7,
          extractFeatures = true,
          recognizeIdentities = false,
          analyzeEmotions = true,
          estimateAge = true,
          estimateGender = true,
          enableQualityCheck = true
        } = options2;
        logger56.info(`Starting face detection for: ${imagePath}`);
        try {
          const detections = await this.runFaceDetection(
            imagePath,
            minFaceSize,
            confidenceThreshold
          );
          for (const detection of detections) {
            await this.extractLandmarks(detection, imagePath);
            if (enableQualityCheck) {
              detection.qualityScore = this.calculateFaceQuality(detection);
            }
            if (extractFeatures || recognizeIdentities) {
              detection.featureVector = await this.extractFaceFeatures(
                imagePath,
                detection.boundingBox
              );
            }
            if (recognizeIdentities && detection.featureVector) {
              const identity = await this.recognizeIdentity(
                detection.featureVector
              );
              if (identity) {
                detection.recognizedIdentity = identity.name;
                detection.identityConfidence = identity.confidence;
              }
            }
            if (analyzeEmotions) {
              detection.emotions = await this.analyzeEmotions(
                imagePath,
                detection.boundingBox
              );
              detection.dominantEmotion = this.getDominantEmotion(
                detection.emotions
              );
            }
            if (estimateAge) {
              detection.estimatedAge = await this.estimateAge(
                imagePath,
                detection.boundingBox
              );
            }
            if (estimateGender) {
              detection.estimatedGender = await this.estimateGender(
                imagePath,
                detection.boundingBox
              );
            }
          }
          const qualifiedDetections = enableQualityCheck ? detections.filter((d) => d.qualityScore && d.qualityScore > 0.5) : detections;
          logger56.info(
            `Face detection completed: ${qualifiedDetections.length} faces detected`
          );
          return qualifiedDetections;
        } catch (error) {
          logger56.error("Face detection failed:", error);
          throw error;
        }
      }
      /**
       * Detect faces in video
       */
      async detectFacesInVideo(videoPath, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          frameRate = 1,
          startTime = 0,
          endTime,
          enableTemporalSmoothing = true,
          trackAcrossFrames = true,
          ...faceOptions
        } = options2;
        logger56.info(`Starting video face detection for: ${videoPath}`);
        try {
          const results = await this.runVideoFaceDetection(
            videoPath,
            frameRate,
            startTime,
            endTime,
            faceOptions
          );
          if (trackAcrossFrames) {
            this.applyVideoFaceTracking(results);
          }
          if (enableTemporalSmoothing) {
            this.applyTemporalSmoothing(results);
          }
          logger56.info(
            `Video face detection completed: ${results.length} frames processed`
          );
          return results;
        } catch (error) {
          logger56.error("Video face detection failed:", error);
          throw error;
        }
      }
      /**
       * Run primary face detection using MTCNN
       */
      async runFaceDetection(imagePath, minFaceSize, confidenceThreshold) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path46.join(
            this.config.modelsPath,
            "mtcnn_detection.py"
          );
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--min-face-size",
            minFaceSize.toString(),
            "--confidence",
            confidenceThreshold.toString()
          ];
          if (this.config.enableGPU) {
            args.push("--device", "cuda");
          }
          const python = spawn5(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code !== 0) {
              reject(
                new Error(
                  `Face detection failed with code ${code}: ${errorOutput}`
                )
              );
              return;
            }
            try {
              const results = JSON.parse(output);
              const detections = this.parseFaceDetectionResults(results);
              resolve2(detections);
            } catch (parseError) {
              reject(parseError);
            }
          });
          python.on("error", (error) => {
            reject(new Error(`Failed to spawn face detection: ${error.message}`));
          });
        });
      }
      /**
       * Run video face detection
       */
      async runVideoFaceDetection(videoPath, frameRate, startTime, endTime, faceOptions) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path46.join(
            this.config.modelsPath,
            "video_face_detection.py"
          );
          const args = [
            pythonScript,
            "--video",
            videoPath,
            "--frame-rate",
            frameRate.toString(),
            "--start-time",
            startTime.toString(),
            "--min-face-size",
            (faceOptions.minFaceSize || 20).toString(),
            "--confidence",
            (faceOptions.confidenceThreshold || 0.7).toString()
          ];
          if (endTime !== void 0) {
            args.push("--end-time", endTime.toString());
          }
          if (this.config.enableGPU) {
            args.push("--device", "cuda");
          }
          const python = spawn5(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code !== 0) {
              reject(new Error(`Video face detection failed: ${errorOutput}`));
              return;
            }
            try {
              const results = JSON.parse(output);
              const frameResults = this.parseVideoFaceDetectionResults(results);
              resolve2(frameResults);
            } catch (parseError) {
              reject(parseError);
            }
          });
          python.on("error", (error) => {
            reject(
              new Error(`Failed to spawn video face detection: ${error.message}`)
            );
          });
        });
      }
      /**
       * Extract facial landmarks
       */
      async extractLandmarks(detection, imagePath) {
        try {
          const landmarks = await this.runLandmarkExtraction(
            imagePath,
            detection.boundingBox
          );
          detection.landmarks = landmarks;
        } catch (error) {
          logger56.warn("Landmark extraction failed:", error);
          detection.landmarks = this.getDefaultLandmarks(detection.boundingBox);
        }
      }
      /**
       * Run landmark extraction
       */
      async runLandmarkExtraction(imagePath, boundingBox) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path46.join(
            this.config.modelsPath,
            "face_landmarks.py"
          );
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--bbox",
            `${boundingBox.x},${boundingBox.y},${boundingBox.width},${boundingBox.height}`
          ];
          const python = spawn5(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.landmarks);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Landmark extraction failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Extract face features for recognition
       */
      async extractFaceFeatures(imagePath, boundingBox) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path46.join(
            this.config.modelsPath,
            "face_features.py"
          );
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--bbox",
            `${boundingBox.x},${boundingBox.y},${boundingBox.width},${boundingBox.height}`
          ];
          const python = spawn5(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.features || []);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Feature extraction failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Analyze emotions
       */
      async analyzeEmotions(imagePath, boundingBox) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path46.join(
            this.config.modelsPath,
            "emotion_analysis.py"
          );
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--bbox",
            `${boundingBox.x},${boundingBox.y},${boundingBox.width},${boundingBox.height}`
          ];
          const python = spawn5(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.emotions || this.getDefaultEmotions());
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              logger56.warn("Emotion analysis failed, using defaults");
              resolve2(this.getDefaultEmotions());
            }
          });
          python.on("error", (error) => {
            logger56.warn("Emotion analysis failed, using defaults");
            resolve2(this.getDefaultEmotions());
          });
        });
      }
      /**
       * Estimate age
       */
      async estimateAge(imagePath, boundingBox) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path46.join(
            this.config.modelsPath,
            "age_estimation.py"
          );
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--bbox",
            `${boundingBox.x},${boundingBox.y},${boundingBox.width},${boundingBox.height}`
          ];
          const python = spawn5(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.age || 30);
              } catch (parseError) {
                resolve2(30);
              }
            } else {
              resolve2(30);
            }
          });
          python.on("error", () => {
            resolve2(30);
          });
        });
      }
      /**
       * Estimate gender
       */
      async estimateGender(imagePath, boundingBox) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path46.join(
            this.config.modelsPath,
            "gender_estimation.py"
          );
          const args = [
            pythonScript,
            "--image",
            imagePath,
            "--bbox",
            `${boundingBox.x},${boundingBox.y},${boundingBox.width},${boundingBox.height}`
          ];
          const python = spawn5(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.gender || "unknown");
              } catch (parseError) {
                resolve2("unknown");
              }
            } else {
              resolve2("unknown");
            }
          });
          python.on("error", () => {
            resolve2("unknown");
          });
        });
      }
      /**
       * Parse face detection results
       */
      parseFaceDetectionResults(results) {
        const detections = [];
        for (const face of results.faces || []) {
          detections.push({
            boundingBox: {
              x: Math.round(face.bbox[0]),
              y: Math.round(face.bbox[1]),
              width: Math.round(face.bbox[2]),
              height: Math.round(face.bbox[3]),
              confidence: face.confidence
            },
            confidence: face.confidence,
            landmarks: this.getDefaultLandmarks({
              x: face.bbox[0],
              y: face.bbox[1],
              width: face.bbox[2],
              height: face.bbox[3]
            })
          });
        }
        return detections;
      }
      /**
       * Parse video face detection results
       */
      parseVideoFaceDetectionResults(results) {
        const frameResults = [];
        for (const frameResult of results.frames || []) {
          const faces = this.parseFaceDetectionResults(frameResult);
          frameResults.push({
            frame: frameResult.frame_number,
            timestamp: frameResult.timestamp,
            faces
          });
        }
        return frameResults;
      }
      /**
       * Apply face tracking across video frames
       */
      applyVideoFaceTracking(frameResults) {
        const tracks = /* @__PURE__ */ new Map();
        let nextTrackId = 0;
        for (const frameResult of frameResults) {
          const unmatchedFaces = [...frameResult.faces];
          const activeTrackIds = /* @__PURE__ */ new Set();
          for (const [trackId, trackHistory] of tracks.entries()) {
            const lastFace = trackHistory[trackHistory.length - 1];
            let bestMatch = null;
            let bestSimilarity = 0;
            let bestIndex = -1;
            for (let i = 0; i < unmatchedFaces.length; i++) {
              const face = unmatchedFaces[i];
              const boxSimilarity = this.calculateBoundingBoxSimilarity(
                face.boundingBox,
                lastFace.boundingBox
              );
              let featureSimilarity = 0;
              if (face.featureVector && lastFace.featureVector) {
                featureSimilarity = this.calculateFeatureSimilarity(
                  face.featureVector,
                  lastFace.featureVector
                );
              }
              const overallSimilarity = (boxSimilarity + featureSimilarity) / 2;
              if (overallSimilarity > bestSimilarity && overallSimilarity > 0.5) {
                bestMatch = face;
                bestSimilarity = overallSimilarity;
                bestIndex = i;
              }
            }
            if (bestMatch) {
              trackHistory.push(bestMatch);
              unmatchedFaces.splice(bestIndex, 1);
              activeTrackIds.add(trackId);
            }
          }
          for (const face of unmatchedFaces) {
            const trackId = `face_track_${nextTrackId++}`;
            tracks.set(trackId, [face]);
            activeTrackIds.add(trackId);
          }
          for (const [trackId, trackHistory] of tracks.entries()) {
            if (!activeTrackIds.has(trackId)) {
              const frameGap = frameResult.frame - this.getLastFrameNumber(trackHistory);
              if (frameGap > 5) {
                tracks.delete(trackId);
              }
            }
          }
        }
      }
      /**
       * Apply temporal smoothing to face attributes
       */
      applyTemporalSmoothing(frameResults) {
        logger56.debug("Temporal smoothing applied to face detection results");
      }
      /**
       * Calculate face quality score
       */
      calculateFaceQuality(detection) {
        let qualityScore = detection.confidence;
        const faceSize = detection.boundingBox.width * detection.boundingBox.height;
        const sizeScore = Math.min(faceSize / (100 * 100), 1);
        let landmarkScore = 1;
        if (detection.landmarks) {
          const landmarks = detection.landmarks;
          landmarkScore = Object.values(landmarks).reduce((sum, conf) => {
            return sum + (typeof conf === "number" ? conf : conf.confidence || 0);
          }, 0) / Object.keys(landmarks).length;
        }
        qualityScore = (qualityScore + sizeScore + landmarkScore) / 3;
        return Math.max(0, Math.min(1, qualityScore));
      }
      /**
       * Recognize identity using feature matching
       */
      async recognizeIdentity(faceVector, threshold = 0.6) {
        let bestMatch = null;
        let bestSimilarity = 0;
        for (const [name, knownVector] of this.identityDatabase.entries()) {
          const similarity = this.calculateFeatureSimilarity(
            faceVector,
            knownVector
          );
          if (similarity > bestSimilarity && similarity > threshold) {
            bestSimilarity = similarity;
            bestMatch = { name, confidence: similarity };
          }
        }
        return bestMatch;
      }
      /**
       * Calculate feature similarity using cosine similarity
       */
      calculateFeatureSimilarity(vector1, vector2) {
        if (vector1.length !== vector2.length) return 0;
        let dotProduct = 0;
        let norm1 = 0;
        let norm2 = 0;
        for (let i = 0; i < vector1.length; i++) {
          dotProduct += vector1[i] * vector2[i];
          norm1 += vector1[i] * vector1[i];
          norm2 += vector2[i] * vector2[i];
        }
        return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
      }
      /**
       * Calculate bounding box similarity
       */
      calculateBoundingBoxSimilarity(box1, box2) {
        const x1 = Math.max(box1.x, box2.x);
        const y1 = Math.max(box1.y, box2.y);
        const x2 = Math.min(box1.x + box1.width, box2.x + box2.width);
        const y2 = Math.min(box1.y + box1.height, box2.y + box2.height);
        if (x2 <= x1 || y2 <= y1) return 0;
        const intersectionArea = (x2 - x1) * (y2 - y1);
        const box1Area = box1.width * box1.height;
        const box2Area = box2.width * box2.height;
        const unionArea = box1Area + box2Area - intersectionArea;
        return intersectionArea / unionArea;
      }
      /**
       * Get dominant emotion from emotion scores
       */
      getDominantEmotion(emotions) {
        const entries = Object.entries(emotions);
        const dominant = entries.reduce(
          (max, [emotion, score]) => score > max.score ? { emotion, score } : max,
          { emotion: "neutral", score: 0 }
        );
        return dominant.emotion;
      }
      /**
       * Get default landmarks based on bounding box
       */
      getDefaultLandmarks(boundingBox) {
        const centerX = boundingBox.x + boundingBox.width / 2;
        const centerY = boundingBox.y + boundingBox.height / 2;
        return {
          leftEye: {
            x: centerX - boundingBox.width * 0.2,
            y: centerY - boundingBox.height * 0.1
          },
          rightEye: {
            x: centerX + boundingBox.width * 0.2,
            y: centerY - boundingBox.height * 0.1
          },
          nose: { x: centerX, y: centerY },
          leftMouth: {
            x: centerX - boundingBox.width * 0.15,
            y: centerY + boundingBox.height * 0.2
          },
          rightMouth: {
            x: centerX + boundingBox.width * 0.15,
            y: centerY + boundingBox.height * 0.2
          }
        };
      }
      /**
       * Get default emotion scores
       */
      getDefaultEmotions() {
        return {
          happy: 0.1,
          sad: 0.1,
          angry: 0.1,
          surprised: 0.1,
          fearful: 0.1,
          disgusted: 0.1,
          neutral: 0.4
        };
      }
      /**
       * Get last frame number from track history
       */
      getLastFrameNumber(trackHistory) {
        return 0;
      }
      /**
       * Identity database methods
       */
      async addIdentity(name, faceVector) {
        this.identityDatabase.set(name, faceVector);
        logger56.info(`Added identity: ${name}`);
      }
      async removeIdentity(name) {
        this.identityDatabase.delete(name);
        logger56.info(`Removed identity: ${name}`);
      }
      async listIdentities() {
        return Array.from(this.identityDatabase.keys());
      }
      /**
       * Verify dependencies
       */
      async verifyDependencies() {
        return new Promise((resolve2, reject) => {
          const python = spawn5(this.config.pythonPath, [
            "-c",
            'import mtcnn, facenet_pytorch, cv2; print("Dependencies OK")'
          ]);
          python.on("close", (code) => {
            if (code === 0) {
              resolve2();
            } else {
              reject(
                new Error(
                  "Required dependencies not found. Please install mtcnn, facenet-pytorch, opencv-python."
                )
              );
            }
          });
          python.on("error", () => {
            reject(new Error("Python not found or dependencies missing."));
          });
        });
      }
      /**
       * Load pre-trained models
       */
      async loadModels() {
        logger56.info("Face detection models loaded");
      }
      /**
       * Initialize identity database
       */
      async initializeIdentityDatabase() {
        logger56.info("Identity database initialized");
      }
      /**
       * Check if face detection engine is ready
       */
      isReady() {
        return this.isInitialized;
      }
      /**
       * Cleanup resources
       */
      async shutdown() {
        logger56.info("Shutting down Face Detection Engine...");
        this.isInitialized = false;
        logger56.info("Face Detection Engine shutdown complete");
      }
    };
  }
});

// src/ai/engines/TextAnalysisEngine.ts
import { spawn as spawn6 } from "child_process";
import path47 from "path";
import pino61 from "pino";
var logger57, TextAnalysisEngine;
var init_TextAnalysisEngine = __esm({
  "src/ai/engines/TextAnalysisEngine.ts"() {
    "use strict";
    logger57 = pino61({ name: "TextAnalysisEngine" });
    TextAnalysisEngine = class {
      config;
      isInitialized = false;
      models = /* @__PURE__ */ new Map();
      constructor(config9) {
        this.config = config9;
      }
      /**
       * Initialize text analysis engine
       */
      async initialize() {
        try {
          await this.verifyDependencies();
          await this.loadLanguageModels();
          this.isInitialized = true;
          logger57.info("Text Analysis Engine initialized successfully");
        } catch (error) {
          logger57.error("Failed to initialize Text Analysis Engine:", error);
          throw error;
        }
      }
      /**
       * Analyze text using multiple NLP techniques
       */
      async analyzeText(text, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          extractEntities = true,
          performSentiment = true,
          extractTopics = false,
          detectLanguage = true,
          extractKeyPhrases = true,
          generateSummary = false,
          analyzeReadability = true,
          detectIntentions = false,
          enableCoreference = false,
          enableDependencyParsing = false
        } = options2;
        logger57.info(`Starting text analysis for ${text.length} characters`);
        try {
          const statistics = this.calculateTextStatistics(text);
          let language = { language: "en", confidence: 1 };
          if (detectLanguage) {
            language = await this.detectLanguage(text);
          }
          const preprocessedText = this.preprocessText(text, language.language);
          let entities = [];
          if (extractEntities) {
            entities = await this.extractNamedEntities(
              preprocessedText,
              language.language
            );
          }
          let sentiment = null;
          if (performSentiment) {
            sentiment = await this.analyzeSentiment(
              preprocessedText,
              language.language
            );
          }
          let topics = [];
          if (extractTopics) {
            topics = await this.extractTopics(preprocessedText, language.language);
          }
          let keyPhrases = [];
          if (extractKeyPhrases) {
            keyPhrases = await this.extractKeyPhrases(
              preprocessedText,
              language.language
            );
          }
          let summary = null;
          if (generateSummary && text.length > 500) {
            summary = await this.generateSummary(
              preprocessedText,
              language.language
            );
          }
          let readabilityScore;
          if (analyzeReadability) {
            readabilityScore = this.calculateReadabilityScore(text, statistics);
          }
          let intentions;
          if (detectIntentions) {
            intentions = await this.detectIntentions(
              preprocessedText,
              language.language
            );
          }
          if (enableCoreference) {
            await this.resolveCoreferences(entities, text);
          }
          const result2 = {
            text: preprocessedText,
            language,
            entities,
            sentiment,
            topics,
            keyPhrases,
            summary,
            readabilityScore,
            intentions,
            statistics
          };
          logger57.info(
            `Text analysis completed: ${entities.length} entities, sentiment: ${sentiment?.label || "N/A"}`
          );
          return result2;
        } catch (error) {
          logger57.error("Text analysis failed:", error);
          throw error;
        }
      }
      /**
       * Detect language of text
       */
      async detectLanguage(text) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path47.join(
            this.config.modelsPath,
            "language_detection.py"
          );
          const args = [
            pythonScript,
            "--text",
            text.substring(0, 1e3)
            // Use first 1000 chars for detection
          ];
          const python = spawn6(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2({
                  language: result2.language || "en",
                  confidence: result2.confidence || 0.8,
                  alternateLanguages: result2.alternatives || []
                });
              } catch (parseError) {
                resolve2({ language: "en", confidence: 0.8 });
              }
            } else {
              resolve2({ language: "en", confidence: 0.8 });
            }
          });
          python.on("error", () => {
            resolve2({ language: "en", confidence: 0.8 });
          });
        });
      }
      /**
       * Extract named entities
       */
      async extractNamedEntities(text, language) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path47.join(
            this.config.modelsPath,
            "named_entity_recognition.py"
          );
          const args = [
            pythonScript,
            "--text",
            text,
            "--language",
            language,
            "--model",
            this.getModelForLanguage(language, "ner")
          ];
          const python = spawn6(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                const entities = this.parseNamedEntities(result2.entities || []);
                resolve2(entities);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`NER failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Analyze sentiment
       */
      async analyzeSentiment(text, language) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path47.join(
            this.config.modelsPath,
            "sentiment_analysis.py"
          );
          const args = [
            pythonScript,
            "--text",
            text,
            "--language",
            language,
            "--enable-aspects"
          ];
          const python = spawn6(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2({
                  label: result2.sentiment || "neutral",
                  score: result2.score || 0,
                  confidence: result2.confidence || 0.8,
                  aspects: result2.aspects || []
                });
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Sentiment analysis failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Extract topics using LDA or similar
       */
      async extractTopics(text, language) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path47.join(
            this.config.modelsPath,
            "topic_modeling.py"
          );
          const args = [
            pythonScript,
            "--text",
            text,
            "--language",
            language,
            "--num-topics",
            "5",
            "--algorithm",
            "lda"
          ];
          const python = spawn6(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                const topics = this.parseTopics(result2.topics || []);
                resolve2(topics);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              logger57.warn("Topic modeling failed, returning empty topics");
              resolve2([]);
            }
          });
          python.on("error", () => {
            resolve2([]);
          });
        });
      }
      /**
       * Extract key phrases
       */
      async extractKeyPhrases(text, language) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path47.join(
            this.config.modelsPath,
            "keyphrase_extraction.py"
          );
          const args = [
            pythonScript,
            "--text",
            text,
            "--language",
            language,
            "--algorithm",
            "textrank"
          ];
          const python = spawn6(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                const keyPhrases = this.parseKeyPhrases(
                  result2.keyphrases || [],
                  text
                );
                resolve2(keyPhrases);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              logger57.warn("Key phrase extraction failed, returning empty phrases");
              resolve2([]);
            }
          });
          python.on("error", () => {
            resolve2([]);
          });
        });
      }
      /**
       * Generate text summary
       */
      async generateSummary(text, language) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path47.join(
            this.config.modelsPath,
            "text_summarization.py"
          );
          const args = [
            pythonScript,
            "--text",
            text,
            "--language",
            language,
            "--extractive-sentences",
            "3",
            "--enable-abstractive"
          ];
          const python = spawn6(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2({
                  extractive: result2.extractive_summary || [],
                  abstractive: result2.abstractive_summary,
                  keyPoints: result2.key_points || [],
                  compressionRatio: (result2.summary_length || text.length) / text.length
                });
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Summarization failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Detect intentions in text
       */
      async detectIntentions(text, language) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path47.join(
            this.config.modelsPath,
            "intention_detection.py"
          );
          const args = [pythonScript, "--text", text, "--language", language];
          const python = spawn6(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.intentions || []);
              } catch (parseError) {
                resolve2([]);
              }
            } else {
              resolve2([]);
            }
          });
          python.on("error", () => {
            resolve2([]);
          });
        });
      }
      /**
       * Preprocess text for analysis
       */
      preprocessText(text, language) {
        let processed = text.replace(/\r\n/g, "\n").replace(/\s+/g, " ").trim();
        if (language === "en") {
          processed = processed.replace(/n't/g, " not").replace(/'ll/g, " will").replace(/'re/g, " are").replace(/'ve/g, " have").replace(/'d/g, " would");
        }
        return processed;
      }
      /**
       * Calculate text statistics
       */
      calculateTextStatistics(text) {
        const characterCount = text.length;
        const words = text.toLowerCase().match(/\b\w+\b/g) || [];
        const wordCount = words.length;
        const sentences = text.split(/[.!?]+/).filter((s) => s.trim().length > 0);
        const sentenceCount = sentences.length;
        const paragraphs = text.split(/\n\s*\n/).filter((p) => p.trim().length > 0);
        const paragraphCount = paragraphs.length;
        const averageWordsPerSentence = sentenceCount > 0 ? wordCount / sentenceCount : 0;
        const averageSentencesPerParagraph = paragraphCount > 0 ? sentenceCount / paragraphCount : 0;
        const uniqueWords = new Set(words);
        const vocabularyDiversity = wordCount > 0 ? uniqueWords.size / wordCount : 0;
        const avgWordLength = words.reduce((sum, word) => sum + word.length, 0) / (wordCount || 1) || 0;
        const complexityScore = averageWordsPerSentence * 0.4 + avgWordLength * 0.3 + (1 - vocabularyDiversity) * 0.3;
        return {
          characterCount,
          wordCount,
          sentenceCount,
          paragraphCount,
          averageWordsPerSentence,
          averageSentencesPerParagraph,
          vocabularyDiversity,
          complexityScore
        };
      }
      /**
       * Calculate readability score (Flesch Reading Ease)
       */
      calculateReadabilityScore(text, statistics) {
        const { wordCount, sentenceCount } = statistics;
        if (sentenceCount === 0 || wordCount === 0) return 0;
        const syllableCount = this.countSyllables(text);
        const score = 206.835 - 1.015 * (wordCount / sentenceCount) - 84.6 * (syllableCount / wordCount);
        return Math.max(0, Math.min(100, score));
      }
      /**
       * Count syllables in text (simplified)
       */
      countSyllables(text) {
        const words = text.toLowerCase().match(/\b\w+\b/g) || [];
        let syllableCount = 0;
        for (const word of words) {
          const vowelGroups = word.match(/[aeiouy]+/g) || [];
          let syllables = vowelGroups.length;
          if (word.endsWith("e") && syllables > 1) syllables--;
          if (syllables === 0) syllables = 1;
          syllableCount += syllables;
        }
        return syllableCount;
      }
      /**
       * Resolve coreferences in entities
       */
      async resolveCoreferences(entities, text) {
        logger57.debug("Coreference resolution applied");
      }
      /**
       * Parse named entities from Python output
       */
      parseNamedEntities(entities) {
        return entities.map((entity) => ({
          text: entity.text,
          label: entity.label,
          start: entity.start,
          end: entity.end,
          confidence: entity.confidence || 0.8,
          description: entity.description,
          canonicalForm: entity.canonical_form,
          entityId: entity.entity_id
        }));
      }
      /**
       * Parse topics from Python output
       */
      parseTopics(topics) {
        return topics.map((topic, index) => ({
          id: `topic_${index}`,
          keywords: topic.keywords || [],
          coherence: topic.coherence || 0.5,
          documents: topic.documents,
          representative_text: topic.representative_text
        }));
      }
      /**
       * Parse key phrases from Python output
       */
      parseKeyPhrases(phrases, text) {
        return phrases.map((phrase) => {
          const positions = [];
          let index = text.toLowerCase().indexOf(phrase.phrase.toLowerCase());
          while (index !== -1) {
            positions.push(index);
            index = text.toLowerCase().indexOf(phrase.phrase.toLowerCase(), index + 1);
          }
          return {
            phrase: phrase.phrase,
            relevance: phrase.relevance || 0.5,
            frequency: positions.length,
            positions
          };
        });
      }
      /**
       * Get appropriate model for language and task
       */
      getModelForLanguage(language, task) {
        const modelMap = {
          en: {
            ner: "en_core_web_lg",
            sentiment: "en_core_web_lg"
          },
          es: {
            ner: "es_core_news_lg",
            sentiment: "es_core_news_lg"
          },
          fr: {
            ner: "fr_core_news_lg",
            sentiment: "fr_core_news_lg"
          },
          de: {
            ner: "de_core_news_lg",
            sentiment: "de_core_news_lg"
          }
        };
        return modelMap[language]?.[task] || modelMap["en"][task] || "en_core_web_lg";
      }
      /**
       * Verify dependencies
       */
      async verifyDependencies() {
        return new Promise((resolve2, reject) => {
          const python = spawn6(this.config.pythonPath, [
            "-c",
            'import spacy, transformers, sklearn, nltk; print("Dependencies OK")'
          ]);
          python.on("close", (code) => {
            if (code === 0) {
              resolve2();
            } else {
              reject(
                new Error(
                  "Required dependencies not found. Please install spacy, transformers, scikit-learn, nltk."
                )
              );
            }
          });
          python.on("error", () => {
            reject(new Error("Python not found or dependencies missing."));
          });
        });
      }
      /**
       * Load language models
       */
      async loadLanguageModels() {
        try {
          logger57.info("Language models loaded successfully");
        } catch (error) {
          logger57.error("Failed to load language models:", error);
          throw error;
        }
      }
      /**
       * Check if text analysis engine is ready
       */
      isReady() {
        return this.isInitialized;
      }
      /**
       * Cleanup resources
       */
      async shutdown() {
        logger57.info("Shutting down Text Analysis Engine...");
        this.models.clear();
        this.isInitialized = false;
        logger57.info("Text Analysis Engine shutdown complete");
      }
    };
  }
});

// src/ai/services/EmbeddingService.ts
import { spawn as spawn7 } from "child_process";
import path48 from "path";
import pino62 from "pino";
var logger58, EmbeddingService2;
var init_EmbeddingService2 = __esm({
  "src/ai/services/EmbeddingService.ts"() {
    "use strict";
    logger58 = pino62({ name: "EmbeddingService" });
    EmbeddingService2 = class {
      config;
      db;
      isInitialized = false;
      availableModels = /* @__PURE__ */ new Map();
      constructor(config9, db2) {
        this.config = config9;
        this.db = db2;
      }
      /**
       * Initialize embedding service
       */
      async initialize() {
        try {
          await this.verifyDependencies();
          await this.loadEmbeddingModels();
          if (this.db) {
            await this.initializeVectorTables();
          }
          this.isInitialized = true;
          logger58.info("Embedding Service initialized successfully");
        } catch (error) {
          logger58.error("Failed to initialize Embedding Service:", error);
          throw error;
        }
      }
      /**
       * Generate text embedding
       */
      async generateTextEmbedding(text, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          model = "sentence-transformers/all-MiniLM-L6-v2",
          normalize: normalize3 = true,
          poolingStrategy = "mean"
        } = options2;
        try {
          logger58.debug(
            `Generating text embedding for: ${text.substring(0, 100)}...`
          );
          const embedding = await this.runTextEmbedding(
            text,
            model,
            normalize3,
            poolingStrategy
          );
          logger58.debug(
            `Generated text embedding with dimension: ${embedding.length}`
          );
          return embedding;
        } catch (error) {
          logger58.error("Text embedding generation failed:", error);
          throw error;
        }
      }
      /**
       * Generate image embedding
       */
      async generateImageEmbedding(imagePath, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const { model = "openai/clip-vit-base-patch32", normalize: normalize3 = true } = options2;
        try {
          logger58.debug(`Generating image embedding for: ${imagePath}`);
          const embedding = await this.runImageEmbedding(
            imagePath,
            model,
            normalize3
          );
          logger58.debug(
            `Generated image embedding with dimension: ${embedding.length}`
          );
          return embedding;
        } catch (error) {
          logger58.error("Image embedding generation failed:", error);
          throw error;
        }
      }
      /**
       * Generate audio embedding
       */
      async generateAudioEmbedding(audioPath, options2 = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const { model = "facebook/wav2vec2-base", normalize: normalize3 = true } = options2;
        try {
          logger58.debug(`Generating audio embedding for: ${audioPath}`);
          const embedding = await this.runAudioEmbedding(
            audioPath,
            model,
            normalize3
          );
          logger58.debug(
            `Generated audio embedding with dimension: ${embedding.length}`
          );
          return embedding;
        } catch (error) {
          logger58.error("Audio embedding generation failed:", error);
          throw error;
        }
      }
      /**
       * Generate multimodal embedding by combining different modalities
       */
      async generateMultimodalEmbedding(inputs, weights = {}) {
        if (!this.isInitialized) {
          await this.initialize();
        }
        const {
          text: textWeight = 1,
          image: imageWeight = 1,
          audio: audioWeight = 1
        } = weights;
        try {
          const embeddings = [];
          const modalityWeights = [];
          if (inputs.text && textWeight > 0) {
            const textEmbedding = await this.generateTextEmbedding(inputs.text);
            embeddings.push(textEmbedding);
            modalityWeights.push(textWeight);
          }
          if (inputs.imagePath && imageWeight > 0) {
            const imageEmbedding = await this.generateImageEmbedding(
              inputs.imagePath
            );
            embeddings.push(imageEmbedding);
            modalityWeights.push(imageWeight);
          }
          if (inputs.audioPath && audioWeight > 0) {
            const audioEmbedding = await this.generateAudioEmbedding(
              inputs.audioPath
            );
            embeddings.push(audioEmbedding);
            modalityWeights.push(audioWeight);
          }
          if (embeddings.length === 0) {
            throw new Error("No valid inputs provided for multimodal embedding");
          }
          const combinedEmbedding = this.fuseEmbeddings(
            embeddings,
            modalityWeights
          );
          logger58.debug(
            `Generated multimodal embedding with dimension: ${combinedEmbedding.length}`
          );
          return combinedEmbedding;
        } catch (error) {
          logger58.error("Multimodal embedding generation failed:", error);
          throw error;
        }
      }
      /**
       * Store embedding in vector database
       */
      async storeEmbedding(id, vector, metadata, modality, source) {
        if (!this.db) {
          throw new Error("Database connection not available");
        }
        try {
          const query3 = `
        INSERT INTO embeddings (id, vector, metadata, modality, source, created_at)
        VALUES ($1, $2, $3, $4, $5, NOW())
        ON CONFLICT (id) DO UPDATE SET
          vector = EXCLUDED.vector,
          metadata = EXCLUDED.metadata,
          modality = EXCLUDED.modality,
          source = EXCLUDED.source,
          created_at = NOW()
      `;
          await this.db.query(query3, [
            id,
            JSON.stringify(vector),
            JSON.stringify(metadata),
            modality,
            source
          ]);
          logger58.debug(`Stored embedding: ${id} (${modality})`);
        } catch (error) {
          logger58.error("Failed to store embedding:", error);
          throw error;
        }
      }
      /**
       * Find similar embeddings using vector similarity
       */
      async findSimilar(queryVector, options2 = {}) {
        if (!this.db) {
          throw new Error("Database connection not available");
        }
        const { topK = 10, threshold = 0, modality, metadata } = options2;
        try {
          let query3 = `
        SELECT 
          id,
          vector,
          metadata,
          modality,
          source,
          (1 - (vector <=> $1::vector)) as similarity
        FROM embeddings
        WHERE (1 - (vector <=> $1::vector)) >= $2
      `;
          const params = [JSON.stringify(queryVector), threshold];
          let paramCount = 2;
          if (modality) {
            query3 += ` AND modality = $${++paramCount}`;
            params.push(modality);
          }
          if (metadata) {
            for (const [key, value] of Object.entries(metadata)) {
              query3 += ` AND metadata->>'${key}' = $${++paramCount}`;
              params.push(value);
            }
          }
          query3 += ` ORDER BY similarity DESC LIMIT $${++paramCount}`;
          params.push(topK);
          const result2 = await this.db.query(query3, params);
          const similarities = result2.rows.map((row) => ({
            id: row.id,
            similarity: row.similarity,
            metadata: row.metadata,
            vector: JSON.parse(row.vector)
          }));
          logger58.debug(`Found ${similarities.length} similar embeddings`);
          return similarities;
        } catch (error) {
          logger58.error("Similarity search failed:", error);
          throw error;
        }
      }
      /**
       * Perform cross-modal search
       */
      async crossModalSearch(query3, options2 = {}) {
        const { topK = 10, threshold = 0.5 } = options2;
        try {
          const queryEmbeddings = [];
          const modalityWeights = [];
          if (query3.textQuery) {
            const textEmbedding = await this.generateTextEmbedding(query3.textQuery);
            queryEmbeddings.push(textEmbedding);
            modalityWeights.push(query3.weights?.text || 1);
          }
          if (query3.imageQuery) {
            const imageEmbedding = await this.generateImageEmbedding(
              query3.imageQuery
            );
            queryEmbeddings.push(imageEmbedding);
            modalityWeights.push(query3.weights?.image || 1);
          }
          if (query3.audioQuery) {
            const audioEmbedding = await this.generateAudioEmbedding(
              query3.audioQuery
            );
            queryEmbeddings.push(audioEmbedding);
            modalityWeights.push(query3.weights?.audio || 1);
          }
          if (queryEmbeddings.length === 0) {
            throw new Error("No valid query modalities provided");
          }
          const combinedQuery = this.fuseEmbeddings(
            queryEmbeddings,
            modalityWeights
          );
          const results = await this.findSimilar(combinedQuery, {
            topK,
            threshold
          });
          logger58.info(`Cross-modal search returned ${results.length} results`);
          return results;
        } catch (error) {
          logger58.error("Cross-modal search failed:", error);
          throw error;
        }
      }
      /**
       * Perform clustering on embeddings
       */
      async clusterEmbeddings(embeddingIds, options2 = {}) {
        const {
          numClusters = 5,
          algorithm = "kmeans",
          minClusterSize = 2
        } = options2;
        try {
          const embeddings = await this.getEmbeddingsByIds(embeddingIds);
          if (embeddings.length < numClusters) {
            throw new Error("Not enough embeddings for clustering");
          }
          const clusters = await this.runClustering(
            embeddings,
            numClusters,
            algorithm
          );
          const validClusters = clusters.filter(
            (cluster) => cluster.members.length >= minClusterSize
          );
          logger58.info(
            `Clustering completed: ${validClusters.length} clusters found`
          );
          return validClusters;
        } catch (error) {
          logger58.error("Clustering failed:", error);
          throw error;
        }
      }
      /**
       * Run text embedding generation
       */
      async runTextEmbedding(text, model, normalize3, poolingStrategy) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path48.join(
            this.config.modelsPath,
            "text_embedding.py"
          );
          const args = [
            pythonScript,
            "--text",
            text,
            "--model",
            model,
            "--pooling",
            poolingStrategy
          ];
          if (normalize3) {
            args.push("--normalize");
          }
          if (this.config.enableGPU) {
            args.push("--device", "cuda");
          }
          const python = spawn7(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.embedding || []);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Text embedding failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Run image embedding generation
       */
      async runImageEmbedding(imagePath, model, normalize3) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path48.join(
            this.config.modelsPath,
            "image_embedding.py"
          );
          const args = [pythonScript, "--image", imagePath, "--model", model];
          if (normalize3) {
            args.push("--normalize");
          }
          if (this.config.enableGPU) {
            args.push("--device", "cuda");
          }
          const python = spawn7(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.embedding || []);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Image embedding failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Run audio embedding generation
       */
      async runAudioEmbedding(audioPath, model, normalize3) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path48.join(
            this.config.modelsPath,
            "audio_embedding.py"
          );
          const args = [pythonScript, "--audio", audioPath, "--model", model];
          if (normalize3) {
            args.push("--normalize");
          }
          if (this.config.enableGPU) {
            args.push("--device", "cuda");
          }
          const python = spawn7(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.embedding || []);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Audio embedding failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Fuse multiple embeddings using weighted combination
       */
      fuseEmbeddings(embeddings, weights) {
        if (embeddings.length === 0) {
          throw new Error("No embeddings to fuse");
        }
        if (embeddings.length !== weights.length) {
          throw new Error("Number of embeddings must match number of weights");
        }
        const totalWeight = weights.reduce((sum, w) => sum + w, 0);
        const normalizedWeights = weights.map((w) => w / totalWeight);
        const dimension = embeddings[0].length;
        for (const embedding of embeddings) {
          if (embedding.length !== dimension) {
            throw new Error("All embeddings must have the same dimension");
          }
        }
        const fusedEmbedding = new Array(dimension).fill(0);
        for (let i = 0; i < dimension; i++) {
          for (let j = 0; j < embeddings.length; j++) {
            fusedEmbedding[i] += embeddings[j][i] * normalizedWeights[j];
          }
        }
        const norm = Math.sqrt(
          fusedEmbedding.reduce((sum, val) => sum + val * val, 0)
        );
        if (norm > 0) {
          for (let i = 0; i < dimension; i++) {
            fusedEmbedding[i] /= norm;
          }
        }
        return fusedEmbedding;
      }
      /**
       * Get embeddings by IDs
       */
      async getEmbeddingsByIds(ids) {
        if (!this.db) {
          throw new Error("Database connection not available");
        }
        const query3 = `
      SELECT id, vector, metadata, modality, source, created_at
      FROM embeddings
      WHERE id = ANY($1)
    `;
        const result2 = await this.db.query(query3, [ids]);
        return result2.rows.map((row) => ({
          id: row.id,
          vector: JSON.parse(row.vector),
          metadata: row.metadata,
          modality: row.modality,
          source: row.source,
          createdAt: row.created_at
        }));
      }
      /**
       * Run clustering algorithm
       */
      async runClustering(embeddings, numClusters, algorithm) {
        return new Promise((resolve2, reject) => {
          const pythonScript = path48.join(this.config.modelsPath, "clustering.py");
          const embeddingData = {
            embeddings: embeddings.map((e) => ({
              id: e.id,
              vector: e.vector,
              metadata: e.metadata
            }))
          };
          const args = [
            pythonScript,
            "--data",
            JSON.stringify(embeddingData),
            "--num-clusters",
            numClusters.toString(),
            "--algorithm",
            algorithm
          ];
          const python = spawn7(this.config.pythonPath, args);
          let output = "";
          let errorOutput = "";
          python.stdout.on("data", (data) => {
            output += data.toString();
          });
          python.stderr.on("data", (data) => {
            errorOutput += data.toString();
          });
          python.on("close", (code) => {
            if (code === 0) {
              try {
                const result2 = JSON.parse(output);
                resolve2(result2.clusters || []);
              } catch (parseError) {
                reject(parseError);
              }
            } else {
              reject(new Error(`Clustering failed: ${errorOutput}`));
            }
          });
          python.on("error", (error) => {
            reject(error);
          });
        });
      }
      /**
       * Initialize vector database tables
       */
      async initializeVectorTables() {
        const createTableQuery = `
      CREATE TABLE IF NOT EXISTS embeddings (
        id VARCHAR(255) PRIMARY KEY,
        vector JSONB NOT NULL,
        metadata JSONB DEFAULT '{}',
        modality VARCHAR(50) NOT NULL,
        source VARCHAR(255),
        created_at TIMESTAMP DEFAULT NOW()
      );

      CREATE INDEX IF NOT EXISTS idx_embeddings_modality ON embeddings(modality);
      CREATE INDEX IF NOT EXISTS idx_embeddings_source ON embeddings(source);
      CREATE INDEX IF NOT EXISTS idx_embeddings_created_at ON embeddings(created_at);
    `;
        await this.db.query(createTableQuery);
        logger58.debug("Vector database tables initialized");
      }
      /**
       * Verify dependencies
       */
      async verifyDependencies() {
        return new Promise((resolve2, reject) => {
          const python = spawn7(this.config.pythonPath, [
            "-c",
            'import sentence_transformers, transformers, torch, sklearn; print("Dependencies OK")'
          ]);
          python.on("close", (code) => {
            if (code === 0) {
              resolve2();
            } else {
              reject(
                new Error(
                  "Required dependencies not found. Please install sentence-transformers, transformers, torch, scikit-learn."
                )
              );
            }
          });
          python.on("error", () => {
            reject(new Error("Python not found or dependencies missing."));
          });
        });
      }
      /**
       * Load embedding models
       */
      async loadEmbeddingModels() {
        try {
          const textModels = [
            "sentence-transformers/all-MiniLM-L6-v2",
            "sentence-transformers/all-mpnet-base-v2",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
          ];
          const imageModels = [
            "openai/clip-vit-base-patch32",
            "openai/clip-vit-large-patch14"
          ];
          const audioModels = ["facebook/wav2vec2-base", "facebook/wav2vec2-large"];
          this.availableModels.set("text", textModels);
          this.availableModels.set("image", imageModels);
          this.availableModels.set("audio", audioModels);
          logger58.info("Embedding models loaded successfully");
        } catch (error) {
          logger58.error("Failed to load embedding models:", error);
          throw error;
        }
      }
      /**
       * Check if embedding service is ready
       */
      isReady() {
        return this.isInitialized;
      }
      /**
       * Get available models for a modality
       */
      getAvailableModels(modality) {
        return this.availableModels.get(modality) || [];
      }
      /**
       * Cleanup resources
       */
      async shutdown() {
        logger58.info("Shutting down Embedding Service...");
        this.availableModels.clear();
        this.isInitialized = false;
        logger58.info("Embedding Service shutdown complete");
      }
    };
  }
});

// src/services/ImageProcessingPipeline.ts
import sharp2 from "sharp";
import exifReader from "exif-reader";
import pino63 from "pino";
var logger59, DEFAULT_PIPELINE_CONFIG, defaultImageProcessingConfig;
var init_ImageProcessingPipeline = __esm({
  "src/services/ImageProcessingPipeline.ts"() {
    "use strict";
    logger59 = pino63({ name: "ImageProcessingPipeline" });
    DEFAULT_PIPELINE_CONFIG = {
      thumbnails: [
        {
          width: 320,
          height: 320,
          postfix: "thumb",
          format: "jpeg",
          quality: 80
        }
      ],
      conversions: [
        {
          format: "webp",
          quality: 82,
          suffix: "webp"
        }
      ],
      optimization: {
        quality: 88,
        progressive: true,
        normalize: true,
        targetFormat: void 0
      },
      watermark: {
        text: void 0,
        imagePath: void 0,
        gravity: "southeast",
        opacity: 0.25,
        size: 42,
        margin: 24
      },
      facialRecognitionHook: void 0
    };
    defaultImageProcessingConfig = DEFAULT_PIPELINE_CONFIG;
  }
});

// src/services/CdnUploadService.ts
import { S3Client, PutObjectCommand } from "@aws-sdk/client-s3";
import pino64 from "pino";
var logger60;
var init_CdnUploadService = __esm({
  "src/services/CdnUploadService.ts"() {
    "use strict";
    logger60 = pino64({ name: "CdnUploadService" });
  }
});

// src/services/MediaUploadService.ts
import sharp3 from "sharp";
import ffprobe from "ffprobe-static";
import ffmpeg from "fluent-ffmpeg";
import pino65 from "pino";
import exifReader2 from "exif-reader";
import ffmpegStatic from "ffmpeg-static";
var logger61, defaultMediaUploadConfig;
var init_MediaUploadService = __esm({
  "src/services/MediaUploadService.ts"() {
    "use strict";
    init_ImageProcessingPipeline();
    init_CdnUploadService();
    logger61 = pino65({ name: "MediaUploadService" });
    ffmpeg.setFfmpegPath(ffmpegStatic);
    ffmpeg.setFfprobePath(ffprobe.path);
    defaultMediaUploadConfig = {
      maxFileSize: 10 * 1024 * 1024 * 1024,
      // 10GB
      allowedTypes: [
        "image/*",
        "video/*",
        "audio/*",
        "text/*",
        "application/pdf",
        "application/msword",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        "application/json",
        "application/xml"
      ],
      uploadPath: process.env.MEDIA_UPLOAD_PATH || "/tmp/intelgraph/uploads",
      thumbnailPath: process.env.MEDIA_THUMBNAIL_PATH || "/tmp/intelgraph/thumbnails",
      chunkSize: 64 * 1024,
      // 64KB chunks
      imageProcessing: defaultImageProcessingConfig,
      cdnUpload: {
        enabled: false,
        bucket: process.env.MEDIA_CDN_BUCKET || "",
        region: process.env.MEDIA_CDN_REGION || "us-east-1",
        basePath: process.env.MEDIA_CDN_BASE_PATH,
        endpoint: process.env.MEDIA_CDN_ENDPOINT,
        publicUrl: process.env.MEDIA_CDN_PUBLIC_URL,
        accessKeyId: process.env.MEDIA_CDN_ACCESS_KEY_ID,
        secretAccessKey: process.env.MEDIA_CDN_SECRET_ACCESS_KEY
      }
    };
  }
});

// src/ai/engines/VideoFrameExtractor.ts
import ffmpeg2 from "fluent-ffmpeg";
import path49 from "path";
import fs39 from "fs/promises";
import pino66 from "pino";
import { randomUUID as randomUUID73 } from "crypto";
var logger62, VideoFrameExtractor;
var init_VideoFrameExtractor = __esm({
  "src/ai/engines/VideoFrameExtractor.ts"() {
    "use strict";
    logger62 = pino66({ name: "VideoFrameExtractor" });
    VideoFrameExtractor = class {
      ffmpegPath;
      ffprobePath;
      tempDir;
      constructor(ffmpegPath, ffprobePath, tempDir) {
        this.ffmpegPath = ffmpegPath;
        this.ffprobePath = ffprobePath;
        this.tempDir = tempDir;
        ffmpeg2.setFfmpegPath(this.ffmpegPath);
        ffmpeg2.setFfprobePath(this.ffprobePath);
      }
      /**
       * Extracts frames and optionally audio from a video file.
       * @param videoPath Absolute path to the input video file.
       * @param options Frame extraction options.
       * @returns An object containing extracted frames and audio (if requested).
       */
      async extract(videoPath, options2 = {}) {
        const {
          frameRate,
          interval,
          outputDir = path49.join(this.tempDir, `frames-${randomUUID73()}`),
          outputFormat = "png",
          startTime,
          endTime,
          extractAudio = false
        } = options2;
        await fs39.mkdir(outputDir, { recursive: true });
        logger62.info(`Extracting frames to: ${outputDir}`);
        const frames = [];
        let audio;
        let frameCount = 0;
        return new Promise(async (resolve2, reject) => {
          const command = ffmpeg2(videoPath);
          if (startTime !== void 0) {
            command.seekInput(startTime);
          }
          if (endTime !== void 0) {
            command.duration(endTime - (startTime || 0));
          }
          if (frameRate) {
            command.fps(frameRate);
          } else if (interval) {
            command.addOption("-vf", `fps=1/${interval}`);
          } else {
            command.fps(1);
          }
          command.output(`${outputDir}/frame-%s.${outputFormat}`).on("filenames", (filenames) => {
            filenames.forEach((filename) => {
              const timestampMatch = filename.match(/frame-(\d+(\.\d+)?)\./);
              const timestamp = timestampMatch ? parseFloat(timestampMatch[1]) : 0;
              frames.push({
                framePath: path49.join(outputDir, filename),
                timestamp,
                frameNumber: frameCount++
                // Simple sequential numbering
              });
            });
          }).on("end", async () => {
            logger62.info(
              `Finished frame extraction for ${videoPath}. Extracted ${frames.length} frames.`
            );
            if (extractAudio) {
              try {
                audio = await this.extractAudioStream(
                  videoPath,
                  outputDir,
                  startTime,
                  endTime
                );
                logger62.info(`Finished audio extraction for ${videoPath}.`);
              } catch (audioErr) {
                logger62.error(`Failed to extract audio: ${audioErr}`);
              }
            }
            resolve2({ frames, audio });
          }).on("error", (err) => {
            logger62.error(
              `Error during frame extraction for ${videoPath}: ${err.message}`
            );
            reject(err);
          }).run();
        });
      }
      /**
       * Extracts the audio stream from a video file.
       * @param videoPath Absolute path to the input video file.
       * @param outputDir Directory to save the extracted audio file.
       * @param startTime Start time in seconds.
       * @param endTime End time in seconds.
       * @returns Path to the extracted audio file.
       */
      async extractAudioStream(videoPath, outputDir, startTime, endTime) {
        const audioFileName = `audio-${randomUUID73()}.mp3`;
        const audioPath = path49.join(outputDir, audioFileName);
        return new Promise((resolve2, reject) => {
          const command = ffmpeg2(videoPath);
          if (startTime !== void 0) {
            command.seekInput(startTime);
          }
          if (endTime !== void 0) {
            command.duration(endTime - (startTime || 0));
          }
          command.output(audioPath).noVideo().audioCodec("libmp3lame").on("end", async () => {
            const metadata = await this.getVideoMetadata(videoPath);
            const duration = metadata.format.duration || 0;
            resolve2({ audioPath, duration });
          }).on("error", (err) => {
            reject(err);
          }).run();
        });
      }
      /**
       * Gets metadata of a video file using ffprobe.
       * @param videoPath Absolute path to the video file.
       * @returns Video metadata.
       */
      async getVideoMetadata(videoPath) {
        return new Promise((resolve2, reject) => {
          ffmpeg2.ffprobe(videoPath, (err, metadata) => {
            if (err) {
              reject(err);
            } else {
              resolve2(metadata);
            }
          });
        });
      }
      /**
       * Cleans up temporary directories created during extraction.
       * @param dirPath Path to the directory to remove.
       */
      async cleanup(dirPath) {
        try {
          await fs39.rm(dirPath, { recursive: true, force: true });
          logger62.info(`Cleaned up temporary directory: ${dirPath}`);
        } catch (error) {
          logger62.error(`Failed to clean up directory ${dirPath}: ${error}`);
        }
      }
    };
  }
});

// src/ai/ExtractionEngine.ts
import pino67 from "pino";
import path50 from "path";
import { createReadStream as createReadStream4, promises as fsPromises } from "fs";
import ffmpegStatic2 from "ffmpeg-static";
import ffprobeStatic from "ffprobe-static";
var logger63, ExtractionEngine;
var init_ExtractionEngine = __esm({
  "src/ai/ExtractionEngine.ts"() {
    "use strict";
    init_OCREngine();
    init_ObjectDetectionEngine();
    init_SpeechToTextEngine();
    init_FaceDetectionEngine();
    init_TextAnalysisEngine();
    init_EmbeddingService2();
    init_MediaUploadService();
    init_VideoFrameExtractor();
    logger63 = pino67({ name: "ExtractionEngine" });
    ExtractionEngine = class {
      config;
      db;
      ocrEngine;
      objectDetectionEngine;
      speechEngine;
      faceEngine;
      textEngine;
      embeddingService;
      videoFrameExtractor;
      // WAR-GAMED SIMULATION - VideoFrameExtractor instance
      activeJobs = /* @__PURE__ */ new Map();
      constructor(config9, db2) {
        this.config = config9;
        this.db = db2;
        this.ocrEngine = new OCREngine(config9);
        this.objectDetectionEngine = new ObjectDetectionEngine(config9);
        this.speechEngine = new SpeechToTextEngine(config9);
        this.faceEngine = new FaceDetectionEngine(config9);
        this.textEngine = new TextAnalysisEngine(config9);
        this.embeddingService = new EmbeddingService2(config9);
        this.videoFrameExtractor = new VideoFrameExtractor(
          ffmpegStatic2 || "ffmpeg",
          // Use static path or default
          ffprobeStatic || "ffprobe",
          // Use static path or default
          config9.tempPath
        );
      }
      /**
       * Validate that the media path is within allowed directories
       */
      async validateMediaPath(mediaPath) {
        const resolvedPath = path50.resolve(mediaPath);
        const allowedPaths = this.config.allowedPaths ? [...this.config.allowedPaths] : [this.config.tempPath];
        const isAllowed = allowedPaths.some((allowed) => {
          const resolvedAllowed = path50.resolve(allowed);
          if (resolvedPath === resolvedAllowed) return true;
          return resolvedPath.startsWith(resolvedAllowed + path50.sep);
        });
        if (!isAllowed) {
          try {
            await fsPromises.access(resolvedPath);
            logger63.warn(`Access denied to file outside allowed paths: ${resolvedPath}`);
          } catch (e) {
          }
          throw new Error(`Access denied: Media path is not in an allowed directory.`);
        }
      }
      /**
       * Process extraction request using multiple AI models
       */
      async processExtraction(request) {
        const { jobId, mediaPath, mediaType, extractionMethods, options: options2 } = request;
        await this.validateMediaPath(mediaPath);
        const startTime = Date.now();
        logger63.info(
          `Starting extraction job: ${jobId}, methods: ${extractionMethods.join(", ")}`
        );
        const results = [];
        const overallErrors = [];
        try {
          this.activeJobs.set(jobId, { startTime, status: "running" });
          for (const method of extractionMethods) {
            try {
              const methodStartTime = Date.now();
              let result2;
              switch (method) {
                case "ocr":
                  result2 = await this.performOCR(
                    jobId,
                    mediaPath,
                    mediaType,
                    options2
                  );
                  break;
                case "object_detection":
                  result2 = await this.performObjectDetection(
                    jobId,
                    mediaPath,
                    mediaType,
                    options2
                  );
                  break;
                case "face_detection":
                  result2 = await this.performFaceDetection(
                    jobId,
                    mediaPath,
                    mediaType,
                    options2
                  );
                  break;
                case "speech_to_text":
                  result2 = await this.performSpeechToText(
                    jobId,
                    mediaPath,
                    mediaType,
                    options2
                  );
                  break;
                case "text_analysis":
                  result2 = await this.performTextAnalysis(
                    jobId,
                    mediaPath,
                    mediaType,
                    options2
                  );
                  break;
                case "scene_analysis":
                  result2 = await this.performSceneAnalysis(
                    jobId,
                    mediaPath,
                    mediaType,
                    options2
                  );
                  break;
                case "entity_extraction":
                  result2 = await this.performEntityExtraction(
                    jobId,
                    mediaPath,
                    mediaType,
                    options2
                  );
                  break;
                // WAR-GAMED SIMULATION - New case for video analysis
                case "video_analysis":
                  if (mediaType !== "VIDEO" /* VIDEO */) {
                    throw new Error(
                      `Video analysis only supported for media type: ${"VIDEO" /* VIDEO */}`
                    );
                  }
                  result2 = await this.performVideoAnalysis(
                    jobId,
                    mediaPath,
                    options2
                  );
                  break;
                default:
                  throw new Error(`Unknown extraction method: ${method}`);
              }
              await this.generateEmbeddings(result2.entities);
              results.push(result2);
              const methodDuration = Date.now() - methodStartTime;
              logger63.info(
                `Completed ${method} for job ${jobId}: ${result2.entities.length} entities, ${methodDuration}ms`
              );
            } catch (methodError) {
              const errorMsg = `Failed ${method}: ${methodError.message}`;
              overallErrors.push(errorMsg);
              logger63.error(
                `Extraction method ${method} failed for job ${jobId}:`,
                methodError
              );
            }
          }
          const totalDuration = Date.now() - startTime;
          logger63.info(
            `Extraction job ${jobId} completed: ${results.length} methods, ${totalDuration}ms`
          );
          return results;
        } catch (error) {
          logger63.error(`Extraction job ${jobId} failed:`, error);
          throw error;
        } finally {
          this.activeJobs.delete(jobId);
        }
      }
      /**
       * Perform OCR using Tesseract and PaddleOCR
       */
      async performOCR(jobId, mediaPath, mediaType, options2) {
        if (mediaType !== "IMAGE" /* IMAGE */ && mediaType !== "VIDEO" /* VIDEO */) {
          throw new Error(`OCR not supported for media type: ${mediaType}`);
        }
        const startTime = Date.now();
        const entities = [];
        const errors = [];
        try {
          const ocrResults = await this.ocrEngine.extractText(mediaPath, {
            language: options2.language || "eng",
            enhanceImage: options2.enhanceImage !== false,
            confidenceThreshold: options2.confidenceThreshold || 0.6
          });
          for (const result2 of ocrResults) {
            entities.push({
              entityType: "text",
              extractedText: result2.text,
              boundingBox: result2.boundingBox,
              confidence: result2.confidence,
              extractionMethod: "ocr",
              extractionVersion: "2.0.0",
              metadata: {
                language: result2.language,
                ocrEngine: result2.engine,
                wordCount: result2.text.split(" ").length,
                characterCount: result2.text.length
              }
            });
          }
          const processingTime = Date.now() - startTime;
          const avgConfidence = entities.reduce((sum, e) => sum + e.confidence, 0) / entities.length || 0;
          return {
            jobId,
            method: "ocr",
            entities,
            metrics: {
              processingTime,
              entitiesExtracted: entities.length,
              averageConfidence: avgConfidence,
              memoryUsage: process.memoryUsage().heapUsed,
              modelVersion: "tesseract-5.3.0"
            },
            errors
          };
        } catch (error) {
          errors.push(error.message);
          throw error;
        }
      }
      /**
       * Perform object detection using YOLO and other models
       */
      async performObjectDetection(jobId, mediaPath, mediaType, options2) {
        if (mediaType !== "IMAGE" /* IMAGE */ && mediaType !== "VIDEO" /* VIDEO */) {
          throw new Error(
            `Object detection not supported for media type: ${mediaType}`
          );
        }
        const startTime = Date.now();
        const entities = [];
        try {
          const detections = await this.objectDetectionEngine.detectObjects(
            mediaPath,
            {
              model: options2.model || "yolov8n",
              confidenceThreshold: options2.confidenceThreshold || 0.5,
              nmsThreshold: options2.nmsThreshold || 0.4,
              maxDetections: options2.maxDetections || 100
            }
          );
          for (const detection of detections) {
            entities.push({
              entityType: detection.className,
              boundingBox: detection.boundingBox,
              confidence: detection.confidence,
              extractionMethod: "object_detection",
              extractionVersion: "2.0.0",
              metadata: {
                model: detection.model,
                classId: detection.classId,
                trackingId: detection.trackingId
              }
            });
          }
          const processingTime = Date.now() - startTime;
          const avgConfidence = entities.reduce((sum, e) => sum + e.confidence, 0) / entities.length || 0;
          return {
            jobId,
            method: "object_detection",
            entities,
            metrics: {
              processingTime,
              entitiesExtracted: entities.length,
              averageConfidence: avgConfidence,
              memoryUsage: process.memoryUsage().heapUsed,
              modelVersion: "yolov8n-1.0"
            },
            errors: []
          };
        } catch (error) {
          throw error;
        }
      }
      /**
       * Perform face detection and recognition
       */
      async performFaceDetection(jobId, mediaPath, mediaType, options2) {
        if (mediaType !== "IMAGE" /* IMAGE */ && mediaType !== "VIDEO" /* VIDEO */) {
          throw new Error(
            `Face detection not supported for media type: ${mediaType}`
          );
        }
        const startTime = Date.now();
        const entities = [];
        try {
          const faces = await this.faceEngine.detectFaces(mediaPath, {
            minFaceSize: options2.minFaceSize || 20,
            confidenceThreshold: options2.confidenceThreshold || 0.7,
            extractFeatures: options2.extractFeatures !== false,
            recognizeIdentities: options2.recognizeIdentities === true
          });
          for (const face of faces) {
            entities.push({
              entityType: "face",
              boundingBox: face.boundingBox,
              confidence: face.confidence,
              extractionMethod: "face_detection",
              extractionVersion: "2.0.0",
              metadata: {
                landmarks: face.landmarks,
                age: face.estimatedAge,
                gender: face.estimatedGender,
                emotion: face.dominantEmotion,
                identity: face.recognizedIdentity,
                features: face.featureVector
              }
            });
          }
          const processingTime = Date.now() - startTime;
          const avgConfidence = entities.reduce((sum, e) => sum + e.confidence, 0) / entities.length || 0;
          return {
            jobId,
            method: "face_detection",
            entities,
            metrics: {
              processingTime,
              entitiesExtracted: entities.length,
              averageConfidence: avgConfidence,
              memoryUsage: process.memoryUsage().heapUsed,
              modelVersion: "mtcnn-pytorch-1.0"
            },
            errors: []
          };
        } catch (error) {
          throw error;
        }
      }
      /**
       * Perform speech-to-text transcription
       */
      async performSpeechToText(jobId, mediaPath, mediaType, options2) {
        if (mediaType !== "AUDIO" /* AUDIO */ && mediaType !== "VIDEO" /* VIDEO */) {
          throw new Error(
            `Speech-to-text not supported for media type: ${mediaType}`
          );
        }
        const startTime = Date.now();
        const entities = [];
        try {
          const transcriptions = await this.speechEngine.transcribe(mediaPath, {
            language: options2.language || "en",
            model: options2.model || "whisper-base",
            enableDiarization: options2.enableDiarization === true,
            enhanceAudio: options2.enhanceAudio !== false,
            timestamping: options2.timestamping !== false
          });
          for (const segment of transcriptions) {
            entities.push({
              entityType: "speech",
              extractedText: segment.text,
              temporalRange: {
                startTime: segment.startTime,
                endTime: segment.endTime,
                confidence: segment.confidence
              },
              confidence: segment.confidence,
              extractionMethod: "speech_to_text",
              extractionVersion: "2.0.0",
              metadata: {
                speaker: segment.speaker,
                language: segment.detectedLanguage,
                wordCount: segment.text.split(" ").length,
                audioQuality: segment.audioQuality,
                noiseLevel: segment.noiseLevel
              }
            });
          }
          const processingTime = Date.now() - startTime;
          const avgConfidence = entities.reduce((sum, e) => sum + e.confidence, 0) / entities.length || 0;
          return {
            jobId,
            method: "speech_to_text",
            entities,
            metrics: {
              processingTime,
              entitiesExtracted: entities.length,
              averageConfidence: avgConfidence,
              memoryUsage: process.memoryUsage().heapUsed,
              modelVersion: "whisper-base-v20231117"
            },
            errors: []
          };
        } catch (error) {
          throw error;
        }
      }
      /**
       * Perform advanced text analysis (NER, sentiment, topics)
       */
      async performTextAnalysis(jobId, mediaPath, mediaType, options2) {
        const startTime = Date.now();
        const entities = [];
        try {
          let textContent = "";
          if (mediaType === "TEXT" /* TEXT */) {
            textContent = await this.readTextFile(mediaPath);
          } else {
            const ocrResult = await this.performOCR(
              jobId,
              mediaPath,
              mediaType,
              options2
            );
            textContent = ocrResult.entities.map((e) => e.extractedText).join(" ");
          }
          if (!textContent.trim()) {
            return {
              jobId,
              method: "text_analysis",
              entities: [],
              metrics: {
                processingTime: Date.now() - startTime,
                entitiesExtracted: 0,
                averageConfidence: 0,
                memoryUsage: process.memoryUsage().heapUsed,
                modelVersion: "spacy-en-3.7.0"
              },
              errors: ["No text content found"]
            };
          }
          const analysis = await this.textEngine.analyzeText(textContent, {
            extractEntities: options2.extractEntities !== false,
            performSentiment: options2.performSentiment !== false,
            extractTopics: options2.extractTopics === true,
            detectLanguage: options2.detectLanguage !== false
          });
          for (const entity of analysis.entities) {
            entities.push({
              entityType: entity.label.toLowerCase(),
              extractedText: entity.text,
              confidence: entity.confidence,
              extractionMethod: "text_analysis",
              extractionVersion: "2.0.0",
              metadata: {
                startOffset: entity.start,
                endOffset: entity.end,
                entityLabel: entity.label,
                description: entity.description
              }
            });
          }
          for (const topic of analysis.topics) {
            entities.push({
              entityType: "topic",
              extractedText: topic.keywords.join(", "),
              confidence: topic.coherence,
              extractionMethod: "text_analysis",
              extractionVersion: "2.0.0",
              metadata: {
                topicId: topic.id,
                keywords: topic.keywords,
                coherenceScore: topic.coherence
              }
            });
          }
          if (analysis.sentiment) {
            entities.push({
              entityType: "sentiment",
              extractedText: analysis.sentiment.label,
              confidence: Math.abs(analysis.sentiment.score),
              extractionMethod: "text_analysis",
              extractionVersion: "2.0.0",
              metadata: {
                sentimentScore: analysis.sentiment.score,
                sentimentLabel: analysis.sentiment.label,
                confidence: analysis.sentiment.confidence
              }
            });
          }
          const processingTime = Date.now() - startTime;
          const avgConfidence = entities.reduce((sum, e) => sum + e.confidence, 0) / entities.length || 0;
          return {
            jobId,
            method: "text_analysis",
            entities,
            metrics: {
              processingTime,
              entitiesExtracted: entities.length,
              averageConfidence: avgConfidence,
              memoryUsage: process.memoryUsage().heapUsed,
              modelVersion: "spacy-en-core-web-lg-3.7.0"
            },
            errors: []
          };
        } catch (error) {
          throw error;
        }
      }
      /**
       * Perform scene analysis for images and videos
       */
      async performSceneAnalysis(jobId, mediaPath, mediaType, options2) {
        if (mediaType !== "IMAGE" /* IMAGE */ && mediaType !== "VIDEO" /* VIDEO */) {
          throw new Error(
            `Scene analysis not supported for media type: ${mediaType}`
          );
        }
        const startTime = Date.now();
        const entities = [];
        try {
          const sceneAnalysis = await this.analyzeScene(mediaPath, {
            detectObjects: true,
            classifyScene: true,
            extractColors: true,
            analyzeComposition: true
          });
          entities.push({
            entityType: "scene",
            extractedText: sceneAnalysis.sceneType,
            confidence: sceneAnalysis.sceneConfidence,
            extractionMethod: "scene_analysis",
            extractionVersion: "2.0.0",
            metadata: {
              sceneCategories: sceneAnalysis.categories,
              dominantColors: sceneAnalysis.colors,
              lighting: sceneAnalysis.lighting,
              composition: sceneAnalysis.composition
            }
          });
          const processingTime = Date.now() - startTime;
          return {
            jobId,
            method: "scene_analysis",
            entities,
            metrics: {
              processingTime,
              entitiesExtracted: entities.length,
              averageConfidence: entities[0]?.confidence || 0,
              memoryUsage: process.memoryUsage().heapUsed,
              modelVersion: "resnet50-places365"
            },
            errors: []
          };
        } catch (error) {
          throw error;
        }
      }
      /**
       * Perform entity extraction across all modalities
       */
      async performEntityExtraction(jobId, mediaPath, mediaType, options2) {
        const startTime = Date.now();
        const allEntities = [];
        try {
          const methods = this.getApplicableMethods(mediaType);
          for (const method of methods) {
            try {
              const result2 = await this.processExtraction({
                jobId: `${jobId}_${method}`,
                mediaSourceId: "",
                mediaPath,
                mediaType,
                extractionMethods: [method],
                options: options2
              });
              if (result2.length > 0) {
                allEntities.push(...result2[0].entities);
              }
            } catch (methodError) {
              logger63.warn(
                `Entity extraction method ${method} failed:`,
                methodError
              );
            }
          }
          const mergedEntities = this.mergeAndDeduplicateEntities(allEntities);
          const processingTime = Date.now() - startTime;
          const avgConfidence = mergedEntities.reduce((sum, e) => sum + e.confidence, 0) / mergedEntities.length || 0;
          return {
            jobId,
            method: "entity_extraction",
            entities: mergedEntities,
            metrics: {
              processingTime,
              entitiesExtracted: mergedEntities.length,
              averageConfidence: avgConfidence,
              memoryUsage: process.memoryUsage().heapUsed,
              modelVersion: "multi-modal-v2.0"
            },
            errors: []
          };
        } catch (error) {
          throw error;
        }
      }
      /**
       * Perform video analysis (frame-by-frame processing)
       */
      async performVideoAnalysis(jobId, videoPath, options2) {
        const startTime = Date.now();
        const allEntities = [];
        const errors = [];
        let tempFrameDir;
        let tempAudioPath;
        try {
          const { frames, audio } = await this.videoFrameExtractor.extract(
            videoPath,
            {
              frameRate: options2.frameRate || 1,
              // Default to 1 frame per second
              extractAudio: options2.extractAudio === true,
              outputDir: path50.join(this.config.tempPath, `video-frames-${jobId}`)
            }
          );
          tempFrameDir = path50.join(this.config.tempPath, `video-frames-${jobId}`);
          if (audio) {
            tempAudioPath = audio.audioPath;
            try {
              const speechResult = await this.performSpeechToText(
                jobId,
                tempAudioPath,
                "AUDIO" /* AUDIO */,
                options2
              );
              allEntities.push(...speechResult.entities);
            } catch (speechError) {
              logger63.warn(
                `Speech-to-text failed for video ${jobId}: ${speechError.message}`
              );
              errors.push(`Speech-to-text failed: ${speechError.message}`);
            }
          }
          for (const frame of frames) {
            const frameEntities = [];
            const frameOptions = {
              ...options2,
              frameTimestamp: frame.timestamp,
              frameNumber: frame.frameNumber
            };
            try {
              const faceResult = await this.performFaceDetection(
                jobId,
                frame.framePath,
                "IMAGE" /* IMAGE */,
                frameOptions
              );
              frameEntities.push(...faceResult.entities);
            } catch (faceError) {
              logger63.warn(
                `Face detection failed for frame ${frame.frameNumber}: ${faceError.message}`
              );
              errors.push(
                `Face detection for frame ${frame.frameNumber} failed: ${faceError.message}`
              );
            }
            try {
              const objectResult = await this.performObjectDetection(
                jobId,
                frame.framePath,
                "IMAGE" /* IMAGE */,
                frameOptions
              );
              frameEntities.push(...objectResult.entities);
            } catch (objectError) {
              logger63.warn(
                `Object detection failed for frame ${frame.frameNumber}: ${objectError.message}`
              );
              errors.push(
                `Object detection for frame ${frame.frameNumber} failed: ${objectError.message}`
              );
            }
            try {
              const ocrResult = await this.performOCR(
                jobId,
                frame.framePath,
                "IMAGE" /* IMAGE */,
                frameOptions
              );
              frameEntities.push(...ocrResult.entities);
            } catch (ocrError) {
              logger63.warn(
                `OCR failed for frame ${frame.frameNumber}: ${ocrError.message}`
              );
              errors.push(
                `OCR for frame ${frame.frameNumber} failed: ${ocrError.message}`
              );
            }
            frameEntities.forEach((entity) => {
              entity.metadata = {
                ...entity.metadata,
                frameTimestamp: frame.timestamp,
                frameNumber: frame.frameNumber,
                framePath: frame.framePath
                // For debugging/reference
              };
              if (!entity.temporalRange) {
                entity.temporalRange = {
                  startTime: frame.timestamp,
                  endTime: frame.timestamp,
                  confidence: entity.confidence
                  // Use entity confidence for temporal range
                };
              }
            });
            allEntities.push(...frameEntities);
          }
          const processingTime = Date.now() - startTime;
          const avgConfidence = allEntities.reduce((sum, e) => sum + e.confidence, 0) / allEntities.length || 0;
          return {
            jobId,
            method: "video_analysis",
            entities: allEntities,
            metrics: {
              processingTime,
              entitiesExtracted: allEntities.length,
              averageConfidence: avgConfidence,
              memoryUsage: process.memoryUsage().heapUsed,
              modelVersion: "multi-modal-video-v1.0"
            },
            errors
          };
        } catch (error) {
          logger63.error(`Video analysis failed for ${videoPath}: ${error.message}`);
          errors.push(`Video analysis failed: ${error.message}`);
          throw error;
        } finally {
          if (tempFrameDir) {
            await this.videoFrameExtractor.cleanup(tempFrameDir);
          }
        }
      }
      /**
       * Generate embeddings for extracted entities
       */
      async generateEmbeddings(entities) {
        for (const entity of entities) {
          try {
            if (entity.extractedText) {
              entity.embeddings = entity.embeddings || {};
              entity.embeddings.text = await this.embeddingService.generateTextEmbedding(
                entity.extractedText
              );
            }
            if (entity.boundingBox && (entity.entityType === "face" || entity.entityType === "object")) {
            }
            if (entity.temporalRange && entity.entityType === "speech") {
            }
          } catch (error) {
            logger63.warn(
              `Failed to generate embeddings for entity ${entity.entityType}:`,
              error.message
            );
          }
        }
      }
      /**
       * Get applicable extraction methods for media type
       */
      getApplicableMethods(mediaType) {
        switch (mediaType) {
          case "IMAGE" /* IMAGE */:
            return ["ocr", "object_detection", "face_detection", "scene_analysis"];
          case "VIDEO" /* VIDEO */:
            return ["video_analysis"];
          // This will orchestrate other methods internally
          case "AUDIO" /* AUDIO */:
            return ["speech_to_text"];
          case "TEXT" /* TEXT */:
            return ["text_analysis"];
          case "DOCUMENT" /* DOCUMENT */:
            return ["ocr", "text_analysis"];
          default:
            return ["text_analysis"];
        }
      }
      /**
       * Merge and deduplicate similar entities
       */
      mergeAndDeduplicateEntities(entities) {
        const merged = [];
        for (const entity of entities) {
          const similar = merged.find(
            (existing) => this.areEntitiesSimilar(existing, entity)
          );
          if (similar) {
            if (entity.confidence > similar.confidence) {
              similar.confidence = entity.confidence;
              similar.extractedText = entity.extractedText || similar.extractedText;
            }
          } else {
            merged.push(entity);
          }
        }
        return merged;
      }
      /**
       * Check if two entities are similar enough to merge
       */
      areEntitiesSimilar(entity1, entity2) {
        if (entity1.entityType !== entity2.entityType) return false;
        if (entity1.extractedText && entity2.extractedText) {
          const textSimilarity = this.calculateTextSimilarity(
            entity1.extractedText,
            entity2.extractedText
          );
          if (textSimilarity > 0.8) return true;
        }
        if (entity1.boundingBox && entity2.boundingBox) {
          const spatialOverlap = this.calculateBoundingBoxOverlap(
            entity1.boundingBox,
            entity2.boundingBox
          );
          if (spatialOverlap > 0.5) return true;
        }
        if (entity1.temporalRange && entity2.temporalRange) {
          const temporalOverlap = this.calculateTemporalOverlap(
            entity1.temporalRange,
            entity2.temporalRange
          );
          if (temporalOverlap > 0.7) return true;
        }
        return false;
      }
      /**
       * Calculate text similarity using simple token overlap
       */
      calculateTextSimilarity(text1, text2) {
        const tokens1 = new Set(text1.toLowerCase().split(/\s+/));
        const tokens2 = new Set(text2.toLowerCase().split(/\s+/));
        const intersection = new Set([...tokens1].filter((x) => tokens2.has(x)));
        const union = /* @__PURE__ */ new Set([...tokens1, ...tokens2]);
        return intersection.size / union.size;
      }
      /**
       * Calculate bounding box overlap (IoU)
       */
      calculateBoundingBoxOverlap(box1, box2) {
        const x1 = Math.max(box1.x, box2.x);
        const y1 = Math.max(box1.y, box2.y);
        const x2 = Math.min(box1.x + box1.width, box2.x + box2.width);
        const y2 = Math.min(box1.y + box1.height, box2.y + box2.height);
        if (x2 <= x1 || y2 <= y1) return 0;
        const intersectionArea = (x2 - x1) * (y2 - y1);
        const box1Area = box1.width * box1.height;
        const box2Area = box2.width * box2.height;
        const unionArea = box1Area + box2Area - intersectionArea;
        return intersectionArea / unionArea;
      }
      /**
       * Calculate temporal range overlap
       */
      calculateTemporalOverlap(range1, range2) {
        const start = Math.max(range1.startTime, range2.startTime);
        const end = Math.min(range1.endTime, range2.endTime);
        if (end <= start) return 0;
        const intersectionDuration = end - start;
        const range1Duration = range1.endTime - range1.startTime;
        const range2Duration = range2.endTime - range2.startTime;
        const unionDuration = range1Duration + range2Duration - intersectionDuration;
        return intersectionDuration / unionDuration;
      }
      /**
       * Read text file content
       */
      async readTextFile(filePath) {
        return new Promise((resolve2, reject) => {
          const chunks = [];
          const stream2 = createReadStream4(filePath);
          stream2.on("data", (chunk) => chunks.push(chunk));
          stream2.on("end", () => resolve2(Buffer.concat(chunks).toString("utf8")));
          stream2.on("error", reject);
        });
      }
      /**
       * Analyze scene content (placeholder implementation)
       */
      async analyzeScene(mediaPath, options2) {
        return {
          sceneType: "outdoor_natural",
          sceneConfidence: 0.85,
          categories: ["outdoor", "natural", "landscape"],
          colors: ["green", "blue", "brown"],
          lighting: "daylight",
          composition: "rule_of_thirds"
        };
      }
      /**
       * Get extraction engine status
       */
      getStatus() {
        return {
          activeJobs: this.activeJobs.size,
          maxConcurrentJobs: this.config.maxConcurrentJobs,
          enableGPU: this.config.enableGPU,
          engines: {
            ocr: this.ocrEngine.isReady(),
            objectDetection: this.objectDetectionEngine.isReady(),
            speech: this.speechEngine.isReady(),
            face: this.faceEngine.isReady(),
            text: this.textEngine.isReady()
          }
        };
      }
      /**
       * Cleanup resources
       */
      async shutdown() {
        logger63.info("Shutting down ExtractionEngine...");
        const shutdownTimeout = 3e4;
        const startTime = Date.now();
        while (this.activeJobs.size > 0 && Date.now() - startTime < shutdownTimeout) {
          await new Promise((resolve2) => setTimeout(resolve2, 1e3));
        }
        await Promise.all([
          this.ocrEngine.shutdown(),
          this.objectDetectionEngine.shutdown(),
          this.speechEngine.shutdown(),
          this.faceEngine.shutdown(),
          this.textEngine.shutdown(),
          this.embeddingService.shutdown()
        ]);
        logger63.info("ExtractionEngine shutdown complete");
      }
    };
  }
});

// src/ai/services/AdversaryAgentService.ts
import { spawn as spawn8 } from "child_process";
import path51 from "path";
import pino68 from "pino";
var AdversaryAgentService, AdversaryAgentService_default;
var init_AdversaryAgentService = __esm({
  "src/ai/services/AdversaryAgentService.ts"() {
    "use strict";
    AdversaryAgentService = class {
      pythonPath;
      modelsPath;
      logger = pino68({ name: "AdversaryAgentService" });
      constructor(pythonPath = process.env.PYTHON_PATH || "python", modelsPath = path51.join(process.cwd(), "server", "src", "ai", "models")) {
        this.pythonPath = pythonPath;
        this.modelsPath = modelsPath;
      }
      async generateChain(context4, options2 = {}) {
        return new Promise((resolve2, reject) => {
          const script = path51.join(this.modelsPath, "adversary_agent.py");
          const args = [
            script,
            "--context",
            JSON.stringify(context4),
            "--temperature",
            String(options2.temperature ?? 0.7),
            "--persistence",
            String(options2.persistence ?? 3)
          ];
          const proc = spawn8(this.pythonPath, args);
          let output = "";
          let error = "";
          proc.stdout.on("data", (d) => {
            output += d.toString();
          });
          proc.stderr.on("data", (d) => {
            error += d.toString();
          });
          proc.on("close", (code) => {
            if (code === 0) {
              try {
                const data = JSON.parse(output);
                resolve2(data);
              } catch (err) {
                reject(err);
              }
            } else {
              reject(new Error(error || "adversary agent failed"));
            }
          });
          proc.on("error", (err) => reject(err));
        });
      }
    };
    AdversaryAgentService_default = AdversaryAgentService;
  }
});

// src/routes/ai.ts
var ai_exports = {};
__export(ai_exports, {
  default: () => ai_default
});
import express56 from "express";
import { body as body2, validationResult as validationResult2 } from "express-validator";
import pino69 from "pino";
import { Queue as Queue3, Worker as Worker3 } from "bullmq";
import { Pool as Pool11 } from "pg";
import { randomUUID as randomUUID74 } from "crypto";
function generateScaffoldSentiment(text) {
  const positiveWords = [
    "good",
    "great",
    "excellent",
    "positive",
    "happy",
    "success"
  ];
  const negativeWords = [
    "bad",
    "terrible",
    "poor",
    "negative",
    "sad",
    "failure"
  ];
  const textLower = text.toLowerCase();
  const positiveCount = positiveWords.filter(
    (word) => textLower.includes(word)
  ).length;
  const negativeCount = negativeWords.filter(
    (word) => textLower.includes(word)
  ).length;
  let sentiment = "neutral";
  let confidence = 0.7;
  if (positiveCount > negativeCount) {
    sentiment = "positive";
    confidence = Math.min(0.6 + positiveCount * 0.1, 0.95);
  } else if (negativeCount > positiveCount) {
    sentiment = "negative";
    confidence = Math.min(0.6 + negativeCount * 0.1, 0.95);
  }
  return {
    sentiment,
    confidence: parseFloat(confidence.toFixed(3)),
    scores: {
      positive: sentiment === "positive" ? confidence : (1 - confidence) * 0.4,
      negative: sentiment === "negative" ? confidence : (1 - confidence) * 0.4,
      neutral: sentiment === "neutral" ? confidence : (1 - confidence) * 0.2
    },
    method: "scaffold"
  };
}
function generateScaffoldEntitySentiment(entityData) {
  const textFields = [];
  const fieldMap = {};
  if (entityData.description) {
    textFields.push(entityData.description);
    fieldMap[textFields.length - 1] = "description";
  }
  if (entityData.notes) {
    textFields.push(entityData.notes);
    fieldMap[textFields.length - 1] = "notes";
  }
  if (textFields.length === 0) {
    return {
      overall_sentiment: "neutral",
      overall_confidence: 0,
      field_sentiments: {},
      summary: "No text content found for analysis"
    };
  }
  const fieldSentiments = {};
  const sentimentScores = { positive: [], negative: [], neutral: [] };
  textFields.forEach((text, index) => {
    const result2 = generateScaffoldSentiment(text);
    const fieldName = fieldMap[index];
    fieldSentiments[fieldName] = result2;
    Object.entries(result2.scores).forEach(([sentiment, score]) => {
      sentimentScores[sentiment].push(score);
    });
  });
  const avgScores = {};
  Object.entries(sentimentScores).forEach(([sentiment, scores]) => {
    avgScores[sentiment] = scores.length > 0 ? scores.reduce((a, b) => a + b, 0) / scores.length : 0;
  });
  const overallSentiment = Object.entries(avgScores).reduce(
    (a, b) => avgScores[a[0]] > avgScores[b[0]] ? a : b
  )[0];
  const overallConfidence = avgScores[overallSentiment];
  return {
    overall_sentiment: overallSentiment,
    overall_confidence: parseFloat(overallConfidence.toFixed(3)),
    field_sentiments: fieldSentiments,
    summary: `Analyzed ${textFields.length} field(s). Overall sentiment: ${overallSentiment} (confidence: ${overallConfidence.toFixed(2)})`,
    analyzed_at: (/* @__PURE__ */ new Date()).toISOString()
  };
}
function generateScaffoldAISummary(entityId, entityData, includeContext) {
  const entityType = entityData?.type || "entity";
  const entityName = entityData?.name || entityId;
  const insights = [
    `${entityName} shows characteristics typical of ${entityType} entities`,
    "Scaffold analysis indicates normal behavior patterns",
    "No anomalies detected in current data set"
  ];
  const recommendations = [
    "Continue monitoring entity for changes",
    "Consider expanding data collection for deeper insights",
    "Review related entities for additional context"
  ];
  if (includeContext) {
    insights.push(
      "Context analysis would provide additional insights when real ML models are integrated"
    );
    recommendations.push(
      "Implement context-aware analysis for enhanced predictions"
    );
  }
  return {
    summary: `AI analysis of ${entityName}: This ${entityType} entity demonstrates standard patterns in the available data. Scaffold predictions suggest normal operational characteristics with no immediate concerns identified.`,
    insights,
    recommendations,
    confidence: 0.75,
    generatedBy: "scaffold-ai-v1",
    timestamp: (/* @__PURE__ */ new Date()).toISOString()
  };
}
var logger64, router103, redisClient3, connection2, videoAnalysisQueue, buildBackgroundKey, enforceBackgroundThrottle, feedbackQueue, dummyPgPool, extractionEngineConfig, extractionEngine, videoAnalysisWorker, aiRateLimit, localizedAIProtection, validatePredictLinks, validateSentiment, validateAISummary, validateExtractVideo, handleValidationErrors2, validateFeedback, validateDeceptionFeedback, adversaryService, ai_default;
var init_ai = __esm({
  "src/routes/ai.ts"() {
    "use strict";
    init_EntityLinkingService();
    init_auth4();
    init_ExtractionEngine();
    init_redis();
    init_AdversaryAgentService();
    init_MediaUploadService();
    init_rateLimit2();
    init_config();
    init_RateLimiter();
    init_residency_guard();
    logger64 = pino69();
    router103 = express56.Router();
    redisClient3 = getRedisClient2();
    if (!redisClient3) {
      throw new Error("Redis connection is required for AI queue rate limiting");
    }
    connection2 = redisClient3.duplicate ? redisClient3.duplicate({ maxRetriesPerRequest: null }) : redisClient3;
    videoAnalysisQueue = new Queue3("videoAnalysisQueue", {
      connection: connection2,
      limiter: {
        max: cfg.BACKGROUND_RATE_LIMIT_MAX_REQUESTS,
        duration: cfg.BACKGROUND_RATE_LIMIT_WINDOW_MS
      }
    });
    buildBackgroundKey = (req, scope) => {
      const user = req.user;
      if (user) return `user:${user.id || user.sub}:${scope}`;
      return `ip:${req.ip}:${scope}`;
    };
    enforceBackgroundThrottle = async (req, scope) => {
      const key = buildBackgroundKey(req, scope);
      const result2 = await rateLimiter.consume(
        key,
        1,
        cfg.BACKGROUND_RATE_LIMIT_MAX_REQUESTS,
        cfg.BACKGROUND_RATE_LIMIT_WINDOW_MS
      );
      if (!result2.allowed) {
        const retryAfterSeconds = Math.max(Math.ceil((result2.reset - Date.now()) / 1e3), 0);
        const error = new Error("Background rate limit exceeded");
        error.status = 429;
        error.retryAfter = retryAfterSeconds;
        throw error;
      }
    };
    feedbackQueue = new Queue3("aiFeedbackQueue", {
      connection: connection2,
      limiter: {
        max: cfg.BACKGROUND_RATE_LIMIT_MAX_REQUESTS,
        duration: cfg.BACKGROUND_RATE_LIMIT_WINDOW_MS
      }
    });
    dummyPgPool = new Pool11();
    extractionEngineConfig = {
      pythonPath: process.env.PYTHON_PATH || "python",
      // Ensure this is configured
      modelsPath: process.env.MODELS_PATH || "./models",
      // Ensure this is configured
      tempPath: process.env.TEMP_PATH || "./temp",
      // Ensure this is configured
      maxConcurrentJobs: 5,
      enableGPU: process.env.ENABLE_GPU === "true",
      allowedPaths: [
        process.env.MEDIA_UPLOAD_PATH || "/tmp/intelgraph/uploads",
        process.env.TEMP_PATH || "./temp"
      ]
    };
    extractionEngine = new ExtractionEngine(
      extractionEngineConfig,
      dummyPgPool
    );
    videoAnalysisWorker = new Worker3(
      "videoAnalysisQueue",
      async (job) => {
        const { jobId, mediaPath, mediaType, extractionMethods, options: options2 } = job.data;
        logger64.info(`Processing video analysis job: ${jobId}`);
        try {
          const results = await extractionEngine.processExtraction({
            jobId,
            mediaPath,
            mediaType,
            extractionMethods,
            options: options2,
            mediaSourceId: options2.mediaSourceId || "unknown"
            // Ensure mediaSourceId is passed
          });
          logger64.info(`Video analysis job ${jobId} completed successfully.`);
          return { status: "completed", results };
        } catch (error) {
          logger64.error(
            `Video analysis job ${jobId} failed: ${error.message}`,
            error
          );
          throw new Error(`Video analysis failed: ${error.message}`);
        }
      },
      { connection: connection2 }
    );
    videoAnalysisWorker.on("completed", (job) => {
      logger64.info(`Job ${job.id} has completed!`);
    });
    videoAnalysisWorker.on("failed", (job, err) => {
      logger64.error(`Job ${job?.id} has failed with error ${err.message}`);
    });
    aiRateLimit = createRateLimiter2("AI" /* AI */);
    router103.use(aiRateLimit);
    router103.use(requirePermission("ai:request"));
    localizedAIProtection = async (req, res, next) => {
      try {
        const tenantId = req.user?.tenantId || req.tenantId;
        if (!tenantId) return next();
        const guard = ResidencyGuard.getInstance();
        const isAllowed = await guard.validateFeatureAccess(tenantId, "aiFeatures");
        if (!isAllowed) {
          logger64.warn(`AI feature access blocked for tenant ${tenantId} due to regional residency policy.`);
          return res.status(403).json({
            error: "FeatureRestricted",
            message: "AI features are not available in your region due to data residency or privacy regulations."
          });
        }
        next();
      } catch (err) {
        next(err);
      }
    };
    router103.use(localizedAIProtection);
    validatePredictLinks = [
      body2("entityId").isString().notEmpty().withMessage("entityId is required"),
      body2("topK").optional().isInt({ min: 1, max: 50 }).withMessage("topK must be between 1 and 50")
    ];
    validateSentiment = [
      body2("entityId").optional().isString().withMessage("entityId must be a string"),
      body2("text").optional().isString().withMessage("text must be a string"),
      body2("entityData").optional().isObject().withMessage("entityData must be an object")
    ];
    validateAISummary = [
      body2("entityId").isString().notEmpty().withMessage("entityId is required"),
      body2("entityData").optional().isObject().withMessage("entityData must be an object"),
      body2("includeContext").optional().isBoolean().withMessage("includeContext must be boolean")
    ];
    validateExtractVideo = [
      body2("mediaPath").isString().notEmpty().withMessage("mediaPath is required"),
      body2("mediaType").isIn(["VIDEO" /* VIDEO */]).withMessage("mediaType must be VIDEO"),
      body2("extractionMethods").isArray().withMessage("extractionMethods must be an array"),
      body2("options").isObject().optional().withMessage("options must be an object")
    ];
    handleValidationErrors2 = (req, res, next) => {
      const errors = validationResult2(req);
      if (!errors.isEmpty()) {
        return res.status(400).json({
          error: "Validation failed",
          details: errors.array()
        });
      }
      next();
    };
    router103.post(
      "/predict-links",
      validatePredictLinks,
      handleValidationErrors2,
      async (req, res) => {
        try {
          const startTime = Date.now();
          const { entityId, topK = 10, investigationId } = req.body;
          logger64.info(`Link prediction request for entity: ${entityId}`);
          const result2 = await EntityLinkingService.suggestLinksForEntity(
            entityId,
            {
              limit: topK,
              investigationId,
              token: req.headers.authorization?.replace("Bearer ", "")
            }
          );
          const responseTime = Date.now() - startTime;
          if (!result2.success) {
            return res.status(500).json({
              error: "Link prediction failed",
              message: result2.error || result2.message || "Unknown error"
            });
          }
          res.json({
            success: true,
            entityId,
            jobId: result2.jobId,
            taskId: result2.taskId,
            candidates: result2.candidates,
            metadata: {
              model: result2.modelName || "default_link_predictor",
              topK,
              executionTime: responseTime
            }
          });
        } catch (error) {
          logger64.error(
            `Error in link prediction: ${error instanceof Error ? error.message : "Unknown error"}`
          );
          res.status(500).json({
            error: "Link prediction failed",
            message: "Internal server error during link prediction"
          });
        }
      }
    );
    router103.post(
      "/analyze-sentiment",
      validateSentiment,
      handleValidationErrors2,
      async (req, res) => {
        try {
          const startTime = Date.now();
          const { entityId, text, entityData } = req.body;
          logger64.info(
            `Sentiment analysis request${entityId ? ` for entity: ${entityId}` : ""}`
          );
          let sentimentResult;
          if (text) {
            sentimentResult = generateScaffoldSentiment(text);
          } else if (entityData) {
            sentimentResult = generateScaffoldEntitySentiment(entityData);
          } else {
            return res.status(400).json({
              error: "Invalid request",
              message: "Either text or entityData must be provided"
            });
          }
          const responseTime = Date.now() - startTime;
          logger64.info(`Sentiment analysis completed in ${responseTime}ms`);
          res.json({
            success: true,
            entityId,
            sentiment: sentimentResult,
            metadata: {
              model: "scaffold-sentiment-v1",
              executionTime: responseTime,
              analyzedFields: sentimentResult.field_sentiments ? Object.keys(sentimentResult.field_sentiments).length : 1
            }
          });
        } catch (error) {
          logger64.error(
            `Error in sentiment analysis: ${error instanceof Error ? error.message : "Unknown error"}`
          );
          res.status(500).json({
            error: "Sentiment analysis failed",
            message: "Internal server error during sentiment analysis"
          });
        }
      }
    );
    router103.post(
      "/generate-summary",
      validateAISummary,
      handleValidationErrors2,
      async (req, res) => {
        try {
          const startTime = Date.now();
          const { entityId, entityData, includeContext = true } = req.body;
          logger64.info(`AI summary generation request for entity: ${entityId}`);
          const summary = generateScaffoldAISummary(
            entityId,
            entityData,
            includeContext
          );
          const responseTime = Date.now() - startTime;
          logger64.info(
            `AI summary generated in ${responseTime}ms for entity ${entityId}`
          );
          res.json({
            success: true,
            entityId,
            summary,
            metadata: {
              model: "scaffold-llm-v1",
              includeContext,
              executionTime: responseTime,
              generatedAt: (/* @__PURE__ */ new Date()).toISOString()
            }
          });
        } catch (error) {
          logger64.error(
            `Error in AI summary generation: ${error instanceof Error ? error.message : "Unknown error"}`
          );
          res.status(500).json({
            error: "AI summary generation failed",
            message: "Internal server error during summary generation"
          });
        }
      }
    );
    router103.get("/models/status", async (req, res) => {
      try {
        const modelStatus = {
          linkPrediction: {
            status: "healthy",
            model: "scaffold-gnn-v1",
            lastUpdated: (/* @__PURE__ */ new Date()).toISOString(),
            version: "1.0.0-scaffold"
          },
          sentimentAnalysis: {
            status: "healthy",
            model: "scaffold-sentiment-v1",
            lastUpdated: (/* @__PURE__ */ new Date()).toISOString(),
            version: "1.0.0-scaffold"
          },
          textGeneration: {
            status: "healthy",
            model: "scaffold-llm-v1",
            lastUpdated: (/* @__PURE__ */ new Date()).toISOString(),
            version: "1.0.0-scaffold"
          }
        };
        res.json({
          success: true,
          models: modelStatus,
          overview: {
            totalModels: Object.keys(modelStatus).length,
            healthyModels: Object.values(modelStatus).filter(
              (m) => m.status === "healthy"
            ).length,
            lastChecked: (/* @__PURE__ */ new Date()).toISOString()
          }
        });
      } catch (error) {
        logger64.error(
          `Error checking model status: ${error instanceof Error ? error.message : "Unknown error"}`
        );
        res.status(500).json({
          error: "Model status check failed",
          message: "Internal server error during model status check"
        });
      }
    });
    router103.get("/capabilities", async (req, res) => {
      try {
        const capabilities = {
          linkPrediction: {
            description: "Predict potential relationships between entities using graph neural networks",
            parameters: {
              topK: { type: "integer", min: 1, max: 50, default: 10 },
              threshold: { type: "float", min: 0, max: 1, default: 0.5 }
            },
            supportedEntityTypes: [
              "person",
              "organization",
              "event",
              "location",
              "document"
            ],
            maxEntities: 1e3
          },
          sentimentAnalysis: {
            description: "Analyze sentiment of text content and entity descriptions",
            parameters: {
              language: { type: "string", options: ["en"], default: "en" }
            },
            supportedFields: ["description", "notes", "comments", "content"],
            maxTextLength: 512
          },
          textGeneration: {
            description: "Generate AI-powered insights and summaries for entities",
            parameters: {
              includeContext: { type: "boolean", default: true },
              maxLength: { type: "integer", min: 50, max: 1e3, default: 200 }
            },
            supportedFormats: ["summary", "insights", "recommendations"]
          }
        };
        res.json({
          success: true,
          capabilities,
          version: "1.0.0-scaffold",
          lastUpdated: (/* @__PURE__ */ new Date()).toISOString()
        });
      } catch (error) {
        logger64.error(
          `Error retrieving capabilities: ${error instanceof Error ? error.message : "Unknown error"}`
        );
        res.status(500).json({
          error: "Failed to retrieve capabilities",
          message: "Internal server error"
        });
      }
    });
    router103.post(
      "/extract-video",
      validateExtractVideo,
      handleValidationErrors2,
      async (req, res) => {
        const { mediaPath, mediaType, extractionMethods, options: options2 } = req.body;
        const jobId = randomUUID74();
        try {
          await enforceBackgroundThrottle(req, "video-analysis");
          await videoAnalysisQueue.add(
            "video-analysis-job",
            {
              jobId,
              mediaPath,
              mediaType,
              extractionMethods,
              options: options2
            },
            { jobId }
          );
          logger64.info(`Video analysis job ${jobId} submitted for ${mediaPath}`);
          res.status(202).json({
            success: true,
            jobId,
            message: "Video analysis job submitted successfully. Use /api/ai/job-status/:jobId to track progress."
          });
        } catch (error) {
          logger64.error(
            `Error submitting video analysis job: ${error instanceof Error ? error.message : String(error)}`,
            error
          );
          if (error && typeof error === "object" && "status" in error && error.status === 429) {
            res.status(429).json({
              error: "Too many video analysis requests",
              retryAfter: "retryAfter" in error ? error.retryAfter : void 0
            });
            return;
          }
          res.status(500).json({
            error: "Failed to submit video analysis job",
            message: error instanceof Error ? error.message : String(error)
          });
        }
      }
    );
    router103.get("/job-status/:jobId", async (req, res) => {
      const { jobId } = req.params;
      try {
        const job = await videoAnalysisQueue.getJob(jobId);
        if (!job) {
          return res.status(404).json({
            error: "Job not found",
            message: `Job with ID ${jobId} does not exist.`
          });
        }
        const state = await job.getState();
        const result2 = job.returnvalue;
        const failedReason = job.failedReason;
        res.json({
          success: true,
          jobId,
          status: state,
          progress: job.progress,
          result: state === "completed" ? result2 : void 0,
          error: state === "failed" ? failedReason : void 0,
          createdAt: new Date(job.timestamp).toISOString(),
          processedAt: job.finishedOn ? new Date(job.finishedOn).toISOString() : void 0
        });
      } catch (error) {
        logger64.error(
          `Error getting job status for ${jobId}: ${error instanceof Error ? error.message : String(error)}`,
          error
        );
        res.status(500).json({
          error: "Failed to retrieve job status",
          message: "Internal server error"
        });
      }
    });
    validateFeedback = [
      body2("insight").isObject().notEmpty().withMessage("insight object is required"),
      body2("feedbackType").isIn(["accept", "reject", "flag"]).withMessage("feedbackType must be 'accept', 'reject', or 'flag'"),
      body2("user").isString().notEmpty().withMessage("user is required"),
      body2("timestamp").isISO8601().withMessage("timestamp must be a valid ISO 8601 date string"),
      body2("originalPrediction").isObject().notEmpty().withMessage("originalPrediction object is required")
    ];
    validateDeceptionFeedback = [
      body2("text").isString().notEmpty().withMessage("text is required"),
      body2("label").isIn(["false_positive", "false_negative"]).withMessage("label must be 'false_positive' or 'false_negative'"),
      body2("user").isString().notEmpty().withMessage("user is required"),
      body2("timestamp").isISO8601().withMessage("timestamp must be a valid ISO 8601 date string"),
      body2("deceptionScore").optional().isFloat({ min: 0, max: 1 }).withMessage("deceptionScore must be between 0 and 1")
    ];
    router103.post(
      "/feedback",
      validateFeedback,
      handleValidationErrors2,
      async (req, res) => {
        try {
          const { insight, feedbackType, user, timestamp, originalPrediction } = req.body;
          logger64.info("AI Feedback received:", {
            insight,
            feedbackType,
            user,
            timestamp,
            originalPrediction
          });
          await enforceBackgroundThrottle(req, "ai-feedback");
          await feedbackQueue.add("logFeedback", {
            insight,
            feedbackType,
            user,
            timestamp,
            originalPrediction
          });
          res.status(200).json({
            success: true,
            message: "Feedback received successfully and queued for processing"
          });
        } catch (error) {
          logger64.error(
            `Error processing feedback: ${error instanceof Error ? error.message : String(error)}`
          );
          if (error && typeof error === "object" && "status" in error && error.status === 429) {
            res.status(429).json({
              error: "Too many feedback events submitted",
              retryAfter: "retryAfter" in error ? error.retryAfter : void 0
            });
            return;
          }
          res.status(500).json({
            error: "Failed to process feedback",
            message: "Internal server error"
          });
        }
      }
    );
    router103.post(
      "/feedback/deception",
      validateDeceptionFeedback,
      handleValidationErrors2,
      async (req, res) => {
        try {
          const { text, label, user, timestamp, deceptionScore } = req.body;
          await enforceBackgroundThrottle(req, "ai-feedback");
          await feedbackQueue.add("logDeceptionFeedback", {
            insight: { text, deceptionScore },
            feedbackType: label,
            user,
            timestamp,
            originalPrediction: { deceptionScore }
          });
          res.status(200).json({ success: true, message: "Feedback received" });
        } catch (error) {
          logger64.error(
            `Error processing deception feedback: ${error instanceof Error ? error.message : String(error)}`
          );
          if (error && typeof error === "object" && "status" in error && error.status === 429) {
            res.status(429).json({
              error: "Too many deception feedback events submitted",
              retryAfter: "retryAfter" in error ? error.retryAfter : void 0
            });
            return;
          }
          res.status(500).json({
            error: "Failed to process feedback",
            message: "Internal server error"
          });
        }
      }
    );
    adversaryService = new AdversaryAgentService_default();
    router103.post("/adversary/generate", async (req, res) => {
      const { context: context4, temperature, persistence } = req.body || {};
      if (!context4) {
        return res.status(400).json({ error: "context is required" });
      }
      try {
        const chain = await adversaryService.generateChain(context4, {
          temperature,
          persistence
        });
        res.json({ ttps: chain });
      } catch (err) {
        res.status(500).json({ error: err instanceof Error ? err.message : String(err) });
      }
    });
    ai_default = router103;
  }
});

// src/services/lifecycle-listeners.ts
async function handleRunEvent(event) {
  console.log(`Handling run event: ${event.type} for run ${event.runId}`);
  try {
    if (event.metadata) {
      const ticket = extractTicketFromMetadata(event.metadata);
      if (ticket) {
        await addTicketRunLink(ticket, event.runId, {
          event_type: event.type,
          timestamp: event.timestamp.toISOString(),
          ...event.metadata
        });
        console.log(
          `Successfully linked ticket ${ticket.provider}:${ticket.externalId} to run ${event.runId} via ${event.type} event`
        );
      } else {
        console.log(
          `No ticket information found in run ${event.runId} metadata`
        );
      }
    }
  } catch (error) {
    console.error(`Failed to handle run event for ${event.runId}:`, error);
  }
}
async function handleDeploymentEvent(event) {
  console.log(
    `Handling deployment event: ${event.type} for deployment ${event.deploymentId}`
  );
  try {
    if (event.metadata) {
      const ticket = extractTicketFromMetadata(event.metadata);
      if (ticket) {
        await addTicketDeploymentLink(ticket, event.deploymentId, {
          event_type: event.type,
          timestamp: event.timestamp.toISOString(),
          ...event.metadata
        });
        console.log(
          `Successfully linked ticket ${ticket.provider}:${ticket.externalId} to deployment ${event.deploymentId} via ${event.type} event`
        );
      } else {
        console.log(
          `No ticket information found in deployment ${event.deploymentId} metadata`
        );
      }
    }
  } catch (error) {
    console.error(
      `Failed to handle deployment event for ${event.deploymentId}:`,
      error
    );
  }
}
var LifecycleManager;
var init_lifecycle_listeners = __esm({
  "src/services/lifecycle-listeners.ts"() {
    "use strict";
    init_ticket_links2();
    LifecycleManager = class {
      static runListeners = [];
      static deploymentListeners = [];
      static onRunEvent(listener) {
        this.runListeners.push(listener);
      }
      static onDeploymentEvent(listener) {
        this.deploymentListeners.push(listener);
      }
      static async emitRunEvent(event) {
        for (const listener of this.runListeners) {
          try {
            await listener(event);
          } catch (error) {
            console.error("Run event listener error:", error);
          }
        }
      }
      static async emitDeploymentEvent(event) {
        for (const listener of this.deploymentListeners) {
          try {
            await listener(event);
          } catch (error) {
            console.error("Deployment event listener error:", error);
          }
        }
      }
    };
    LifecycleManager.onRunEvent(handleRunEvent);
    LifecycleManager.onDeploymentEvent(handleDeploymentEvent);
  }
});

// src/webhooks/webhook.queue.ts
import { Queue as Queue4 } from "bullmq";
import IORedis from "ioredis";
var connection3, webhookQueue;
var init_webhook_queue = __esm({
  "src/webhooks/webhook.queue.ts"() {
    "use strict";
    connection3 = new IORedis(process.env.REDIS_URL || "redis://localhost:6379", {
      maxRetriesPerRequest: null
    });
    webhookQueue = new Queue4("webhooks", {
      connection: connection3,
      defaultJobOptions: {
        attempts: 5,
        backoff: {
          type: "exponential",
          delay: 1e3
        },
        removeOnComplete: true,
        removeOnFail: false
        // Keep failed jobs for inspection
      }
    });
  }
});

// src/webhooks/webhook.service.ts
import crypto46 from "crypto";
import { z as z50 } from "zod/v4";
var CreateWebhookSchema, UpdateWebhookSchema, WebhookService, webhookService;
var init_webhook_service = __esm({
  "src/webhooks/webhook.service.ts"() {
    "use strict";
    init_pg();
    init_webhook_queue();
    CreateWebhookSchema = z50.object({
      url: z50.string().url(),
      event_types: z50.array(z50.string()).min(1),
      secret: z50.string().optional()
      // If not provided, one will be generated
    });
    UpdateWebhookSchema = z50.object({
      url: z50.string().url().optional(),
      event_types: z50.array(z50.string()).min(1).optional(),
      is_active: z50.boolean().optional(),
      secret: z50.string().optional()
    });
    WebhookService = class {
      /**
       * Build signing headers with timestamped HMAC for payload integrity
       */
      generateSignature(payload, secret, timestamp) {
        const ts = timestamp || Date.now();
        const payloadString = JSON.stringify(payload);
        const dataToSign = `${ts}.${payloadString}`;
        const hmac = crypto46.createHmac("sha256", secret);
        hmac.update(dataToSign);
        const digest = hmac.digest("hex");
        return {
          signature: `t=${ts},v1=${digest}`,
          timestamp: ts
        };
      }
      /**
       * Create a new webhook registration
       */
      async createWebhook(tenantId, input) {
        const secret = input.secret || crypto46.randomBytes(32).toString("hex");
        const result2 = await pg.oneOrNone(
          `INSERT INTO webhooks (tenant_id, url, secret, event_types)
       VALUES ($1, $2, $3, $4)
       RETURNING *`,
          [tenantId, input.url, secret, input.event_types],
          { tenantId }
        );
        return result2;
      }
      /**
       * Get webhooks for a tenant
       */
      async getWebhooks(tenantId) {
        return pg.many(
          `SELECT * FROM webhooks WHERE tenant_id = $1 ORDER BY created_at DESC`,
          [tenantId],
          { tenantId }
        );
      }
      /**
       * Get a specific webhook
       */
      async getWebhook(tenantId, webhookId) {
        return pg.oneOrNone(
          `SELECT * FROM webhooks WHERE id = $1 AND tenant_id = $2`,
          [webhookId, tenantId],
          { tenantId }
        );
      }
      /**
       * Update a webhook
       */
      async updateWebhook(tenantId, webhookId, input) {
        const updates = [];
        const values = [];
        let idx = 1;
        if (input.url) {
          updates.push(`url = $${idx++}`);
          values.push(input.url);
        }
        if (input.event_types) {
          updates.push(`event_types = $${idx++}`);
          values.push(input.event_types);
        }
        if (input.is_active !== void 0) {
          updates.push(`is_active = $${idx++}`);
          values.push(input.is_active);
        }
        if (input.secret) {
          updates.push(`secret = $${idx++}`);
          values.push(input.secret);
        }
        if (updates.length === 0) {
          return this.getWebhook(tenantId, webhookId);
        }
        updates.push(`updated_at = NOW()`);
        values.push(webhookId);
        values.push(tenantId);
        const query3 = `
      UPDATE webhooks
      SET ${updates.join(", ")}
      WHERE id = $${idx} AND tenant_id = $${idx + 1}
      RETURNING *
    `;
        return pg.oneOrNone(query3, values, { tenantId });
      }
      /**
       * Delete a webhook
       */
      async deleteWebhook(tenantId, webhookId) {
        const result2 = await pg.oneOrNone(
          `DELETE FROM webhooks WHERE id = $1 AND tenant_id = $2 RETURNING id`,
          [webhookId, tenantId],
          { tenantId }
        );
        return !!result2;
      }
      /**
       * Trigger a webhook event for all subscribers
       */
      async triggerEvent(tenantId, eventType, payload) {
        const webhooks = await pg.many(
          `SELECT * FROM webhooks
       WHERE tenant_id = $1
       AND is_active = true
       AND event_types @> $2`,
          [tenantId, [eventType]],
          { tenantId }
        );
        if (webhooks.length === 0) {
          return;
        }
        for (const webhook of webhooks) {
          const delivery = await pg.oneOrNone(
            `INSERT INTO webhook_deliveries (webhook_id, tenant_id, event_type, payload)
         VALUES ($1, $2, $3, $4)
         RETURNING *`,
            [webhook.id, tenantId, eventType, payload],
            { tenantId }
          );
          if (delivery) {
            await webhookQueue.add("deliver-webhook", {
              deliveryId: delivery.id,
              webhookId: webhook.id,
              tenantId,
              url: webhook.url,
              secret: webhook.secret,
              payload,
              eventType,
              triggerType: "event"
            });
          }
        }
      }
      /**
       * Trigger a single webhook, useful for testing or manual resend
       */
      async triggerWebhook(tenantId, webhookId, eventType, payload, triggerType = "event") {
        const webhook = await this.getWebhook(tenantId, webhookId);
        if (!webhook || !webhook.is_active) {
          return null;
        }
        const delivery = await pg.oneOrNone(
          `INSERT INTO webhook_deliveries (webhook_id, tenant_id, event_type, payload)
       VALUES ($1, $2, $3, $4)
       RETURNING *`,
          [webhookId, tenantId, eventType, payload],
          { tenantId }
        );
        if (!delivery) {
          return null;
        }
        await webhookQueue.add("deliver-webhook", {
          deliveryId: delivery.id,
          webhookId,
          tenantId,
          url: webhook.url,
          secret: webhook.secret,
          payload,
          eventType,
          triggerType
        });
        return delivery;
      }
      /**
       * Get delivery history for a webhook
       */
      async getDeliveries(tenantId, webhookId, limit = 20, offset = 0) {
        const webhook = await this.getWebhook(tenantId, webhookId);
        if (!webhook) {
          throw new Error("Webhook not found");
        }
        return pg.many(
          `SELECT * FROM webhook_deliveries
       WHERE webhook_id = $1
       ORDER BY created_at DESC
       LIMIT $2 OFFSET $3`,
          [webhookId, limit, offset],
          { tenantId }
        );
      }
      /**
       * Delivery attempt history for observability and dashboards
       */
      async getDeliveryAttempts(tenantId, webhookId, deliveryId, limit = 50) {
        const webhook = await this.getWebhook(tenantId, webhookId);
        if (!webhook) {
          throw new Error("Webhook not found");
        }
        const params = [webhookId, limit];
        const deliveryFilter = deliveryId ? "AND delivery_id = $3" : "";
        if (deliveryId) {
          params.push(deliveryId);
        }
        return pg.many(
          `SELECT *
       FROM webhook_delivery_attempts
       WHERE webhook_id = $1
       ${deliveryFilter}
       ORDER BY created_at DESC
       LIMIT $2`,
          params,
          { tenantId }
        );
      }
    };
    webhookService = new WebhookService();
  }
});

// src/routes/webhooks.ts
var webhooks_exports = {};
__export(webhooks_exports, {
  default: () => webhooks_default
});
import { Router as Router53 } from "express";
import { body as body3, validationResult as validationResult3 } from "express-validator";
import { SpanKind as SpanKind5, SpanStatusCode as SpanStatusCode6 } from "@opentelemetry/api";
import crypto47 from "crypto";
var router104, verifyGitHubSignature, verifyJiraSecret, verifyLifecycleSecret, validate2, getTenantId5, webhooks_default;
var init_webhooks = __esm({
  "src/routes/webhooks.ts"() {
    "use strict";
    init_ticket_links2();
    init_lifecycle_listeners();
    init_webhook_service();
    init_observability();
    router104 = Router53();
    verifyGitHubSignature = (req, res, next) => {
      const signature = req.headers["x-hub-signature-256"];
      const secret = process.env.GITHUB_WEBHOOK_SECRET;
      if (!secret) {
        if (process.env.NODE_ENV === "production") {
          logger33.error("GITHUB_WEBHOOK_SECRET is missing in production");
          return res.status(500).json({ error: "Server configuration error" });
        }
        return next();
      }
      if (!signature) {
        return res.status(401).json({ error: "Missing X-Hub-Signature-256 header" });
      }
      const rawBody = req.rawBody;
      if (!rawBody) {
        logger33.warn("Webhook received without rawBody (express.json verify hook missing?)");
        return res.status(400).json({ error: "Payload verification failed" });
      }
      const hmac = crypto47.createHmac("sha256", secret);
      const digest = "sha256=" + hmac.update(rawBody).digest("hex");
      const sigString = Array.isArray(signature) ? signature[0] : signature;
      const sigBuffer = Buffer.from(sigString);
      const digestBuffer = Buffer.from(digest);
      try {
        if (sigBuffer.length !== digestBuffer.length || !crypto47.timingSafeEqual(digestBuffer, sigBuffer)) {
          return res.status(401).json({ error: "Invalid signature" });
        }
      } catch (error) {
        return res.status(401).json({ error: "Invalid signature format" });
      }
      next();
    };
    verifyJiraSecret = (req, res, next) => {
      const secret = process.env.JIRA_WEBHOOK_SECRET;
      if (!secret) {
        if (process.env.NODE_ENV === "production") {
          logger33.error("JIRA_WEBHOOK_SECRET is missing in production. Blocking request.");
          return res.status(500).json({ error: "Server configuration error" });
        }
        return next();
      }
      const incomingSecret = req.headers["x-webhook-secret"] || req.query.secret;
      if (!incomingSecret || incomingSecret !== secret) {
        logger33.warn("Jira webhook rejected: Invalid secret");
        return res.status(401).json({ error: "Unauthorized: Invalid webhook secret" });
      }
      next();
    };
    verifyLifecycleSecret = (req, res, next) => {
      const secret = process.env.LIFECYCLE_WEBHOOK_SECRET;
      if (!secret) {
        if (process.env.NODE_ENV === "production") {
          logger33.error("LIFECYCLE_WEBHOOK_SECRET is missing in production. Blocking request.");
          return res.status(500).json({ error: "Server configuration error" });
        }
        return next();
      }
      const incomingSecret = req.headers["x-lifecycle-secret"] || req.headers["x-webhook-secret"];
      if (!incomingSecret || incomingSecret !== secret) {
        logger33.warn("Lifecycle webhook rejected: Invalid secret");
        return res.status(401).json({ error: "Unauthorized: Invalid webhook secret" });
      }
      next();
    };
    validate2 = (schema2) => (req, res, next) => {
      try {
        req.body = schema2.parse(req.body);
        next();
      } catch (error) {
        res.status(400).json({ error: "Invalid input", details: error });
      }
    };
    getTenantId5 = (req) => {
      if (req.user?.tenantId || req.user?.tenant_id) {
        return req.user.tenantId || req.user.tenant_id;
      }
      throw new Error("Tenant ID not found in authenticated session");
    };
    router104.post("/", validate2(CreateWebhookSchema), async (req, res) => {
      try {
        const tenantId = getTenantId5(req);
        const webhook = await webhookService.createWebhook(tenantId, req.body);
        res.status(201).json(webhook);
      } catch (error) {
        res.status(500).json({ error: error.message });
      }
    });
    router104.get("/", async (req, res) => {
      try {
        const tenantId = getTenantId5(req);
        const webhooks = await webhookService.getWebhooks(tenantId);
        res.json(webhooks);
      } catch (error) {
        res.status(500).json({ error: error.message });
      }
    });
    router104.get("/:id", async (req, res) => {
      try {
        const tenantId = getTenantId5(req);
        const webhook = await webhookService.getWebhook(tenantId, req.params.id);
        if (!webhook) {
          return res.status(404).json({ error: "Webhook not found" });
        }
        res.json(webhook);
      } catch (error) {
        res.status(500).json({ error: error.message });
      }
    });
    router104.patch("/:id", validate2(UpdateWebhookSchema), async (req, res) => {
      try {
        const tenantId = getTenantId5(req);
        const webhook = await webhookService.updateWebhook(tenantId, req.params.id, req.body);
        if (!webhook) {
          return res.status(404).json({ error: "Webhook not found" });
        }
        res.json(webhook);
      } catch (error) {
        res.status(500).json({ error: error.message });
      }
    });
    router104.delete("/:id", async (req, res) => {
      try {
        const tenantId = getTenantId5(req);
        const success = await webhookService.deleteWebhook(tenantId, req.params.id);
        if (!success) {
          return res.status(404).json({ error: "Webhook not found" });
        }
        res.status(204).send();
      } catch (error) {
        res.status(500).json({ error: error.message });
      }
    });
    router104.get("/:id/deliveries", async (req, res) => {
      try {
        const tenantId = getTenantId5(req);
        const limit = parseInt(req.query.limit) || 20;
        const offset = parseInt(req.query.offset) || 0;
        const deliveries = await webhookService.getDeliveries(tenantId, req.params.id, limit, offset);
        res.json(deliveries);
      } catch (error) {
        res.status(500).json({ error: error.message });
      }
    });
    router104.post("/trigger-test", async (req, res) => {
      try {
        const tenantId = getTenantId5(req);
        const { eventType, payload } = req.body;
        if (!eventType || !payload) {
          return res.status(400).json({ error: "eventType and payload required" });
        }
        await webhookService.triggerEvent(tenantId, eventType, payload);
        res.json({ message: "Event triggered" });
      } catch (error) {
        res.status(500).json({ error: error.message });
      }
    });
    router104.post(
      "/github",
      verifyGitHubSignature,
      body3("action").isString(),
      body3("number").optional().isNumeric(),
      body3("pull_request").optional().isObject(),
      body3("issue").optional().isObject(),
      async (req, res) => {
        return tracer6.trace("webhook.receive", async (span) => {
          const errors = validationResult3(req);
          if (!errors.isEmpty()) {
            span.setStatus({ code: SpanStatusCode6.ERROR, message: "Validation failed" });
            return res.status(400).json({ errors: errors.array() });
          }
          span.setAttributes({
            "webhook.provider": "github",
            "webhook.event": req.body.action
          });
          try {
            const { action, pull_request, issue, repository } = req.body;
            logger33.info(`Received GitHub webhook: ${action}`, {
              pr: pull_request?.number,
              issue: issue?.number,
              repo: repository?.name,
              provider: "github"
            });
            metrics4.incrementCounter("summit_webhook_deliveries_total", { status: "received", provider: "github" });
            if (pull_request && (action === "opened" || action === "closed" || action === "merged")) {
              const prUrl = pull_request.html_url;
              const prBody = pull_request.body || "";
              const ticket = extractTicketFromPR(prUrl, prBody);
              if (ticket) {
                const runIdMatch = prBody.match(/runId[:\s]*([a-f0-9-]{36})/i);
                const deploymentIdMatch = prBody.match(
                  /deploymentId[:\s]*([a-f0-9-]{36})/i
                );
                const metadata = {
                  pr_url: prUrl,
                  pr_number: pull_request.number,
                  pr_title: pull_request.title,
                  pr_body: prBody,
                  pr_action: action,
                  repository: repository?.full_name,
                  author: pull_request.user?.login
                };
                if (runIdMatch) {
                  await addTicketRunLink(ticket, runIdMatch[1], metadata);
                }
                if (deploymentIdMatch) {
                  await addTicketDeploymentLink(
                    ticket,
                    deploymentIdMatch[1],
                    metadata
                  );
                }
              }
            }
            if (issue && (action === "opened" || action === "closed")) {
              console.log(`GitHub issue ${action}: #${issue.number}`);
            }
            res.status(200).json({ status: "processed" });
          } catch (error) {
            logger33.error("GitHub webhook error:", { error: error.message });
            metrics4.incrementCounter("summit_webhook_deliveries_total", { status: "failed", provider: "github" });
            span.recordException(error);
            span.setStatus({ code: SpanStatusCode6.ERROR, message: error.message });
            res.status(500).json({ error: "Webhook processing failed" });
          }
        }, {
          kind: SpanKind5.SERVER,
          attributes: {
            "webhook.provider": "github"
          }
        });
      }
    );
    router104.post(
      "/jira",
      verifyJiraSecret,
      body3("webhookEvent").isString(),
      body3("issue").optional().isObject(),
      async (req, res) => {
        const errors = validationResult3(req);
        if (!errors.isEmpty()) {
          return res.status(400).json({ errors: errors.array() });
        }
        try {
          const { webhookEvent, issue, changelog } = req.body;
          console.log(`Received Jira webhook: ${webhookEvent}`, {
            issue: issue?.key,
            status: issue?.fields?.status?.name
          });
          if (issue && (webhookEvent === "jira:issue_created" || webhookEvent === "jira:issue_updated")) {
            const ticket = {
              provider: "jira",
              externalId: issue.key
            };
            const description = issue.fields?.description || "";
            const runIdMatch = description.match(/runId[:\s]*([a-f0-9-]{36})/i);
            const deploymentIdMatch = description.match(
              /deploymentId[:\s]*([a-f0-9-]{36})/i
            );
            const metadata = {
              issue_key: issue.key,
              issue_summary: issue.fields?.summary,
              issue_description: description,
              issue_status: issue.fields?.status?.name,
              webhook_event: webhookEvent,
              assignee: issue.fields?.assignee?.displayName,
              reporter: issue.fields?.reporter?.displayName
            };
            if (runIdMatch) {
              await addTicketRunLink(ticket, runIdMatch[1], metadata);
            }
            if (deploymentIdMatch) {
              await addTicketDeploymentLink(ticket, deploymentIdMatch[1], metadata);
            }
          }
          res.status(200).json({ status: "processed" });
        } catch (error) {
          console.error("Jira webhook error:", error);
          res.status(500).json({ error: "Webhook processing failed" });
        }
      }
    );
    router104.post(
      "/lifecycle",
      verifyLifecycleSecret,
      body3("event_type").isIn([
        "run_created",
        "run_completed",
        "run_failed",
        "deployment_started",
        "deployment_completed",
        "deployment_failed"
      ]),
      body3("id").isString(),
      body3("metadata").optional().isObject(),
      async (req, res) => {
        const errors = validationResult3(req);
        if (!errors.isEmpty()) {
          return res.status(400).json({ errors: errors.array() });
        }
        try {
          const { event_type, id, metadata } = req.body;
          if (event_type.startsWith("run_")) {
            await LifecycleManager.emitRunEvent({
              type: event_type,
              runId: id,
              metadata,
              timestamp: /* @__PURE__ */ new Date()
            });
          } else if (event_type.startsWith("deployment_")) {
            await LifecycleManager.emitDeploymentEvent({
              type: event_type,
              deploymentId: id,
              metadata,
              timestamp: /* @__PURE__ */ new Date()
            });
          }
          res.status(200).json({ status: "processed" });
        } catch (error) {
          console.error("Lifecycle webhook error:", error);
          res.status(500).json({ error: "Webhook processing failed" });
        }
      }
    );
    webhooks_default = router104;
  }
});

// src/middleware/policy-action-gate.ts
import { randomUUID as randomUUID75 } from "crypto";
function buildPolicyContextFromRequest(req, action, resource, options2 = {}) {
  const user = req.user || {};
  const tenantId = user.tenantId || user.tenant_id || req.headers["x-tenant-id"] || "unknown";
  return {
    tenantId,
    userId: user.id || user.sub || user.email,
    role: user.role || "user",
    action,
    resource,
    resourceAttributes: options2.resourceAttributes,
    subjectAttributes: user.attributes || {},
    sessionContext: {
      ipAddress: req.ip,
      userAgent: req.get("User-Agent"),
      timestamp: Date.now(),
      sessionId: req.sessionID,
      traceId: req.traceId || randomUUID75()
    },
    policyVersion: options2.policyVersion
  };
}
function policyActionGate(options2) {
  return async (req, res, next) => {
    const user = req.user;
    if (!user) {
      return res.status(401).json({ error: "Unauthorized" });
    }
    const resourceAttributes = {
      ...options2.buildResourceAttributes ? options2.buildResourceAttributes(req) : {},
      resourceId: options2.resolveResourceId?.(req)
    };
    const context4 = buildPolicyContextFromRequest(
      req,
      options2.action,
      options2.resource,
      { resourceAttributes }
    );
    try {
      const decision = await opaPolicyEngine.evaluatePolicy(
        options2.policyName || "maestro/authz",
        context4
      );
      if (!decision.allow) {
        return res.status(403).json({
          error: "Forbidden",
          reason: decision.reason,
          decision
        });
      }
      req.policyDecision = decision;
      return next();
    } catch (error) {
      return res.status(500).json({
        error: "Policy evaluation failed",
        message: error instanceof Error ? error.message : "Unknown error"
      });
    }
  };
}
var init_policy_action_gate = __esm({
  "src/middleware/policy-action-gate.ts"() {
    "use strict";
    init_opa_integration();
  }
});

// src/maestro/budget.ts
function clampNonNegative(value, fallback) {
  if (!Number.isFinite(value)) {
    return fallback;
  }
  return Math.max(MIN_BUDGET, Math.floor(value));
}
function normalizeReasoningBudget(input) {
  const base = DEFAULT_REASONING_BUDGET;
  const thinkMode = input?.thinkMode ?? base.thinkMode;
  const thinkingBudget = clampNonNegative(
    input?.thinkingBudget ?? base.thinkingBudget,
    base.thinkingBudget
  );
  const maxTokens = clampNonNegative(
    input?.maxTokens ?? base.maxTokens,
    base.maxTokens
  );
  const toolBudget = clampNonNegative(
    input?.toolBudget ?? base.toolBudget,
    base.toolBudget
  );
  const timeBudgetMs = clampNonNegative(
    input?.timeBudgetMs ?? base.timeBudgetMs,
    base.timeBudgetMs
  );
  const redactionPolicy = input?.redactionPolicy ?? base.redactionPolicy;
  return {
    thinkMode,
    thinkingBudget: thinkMode === "off" ? 0 : thinkingBudget,
    maxTokens,
    toolBudget,
    timeBudgetMs,
    redactionPolicy
  };
}
function buildBudgetEvidence(budget, outcome, recordedAt = (/* @__PURE__ */ new Date()).toISOString()) {
  return {
    budget,
    outcome,
    recordedAt
  };
}
function summarizeBudgetForPolicy(budget) {
  return {
    think_mode: budget.thinkMode,
    thinking_budget: budget.thinkingBudget,
    max_tokens: budget.maxTokens,
    tool_budget: budget.toolBudget,
    time_budget_ms: budget.timeBudgetMs,
    redaction_policy: budget.redactionPolicy
  };
}
var DEFAULT_REASONING_BUDGET, MIN_BUDGET;
var init_budget = __esm({
  "src/maestro/budget.ts"() {
    "use strict";
    DEFAULT_REASONING_BUDGET = {
      thinkMode: "normal",
      thinkingBudget: 1024,
      maxTokens: 2048,
      toolBudget: 16,
      timeBudgetMs: 6e4,
      redactionPolicy: "summary_only"
    };
    MIN_BUDGET = 0;
  }
});

// src/routes/maestro_routes.ts
var maestro_routes_exports = {};
__export(maestro_routes_exports, {
  buildMaestroRouter: () => buildMaestroRouter,
  createMaestroOPAEnforcer: () => createMaestroOPAEnforcer
});
import { randomUUID as randomUUID76 } from "crypto";
import { Router as Router54 } from "express";
function normalizeDecision(result2) {
  if (typeof result2 === "boolean") {
    return { allow: result2 };
  }
  if (result2 && typeof result2 === "object") {
    if (typeof result2.allow === "boolean") {
      return { allow: result2.allow, reason: result2.reason || result2.message };
    }
    if (typeof result2.result === "boolean") {
      return { allow: result2.result, reason: result2.reason || result2.message };
    }
  }
  return { allow: false, reason: "invalid_decision" };
}
function createMaestroOPAEnforcer(opa = opaClient, policyPath = DEFAULT_POLICY_PATH, options2 = {}) {
  return async (req, res, next) => {
    const correlation = getCorrelationContext(req);
    const traceId = correlation.traceId || req.traceId || correlation.correlationId || req.headers["x-trace-id"] || randomUUID76();
    const principal = {
      id: req.user?.id || req.body?.userId || "anonymous",
      role: req.user?.role,
      tenantId: req.user?.tenantId || req.user?.tenant_id || req.headers["x-tenant-id"] || "unknown"
    };
    const resourceType = typeof options2.resourceType === "function" ? options2.resourceType(req) : options2.resourceType || DEFAULT_RESOURCE_TYPE;
    const resourceId = options2.resolveResourceId?.(req) || req.params.runId || req.params.taskId || req.body?.pipeline_id;
    const resourceAttributes = {
      method: req.method.toLowerCase(),
      path: req.path,
      runId: req.params.runId,
      pipelineId: req.body?.pipeline_id,
      taskId: req.params.taskId,
      ...options2.buildResourceAttributes ? options2.buildResourceAttributes(req) : {}
    };
    const opaInput = {
      action: typeof options2.action === "function" ? options2.action(req) : options2.action || "maestro.run.create",
      principal,
      resource: {
        type: resourceType,
        id: resourceId,
        attributes: resourceAttributes
      },
      traceId,
      correlationId: correlation.correlationId
    };
    try {
      const rawDecision = await opa.evaluateQuery(policyPath, opaInput);
      const decision = normalizeDecision(rawDecision);
      const decisionLog = {
        event: "maestro_opa_decision",
        traceId,
        correlationId: correlation.correlationId,
        principalId: principal.id,
        principalRole: principal.role,
        tenantId: principal.tenantId,
        resourceType,
        resourceId: opaInput.resource.id,
        action: opaInput.action,
        decision: decision.allow ? "allow" : "deny",
        allow: decision.allow,
        reason: decision.reason,
        resourceAttributes
      };
      const logMessage = "Maestro OPA decision evaluated";
      if (!decision.allow) {
        logger2.warn(decisionLog, logMessage);
        return res.status(403).json({
          error: "Forbidden",
          reason: decision.reason || "Access denied by policy"
        });
      }
      logger2.info(decisionLog, logMessage);
      req.opaDecision = decision;
      return next();
    } catch (error) {
      logger2.error(
        {
          event: "maestro_opa_error",
          traceId,
          correlationId: correlation.correlationId,
          principalId: principal.id,
          tenantId: principal.tenantId,
          resourceType,
          error: error instanceof Error ? error.message : "Unknown OPA evaluation error"
        },
        "Failed to evaluate Maestro OPA policy"
      );
      return res.status(500).json({ error: "Policy evaluation failed", code: "opa_error" });
    }
  };
}
function buildMaestroRouter(maestro, queries, opa = opaClient) {
  const router105 = Router54();
  const singleParam27 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
  const enforceStartRunPolicy = policyActionGate({
    action: "start_run",
    resource: "maestro_run",
    resolveResourceId: (req) => req.body?.pipeline_id,
    buildResourceAttributes: (req) => ({
      pipelineId: req.body?.pipeline_id,
      requestText: req.body?.requestText,
      reasoningBudget: summarizeBudgetForPolicy(
        normalizeReasoningBudget(req.body?.reasoningBudget)
      )
    })
  });
  const enforceRunReadPolicy = createMaestroOPAEnforcer(opa, DEFAULT_POLICY_PATH, {
    action: "maestro.run.read",
    resourceType: "maestro/run",
    resolveResourceId: (req) => singleParam27(req.params.runId)
  });
  const enforceTaskReadPolicy = createMaestroOPAEnforcer(opa, DEFAULT_POLICY_PATH, {
    action: "maestro.task.read",
    resourceType: "maestro/task",
    resolveResourceId: (req) => singleParam27(req.params.taskId) || singleParam27(req.params.runId),
    buildResourceAttributes: (req) => ({
      taskId: singleParam27(req.params.taskId),
      runId: singleParam27(req.params.runId)
    })
  });
  router105.post("/runs", enforceStartRunPolicy, async (req, res, next) => {
    try {
      const { userId, requestText } = req.body ?? {};
      if (!userId || !requestText) {
        return res.status(400).json({
          error: "Missing userId or requestText"
        });
      }
      const reasoningBudget = normalizeReasoningBudget(
        req.body?.reasoningBudget
      );
      const tenantId = req.user?.tenantId || req.user?.tenant_id || req.body?.tenantId;
      const result2 = await maestro.runPipeline(userId, requestText, {
        tenantId,
        reasoningBudget
      });
      return res.json(result2);
    } catch (e) {
      next(e);
    }
  });
  router105.get("/runs/:runId", enforceRunReadPolicy, async (req, res, next) => {
    try {
      const runId = singleParam27(req.params.runId) ?? "";
      const response = await queries.getRunResponse(runId);
      if (!response) {
        return res.status(404).json({ error: "Run not found" });
      }
      return res.json(response);
    } catch (e) {
      next(e);
    }
  });
  router105.get("/runs/:runId/tasks", enforceRunReadPolicy, async (req, res, next) => {
    try {
      const runId = singleParam27(req.params.runId) ?? "";
      const run = await queries.getRunResponse(runId);
      if (!run) return res.status(404).json({ error: "Run not found" });
      return res.json(run.tasks);
    } catch (e) {
      next(e);
    }
  });
  router105.get("/tasks/:taskId", enforceTaskReadPolicy, async (req, res, next) => {
    try {
      const taskId = singleParam27(req.params.taskId) ?? "";
      const result2 = await queries.getTaskWithArtifacts(taskId);
      if (!result2) return res.status(404).json({ error: "Task not found" });
      return res.json(result2);
    } catch (e) {
      next(e);
    }
  });
  return router105;
}
var DEFAULT_POLICY_PATH, DEFAULT_RESOURCE_TYPE;
var init_maestro_routes = __esm({
  "src/routes/maestro_routes.ts"() {
    "use strict";
    init_opa_client();
    init_correlation_id();
    init_logger2();
    init_policy_action_gate();
    init_budget();
    DEFAULT_POLICY_PATH = "maestro/authz/allow";
    DEFAULT_RESOURCE_TYPE = "maestro/run";
  }
});

// src/maestro/governance-service.ts
var AgentGovernanceService, agentGovernance;
var init_governance_service2 = __esm({
  "src/maestro/governance-service.ts"() {
    "use strict";
    init_ledger();
    init_logger2();
    AgentGovernanceService = class _AgentGovernanceService {
      static instance;
      safetyViolations = /* @__PURE__ */ new Map();
      // tenantId -> violations
      ledger;
      policies = /* @__PURE__ */ new Map();
      // tenantId -> { policyId -> config }
      riskThresholds = {
        low: 0.3,
        medium: 0.6,
        high: 0.8,
        critical: 0.95
      };
      // Default governance configuration
      defaultConfig = {
        maxConcurrency: 5,
        maxBudget: 1e4,
        // $10,000 budget ceiling
        maxExecutionTimeMs: 6e4,
        // 60 seconds max
        requiredApprovals: 1,
        capabilitiesWhitelist: [
          "data-query",
          "graph-analysis",
          "text-summarization",
          "entity-resolution",
          "threat-correlation",
          "provenance-tracing"
        ],
        allowedDomains: [
          "localhost",
          "127.0.0.1",
          "intelgraph.local",
          "intelgraph.cloud"
        ],
        dataClassification: "internal",
        auditLevel: "standard"
      };
      constructor() {
        this.ledger = ProvenanceLedgerV2.getInstance();
        this.initializePolicies();
      }
      static getInstance() {
        if (!_AgentGovernanceService.instance) {
          _AgentGovernanceService.instance = new _AgentGovernanceService();
        }
        return _AgentGovernanceService.instance;
      }
      /**
       * Initialize default policies and governance rules
       */
      initializePolicies() {
        const systemPolicies = /* @__PURE__ */ new Map();
        systemPolicies.set("*", this.defaultConfig);
        systemPolicies.set("tier-0", {
          ...this.defaultConfig,
          maxBudget: 5e3,
          maxExecutionTimeMs: 3e4,
          requiredApprovals: 0,
          capabilitiesWhitelist: [
            "data-query",
            "graph-analysis",
            "text-summarization"
          ],
          dataClassification: "internal",
          auditLevel: "minimal"
        });
        systemPolicies.set("tier-1", {
          ...this.defaultConfig,
          maxBudget: 15e3,
          maxExecutionTimeMs: 12e4,
          requiredApprovals: 1,
          capabilitiesWhitelist: [
            "data-query",
            "graph-analysis",
            "text-summarization",
            "entity-resolution",
            "threat-correlation"
          ],
          dataClassification: "confidential",
          auditLevel: "standard"
        });
        systemPolicies.set("tier-2", this.defaultConfig);
        this.policies.set("system", systemPolicies);
        logger_default2.info("Agent governance policies initialized");
      }
      /**
       * Evaluate whether an agent action is permitted
       */
      async evaluateAction(agent, action, context4, metadata) {
        if (context4.taskId) {
          try {
            const { getPostgresPool: getPostgresPool3 } = await Promise.resolve().then(() => (init_postgres(), postgres_exports));
            const pool4 = getPostgresPool3();
            const result2 = await pool4.query(
              `SELECT status FROM approvals 
           WHERE (payload->>'taskId' = $1 OR run_id = $2)
           AND status = 'approved' 
           ORDER BY created_at DESC LIMIT 1`,
              [context4.taskId, context4.runId]
            );
            if (result2.rows && result2.rows.length > 0) {
              logger_default2.info({ taskId: context4.taskId }, "Governance: Human approval detected, allowing action");
              return {
                allowed: true,
                reason: "Action manually authorized by human operator",
                riskScore: 0,
                violations: []
              };
            }
          } catch (err) {
            logger_default2.warn({ taskId: context4.taskId, error: err.message }, "Governance: Approval check failed, falling back to policy");
          }
        }
        const config9 = this.getAgentConfig(agent);
        const violations = [];
        if (!config9.capabilitiesWhitelist.includes(action)) {
          const violation = {
            id: `violation-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
            agentId: agent.id,
            violationType: "UNAUTHORIZED_CAPABILITY",
            severity: "high",
            details: `Agent attempted to use unauthorized capability: ${action}`,
            timestamp: /* @__PURE__ */ new Date(),
            context: context4
          };
          violations.push(violation);
          logger_default2.warn({
            agentId: agent.id,
            action,
            violationId: violation.id
          }, "Unauthorized capability attempt detected");
        }
        const estimatedCost = this.estimateActionCost(action, context4);
        if (estimatedCost > config9.maxBudget) {
          const violation = {
            id: `violation-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
            agentId: agent.id,
            violationType: "BUDGET_EXCEEDED",
            severity: "medium",
            details: `Estimated cost $${estimatedCost} exceeds max budget $${config9.maxBudget}`,
            timestamp: /* @__PURE__ */ new Date(),
            context: context4
          };
          violations.push(violation);
          logger_default2.warn({
            agentId: agent.id,
            estimatedCost,
            maxBudget: config9.maxBudget,
            violationId: violation.id
          }, "Budget constraint violation detected");
        }
        const riskScore = this.calculateRiskScore(agent, action, context4);
        let requiredApprovals = config9.requiredApprovals;
        if (riskScore > this.riskThresholds.high) {
          requiredApprovals = Math.max(requiredApprovals, 2);
        }
        if (riskScore > this.riskThresholds.critical) {
          const violation = {
            id: `violation-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`,
            agentId: agent.id,
            violationType: "ETHICAL_VIOLATION",
            severity: "critical",
            details: `Action risk score ${riskScore} exceeds critical threshold ${this.riskThresholds.critical}`,
            timestamp: /* @__PURE__ */ new Date(),
            context: context4
          };
          violations.push(violation);
          logger_default2.error({
            agentId: agent.id,
            riskScore,
            criticalThreshold: this.riskThresholds.critical,
            violationId: violation.id
          }, "Critical risk threshold exceeded");
        }
        const tenantId = agent.tenantId || "system";
        if (violations.length > 0) {
          if (!this.safetyViolations.has(tenantId)) {
            this.safetyViolations.set(tenantId, []);
          }
          this.safetyViolations.get(tenantId).push(...violations);
        }
        for (const violation of violations) {
          await this.ledger.appendEntry({
            tenantId,
            actionType: "GOVERNANCE_VIOLATION",
            resourceType: "SafetyViolation",
            resourceId: violation.id,
            actorId: agent.id,
            actorType: "system",
            timestamp: /* @__PURE__ */ new Date(),
            payload: {
              mutationType: "CREATE",
              entityId: violation.id,
              entityType: "SafetyViolation",
              violationType: violation.violationType,
              severity: violation.severity,
              details: violation.details
            },
            metadata: {
              agentId: agent.id,
              action,
              riskScore,
              originalContext: context4,
              ...metadata
            }
          });
        }
        const allowed = violations.length === 0 && riskScore <= this.riskThresholds.critical;
        const decision = {
          allowed,
          reason: allowed ? "Action passes all governance checks" : `Action violates ${violations.length} governance policies`,
          riskScore,
          requiredApprovals: riskScore > this.riskThresholds.high ? requiredApprovals : config9.requiredApprovals,
          violations
        };
        logger_default2.info({
          agentId: agent.id,
          action,
          allowed,
          riskScore,
          violationCount: violations.length
        }, "Governance decision made");
        return decision;
      }
      /**
       * Calculate risk score for an agent action based on multiple factors
       */
      calculateRiskScore(agent, action, context4) {
        let score = 0;
        switch (action) {
          case "delete":
          case "modify_classification":
          case "export_sensitive_data":
            score += 0.8;
            break;
          case "query":
          case "read":
            score += 0.2;
            break;
          case "execute_arbitrary_code":
            score += 0.95;
            break;
          default:
            score += 0.5;
            break;
        }
        if (context4.tenantId === "restricted") {
          score += 0.2;
        }
        if (context4.riskFactors?.includes("high_value_target")) {
          score += 0.3;
        }
        if (context4.dataClassification) {
          switch (context4.dataClassification) {
            case "secret":
              score += 0.4;
              break;
            case "confidential":
              score += 0.2;
              break;
            case "internal":
              score += 0.1;
              break;
          }
        }
        const hour = (/* @__PURE__ */ new Date()).getUTCHours();
        if (hour < 6 || hour > 22) {
          score += 0.1;
        }
        return Math.min(1, score);
      }
      /**
       * Estimate computational cost of an action
       */
      estimateActionCost(action, context4) {
        const baseCosts = {
          "query": 0.01,
          "read": 5e-3,
          "create": 0.02,
          "update": 0.02,
          "delete": 0.03,
          "graph_analysis": 0.15,
          "entity_resolution": 0.1,
          "threat_correlation": 0.2,
          "export_pdf": 0.05,
          "export_csv": 0.05,
          "execute_arbitrary_code": 1e4
          // Very high cost for unapproved code execution
        };
        const baseCost = baseCosts[action] || 0.05;
        const dataSize = context4.dataSize || 1;
        const volumeFactor = Math.log10(Math.max(1, dataSize)) / 10;
        return baseCost * (1 + volumeFactor) * 100;
      }
      /**
       * Get governance configuration for an agent
       */
      getAgentConfig(agent) {
        const tenantId = agent.tenantId || "system";
        const tenantPolicies = this.policies.get(tenantId) || this.policies.get("system");
        if (!tenantPolicies) return this.defaultConfig;
        const policyId = agent.metadata?.governanceTier || "*";
        return tenantPolicies.get(policyId) || tenantPolicies.get("*") || this.defaultConfig;
      }
      /**
       * Enforce security checks before allowing agent coordination
       */
      async enforceSecurityConstraints(agentIds, action, context4) {
        return this.evaluateAction(
          { id: "coordinator", name: "SubagentCoordinator", tenantId: "system", capabilities: [], metadata: {}, status: "idle", health: { cpuUsage: 0, memoryUsage: 0, lastHeartbeat: /* @__PURE__ */ new Date(), activeTasks: 0, errorRate: 0 }, templateId: "system", config: {} },
          `COORDINATION:${action}`,
          context4
        );
      }
      /**
       * Register a governance policy for a specific agent or tenant
       */
      registerPolicy(id, config9, tenantId = "system") {
        if (!this.policies.has(tenantId)) {
          this.policies.set(tenantId, /* @__PURE__ */ new Map());
        }
        this.policies.get(tenantId).set(id, config9);
        logger_default2.info({
          policyId: id,
          tenantId,
          config: config9
        }, "Agent governance policy registered");
      }
      /**
       * Get recent safety violations for an agent
       */
      getRecentViolations(agentId, tenantId = "system", sinceDays = 7) {
        const cutoffTime = new Date(Date.now() - sinceDays * 24 * 60 * 60 * 1e3);
        const tenantViolations = this.safetyViolations.get(tenantId) || [];
        return tenantViolations.filter((violation) => violation.agentId === agentId && violation.timestamp > cutoffTime).sort((a, b) => b.timestamp.getTime() - a.timestamp.getTime());
      }
      /**
       * Check if an agent is in safe/governed state
       */
      isAgentOperational(agent) {
        const isHealthy = agent.health.memoryUsage < 90 && agent.health.cpuUsage < 90;
        const recentViolations = this.getRecentViolations(agent.id, agent.tenantId || "system", 1);
        const hasCriticalViolations = recentViolations.some((v) => v.severity === "critical");
        return isHealthy && !hasCriticalViolations;
      }
      /**
       * Generate governance compliance report
       */
      async generateComplianceReport(tenantId = "system") {
        const allViolations = this.safetyViolations.get(tenantId) || [];
        const tenantPolicies = this.policies.get(tenantId) || /* @__PURE__ */ new Map();
        const report = {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          tenantId,
          summary: {
            totalViolations: allViolations.length,
            criticalViolations: allViolations.filter((v) => v.severity === "critical").length,
            highViolations: allViolations.filter((v) => v.severity === "high").length,
            policyCoverage: tenantPolicies.size
          },
          violationsByType: this.groupViolationsByType(allViolations),
          violationsBySeverity: this.groupViolationsBySeverity(allViolations),
          agentCompliance: this.calculateAgentCompliance()
        };
        logger_default2.info({
          reportId: `gov-report-${Date.now()}`,
          violations: allViolations.length
        }, "Governance compliance report generated");
        return report;
      }
      /**
       * Helper to group violations by type
       */
      groupViolationsByType(violations) {
        return violations.reduce((acc, violation) => {
          acc[violation.violationType] = (acc[violation.violationType] || 0) + 1;
          return acc;
        }, {});
      }
      /**
       * Helper to group violations by severity
       */
      groupViolationsBySeverity(violations) {
        return violations.reduce((acc, violation) => {
          acc[violation.severity] = (acc[violation.severity] || 0) + 1;
          return acc;
        }, {});
      }
      /**
       * Calculate compliance metrics for agents
       */
      calculateAgentCompliance() {
        return {};
      }
    };
    agentGovernance = AgentGovernanceService.getInstance();
  }
});

// src/conductor/mcp/transport/websocket-jsonrpc.ts
import WebSocket2 from "ws";
var init_websocket_jsonrpc = __esm({
  "src/conductor/mcp/transport/websocket-jsonrpc.ts"() {
    "use strict";
  }
});

// src/conductor/mcp/transport/registry.ts
var init_registry2 = __esm({
  "src/conductor/mcp/transport/registry.ts"() {
    "use strict";
    init_websocket_jsonrpc();
  }
});

// src/conductor/mcp/client.ts
var allowedExecutorUrls, MCPServerRegistry, mcpRegistry, mcpClient;
var init_client = __esm({
  "src/conductor/mcp/client.ts"() {
    "use strict";
    init_logger();
    init_registry2();
    init_prometheus();
    allowedExecutorUrls = process.env.MCP_ALLOWED_EXECUTOR_URLS ? process.env.MCP_ALLOWED_EXECUTOR_URLS.split(",").map((url) => url.trim()) : [];
    MCPServerRegistry = class {
      servers = {};
      /**
       * Register an MCP server
       */
      register(name, config9) {
        this.servers[name] = { ...config9, name };
      }
      /**
       * Unregister an MCP server
       */
      unregister(name) {
        delete this.servers[name];
      }
      /**
       * Get server configuration
       */
      getServer(name) {
        return this.servers[name];
      }
      /**
       * List all registered servers
       */
      listServers() {
        return Object.keys(this.servers);
      }
      /**
       * Get all server configurations
       */
      getAllServers() {
        return { ...this.servers };
      }
      /**
       * Find servers that have a specific tool
       */
      findServersWithTool(toolName) {
        return Object.entries(this.servers).filter(
          ([_2, config9]) => config9.tools.some((tool) => tool.name === toolName)
        ).map(([name, _2]) => name);
      }
    };
    mcpRegistry = new MCPServerRegistry();
  }
});

// src/maestro/agents/MaestroGlobalAgent.ts
var MaestroGlobalAgent_exports = {};
__export(MaestroGlobalAgent_exports, {
  MaestroGlobalAgent: () => MaestroGlobalAgent,
  maestroGlobalAgent: () => maestroGlobalAgent
});
var MaestroGlobalAgent, maestroGlobalAgent;
var init_MaestroGlobalAgent = __esm({
  "src/maestro/agents/MaestroGlobalAgent.ts"() {
    "use strict";
    init_GlobalTrafficSteering();
    init_logger2();
    MaestroGlobalAgent = class _MaestroGlobalAgent {
      static instance;
      constructor() {
      }
      static getInstance() {
        if (!_MaestroGlobalAgent.instance) {
          _MaestroGlobalAgent.instance = new _MaestroGlobalAgent();
        }
        return _MaestroGlobalAgent.instance;
      }
      /**
       * Determines if a task can be executed in the current region for the given tenant.
       * If not, it provides steering advice.
       */
      async evaluateRouting(task) {
        const tenantId = task.input?.tenantId;
        if (!tenantId) {
          return {
            allowed: true,
            action: "STAY",
            reason: "System operation or missing tenant context"
          };
        }
        const decision = await globalTrafficSteering.resolveRegion(tenantId);
        if (!decision.isOptimal) {
          logger_default2.warn({
            taskId: task.id,
            tenantId,
            currentRegion: process.env.SUMMIT_REGION || "us-east-1",
            optimalRegion: decision.targetRegion,
            reason: decision.reason
          }, "Maestro Global Steering Advice: Sub-optimal region detected");
          return {
            allowed: false,
            action: "REDIRECT",
            advice: decision.targetRegion,
            reason: decision.reason
          };
        }
        return {
          allowed: true,
          action: "STAY",
          reason: "Routing is optimal"
        };
      }
    };
    maestroGlobalAgent = MaestroGlobalAgent.getInstance();
  }
});

// src/maestro/handoff-service.ts
var handoff_service_exports = {};
__export(handoff_service_exports, {
  MaestroHandoffService: () => MaestroHandoffService,
  maestroHandoffService: () => maestroHandoffService
});
var MaestroHandoffService, maestroHandoffService;
var init_handoff_service = __esm({
  "src/maestro/handoff-service.ts"() {
    "use strict";
    init_logger2();
    MaestroHandoffService = class _MaestroHandoffService {
      static instance;
      constructor() {
      }
      static getInstance() {
        if (!_MaestroHandoffService.instance) {
          _MaestroHandoffService.instance = new _MaestroHandoffService();
        }
        return _MaestroHandoffService.instance;
      }
      /**
       * Initiates a handoff to a target region.
       * In the current implementation, this marks the task for external execution
       * and logs the handoff event.
       */
      async initiateHandoff(task, targetRegion) {
        logger_default2.info({
          taskId: task.id,
          runId: task.runId,
          currentRegion: process.env.SUMMIT_REGION || "us-east-1",
          targetRegion
        }, "Maestro Handoff: Initiating task transfer to optimal region");
        return {
          success: true,
          handoffId: crypto.randomUUID(),
          message: `Task successfully transitioned to ${targetRegion}`
        };
      }
    };
    maestroHandoffService = MaestroHandoffService.getInstance();
  }
});

// src/services/VisionService.ts
var VisionService, VisionService_default;
var init_VisionService = __esm({
  "src/services/VisionService.ts"() {
    "use strict";
    init_LLMService();
    init_logger2();
    VisionService = class {
      llmService;
      constructor() {
        this.llmService = new LLMService_default();
      }
      /**
       * Analyze an image and return a description of entities and relationships
       * 
       * @param imageUri - The URI or base64 data of the image
       * @param prompt - Optional custom prompt for analysis
       */
      async analyzeImage(imageUri, prompt) {
        logger_default2.info({ message: "Analyzing image with vision model", imageUri });
        const systemPrompt = prompt || `
      You are an expert intelligence analyst. 
      Analyze the provided image and describe all relevant entities (People, Organizations, Locations, Equipment) 
      and the relationships between them. 
      Provide a detailed, factual summary that can be used for entity extraction.
    `;
        try {
          const response = await this.llmService.complete(
            `[IMAGE_ANALYSIS_REQUEST] ${imageUri}

${systemPrompt}`,
            {
              model: "gpt-4o",
              // Vision-capable model
              temperature: 0.2
            }
          );
          return response;
        } catch (error) {
          logger_default2.error({ message: "Vision analysis failed", error: error.message, imageUri });
          throw error;
        }
      }
    };
    VisionService_default = VisionService;
  }
});

// src/services/DeepfakeDetectionService.ts
var DeepfakeDetectionService_exports = {};
__export(DeepfakeDetectionService_exports, {
  DeepfakeDetectionService: () => DeepfakeDetectionService
});
var DeepfakeDetectionService;
var init_DeepfakeDetectionService = __esm({
  "src/services/DeepfakeDetectionService.ts"() {
    "use strict";
    init_LLMService();
    init_VisionService();
    init_logger2();
    DeepfakeDetectionService = class {
      llmService;
      visionService;
      constructor(llmService, visionService) {
        this.llmService = llmService || new LLMService_default();
        this.visionService = visionService || new VisionService_default();
      }
      /**
       * Analyze media for synthetic manipulation (deepfakes)
       */
      async analyze(contentUri, mediaType, tenantId) {
        logger_default2.info({ message: "Starting deepfake analysis", mediaType, tenantId });
        try {
          let analysisPrompt = "";
          switch (mediaType) {
            case "IMAGE":
              return await this.analyzeImage(contentUri, tenantId);
            case "AUDIO":
              return await this.analyzeAudio(contentUri, tenantId);
            case "VIDEO":
              return await this.analyzeVideo(contentUri, tenantId);
            case "TEXT":
              return await this.analyzeText(contentUri, tenantId);
            default:
              throw new Error(`Unsupported media type for deepfake detection: ${mediaType}`);
          }
        } catch (error) {
          logger_default2.error({ message: "Deepfake analysis failed", error: error.message, mediaType });
          throw error;
        }
      }
      async analyzeImage(uri, tenantId) {
        const prompt = `
            Analyze this image for signs of AI generation or manipulation.
            Look for:
            1. Anatomic inconsistencies (extra fingers, mismatched eyes).
            2. Lighting and shadow anomalies.
            3. Blurred or overly smooth textures in complex areas.
            4. Inconsistent backgrounds or geometric distortions.
            
            Return a JSON object:
            {
                "isDeepfake": boolean,
                "confidence": number (0-1),
                "riskScore": number (0-100),
                "markers": string[],
                "details": string
            }
        `;
        const analysis = await this.visionService.analyzeImage(uri, prompt);
        return this.parseResult(analysis);
      }
      async analyzeAudio(uri, tenantId) {
        const response = await this.llmService.complete(
          `Analyze audio metadata and spectral summary for synthetic voice characteristics: ${uri}`,
          { model: "gpt-4o", responseFormat: "json" }
        );
        return this.parseResult(response);
      }
      async analyzeVideo(uri, tenantId) {
        const response = await this.llmService.complete(
          `Analyze video temporal consistency for deepfake markers (lip-sync, eye-blink frequency): ${uri}`,
          { model: "gpt-4o", responseFormat: "json" }
        );
        return this.parseResult(response);
      }
      async analyzeText(content, tenantId) {
        const response = await this.llmService.complete(
          `Analyze the following text for signs of LLM generation (hallucination patterns, repetitive structures, lack of contextual grounding): ${content}`,
          { model: "gpt-4o", responseFormat: "json" }
        );
        return this.parseResult(response);
      }
      parseResult(raw) {
        try {
          const cleanJson = raw.replace(/```json/g, "").replace(/```/g, "").trim();
          const parsed = JSON.parse(cleanJson);
          return {
            isDeepfake: !!parsed.isDeepfake,
            confidence: parsed.confidence ?? 0.5,
            riskScore: parsed.riskScore ?? 50,
            markers: Array.isArray(parsed.markers) ? parsed.markers : [],
            details: parsed.details ?? "Analysis completed."
          };
        } catch (e) {
          logger_default2.warn({ message: "Failed to parse deepfake analysis JSON", raw });
          return {
            isDeepfake: false,
            confidence: 0,
            riskScore: 0,
            markers: ["parsing_error"],
            details: "Raw output: " + raw
          };
        }
      }
    };
  }
});

// src/narrative/adapters/neo4j-loader.ts
var neo4j_loader_exports = {};
__export(neo4j_loader_exports, {
  Neo4jNarrativeLoader: () => Neo4jNarrativeLoader
});
var Neo4jNarrativeLoader;
var init_neo4j_loader = __esm({
  "src/narrative/adapters/neo4j-loader.ts"() {
    "use strict";
    init_neo4j();
    Neo4jNarrativeLoader = class {
      /**
       * Loads a subgraph from Neo4j starting from a root node and transforms it into SimulationEntities.
       * Uses APOC if available, otherwise falls back to a standard Cypher path pattern.
       */
      static async loadFromGraph(rootId, depth = 2) {
        const query3 = `
      MATCH (n {id: $rootId})
      CALL apoc.path.subgraphAll(n, {
        maxLevel: $depth,
        relationshipFilter: 'SUPPORTS|OPPOSES|INFLUENCES|MEMBER_OF'
      })
      YIELD nodes, relationships
      RETURN nodes, relationships
    `;
        try {
          const result2 = await neo.run(query3, { rootId, depth });
          if (result2.records.length === 0) return [];
          const record2 = result2.records[0];
          const rawNodes = transformNeo4jIntegers(record2.get("nodes"));
          const rawRels = transformNeo4jIntegers(record2.get("relationships"));
          const internalToPublicId = /* @__PURE__ */ new Map();
          const entities = rawNodes.map((node) => {
            const props = node.properties || {};
            const id = props.id || node.elementId;
            internalToPublicId.set(node.elementId, id);
            return this.mapNodeToEntity(node);
          });
          const entityMap = new Map(entities.map((e) => [e.id, e]));
          rawRels.forEach((rel) => {
            const fromPublicId = internalToPublicId.get(rel.startNodeElementId);
            const toPublicId = internalToPublicId.get(rel.endNodeElementId);
            if (fromPublicId && toPublicId && entityMap.has(fromPublicId)) {
              const entity = entityMap.get(fromPublicId);
              const strength = this.calculateRelationshipStrength(rel);
              entity.relationships.push({
                targetId: toPublicId,
                strength
              });
            }
          });
          return entities;
        } catch (error) {
          console.error("Failed to load narrative graph from Neo4j:", error);
          return this.fallbackLoad(rootId, depth);
        }
      }
      static mapNodeToEntity(node) {
        const props = node.properties || {};
        const labels2 = node.labels || [];
        const influence = props.pageRank !== void 0 ? props.pageRank : props.influence || 0.5;
        const resilience = props.resilience !== void 0 ? props.resilience : 0.7;
        const sentiment = props.sentiment !== void 0 ? props.sentiment : 0;
        const volatility = props.volatility !== void 0 ? props.volatility : 0.1;
        return {
          id: props.id || node.elementId,
          name: props.name || props.title || props.id || "Unnamed Entity",
          type: labels2.includes("Group") || labels2.includes("Organization") ? "group" : "actor",
          alignment: props.alignment || "neutral",
          influence,
          sentiment,
          volatility,
          resilience,
          themes: props.themes || {},
          relationships: [],
          metadata: {
            labels: labels2,
            ...props
          }
        };
      }
      static calculateRelationshipStrength(rel) {
        const props = rel.properties || {};
        if (props.weight !== void 0) return props.weight;
        if (props.strength !== void 0) return props.strength;
        switch (rel.type) {
          case "SUPPORTS":
            return 0.8;
          case "OPPOSES":
            return -0.8;
          case "MEMBER_OF":
            return 0.9;
          case "INFLUENCES":
            return 0.5;
          default:
            return 0.3;
        }
      }
      static async fallbackLoad(rootId, depth) {
        const query3 = `
      MATCH (n {id: $rootId})
      MATCH path = (n)-[*1..${depth}]-(m)
      WITH nodes(path) as ns, relationships(path) as rs
      UNWIND ns as node
      UNWIND rs as rel
      WITH collect(distinct node) as nodes, collect(distinct rel) as rels
      RETURN nodes, rels
    `;
        const result2 = await neo.run(query3, { rootId });
        if (result2.records.length === 0) return [];
        return [];
      }
    };
  }
});

// src/maestro/core.ts
var core_exports = {};
__export(core_exports, {
  Maestro: () => Maestro
});
var Maestro;
var init_core = __esm({
  "src/maestro/core.ts"() {
    "use strict";
    init_residency_guard();
    init_governance_service2();
    init_logger2();
    init_metrics2();
    init_budget();
    init_client();
    Maestro = class {
      constructor(ig, costMeter, llm, config9) {
        this.ig = ig;
        this.costMeter = costMeter;
        this.llm = llm;
        this.config = config9;
      }
      async getTask(taskId) {
        return this.ig.getTask(taskId);
      }
      async createRun(userId, requestText, options2) {
        const reasoningBudget = normalizeReasoningBudget(
          options2?.reasoningBudget
        );
        const run = {
          id: crypto.randomUUID(),
          user: { id: userId },
          createdAt: (/* @__PURE__ */ new Date()).toISOString(),
          requestText,
          // Pass tenant context if available (will need DB schema update for full persistence)
          ...options2?.tenantId ? { tenantId: options2.tenantId } : {},
          reasoningBudget
        };
        await this.ig.createRun(run);
        return run;
      }
      async planRequest(run) {
        const tenantId = run.tenantId;
        const planTask = {
          id: crypto.randomUUID(),
          runId: run.id,
          tenantId: run.tenantId,
          status: "succeeded",
          // planning is instant for v0.1
          agent: {
            id: this.config.defaultPlannerAgent,
            name: "planner",
            kind: "llm",
            modelId: this.config.defaultPlannerAgent
          },
          kind: "plan",
          description: `Plan for: ${run.requestText}`,
          input: { requestText: run.requestText, tenantId },
          output: { steps: ["single_action"] },
          createdAt: (/* @__PURE__ */ new Date()).toISOString(),
          updatedAt: (/* @__PURE__ */ new Date()).toISOString()
        };
        const actionTask = {
          id: crypto.randomUUID(),
          runId: run.id,
          tenantId: run.tenantId,
          parentTaskId: planTask.id,
          status: "queued",
          agent: {
            id: this.config.defaultActionAgent,
            name: "action-llm",
            kind: "llm",
            modelId: this.config.defaultActionAgent
          },
          kind: "action",
          description: `Execute user request: ${run.requestText}`,
          input: { requestText: run.requestText, tenantId },
          createdAt: (/* @__PURE__ */ new Date()).toISOString(),
          updatedAt: (/* @__PURE__ */ new Date()).toISOString()
        };
        await this.ig.createTask(planTask);
        await this.ig.createTask(actionTask);
        return [planTask, actionTask];
      }
      async executeTask(task) {
        const now = (/* @__PURE__ */ new Date()).toISOString();
        const timer3 = metrics2.maestroJobExecutionDurationSeconds.startTimer({ job_type: task.kind, status: "success" });
        await this.ig.updateTask(task.id, { status: "running", updatedAt: now });
        try {
          const tenantId = task.input?.tenantId;
          if (tenantId) {
            const guard = ResidencyGuard.getInstance();
            await guard.validateAgentExecution(tenantId);
            const { maestroGlobalAgent: maestroGlobalAgent2 } = await Promise.resolve().then(() => (init_MaestroGlobalAgent(), MaestroGlobalAgent_exports));
            const steeringResult = await maestroGlobalAgent2.evaluateRouting(task);
            if (steeringResult.action === "STOP") {
              const errorMsg = `Task execution forced STOP by Global Steering: ${steeringResult.reason}`;
              logger_default2.error({ taskId: task.id, reason: steeringResult.reason }, "Maestro Steering: Execution stopped");
              await this.ig.updateTask(task.id, { status: "failed", errorMessage: errorMsg, updatedAt: now });
              throw new Error(errorMsg);
            }
            if (steeringResult.action === "REDIRECT") {
              const { maestroHandoffService: maestroHandoffService2 } = await Promise.resolve().then(() => (init_handoff_service(), handoff_service_exports));
              const handoff = await maestroHandoffService2.initiateHandoff(task, steeringResult.advice);
              if (handoff.success) {
                const msg = `Task handed off to region ${steeringResult.advice}: ${handoff.message}`;
                await this.ig.updateTask(task.id, {
                  status: "succeeded",
                  // In handoff scenario, local task is 'done' once handed off
                  output: { result: msg, handoffId: handoff.handoffId },
                  updatedAt: now
                });
                return {
                  task: { ...task, status: "succeeded", output: { result: msg } },
                  artifact: null
                };
              }
            }
            if (!task.input?._isShadow) {
            }
            const mediaUri = task.input?.mediaUri || task.input?.uri;
            const mediaType = task.input?.mediaType;
            if (mediaUri && mediaType) {
              const { DeepfakeDetectionService: DeepfakeDetectionService2 } = await Promise.resolve().then(() => (init_DeepfakeDetectionService(), DeepfakeDetectionService_exports));
              const deepfakeService = new DeepfakeDetectionService2();
              const analysis = await deepfakeService.analyze(mediaUri, mediaType, tenantId);
              if (analysis.isDeepfake && analysis.riskScore > 80) {
                const errorMsg = `Security Alert: High-risk deepfake detected in task input. Risk score: ${analysis.riskScore}. Markers: ${analysis.markers.join(", ")}. Details: ${analysis.details}`;
                logger_default2.error({ taskId: task.id, analysis }, "Deepfake detection blocked task execution");
                await this.ig.updateTask(task.id, { status: "failed", errorMessage: errorMsg, updatedAt: now });
                throw new Error(errorMsg);
              }
              if (analysis.isDeepfake) {
                logger_default2.warn({ taskId: task.id, analysis }, "Deepfake detected but risk score below threshold. Proceeding with caution.");
                task.output = { ...task.output, deepfakeAnalysis: analysis };
              }
            }
          }
          const governanceService = AgentGovernanceService.getInstance();
          const maestroAgent = {
            id: task.agent.id,
            name: task.agent.name,
            tenantId: tenantId || "system",
            capabilities: [],
            // Could be extracted from agent metadata
            metadata: {
              modelId: task.agent.modelId,
              kind: task.agent.kind
            },
            status: "idle",
            health: {
              cpuUsage: 0,
              memoryUsage: 0,
              lastHeartbeat: /* @__PURE__ */ new Date(),
              activeTasks: 1,
              errorRate: 0
            },
            templateId: task.agent.kind,
            config: {}
          };
          const governanceDecision = await governanceService.evaluateAction(
            maestroAgent,
            task.kind,
            // action type: 'plan', 'action', 'subworkflow', 'graph.analysis'
            {
              taskId: task.id,
              runId: task.runId,
              description: task.description,
              input: task.input,
              tenantId
            },
            {
              source: "maestro_core_executeTask",
              timestamp: now
            }
          );
          if (!governanceDecision.allowed) {
            const errorMessage = `Governance policy violation: ${governanceDecision.reason}. Risk score: ${governanceDecision.riskScore.toFixed(2)}. Violations: ${governanceDecision.violations?.map((v) => v.violationType).join(", ") || "none"}`;
            logger_default2.error({
              taskId: task.id,
              runId: task.runId,
              agentId: task.agent.id,
              decision: governanceDecision
            }, "Task blocked by governance policy");
            await this.ig.updateTask(task.id, {
              status: "failed",
              errorMessage,
              updatedAt: now
            });
            throw new Error(errorMessage);
          }
          if (governanceDecision.requiredApprovals && governanceDecision.requiredApprovals > 0) {
            logger_default2.warn({
              taskId: task.id,
              runId: task.runId,
              agentId: task.agent.id,
              requiredApprovals: governanceDecision.requiredApprovals,
              riskScore: governanceDecision.riskScore
            }, "Task requires human approval");
            await this.ig.updateTask(task.id, {
              status: "pending_approval",
              errorMessage: `Awaiting ${governanceDecision.requiredApprovals} approval(s). Risk score: ${governanceDecision.riskScore.toFixed(2)}`,
              updatedAt: now
            });
            try {
              const { createApproval: createApproval2 } = await Promise.resolve().then(() => (init_approvals(), approvals_exports));
              await createApproval2({
                requesterId: task.agent.id,
                action: "maestro_task_execution",
                payload: { taskId: task.id, taskKind: task.kind, riskScore: governanceDecision.riskScore },
                reason: `Governance policy flagged for review. Risk: ${governanceDecision.riskScore.toFixed(2)}. ${governanceDecision.reason}`,
                runId: task.runId
              });
            } catch (approvalError) {
              logger_default2.error({ taskId: task.id, error: approvalError.message }, "Maestro: Failed to create approval record");
            }
            return {
              task: {
                ...task,
                status: "pending_approval",
                errorMessage: `Awaiting ${governanceDecision.requiredApprovals} approval(s)`,
                updatedAt: now
              },
              artifact: null
            };
          }
          logger_default2.info({
            taskId: task.id,
            runId: task.runId,
            agentId: task.agent.id,
            riskScore: governanceDecision.riskScore
          }, "Task passed governance checks, proceeding with execution");
          if (task.kind === "action" && tenantId) {
            try {
              const { Neo4jNarrativeLoader: Neo4jNarrativeLoader2 } = await Promise.resolve().then(() => (init_neo4j_loader(), neo4j_loader_exports));
              const { narrativeSimulationManager: narrativeSimulationManager2 } = await Promise.resolve().then(() => (init_manager(), manager_exports));
              const rootId = task.input?.rootId || task.input?.targetId;
              if (rootId) {
                const initialEntities = await Neo4jNarrativeLoader2.loadFromGraph(rootId, 2);
                if (initialEntities.length > 0) {
                  const sim = narrativeSimulationManager2.createSimulation({
                    name: `Impact Prediction: ${task.id}`,
                    themes: ["Security", "Trust"],
                    initialEntities,
                    metadata: { taskId: task.id, isShadow: true }
                  });
                  narrativeSimulationManager2.injectActorAction(sim.id, task.agent.id, task.description);
                  const predictedState = await narrativeSimulationManager2.tick(sim.id, 5);
                  task.output = {
                    ...task.output,
                    impactForecast: {
                      summary: predictedState.narrative.summary,
                      arcs: predictedState.arcs.map((arc) => ({ theme: arc.theme, momentum: arc.momentum, outlook: arc.outlook }))
                    }
                  };
                  narrativeSimulationManager2.remove(sim.id);
                }
              }
            } catch (simError) {
              logger_default2.warn({ taskId: task.id, error: simError.message }, "Maestro: Narrative impact prediction failed (non-blocking)");
            }
          }
          let result2 = "";
          if (task.agent.kind === "llm") {
            metrics2.maestroAiModelRequests.inc({ model: task.agent.modelId, operation: "executeTask", status: "attempt" });
            let attempts = 0;
            const maxRetries = 3;
            let lastError;
            while (attempts < maxRetries) {
              const controller = new AbortController();
              const signal = controller.signal;
              let timeoutId;
              try {
                const timeout = new Promise((_2, reject) => {
                  timeoutId = setTimeout(() => {
                    controller.abort();
                    reject(new Error("LLM execution timed out"));
                  }, 6e4);
                });
                const llmCall = this.llm.callCompletion(
                  task.runId,
                  task.id,
                  {
                    model: task.agent.modelId,
                    messages: [
                      { role: "system", content: "You are an execution agent." },
                      { role: "user", content: task.description },
                      ...task.input.requestText ? [{ role: "user", content: String(task.input.requestText) }] : []
                    ],
                    tools: mcpRegistry.listServers().flatMap((s) => {
                      const srv = mcpRegistry.getServer(s);
                      return srv?.tools.map((t) => ({
                        type: "function",
                        function: {
                          name: t.name,
                          description: t.description,
                          parameters: t.schema
                        }
                      })) || [];
                    })
                  },
                  {
                    feature: `maestro_${task.kind}`,
                    tenantId: typeof task.input?.tenantId === "string" ? task.input.tenantId : void 0,
                    environment: process.env.NODE_ENV || "unknown",
                    // @ts-ignore
                    signal
                  }
                );
                const llmResult = await Promise.race([llmCall, timeout]);
                clearTimeout(timeoutId);
                if (llmResult.tool_calls && llmResult.tool_calls.length > 0) {
                  const toolLogs = [];
                  const toolOutputs = [];
                  for (const call of llmResult.tool_calls) {
                    const { name, arguments: argsJson } = call.function;
                    const args = JSON.parse(argsJson);
                    const servers = mcpRegistry.findServersWithTool(name);
                    if (servers.length > 0) {
                      const toolResult = await mcpClient.executeTool(servers[0], name, args);
                      toolOutputs.push({ tool: name, result: toolResult });
                      toolLogs.push(`Executed tool ${name}`);
                    }
                  }
                  result2 = JSON.stringify({
                    explanation: llmResult.content,
                    tool_results: toolOutputs
                  });
                  task.output = { ...task.output, logs: [...task.output?.logs || [], ...toolLogs] };
                } else {
                  result2 = llmResult.content;
                }
                break;
              } catch (err) {
                clearTimeout(timeoutId);
                lastError = err;
                attempts++;
                if (!signal.aborted) controller.abort();
                if (attempts >= maxRetries) break;
                await new Promise((resolve2) => setTimeout(resolve2, 1e3 * Math.pow(2, attempts - 1)));
              }
            }
            if (!result2 && lastError) throw lastError;
          } else {
            result2 = "TODO: implement non-LLM agent";
          }
          const artifact = {
            id: crypto.randomUUID(),
            runId: task.runId,
            taskId: task.id,
            tenantId: task.tenantId,
            kind: "text",
            label: "task-output",
            data: result2,
            createdAt: (/* @__PURE__ */ new Date()).toISOString()
          };
          const updatedTask = {
            status: "succeeded",
            output: { ...task.output, result: result2 },
            updatedAt: (/* @__PURE__ */ new Date()).toISOString()
          };
          await this.ig.createArtifact(artifact);
          await this.ig.updateTask(task.id, updatedTask);
          timer3();
          return {
            task: { ...task, ...updatedTask },
            artifact
          };
        } catch (err) {
          const updatedTask = {
            status: "failed",
            errorMessage: err?.message ?? String(err),
            updatedAt: (/* @__PURE__ */ new Date()).toISOString()
          };
          await this.ig.updateTask(task.id, updatedTask);
          metrics2.maestroAiModelErrors.inc({ model: task.agent.modelId || "unknown" });
          timer3({ status: "failed" });
          return { task: { ...task, ...updatedTask }, artifact: null };
        }
      }
      async runPipeline(userId, requestText, options2) {
        const end = metrics2.maestroOrchestrationDuration.startTimer({ endpoint: "runPipeline" });
        metrics2.maestroOrchestrationRequests.inc({ method: "runPipeline", endpoint: "runPipeline", status: "started" });
        metrics2.maestroActiveSessions.inc({ type: "pipeline" });
        const startTime = Date.now();
        try {
          const run = await this.createRun(userId, requestText, options2);
          const tasks = await this.planRequest(run);
          const executable = tasks.filter((t) => t.status === "queued");
          const results = await Promise.all(
            executable.map((task) => this.executeTask(task))
          );
          const costSummary = await this.costMeter.summarize(run.id);
          const budgetEvidence = buildBudgetEvidence(run.reasoningBudget, {
            success: results.every((result2) => result2.task.status === "succeeded"),
            latencyMs: Date.now() - startTime,
            totalCostUSD: costSummary.totalCostUSD,
            totalInputTokens: costSummary.totalInputTokens,
            totalOutputTokens: costSummary.totalOutputTokens
          });
          await this.ig.updateRun(run.id, {
            reasoningBudgetEvidence: budgetEvidence
          });
          end();
          metrics2.maestroOrchestrationRequests.inc({ method: "runPipeline", endpoint: "runPipeline", status: "success" });
          return {
            run,
            tasks: tasks.map((t) => ({
              id: t.id,
              status: t.status,
              description: t.description
            })),
            results,
            costSummary
          };
        } catch (error) {
          metrics2.maestroOrchestrationErrors.inc({ error_type: "pipeline_error", endpoint: "runPipeline" });
          metrics2.maestroOrchestrationRequests.inc({ method: "runPipeline", endpoint: "runPipeline", status: "error" });
          throw error;
        } finally {
          metrics2.maestroActiveSessions.dec({ type: "pipeline" });
        }
      }
    };
  }
});

// src/maestro/queries.ts
var queries_exports = {};
__export(queries_exports, {
  MaestroQueries: () => MaestroQueries
});
var MaestroQueries;
var init_queries = __esm({
  "src/maestro/queries.ts"() {
    "use strict";
    MaestroQueries = class {
      constructor(ig) {
        this.ig = ig;
      }
      async getTaskWithArtifacts(taskId) {
        const task = await this.ig.getTask(taskId);
        if (!task) return null;
        const artifacts = await this.ig.getArtifactsForTask(taskId);
        return { task, artifacts };
      }
      /**
       * Reconstruct a MaestroRunResponse from stored graph data.
       */
      async getRunResponse(runId) {
        const run = await this.ig.getRun(runId);
        if (!run) return null;
        const [tasks, artifacts, costSummary] = await Promise.all([
          this.ig.getTasksForRun(runId),
          this.ig.getArtifactsForRun(runId),
          this.ig.getRunCostSummary(runId)
        ]);
        const taskSummaries = tasks.map((t) => ({
          id: t.id,
          status: t.status,
          description: t.description
        }));
        const results = tasks.map((task) => {
          const taskArtifacts = artifacts.filter((a) => a.taskId === task.id);
          const artifact = taskArtifacts[0] ?? null;
          return {
            task: {
              id: task.id,
              status: task.status,
              description: task.description,
              errorMessage: task.errorMessage
            },
            artifact
          };
        });
        return {
          run,
          tasks: taskSummaries,
          results,
          costSummary
        };
      }
    };
  }
});

// src/intelgraph/client-impl.ts
var client_impl_exports = {};
__export(client_impl_exports, {
  IntelGraphClientImpl: () => IntelGraphClientImpl
});
var IntelGraphClientImpl;
var init_client_impl = __esm({
  "src/intelgraph/client-impl.ts"() {
    "use strict";
    init_database();
    IntelGraphClientImpl = class {
      get driver() {
        return getNeo4jDriver2();
      }
      serializeRunProps(run) {
        const { reasoningBudget, reasoningBudgetEvidence, ...props } = run;
        return {
          ...props,
          ...reasoningBudget ? { reasoningBudget: JSON.stringify(reasoningBudget) } : {},
          ...reasoningBudgetEvidence ? { reasoningBudgetEvidence: JSON.stringify(reasoningBudgetEvidence) } : {}
        };
      }
      async createRun(run) {
        const session = this.driver.session();
        try {
          const { user, ...rest } = run;
          const props = this.serializeRunProps(rest);
          await session.run(
            `
        MERGE (r:MaestroRun {id: $props.id})
        SET r += $props
        SET r.userId = $userId
        WITH r
        MERGE (u:User {id: $userId})
        MERGE (r)-[:REQUESTED_BY]->(u)
        `,
            { props, userId: user?.id || "anonymous" }
          );
        } finally {
          await session.close();
        }
      }
      async updateRun(runId, patch) {
        const session = this.driver.session();
        try {
          const { user, ...rest } = patch;
          const props = this.serializeRunProps(rest);
          await session.run(
            `
        MATCH (r:MaestroRun {id: $runId})
        SET r += $props
        `,
            { runId, props }
          );
        } finally {
          await session.close();
        }
      }
      async createTask(task) {
        const session = this.driver.session();
        try {
          const { agent, ...props } = task;
          await session.run(
            `
        MATCH (r:MaestroRun {id: $props.runId})
        MERGE (t:MaestroTask {id: $props.id})
        SET t += $props
        SET t.agentId = $agentId, t.agentName = $agentName, t.agentKind = $agentKind
        MERGE (r)-[:HAS_TASK]->(t)
        `,
            {
              props,
              agentId: agent?.id,
              agentName: agent?.name,
              agentKind: agent?.kind
            }
          );
        } finally {
          await session.close();
        }
      }
      async updateTask(taskId, patch) {
        const session = this.driver.session();
        try {
          const { agent, ...props } = patch;
          await session.run(
            `
        MATCH (t:MaestroTask {id: $taskId})
        WHERE t.tenantId = $tenantId OR $tenantId IS NULL
        SET t += $props
        `,
            { taskId, props, tenantId: props.tenantId }
          );
        } finally {
          await session.close();
        }
      }
      async createArtifact(artifact) {
        const session = this.driver.session();
        try {
          await session.run(
            `
        MATCH (t:MaestroTask {id: $artifact.taskId})
        MERGE (a:MaestroArtifact {id: $artifact.id})
        SET a += $artifact
        SET a.tenantId = t.tenantId
        MERGE (t)-[:HAS_ARTIFACT]->(a)
        MERGE (r:MaestroRun {id: $artifact.runId})
        MERGE (r)-[:HAS_ARTIFACT]->(a)
        `,
            { artifact }
          );
        } finally {
          await session.close();
        }
      }
      async recordCostSample(sample) {
        const session = this.driver.session();
        try {
          await session.run(
            `
        MATCH (r:MaestroRun {id: $sample.runId})
        CREATE (c:MaestroCostSample)
        SET c += $sample
        MERGE (r)-[:HAS_COST]->(c)
        `,
            { sample }
          );
        } finally {
          await session.close();
        }
      }
      async getRunCostSummary(runId) {
        const session = this.driver.session();
        try {
          const result2 = await session.run(
            `
        MATCH (r:MaestroRun {id: $runId})-[:HAS_COST]->(c:MaestroCostSample)
        RETURN
          sum(c.costUSD) as totalCostUSD,
          sum(c.inputTokens) as totalInputTokens,
          sum(c.outputTokens) as totalOutputTokens,
          collect({model: c.model, costUSD: c.costUSD, inputTokens: c.inputTokens, outputTokens: c.outputTokens}) as samples
        `,
            { runId }
          );
          if (result2.records.length === 0) {
            return {
              runId,
              totalCostUSD: 0,
              totalInputTokens: 0,
              totalOutputTokens: 0,
              byModel: {}
            };
          }
          const record2 = result2.records[0];
          const samples = record2.get("samples");
          const byModel = {};
          for (const s of samples) {
            if (!byModel[s.model]) {
              byModel[s.model] = { costUSD: 0, inputTokens: 0, outputTokens: 0 };
            }
            byModel[s.model].costUSD += s.costUSD;
            byModel[s.model].inputTokens += s.inputTokens;
            byModel[s.model].outputTokens += s.outputTokens;
          }
          return {
            runId,
            totalCostUSD: record2.get("totalCostUSD") || 0,
            totalInputTokens: record2.get("totalInputTokens").toNumber(),
            totalOutputTokens: record2.get("totalOutputTokens").toNumber(),
            byModel
          };
        } finally {
          await session.close();
        }
      }
      async getRun(runId) {
        const session = this.driver.session();
        try {
          const result2 = await session.run(
            `
        MATCH (r:MaestroRun {id: $runId})
        RETURN r
        `,
            { runId }
          );
          if (result2.records.length === 0) return null;
          const props = result2.records[0].get("r").properties;
          return {
            ...props,
            user: { id: props.userId }
          };
        } finally {
          await session.close();
        }
      }
      async getTasksForRun(runId) {
        const session = this.driver.session();
        try {
          const result2 = await session.run(
            `
        MATCH (r:MaestroRun {id: $runId})-[:HAS_TASK]->(t:MaestroTask)
        RETURN t
        ORDER BY t.createdAt
        `,
            { runId }
          );
          return result2.records.map((r) => {
            const props = r.get("t").properties;
            return {
              ...props,
              agent: props.agentId ? { id: props.agentId, name: props.agentName, kind: props.agentKind } : void 0
            };
          });
        } finally {
          await session.close();
        }
      }
      async getTask(taskId) {
        const session = this.driver.session();
        try {
          const result2 = await session.run(
            `
        MATCH (t:MaestroTask {id: $taskId})
        RETURN t
        `,
            { taskId }
          );
          if (result2.records.length === 0) return null;
          const props = result2.records[0].get("t").properties;
          return {
            ...props,
            agent: props.agentId ? { id: props.agentId, name: props.agentName, kind: props.agentKind } : void 0,
            tenantId: props.tenantId
          };
        } finally {
          await session.close();
        }
      }
      async getArtifactsForRun(runId) {
        const session = this.driver.session();
        try {
          const result2 = await session.run(
            `
        MATCH (r:MaestroRun {id: $runId})-[:HAS_ARTIFACT]->(a:MaestroArtifact)
        RETURN a
        ORDER BY a.createdAt
        `,
            { runId }
          );
          return result2.records.map((r) => r.get("a").properties);
        } finally {
          await session.close();
        }
      }
      async getArtifactsForTask(taskId) {
        const session = this.driver.session();
        try {
          const result2 = await session.run(
            `
        MATCH (t:MaestroTask {id: $taskId})-[:HAS_ARTIFACT]->(a:MaestroArtifact)
        RETURN a
        ORDER BY a.createdAt
        `,
            { taskId }
          );
          return result2.records.map((r) => r.get("a").properties);
        } finally {
          await session.close();
        }
      }
    };
  }
});

// src/maestro/dsl.ts
var MaestroDSL;
var init_dsl = __esm({
  "src/maestro/dsl.ts"() {
    "use strict";
    MaestroDSL = class {
      /**
       * Validates a MaestroSpec to ensure it is a valid DAG (no cycles)
       * and that all references are valid.
       */
      static validate(spec) {
        if (!spec.nodes || spec.nodes.length === 0) {
          return { valid: false, error: "Spec must have at least one node" };
        }
        const nodeIds = /* @__PURE__ */ new Set();
        for (const node of spec.nodes) {
          if (nodeIds.has(node.id)) {
            return { valid: false, error: `Duplicate node ID: ${node.id}` };
          }
          nodeIds.add(node.id);
        }
        for (const edge of spec.edges) {
          if (!nodeIds.has(edge.from)) {
            return { valid: false, error: `Edge source invalid: ${edge.from}` };
          }
          if (!nodeIds.has(edge.to)) {
            return { valid: false, error: `Edge target invalid: ${edge.to}` };
          }
        }
        if (this.hasCycle(spec)) {
          return { valid: false, error: "Spec contains a cycle" };
        }
        return { valid: true };
      }
      static hasCycle(spec) {
        const visited = /* @__PURE__ */ new Set();
        const recursionStack = /* @__PURE__ */ new Set();
        const adj = /* @__PURE__ */ new Map();
        for (const node of spec.nodes) {
          adj.set(node.id, []);
        }
        for (const edge of spec.edges) {
          adj.get(edge.from)?.push(edge.to);
        }
        const detect = (nodeId) => {
          visited.add(nodeId);
          recursionStack.add(nodeId);
          const neighbors = adj.get(nodeId) || [];
          for (const neighbor of neighbors) {
            if (!visited.has(neighbor)) {
              if (detect(neighbor)) return true;
            } else if (recursionStack.has(neighbor)) {
              return true;
            }
          }
          recursionStack.delete(nodeId);
          return false;
        };
        for (const node of spec.nodes) {
          if (!visited.has(node.id)) {
            if (detect(node.id)) return true;
          }
        }
        return false;
      }
      /**
       * Compiles a Spec into a set of initial Tasks for a Run.
       * This handles creating the Task entities with correct dependencies.
       */
      static compileToTasks(spec, runId, tenantId, input) {
        const tasks = [];
        const nodeIdToTaskId = /* @__PURE__ */ new Map();
        for (const node of spec.nodes) {
          const taskId = crypto.randomUUID();
          nodeIdToTaskId.set(node.id, taskId);
          let taskKind = "custom";
          if (node.kind === "task") {
            taskKind = node.ref;
          } else if (node.kind === "agent_call") {
            taskKind = "agent_call";
          } else if (node.kind === "subflow") {
            taskKind = "subflow";
          }
          const task = {
            id: taskId,
            runId,
            tenantId,
            name: node.name || node.id,
            kind: taskKind,
            // Cast to known kind
            status: "pending",
            dependsOn: [],
            // filled later
            attempt: 0,
            maxAttempts: node.config?.maxAttempts || 3,
            backoffStrategy: node.config?.backoffStrategy || "exponential",
            payload: {
              ...node.config || {},
              // Merge inputMapping logic would happen at runtime execution,
              // or we store the mapping definition here in payload/metadata
              inputMapping: node.inputMapping,
              ref: node.ref
            },
            metadata: {
              nodeId: node.id
            }
          };
          tasks.push(task);
        }
        for (const edge of spec.edges) {
          const parentTaskId = nodeIdToTaskId.get(edge.from);
          const childTaskId = nodeIdToTaskId.get(edge.to);
          if (parentTaskId && childTaskId) {
            const childTask = tasks.find((t) => t.id === childTaskId);
            if (childTask) {
              childTask.dependsOn.push(parentTaskId);
            }
          }
        }
        for (const task of tasks) {
          if (task.dependsOn.length === 0) {
            task.status = "ready";
          }
        }
        return tasks;
      }
    };
  }
});

// src/maestro/coordination/budget-manager.ts
var CoordinationBudgetManager, budgetManager;
var init_budget_manager = __esm({
  "src/maestro/coordination/budget-manager.ts"() {
    "use strict";
    CoordinationBudgetManager = class {
      // In-memory store for now. In production, this would be Redis/Postgres.
      contexts = /* @__PURE__ */ new Map();
      initialize(context4) {
        if (this.contexts.has(context4.coordinationId)) {
          throw new Error(`Budget/Context already exists for coordinationId: ${context4.coordinationId}`);
        }
        this.contexts.set(context4.coordinationId, context4);
      }
      get(coordinationId) {
        return this.contexts.get(coordinationId);
      }
      checkBudget(coordinationId) {
        const context4 = this.contexts.get(coordinationId);
        if (!context4) {
          return { allowed: false, reason: "Coordination context not found" };
        }
        if (context4.status !== "ACTIVE") {
          return { allowed: false, reason: `Context is ${context4.status}` };
        }
        const elapsed = Date.now() - context4.startTime.getTime();
        if (elapsed > context4.budget.wallClockTimeMs) {
          return { allowed: false, reason: "Wall clock time exceeded" };
        }
        if (context4.budgetConsumed.totalSteps >= context4.budget.totalSteps) {
          return { allowed: false, reason: "Step limit exceeded" };
        }
        if (context4.budgetConsumed.totalTokens >= context4.budget.totalTokens) {
          return { allowed: false, reason: "Token limit exceeded" };
        }
        return { allowed: true };
      }
      consumeBudget(coordinationId, usage) {
        const context4 = this.contexts.get(coordinationId);
        if (!context4) {
          throw new Error(`Coordination context not found: ${coordinationId}`);
        }
        if (usage.totalSteps) {
          context4.budgetConsumed.totalSteps += usage.totalSteps;
        }
        if (usage.totalTokens) {
          context4.budgetConsumed.totalTokens += usage.totalTokens;
        }
        const check = this.checkBudget(coordinationId);
        if (!check.allowed && check.reason !== `Context is ${context4.status}`) {
        }
      }
      delete(coordinationId) {
        return this.contexts.delete(coordinationId);
      }
    };
    budgetManager = new CoordinationBudgetManager();
  }
});

// src/maestro/subagent-coordinator.ts
var SubagentCoordinator, subagentCoordinator;
var init_subagent_coordinator = __esm({
  "src/maestro/subagent-coordinator.ts"() {
    "use strict";
    init_ledger();
    init_logger2();
    init_governance_service2();
    SubagentCoordinator = class _SubagentCoordinator {
      static instance;
      tasks = /* @__PURE__ */ new Map();
      channels = /* @__PURE__ */ new Map();
      activeProposals = /* @__PURE__ */ new Map();
      metrics = /* @__PURE__ */ new Map();
      ledger;
      agentStatusCallbacks = /* @__PURE__ */ new Map();
      constructor() {
        this.ledger = ProvenanceLedgerV2.getInstance();
      }
      static getInstance() {
        if (!_SubagentCoordinator.instance) {
          _SubagentCoordinator.instance = new _SubagentCoordinator();
        }
        return _SubagentCoordinator.instance;
      }
      /**
       * Create a new coordination channel for agents to collaborate
       */
      async createChannel(topic, participants, metadata) {
        const channelId = `channel-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`;
        const channel = {
          id: channelId,
          participants,
          topic,
          messages: [],
          createdAt: /* @__PURE__ */ new Date(),
          isActive: true
        };
        this.channels.set(channelId, channel);
        await this.ledger.appendEntry({
          tenantId: "system",
          actionType: "CHANNEL_CREATED",
          resourceType: "CoordinationChannel",
          resourceId: channelId,
          actorId: "coordinator",
          actorType: "system",
          timestamp: /* @__PURE__ */ new Date(),
          payload: {
            mutationType: "CREATE",
            entityId: channelId,
            entityType: "CoordinationChannel",
            topic,
            participants,
            metadata
          },
          metadata: {
            topic,
            participants: participants.length,
            createdChannel: true
          }
        });
        logger_default2.info({
          channelId,
          participants: participants.length,
          topic
        }, "Coordination channel created");
        return channel;
      }
      /**
       * Assign a task to specific agents with workload balancing
       */
      async assignTask(task, agentIds) {
        const taskId = `task-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`;
        const coordinationTask = {
          ...task,
          id: taskId,
          assignedAgentIds: agentIds,
          status: "delegated",
          createdAt: /* @__PURE__ */ new Date(),
          updatedAt: /* @__PURE__ */ new Date()
        };
        this.tasks.set(taskId, coordinationTask);
        logger_default2.info({
          taskId,
          agentsAssigned: agentIds.length,
          title: task.title
        }, "Task assigned to agents for coordination");
        await this.ledger.appendEntry({
          tenantId: "system",
          actionType: "TASK_ASSIGNED",
          resourceType: "CoordinationTask",
          resourceId: taskId,
          actorId: "coordinator",
          actorType: "system",
          timestamp: /* @__PURE__ */ new Date(),
          payload: {
            mutationType: "CREATE",
            entityId: taskId,
            entityType: "CoordinationTask",
            title: task.title,
            description: task.description,
            assignedAgentIds: agentIds,
            priority: task.priority,
            payload: task.payload
          },
          metadata: {
            assignedAgentCount: agentIds.length,
            priority: task.priority,
            topic: task.title
          }
        });
        return coordinationTask;
      }
      /**
       * Submit a proposal for consensus among agents
       */
      async submitConsensusProposal(coordinatorId, topic, proposal, voterAgentIds, deadlineHours = 24) {
        const proposalId = `proposal-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`;
        const consensusProposal = {
          id: proposalId,
          coordinatorId,
          topic,
          proposal,
          voters: voterAgentIds,
          votingDeadline: new Date(Date.now() + deadlineHours * 60 * 60 * 1e3),
          votes: /* @__PURE__ */ new Map(),
          status: "in_voting",
          createdAt: /* @__PURE__ */ new Date()
        };
        this.activeProposals.set(proposalId, consensusProposal);
        logger_default2.info({
          proposalId,
          topic,
          voters: voterAgentIds.length
        }, "Consensus proposal submitted");
        for (const agentId of voterAgentIds) {
          await this.sendMessage(
            agentId,
            coordinatorId,
            "CONSENSUS_PROPOSAL",
            `Consensus proposal submitted: ${topic}`,
            [{
              proposalId,
              proposal: consensusProposal.proposal,
              deadline: consensusProposal.votingDeadline
            }]
          );
        }
        await this.ledger.appendEntry({
          tenantId: "system",
          actionType: "PROPOSAL_SUBMITTED",
          resourceType: "ConsensusProposal",
          resourceId: proposalId,
          actorId: coordinatorId,
          actorType: "system",
          timestamp: /* @__PURE__ */ new Date(),
          payload: {
            mutationType: "CREATE",
            entityId: proposalId,
            entityType: "ConsensusProposal",
            topic,
            voters: voterAgentIds,
            deadlineHours
          },
          metadata: {
            proposalId,
            topic,
            voterCount: voterAgentIds.length
          }
        });
        return consensusProposal;
      }
      /**
       * Cast a vote on a consensus proposal
       */
      async voteOnProposal(agentId, proposalId, vote, rationale) {
        const proposal = this.activeProposals.get(proposalId);
        if (!proposal) {
          throw new Error(`Proposal ${proposalId} not found`);
        }
        if (!proposal.voters.includes(agentId)) {
          throw new Error(`Agent ${agentId} is not authorized to vote on proposal ${proposalId}`);
        }
        proposal.votes.set(agentId, {
          vote,
          timestamp: /* @__PURE__ */ new Date(),
          rationale
        });
        proposal.updatedAt = /* @__PURE__ */ new Date();
        const allVoted = proposal.voters.every((voter) => proposal.votes.has(voter));
        const deadlinePassed = /* @__PURE__ */ new Date() > proposal.votingDeadline;
        if (allVoted || deadlinePassed) {
          const approveCount = Array.from(proposal.votes.values()).filter((v) => v.vote === "approve").length;
          const rejectCount = Array.from(proposal.votes.values()).filter((v) => v.vote === "reject").length;
          const quorumMet = proposal.votes.size >= Math.floor(proposal.voters.length * 0.6);
          if (quorumMet && approveCount > rejectCount) {
            proposal.status = "passed";
            logger_default2.info({
              proposalId,
              approveCount,
              rejectCount,
              voterCount: proposal.votes.size
            }, "Consensus proposal passed");
          } else if (quorumMet) {
            proposal.status = "rejected";
            logger_default2.info({
              proposalId,
              approveCount,
              rejectCount,
              voterCount: proposal.votes.size
            }, "Consensus proposal rejected");
          }
          proposal.closedAt = /* @__PURE__ */ new Date();
        }
        await this.ledger.appendEntry({
          tenantId: "system",
          actionType: "VOTE_CAST",
          resourceType: "ConsensusProposal",
          resourceId: proposalId,
          actorId: agentId,
          actorType: "system",
          timestamp: /* @__PURE__ */ new Date(),
          payload: {
            mutationType: "UPDATE",
            entityId: proposalId,
            entityType: "ConsensusProposal",
            vote,
            rationale
          },
          metadata: {
            agentId,
            vote,
            proposalTopic: proposal.topic
          }
        });
        return proposal;
      }
      /**
       * Send a coordination message to one or more agents
       */
      async sendMessage(recipientId, senderId, type, content, attachments, correlationId, channelId) {
        const messageId = `msg-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`;
        const message = {
          id: messageId,
          senderId,
          recipientId,
          timestamp: /* @__PURE__ */ new Date(),
          type,
          content,
          attachments,
          correlationId,
          metadata: {
            direction: "direct",
            sender: senderId,
            recipient: recipientId
          }
        };
        if (channelId) {
          const channel = this.channels.get(channelId);
          if (channel) {
            message.metadata = {
              ...message.metadata,
              channelId,
              direction: "broadcast"
            };
            channel.messages.push(message);
            channel.updatedAt = /* @__PURE__ */ new Date();
            logger_default2.info({
              messageId,
              channel: channelId,
              sender: senderId,
              type
            }, "Broadcast coordination message");
          }
        }
        await this.updateAgentMetrics(senderId, { coordinationMessagesSent: 1 });
        if (recipientId !== senderId) {
          await this.updateAgentMetrics(recipientId, { coordinationMessagesReceived: 1 });
        }
        logger_default2.info({
          messageId,
          sender: senderId,
          recipient: recipientId,
          channel: channelId,
          type
        }, "Coordination message sent");
        await this.ledger.appendEntry({
          tenantId: "system",
          actionType: "MESSAGE_SENT",
          resourceType: "CoordinationMessage",
          resourceId: messageId,
          actorId: senderId,
          actorType: "system",
          timestamp: /* @__PURE__ */ new Date(),
          payload: {
            mutationType: "CREATE",
            entityId: messageId,
            entityType: "CoordinationMessage",
            senderId,
            recipientId,
            type,
            content,
            correlationId
          },
          metadata: {
            messageType: type,
            hasAttachments: !!attachments?.length,
            channel: channelId,
            messageLength: content.length
          }
        });
        return message;
      }
      /**
       * Request help from other agents when facing complex tasks
       */
      async requestHelp(requestingAgentId, taskDescription, urgency, requiredCapabilities) {
        logger_default2.info({
          requestingAgentId,
          taskDescription,
          urgency,
          requiredCapabilities
        }, "Agent requested help from peers");
        const availableAgents = this.findAvailableAgents(requiredCapabilities);
        const helpTask = await this.assignTask({
          title: `Help Request from Agent ${requestingAgentId}`,
          description: `Agent ${requestingAgentId} needs help with: ${taskDescription}`,
          assignedAgentIds: availableAgents,
          priority: urgency,
          payload: {
            originalRequestId: requestingAgentId,
            task: taskDescription,
            capabilities: requiredCapabilities
          }
        }, availableAgents);
        await this.ledger.appendEntry({
          tenantId: "system",
          actionType: "HELP_REQUESTED",
          resourceType: "CoordinationRequest",
          resourceId: helpTask.id,
          actorId: requestingAgentId,
          actorType: "system",
          timestamp: /* @__PURE__ */ new Date(),
          payload: {
            mutationType: "CREATE",
            entityId: helpTask.id,
            entityType: "CoordinationRequest",
            requester: requestingAgentId,
            task: taskDescription,
            urgency,
            requiredCapabilities
          },
          metadata: {
            requesterAgentId: requestingAgentId,
            urgency,
            capabilityCount: requiredCapabilities.length
          }
        });
        return [helpTask];
      }
      /**
       * Find agents available for coordination work
       */
      findAvailableAgents(requiredCapabilities) {
        const allAgentIds = Array.from(this.agentStatusCallbacks.keys());
        return allAgentIds;
      }
      /**
       * Update coordination metrics for an agent
       */
      async updateAgentMetrics(agentId, metricsUpdate) {
        if (!this.metrics.has(agentId)) {
          this.metrics.set(agentId, {
            coordinationMessagesSent: 0,
            coordinationMessagesReceived: 0,
            collaborativeTasksCompleted: 0,
            consensusDecisionsMade: 0,
            resourceSharingEvents: 0,
            conflictResolutionEvents: 0,
            averageCollaborationTimeMs: 0
          });
        }
        const current = this.metrics.get(agentId);
        Object.entries(metricsUpdate).forEach(([key, value]) => {
          if (typeof value === "number" && typeof current[key] === "number") {
            current[key] += value;
          } else {
            current[key] = value;
          }
        });
        current.updatedAt = /* @__PURE__ */ new Date();
      }
      /**
       * Get coordination metrics for an agent
       */
      getAgentMetrics(agentId) {
        return this.metrics.get(agentId) || null;
      }
      /**
       * Register a callback for agent status changes
       */
      registerAgentStatusCallback(agentId, callback) {
        this.agentStatusCallbacks.set(agentId, callback);
      }
      /**
       * Remove an agent status callback
       */
      removeAgentStatusCallback(agentId) {
        this.agentStatusCallbacks.delete(agentId);
      }
    };
    subagentCoordinator = SubagentCoordinator.getInstance();
  }
});

// src/services/KillSwitchService.ts
var KillSwitchService, killSwitchService;
var init_KillSwitchService = __esm({
  "src/services/KillSwitchService.ts"() {
    "use strict";
    init_logger();
    KillSwitchService = class _KillSwitchService {
      static instance;
      state = {
        global: false,
        agents: /* @__PURE__ */ new Set(),
        features: /* @__PURE__ */ new Set(),
        tenants: /* @__PURE__ */ new Set()
      };
      constructor() {
      }
      static getInstance() {
        if (!_KillSwitchService.instance) {
          _KillSwitchService.instance = new _KillSwitchService();
        }
        return _KillSwitchService.instance;
      }
      engageGlobalKillSwitch(actor, reason) {
        this.state.global = true;
        logger.fatal({ actor, reason }, "GLOBAL KILL SWITCH ENGAGED");
      }
      disengageGlobalKillSwitch(actor, reason) {
        this.state.global = false;
        logger.info({ actor, reason }, "Global Kill Switch Disengaged");
      }
      isGlobalKillSwitchActive() {
        return this.state.global;
      }
      killAgent(agentId, actor, reason) {
        this.state.agents.add(agentId);
        logger.warn({ agentId, actor, reason }, "Agent Kill Switch Engaged");
      }
      reviveAgent(agentId, actor, reason) {
        this.state.agents.delete(agentId);
        logger.info({ agentId, actor, reason }, "Agent Kill Switch Disengaged");
      }
      isAgentKilled(agentId) {
        return this.state.global || this.state.agents.has(agentId);
      }
      killFeature(featureKey, actor, reason) {
        this.state.features.add(featureKey);
        logger.warn({ featureKey, actor, reason }, "Feature Kill Switch Engaged");
      }
      reviveFeature(featureKey, actor, reason) {
        this.state.features.delete(featureKey);
        logger.info({ featureKey, actor, reason }, "Feature Kill Switch Disengaged");
      }
      isFeatureKilled(featureKey) {
        return this.state.global || this.state.features.has(featureKey);
      }
      killTenant(tenantId, actor, reason) {
        this.state.tenants.add(tenantId);
        logger.warn({ tenantId, actor, reason }, "Tenant Kill Switch Engaged");
      }
      reviveTenant(tenantId, actor, reason) {
        this.state.tenants.delete(tenantId);
        logger.info({ tenantId, actor, reason }, "Tenant Kill Switch Disengaged");
      }
      isTenantKilled(tenantId) {
        return this.state.global || this.state.tenants.has(tenantId);
      }
      checkSystemHealth(context4) {
        if (this.state.global) {
          return { allowed: false, reason: "Global Kill Switch is Active" };
        }
        if (context4?.agentId && this.state.agents.has(context4.agentId)) {
          return { allowed: false, reason: `Agent ${context4.agentId} is kill-switched` };
        }
        if (context4?.tenantId && this.state.tenants.has(context4.tenantId)) {
          return { allowed: false, reason: `Tenant ${context4.tenantId} is kill-switched` };
        }
        if (context4?.feature && this.state.features.has(context4.feature)) {
          return { allowed: false, reason: `Feature ${context4.feature} is kill-switched` };
        }
        return { allowed: true };
      }
    };
    killSwitchService = KillSwitchService.getInstance();
  }
});

// src/services/TelemetryService.ts
var TelemetryService2, telemetryService2;
var init_TelemetryService2 = __esm({
  "src/services/TelemetryService.ts"() {
    "use strict";
    init_logger();
    TelemetryService2 = class _TelemetryService {
      static instance;
      constructor() {
      }
      static getInstance() {
        if (!_TelemetryService.instance) {
          _TelemetryService.instance = new _TelemetryService();
        }
        return _TelemetryService.instance;
      }
      logEvent(eventType, event) {
        logger.info({
          eventType,
          ...event
        }, `Telemetry Event: ${eventType}`);
      }
    };
    telemetryService2 = TelemetryService2.getInstance();
  }
});

// src/lib/telemetry/alerting-service.ts
var AlertingService, alertingService;
var init_alerting_service = __esm({
  "src/lib/telemetry/alerting-service.ts"() {
    "use strict";
    AlertingService = class {
      sendAlert(message, context4) {
        console.log(`[ALERT] ${message}`);
        if (context4) {
          console.log(`[ALERT CONTEXT] Logs: ${context4.logs?.length || 0}, Traces: ${context4.relatedTraces?.length || 0}`);
          if (context4.metricsContext) {
            console.log(`[ALERT METRICS] ${JSON.stringify(context4.metricsContext)}`);
          }
        }
      }
    };
    alertingService = new AlertingService();
  }
});

// src/services/DriftDetectionService.ts
import fs40 from "fs/promises";
import crypto48 from "crypto";
var DriftDetectionService, driftDetectionService;
var init_DriftDetectionService = __esm({
  "src/services/DriftDetectionService.ts"() {
    "use strict";
    init_logger();
    init_alerting_service();
    DriftDetectionService = class _DriftDetectionService {
      static instance;
      knownFileHashes = /* @__PURE__ */ new Map();
      baselineAgentMetrics = /* @__PURE__ */ new Map();
      monitoringInterval = null;
      constructor() {
        this.baselineAgentMetrics.set("planner", { successRate: 0.95, errorRate: 0.05 });
      }
      static getInstance() {
        if (!_DriftDetectionService.instance) {
          _DriftDetectionService.instance = new _DriftDetectionService();
        }
        return _DriftDetectionService.instance;
      }
      startMonitoring(intervalMs = 36e5) {
        if (this.monitoringInterval) return;
        logger.info("Starting Drift Detection Monitoring");
        this.runChecks();
        this.monitoringInterval = setInterval(() => {
          this.runChecks();
        }, intervalMs);
      }
      stopMonitoring() {
        if (this.monitoringInterval) {
          clearInterval(this.monitoringInterval);
          this.monitoringInterval = null;
        }
      }
      async runChecks() {
        const criticalPaths = [
          "server/src/maestro/governance-service.js",
          "server/src/services/AuthService.js"
          // Assuming existence or common path
        ];
        const existingPaths = [];
        for (const p of criticalPaths) {
          try {
            await fs40.access(p);
            existingPaths.push(p);
          } catch {
          }
        }
        await this.checkCodeDrift(existingPaths);
        await this.checkPolicyDrift();
      }
      async checkCodeDrift(criticalPaths) {
        let drifted = false;
        for (const filePath of criticalPaths) {
          try {
            const content = await fs40.readFile(filePath);
            const hash3 = crypto48.createHash("sha256").update(content).digest("hex");
            if (this.knownFileHashes.has(filePath)) {
              const knownHash = this.knownFileHashes.get(filePath);
              if (knownHash !== hash3) {
                logger.error({ filePath, knownHash, currentHash: hash3 }, "CODE DRIFT DETECTED");
                alertingService.sendAlert(`Code drift detected in ${filePath}`);
                drifted = true;
              }
            } else {
              this.knownFileHashes.set(filePath, hash3);
            }
          } catch (err) {
            logger.warn({ filePath, error: err }, "Failed to check code drift for file");
          }
        }
        return drifted;
      }
      async checkPolicyDrift() {
      }
      checkBehavioralDrift(agentId, currentMetrics) {
        const baseline = this.baselineAgentMetrics.get(agentId);
        if (!baseline) return;
        if (currentMetrics.successRate < baseline.successRate * 0.9) {
          logger.warn({ agentId, current: currentMetrics.successRate, baseline: baseline.successRate }, "BEHAVIORAL DRIFT: Success Rate Drop");
          alertingService.sendAlert(`Agent ${agentId} success rate dropped below baseline`);
        }
        if (currentMetrics.errorRate > baseline.errorRate * 1.5) {
          logger.warn({ agentId, current: currentMetrics.errorRate, baseline: baseline.errorRate }, "BEHAVIORAL DRIFT: Error Rate Spike");
          alertingService.sendAlert(`Agent ${agentId} error rate spiked above baseline`);
        }
      }
    };
    driftDetectionService = DriftDetectionService.getInstance();
  }
});

// src/maestro/store/orchestrator-store.ts
import { v4 as uuidv435 } from "uuid";
var OrchestratorPostgresStore;
var init_orchestrator_store = __esm({
  "src/maestro/store/orchestrator-store.ts"() {
    "use strict";
    init_logger();
    OrchestratorPostgresStore = class {
      pool;
      constructor(pool4) {
        this.pool = pool4;
      }
      async initialize() {
        const createTablesQuery = `
      -- Maestro loops table
      CREATE TABLE IF NOT EXISTS maestro_loops (
        id VARCHAR(255) PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        type VARCHAR(100) NOT NULL,
        status VARCHAR(50) NOT NULL DEFAULT 'active',
        last_decision TEXT,
        last_run TIMESTAMP WITH TIME ZONE,
        config JSONB DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
      );

      -- Maestro agents table
      CREATE TABLE IF NOT EXISTS maestro_agents (
        id VARCHAR(255) PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        role VARCHAR(255),
        model VARCHAR(255),
        status VARCHAR(50) NOT NULL DEFAULT 'active',
        routing_weight INTEGER DEFAULT 100,
        metrics JSONB DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
      );

      -- Maestro experiments table
      CREATE TABLE IF NOT EXISTS maestro_experiments (
        id VARCHAR(255) PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        hypothesis TEXT,
        status VARCHAR(50) NOT NULL DEFAULT 'running',
        variants TEXT[],
        metrics JSONB DEFAULT '{}',
        start_date TIMESTAMP WITH TIME ZONE,
        end_date TIMESTAMP WITH TIME ZONE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
      );

      -- Maestro playbooks table
      CREATE TABLE IF NOT EXISTS maestro_playbooks (
        id VARCHAR(255) PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        description TEXT,
        triggers TEXT[],
        actions TEXT[],
        is_enabled BOOLEAN DEFAULT TRUE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
      );

      -- Maestro audit log table
      CREATE TABLE IF NOT EXISTS maestro_audit_log (
        id SERIAL PRIMARY KEY,
        actor VARCHAR(255) NOT NULL,
        action VARCHAR(255) NOT NULL,
        resource VARCHAR(255) NOT NULL,
        details TEXT,
        status VARCHAR(50) DEFAULT 'allowed',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
      );

      -- Coordination tasks table
      CREATE TABLE IF NOT EXISTS maestro_coordination_tasks (
        id VARCHAR(255) PRIMARY KEY,
        title VARCHAR(255) NOT NULL,
        description TEXT,
        status VARCHAR(50) NOT NULL DEFAULT 'pending',
        owner_id VARCHAR(255),
        participants TEXT[],
        priority INTEGER DEFAULT 0,
        result JSONB,
        error TEXT,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        completed_at TIMESTAMP WITH TIME ZONE
      );

      -- Coordination channels table
      CREATE TABLE IF NOT EXISTS maestro_coordination_channels (
        id VARCHAR(255) PRIMARY KEY,
        topic VARCHAR(255) NOT NULL,
        participants TEXT[],
        status VARCHAR(50) NOT NULL DEFAULT 'active',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
      );

      -- Consensus proposals table
      CREATE TABLE IF NOT EXISTS maestro_consensus_proposals (
        id VARCHAR(255) PRIMARY KEY,
        topic VARCHAR(255) NOT NULL,
        proposal_data JSONB NOT NULL,
        coordinator_id VARCHAR(255) NOT NULL,
        voters TEXT[],
        votes JSONB DEFAULT '{}',
        status VARCHAR(50) NOT NULL DEFAULT 'voting',
        deadline TIMESTAMP WITH TIME ZONE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
      );

      -- Create indexes for performance
      CREATE INDEX IF NOT EXISTS idx_maestro_loops_status ON maestro_loops(status);
      CREATE INDEX IF NOT EXISTS idx_maestro_agents_status ON maestro_agents(status);
      CREATE INDEX IF NOT EXISTS idx_maestro_experiments_status ON maestro_experiments(status);
      CREATE INDEX IF NOT EXISTS idx_maestro_audit_log_created_at ON maestro_audit_log(created_at);
      CREATE INDEX IF NOT EXISTS idx_maestro_coordination_tasks_status ON maestro_coordination_tasks(status);
      CREATE INDEX IF NOT EXISTS idx_maestro_coordination_channels_topic ON maestro_coordination_channels(topic);
      CREATE INDEX IF NOT EXISTS idx_maestro_consensus_proposals_status ON maestro_consensus_proposals(status);
    `;
        try {
          await this.pool.query(createTablesQuery);
          logger.info("OrchestratorPostgresStore initialized successfully");
        } catch (error) {
          logger.error("Failed to initialize OrchestratorPostgresStore:", error);
          throw error;
        }
      }
      // Maestro Loops methods
      async getLoops() {
        const result2 = await this.pool.query(
          "SELECT id, name, type, status, last_decision, last_run, config FROM maestro_loops ORDER BY created_at DESC"
        );
        return result2.rows.map((row) => ({
          id: row.id,
          name: row.name,
          type: row.type,
          status: row.status,
          lastDecision: row.last_decision,
          lastRun: row.last_run ? new Date(row.last_run).toISOString() : "",
          config: row.config || {}
        }));
      }
      async getLoopById(id) {
        const result2 = await this.pool.query(
          "SELECT id, name, type, status, last_decision, last_run, config FROM maestro_loops WHERE id = $1",
          [id]
        );
        if (result2.rows.length === 0) return null;
        const row = result2.rows[0];
        return {
          id: row.id,
          name: row.name,
          type: row.type,
          status: row.status,
          lastDecision: row.last_decision,
          lastRun: row.last_run ? new Date(row.last_run).toISOString() : "",
          config: row.config || {}
        };
      }
      async updateLoopStatus(id, status) {
        const result2 = await this.pool.query(
          "UPDATE maestro_loops SET status = $1, updated_at = CURRENT_TIMESTAMP WHERE id = $2",
          [status, id]
        );
        return result2.rowCount !== 0;
      }
      // Maestro Agents methods
      async getAgents() {
        const result2 = await this.pool.query(
          "SELECT id, name, role, model, status, routing_weight, metrics FROM maestro_agents ORDER BY created_at DESC"
        );
        return result2.rows.map((row) => ({
          id: row.id,
          name: row.name,
          role: row.role,
          model: row.model,
          status: row.status,
          routingWeight: row.routing_weight,
          metrics: row.metrics || {}
        }));
      }
      async getAgentById(id) {
        const result2 = await this.pool.query(
          "SELECT id, name, role, model, status, routing_weight, metrics FROM maestro_agents WHERE id = $1",
          [id]
        );
        if (result2.rows.length === 0) return null;
        const row = result2.rows[0];
        return {
          id: row.id,
          name: row.name,
          role: row.role,
          model: row.model,
          status: row.status,
          routingWeight: row.routing_weight,
          metrics: row.metrics || {}
        };
      }
      async updateAgent(id, updates, actor) {
        const client6 = await this.pool.connect();
        try {
          await client6.query("BEGIN");
          const updateResult = await client6.query(
            `UPDATE maestro_agents SET 
          name = COALESCE($1, name), 
          role = COALESCE($2, role), 
          model = COALESCE($3, model), 
          status = COALESCE($4, status), 
          routing_weight = COALESCE($5, routing_weight), 
          metrics = COALESCE($6, metrics), 
          updated_at = CURRENT_TIMESTAMP 
         WHERE id = $7 
         RETURNING *`,
            [
              updates.name,
              updates.role,
              updates.model,
              updates.status,
              updates.routingWeight,
              updates.metrics,
              id
            ]
          );
          if (updateResult.rows.length === 0) {
            return null;
          }
          await client6.query(
            "INSERT INTO maestro_audit_log (actor, action, resource, details) VALUES ($1, $2, $3, $4)",
            [actor, "update_agent", `agent:${id}`, `Updated agent ${id}`]
          );
          await client6.query("COMMIT");
          const row = updateResult.rows[0];
          return {
            id: row.id,
            name: row.name,
            role: row.role,
            model: row.model,
            status: row.status,
            routingWeight: row.routing_weight,
            metrics: row.metrics || {}
          };
        } catch (error) {
          await client6.query("ROLLBACK");
          throw error;
        } finally {
          client6.release();
        }
      }
      // Maestro Experiments methods
      async getExperiments() {
        const result2 = await this.pool.query(
          "SELECT id, name, hypothesis, status, variants, metrics, start_date, end_date FROM maestro_experiments ORDER BY created_at DESC"
        );
        return result2.rows.map((row) => ({
          id: row.id,
          name: row.name,
          hypothesis: row.hypothesis,
          status: row.status,
          variants: row.variants || [],
          metrics: row.metrics || {},
          startDate: row.start_date ? new Date(row.start_date).toISOString() : "",
          endDate: row.end_date ? new Date(row.end_date).toISOString() : ""
        }));
      }
      async createExperiment(experiment, actor) {
        const result2 = await this.pool.query(
          `INSERT INTO maestro_experiments 
       (id, name, hypothesis, status, variants, metrics, start_date) 
       VALUES ($1, $2, $3, $4, $5, $6, $7) 
       RETURNING id, name, hypothesis, status, variants, metrics, start_date, created_at`,
          [
            experiment.id || uuidv435(),
            experiment.name,
            experiment.hypothesis,
            experiment.status,
            experiment.variants,
            experiment.metrics,
            experiment.startDate ? new Date(experiment.startDate) : null
          ]
        );
        await this.pool.query(
          "INSERT INTO maestro_audit_log (actor, action, resource, details) VALUES ($1, $2, $3, $4)",
          [actor, "create_experiment", `experiment:${result2.rows[0].id}`, `Created experiment ${result2.rows[0].name}`]
        );
        const row = result2.rows[0];
        return {
          id: row.id,
          name: row.name,
          hypothesis: row.hypothesis,
          status: row.status,
          variants: row.variants || [],
          metrics: row.metrics || {},
          startDate: row.start_date ? new Date(row.start_date).toISOString() : "",
          endDate: row.end_date ? new Date(row.end_date).toISOString() : ""
        };
      }
      // Maestro Playbooks methods
      async getPlaybooks() {
        const result2 = await this.pool.query(
          "SELECT id, name, description, triggers, actions, is_enabled FROM maestro_playbooks ORDER BY created_at DESC"
        );
        return result2.rows.map((row) => ({
          id: row.id,
          name: row.name,
          description: row.description,
          triggers: row.triggers || [],
          actions: row.actions || [],
          isEnabled: row.is_enabled
        }));
      }
      // Coordination methods
      async createCoordinationTask(task, actor) {
        const id = `task_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
        const result2 = await this.pool.query(
          `INSERT INTO maestro_coordination_tasks 
       (id, title, description, owner_id, participants, priority) 
       VALUES ($1, $2, $3, $4, $5, $6) 
       RETURNING id, title, description, status, owner_id, participants, priority, created_at`,
          [id, task.title, task.description, task.ownerId, task.participants, task.priority]
        );
        await this.pool.query(
          "INSERT INTO maestro_audit_log (actor, action, resource, details) VALUES ($1, $2, $3, $4)",
          [actor, "create_coordination_task", `coordination_task:${id}`, `Created coordination task ${task.title}`]
        );
        const row = result2.rows[0];
        return {
          id: row.id,
          title: row.title,
          description: row.description,
          status: row.status || "pending",
          ownerId: row.owner_id,
          participants: row.participants || [],
          priority: row.priority || 0,
          createdAt: row.created_at ? new Date(row.created_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString(),
          updatedAt: row.created_at ? new Date(row.created_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      async getCoordinationTaskById(id) {
        const result2 = await this.pool.query(
          "SELECT id, title, description, status, owner_id, participants, priority, created_at, updated_at FROM maestro_coordination_tasks WHERE id = $1",
          [id]
        );
        if (result2.rows.length === 0) return null;
        const row = result2.rows[0];
        return {
          id: row.id,
          title: row.title,
          description: row.description,
          status: row.status,
          ownerId: row.owner_id,
          participants: row.participants || [],
          priority: row.priority || 0,
          createdAt: row.created_at ? new Date(row.created_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString(),
          updatedAt: row.updated_at ? new Date(row.updated_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      async updateCoordinationTaskStatus(id, status) {
        const result2 = await this.pool.query(
          "UPDATE maestro_coordination_tasks SET status = $1, updated_at = CURRENT_TIMESTAMP WHERE id = $2",
          [status, id]
        );
        return result2.rowCount !== 0;
      }
      async createCoordinationChannel(topic, participantAgentIds, actor) {
        const id = `channel_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
        const result2 = await this.pool.query(
          `INSERT INTO maestro_coordination_channels 
       (id, topic, participants) 
       VALUES ($1, $2, $3) 
       RETURNING id, topic, participants, status, created_at`,
          [id, topic, participantAgentIds]
        );
        await this.pool.query(
          "INSERT INTO maestro_audit_log (actor, action, resource, details) VALUES ($1, $2, $3, $4)",
          [actor, "create_coordination_channel", `coordination_channel:${id}`, `Created coordination channel for topic: ${topic}`]
        );
        const row = result2.rows[0];
        return {
          id: row.id,
          topic: row.topic,
          participants: row.participants || [],
          status: row.status || "active",
          createdAt: row.created_at ? new Date(row.created_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString(),
          updatedAt: row.created_at ? new Date(row.created_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      async getCoordinationChannelById(id) {
        const result2 = await this.pool.query(
          "SELECT id, topic, participants, status, created_at, updated_at FROM maestro_coordination_channels WHERE id = $1",
          [id]
        );
        if (result2.rows.length === 0) return null;
        const row = result2.rows[0];
        return {
          id: row.id,
          topic: row.topic,
          participants: row.participants || [],
          status: row.status,
          createdAt: row.created_at ? new Date(row.created_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString(),
          updatedAt: row.updated_at ? new Date(row.updated_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      async initiateConsensus(coordinatorId, topic, proposal, voterAgentIds, deadlineHours, actor) {
        const id = `consensus_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
        const deadline = new Date(Date.now() + deadlineHours * 60 * 60 * 1e3);
        const result2 = await this.pool.query(
          `INSERT INTO maestro_consensus_proposals 
       (id, topic, proposal_data, coordinator_id, voters, deadline) 
       VALUES ($1, $2, $3, $4, $5, $6) 
       RETURNING id, topic, proposal_data, coordinator_id, voters, status, deadline, created_at`,
          [id, topic, JSON.stringify(proposal), coordinatorId, voterAgentIds, deadline]
        );
        await this.pool.query(
          "INSERT INTO maestro_audit_log (actor, action, resource, details) VALUES ($1, $2, $3, $4)",
          [actor, "initiate_consensus", `consensus_proposal:${id}`, `Initiated consensus for topic: ${topic}`]
        );
        const row = result2.rows[0];
        return {
          id: row.id,
          topic: row.topic,
          proposal: JSON.parse(row.proposal_data),
          coordinatorId: row.coordinator_id,
          voters: row.voters || [],
          votes: row.votes || {},
          status: row.status || "voting",
          deadline: row.deadline ? new Date(row.deadline).toISOString() : (/* @__PURE__ */ new Date()).toISOString(),
          createdAt: row.created_at ? new Date(row.created_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      async getConsensusProposalById(id) {
        const result2 = await this.pool.query(
          "SELECT id, topic, proposal_data, coordinator_id, voters, votes, status, deadline, created_at FROM maestro_consensus_proposals WHERE id = $1",
          [id]
        );
        if (result2.rows.length === 0) return null;
        const row = result2.rows[0];
        return {
          id: row.id,
          topic: row.topic,
          proposal: JSON.parse(row.proposal_data),
          coordinatorId: row.coordinator_id,
          voters: row.voters || [],
          votes: row.votes || {},
          status: row.status,
          deadline: row.deadline ? new Date(row.deadline).toISOString() : (/* @__PURE__ */ new Date()).toISOString(),
          createdAt: row.created_at ? new Date(row.created_at).toISOString() : (/* @__PURE__ */ new Date()).toISOString()
        };
      }
      async recordVote(proposalId, agentId, vote) {
        const client6 = await this.pool.connect();
        try {
          await client6.query("BEGIN");
          const result2 = await client6.query(
            "SELECT votes FROM maestro_consensus_proposals WHERE id = $1",
            [proposalId]
          );
          if (result2.rows.length === 0) {
            return false;
          }
          const currentVotes = result2.rows[0].votes || {};
          currentVotes[agentId] = {
            ...vote,
            timestamp: (/* @__PURE__ */ new Date()).toISOString(),
            weight: vote.weight || 1
          };
          await client6.query(
            "UPDATE maestro_consensus_proposals SET votes = $1, updated_at = CURRENT_TIMESTAMP WHERE id = $2",
            [JSON.stringify(currentVotes), proposalId]
          );
          await client6.query("COMMIT");
          return true;
        } catch (error) {
          await client6.query("ROLLBACK");
          throw error;
        } finally {
          client6.release();
        }
      }
      // Audit log methods
      async getAuditLog(limit = 100) {
        const result2 = await this.pool.query(
          `SELECT id, actor, action, resource, details, status, created_at 
       FROM maestro_audit_log 
       ORDER BY created_at DESC 
       LIMIT $1`,
          [limit]
        );
        return result2.rows.map((row) => ({
          id: String(row.id),
          timestamp: new Date(row.created_at).toISOString(),
          actor: row.actor,
          action: row.action,
          resource: row.resource,
          details: row.details,
          status: row.status
        }));
      }
      async logAudit(actor, action, resource, details, status = "allowed") {
        await this.pool.query(
          "INSERT INTO maestro_audit_log (actor, action, resource, details, status) VALUES ($1, $2, $3, $4, $5)",
          [actor, action, resource, details, status]
        );
      }
    };
  }
});

// src/maestro/MaestroService.ts
var MaestroService, maestroService;
var init_MaestroService = __esm({
  "src/maestro/MaestroService.ts"() {
    "use strict";
    init_runs_repo();
    init_subagent_coordinator();
    init_KillSwitchService();
    init_TelemetryService2();
    init_DriftDetectionService();
    init_orchestrator_store();
    MaestroService = class _MaestroService {
      static instance;
      orchestratorStore;
      subagentCoordinator;
      constructor(dbPool) {
        this.subagentCoordinator = SubagentCoordinator.getInstance();
        this.orchestratorStore = dbPool ? new OrchestratorPostgresStore(dbPool) : this.createInMemoryStore();
        driftDetectionService.startMonitoring();
      }
      static getInstance(dbPool) {
        if (!_MaestroService.instance) {
          _MaestroService.instance = new _MaestroService(dbPool);
        }
        return _MaestroService.instance;
      }
      createInMemoryStore() {
        const inMemoryData = {
          loops: [
            {
              id: "cost-optimization",
              name: "Cost Optimization Loop",
              type: "cost",
              status: "active",
              lastDecision: "Shifted 20% traffic to Haiku for non-critical tasks",
              lastRun: (/* @__PURE__ */ new Date()).toISOString(),
              config: { threshold: 0.8, interval: "15m" }
            },
            {
              id: "reliability-guardian",
              name: "Reliability Guardian",
              type: "reliability",
              status: "active",
              lastDecision: "Quarantined node agent-worker-3 due to high error rate",
              lastRun: (/* @__PURE__ */ new Date()).toISOString(),
              config: { maxRetries: 3 }
            },
            {
              id: "safety-sentinel",
              name: "Safety Sentinel",
              type: "safety",
              status: "active",
              lastDecision: "Blocked 2 prompts for PII violation",
              lastRun: (/* @__PURE__ */ new Date()).toISOString(),
              config: { strictMode: true }
            }
          ],
          experiments: [
            {
              id: "exp-001",
              name: "Planner Agent V2",
              hypothesis: "New prompt structure reduces planning latency by 15%",
              status: "running",
              variants: ["control", "v2-prompt"],
              metrics: { latency: -12, successRate: 2 },
              startDate: new Date(Date.now() - 864e5 * 2).toISOString()
            }
          ],
          playbooks: [
            {
              id: "pb-restart-pods",
              name: "Restart Stuck Pods",
              description: "Automatically restarts pods that are in CrashLoopBackOff for > 10m",
              triggers: ["pod_crash_loop"],
              actions: ["k8s.delete_pod"],
              isEnabled: true
            }
          ],
          auditLog: [],
          agents: [
            {
              id: "planner",
              name: "Planner",
              role: "Orchestration",
              model: "gpt-4-turbo",
              status: "healthy",
              metrics: { successRate: 98.5, latencyP95: 1200, costPerTask: 0.03 },
              routingWeight: 100
            },
            {
              id: "coder",
              name: "Coder",
              role: "Implementation",
              model: "claude-3-opus",
              status: "healthy",
              metrics: { successRate: 95.2, latencyP95: 4500, costPerTask: 0.15 },
              routingWeight: 80
            },
            {
              id: "reviewer",
              name: "Reviewer",
              role: "Quality Assurance",
              model: "gpt-4o",
              status: "healthy",
              metrics: { successRate: 99.1, latencyP95: 2100, costPerTask: 0.05 },
              routingWeight: 100
            }
          ]
        };
        return {
          initialize: async () => Promise.resolve(),
          getLoops: async () => inMemoryData.loops,
          getLoopById: async (id) => inMemoryData.loops.find((loop) => loop.id === id) || null,
          updateLoopStatus: async (id, status) => {
            const loop = inMemoryData.loops.find((l) => l.id === id);
            if (loop) {
              loop.status = status;
              return true;
            }
            return false;
          },
          getAgents: async () => {
            return inMemoryData.agents.map((agent) => ({
              id: agent.id,
              name: agent.name,
              role: agent.role,
              model: agent.model,
              status: agent.status,
              routingWeight: agent.routingWeight,
              metrics: agent.metrics
            }));
          },
          getAgentById: async (id) => {
            const agent = inMemoryData.agents.find((a) => a.id === id);
            if (agent) {
              return {
                id: agent.id,
                name: agent.name,
                role: agent.role,
                model: agent.model,
                status: agent.status,
                routingWeight: agent.routingWeight,
                metrics: agent.metrics
              };
            }
            return null;
          },
          updateAgent: async (id, updates, actor) => {
            const idx = inMemoryData.agents.findIndex((a) => a.id === id);
            if (idx !== -1) {
              inMemoryData.agents[idx] = { ...inMemoryData.agents[idx], ...updates };
              await this.logAudit(actor, "update_agent", id, `Updated agent ${id}`);
              return {
                id: inMemoryData.agents[idx].id,
                name: inMemoryData.agents[idx].name,
                role: inMemoryData.agents[idx].role,
                model: inMemoryData.agents[idx].model,
                status: inMemoryData.agents[idx].status,
                routingWeight: inMemoryData.agents[idx].routingWeight,
                metrics: inMemoryData.agents[idx].metrics
              };
            }
            return null;
          },
          getExperiments: async () => inMemoryData.experiments,
          createExperiment: async (experiment, actor) => {
            inMemoryData.experiments.push(experiment);
            await this.logAudit(actor, "create_experiment", experiment.id, `Created experiment ${experiment.name}`);
            return experiment;
          },
          getPlaybooks: async () => inMemoryData.playbooks,
          createCoordinationTask: async (task, actor) => {
            const id = `task_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
            const coordinationTask = {
              id,
              title: task.title,
              description: task.description,
              status: "pending",
              ownerId: task.ownerId,
              participants: task.participants,
              priority: task.priority,
              createdAt: (/* @__PURE__ */ new Date()).toISOString(),
              updatedAt: (/* @__PURE__ */ new Date()).toISOString()
            };
            await this.logAudit(actor, "create_coordination_task", `coordination_task:${id}`, `Created coordination task ${task.title}`);
            return coordinationTask;
          },
          getCoordinationTaskById: async (id) => null,
          // Implement as needed
          updateCoordinationTaskStatus: async (id, status) => false,
          // Implement as needed
          createCoordinationChannel: async (topic, participantAgentIds, actor) => {
            const id = `channel_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
            const channel = {
              id,
              topic,
              participants: participantAgentIds,
              status: "active",
              createdAt: (/* @__PURE__ */ new Date()).toISOString(),
              updatedAt: (/* @__PURE__ */ new Date()).toISOString()
            };
            await this.logAudit(actor, "create_coordination_channel", `coordination_channel:${id}`, `Created coordination channel for topic: ${topic}`);
            return channel;
          },
          getCoordinationChannelById: async (id) => null,
          // Implement as needed
          initiateConsensus: async (coordinatorId, topic, proposal, voterAgentIds, deadlineHours, actor) => {
            const id = `consensus_${Date.now()}_${Math.random().toString(36).substr(2, 5)}`;
            const deadline = new Date(Date.now() + deadlineHours * 60 * 60 * 1e3);
            const consensusProposal = {
              id,
              topic,
              proposal,
              coordinatorId,
              voters: voterAgentIds,
              votes: {},
              status: "voting",
              deadline: deadline.toISOString(),
              createdAt: (/* @__PURE__ */ new Date()).toISOString()
            };
            await this.logAudit(actor, "initiate_consensus", `consensus_proposal:${id}`, `Initiated consensus for topic: ${topic}`);
            return consensusProposal;
          },
          getConsensusProposalById: async (id) => null,
          // Implement as needed
          recordVote: async (proposalId, agentId, vote) => false,
          // Implement as needed
          getAuditLog: async (limit = 100) => inMemoryData.auditLog.slice(0, limit),
          logAudit: async (actor, action, resource, details, status = "allowed") => {
            inMemoryData.auditLog.unshift({
              id: Math.random().toString(36).substr(2, 9),
              timestamp: (/* @__PURE__ */ new Date()).toISOString(),
              actor,
              action,
              resource,
              details,
              status
            });
            if (inMemoryData.auditLog.length > 1e3) inMemoryData.auditLog = inMemoryData.auditLog.slice(0, 1e3);
          }
        };
      }
      async initialize() {
        await this.orchestratorStore.initialize();
      }
      // --- Dashboard ---
      async getHealthSnapshot(tenantId) {
        const runs = await runsRepo.list(tenantId, 100);
        const recentFailures = runs.filter(
          (r) => r.status === "failed" && new Date(r.created_at).getTime() > Date.now() - 36e5
        ).length;
        let overallScore = 100;
        if (recentFailures > 5) overallScore -= 20;
        if (recentFailures > 15) overallScore -= 40;
        return {
          overallScore: Math.max(0, overallScore),
          workstreams: [
            { name: "MC-Core", status: overallScore > 80 ? "healthy" : "degraded", score: overallScore },
            { name: "MergeTrain", status: "healthy", score: 98 },
            { name: "IntelGraph-Ingest", status: "healthy", score: 95 },
            { name: "UI/UX", status: "healthy", score: 100 }
          ],
          activeAlerts: overallScore < 90 ? [
            {
              id: "alert-1",
              title: "High failure rate in last hour",
              severity: "warning",
              timestamp: (/* @__PURE__ */ new Date()).toISOString()
            }
          ] : []
        };
      }
      async getDashboardStats(tenantId) {
        const runs = await runsRepo.list(tenantId, 1e3);
        const activeRuns = runs.filter((r) => r.status === "running").length;
        const completedRuns = runs.filter((r) => r.status === "succeeded").length;
        const failedRuns = runs.filter((r) => r.status === "failed").length;
        return {
          activeRuns,
          completedRuns,
          failedRuns,
          totalRuns: runs.length,
          tasksPerMinute: 42,
          // Mock for now, would need task table
          successRate: runs.length ? completedRuns / runs.length * 100 : 100
        };
      }
      // --- Runs ---
      // Proxied to runsRepo
      // --- Agents ---
      async getAgents() {
        return await this.orchestratorStore.getAgents();
      }
      async updateAgent(id, updates, actor) {
        return await this.orchestratorStore.updateAgent(id, updates, actor);
      }
      // --- Autonomic ---
      async getControlLoops() {
        return await this.orchestratorStore.getLoops();
      }
      async toggleLoop(id, status, actor) {
        return await this.orchestratorStore.updateLoopStatus(id, status);
      }
      // --- Experiments & Playbooks ---
      async getExperiments() {
        return await this.orchestratorStore.getExperiments();
      }
      async createExperiment(experiment, actor) {
        return await this.orchestratorStore.createExperiment(experiment, actor);
      }
      async getPlaybooks() {
        return await this.orchestratorStore.getPlaybooks();
      }
      // --- Audit ---
      async getAuditLog(limit = 100) {
        return await this.orchestratorStore.getAuditLog(limit);
      }
      async logAudit(actor, action, resource, details, status = "allowed") {
        await this.orchestratorStore.logAudit(actor, action, resource, details, status);
      }
      // --- Subagent Coordination Methods ---
      /**
       * Coordinate tasks between multiple agents
       */
      async coordinateAgents(task, participantAgentIds, actor) {
        const healthCheck = killSwitchService.checkSystemHealth({ agentId: actor, feature: "agent_coordination" });
        if (!healthCheck.allowed) {
          throw new Error(`System Kill Switch Active: ${healthCheck.reason}`);
        }
        const governanceCheck = await agentGovernance.evaluateAction(
          {
            id: actor,
            name: "MaestroService",
            tenantId: "system",
            capabilities: ["coordination", "orchestration"],
            metadata: {},
            status: "active",
            health: { cpuUsage: 0, memoryUsage: 0, lastHeartbeat: /* @__PURE__ */ new Date(), activeTasks: 0, errorRate: 0 }
          },
          "coordinate_agents",
          { task, participants: participantAgentIds }
        );
        telemetryService2.logEvent("policy_decision", {
          id: `pol-${Date.now()}`,
          policyId: "agent_governance",
          decision: governanceCheck.allowed ? "allow" : "deny",
          reason: governanceCheck.reason,
          actorId: actor,
          resourceId: "coordinate_agents",
          timestamp: /* @__PURE__ */ new Date(),
          context: { task, participants: participantAgentIds }
        });
        if (!governanceCheck.allowed) {
          throw new Error(`Agent coordination prohibited: ${governanceCheck.reason}`);
        }
        const coordinationTask = await this.orchestratorStore.createCoordinationTask(task, actor);
        await this.logAudit(
          actor,
          "coordinate_agents",
          `coordination_task:${coordinationTask.id}`,
          `Coordinated task ${task.title} among ${participantAgentIds.length} agents`
        );
        const duration = Date.now() - (task.createdAt ? new Date(task.createdAt).getTime() : Date.now());
        telemetryService2.logEvent("agent_action", {
          id: `act-${Date.now()}`,
          agentId: actor,
          actionType: "coordinate_agents",
          status: "success",
          durationMs: duration > 0 ? duration : 0,
          timestamp: /* @__PURE__ */ new Date(),
          metadata: { taskId: coordinationTask.id }
        });
        return coordinationTask;
      }
      /**
       * Create a coordination channel for multi-agent collaboration
       */
      async createCoordinationChannel(topic, participantAgentIds, actor) {
        const healthCheck = killSwitchService.checkSystemHealth({ agentId: actor, feature: "coordination_channel" });
        if (!healthCheck.allowed) {
          throw new Error(`System Kill Switch Active: ${healthCheck.reason}`);
        }
        const governanceCheck = await agentGovernance.evaluateAction(
          {
            id: actor,
            name: "MaestroService",
            tenantId: "system",
            capabilities: ["coordination", "orchestration"],
            metadata: {},
            status: "active",
            health: { cpuUsage: 0, memoryUsage: 0, lastHeartbeat: /* @__PURE__ */ new Date(), activeTasks: 0, errorRate: 0 }
          },
          "create_coordination_channel",
          { topic, participants: participantAgentIds }
        );
        telemetryService2.logEvent("policy_decision", {
          id: `pol-${Date.now()}`,
          policyId: "agent_governance",
          decision: governanceCheck.allowed ? "allow" : "deny",
          reason: governanceCheck.reason,
          actorId: actor,
          resourceId: "create_coordination_channel",
          timestamp: /* @__PURE__ */ new Date(),
          context: { topic, participants: participantAgentIds }
        });
        if (!governanceCheck.allowed) {
          throw new Error(`Coordination channel creation prohibited: ${governanceCheck.reason}`);
        }
        const channel = await this.orchestratorStore.createCoordinationChannel(topic, participantAgentIds, actor);
        await this.logAudit(
          actor,
          "create_coordination_channel",
          `channel:${channel.id}`,
          `Created coordination channel for topic: ${topic}`
        );
        telemetryService2.logEvent("agent_action", {
          id: `act-${Date.now()}`,
          agentId: actor,
          actionType: "create_coordination_channel",
          status: "success",
          durationMs: 50,
          // Mocked latency for channel creation (usually fast)
          timestamp: /* @__PURE__ */ new Date(),
          metadata: { channelId: channel.id }
        });
        return channel;
      }
      /**
       * Initiate consensus process among agents
       */
      async initiateConsensus(coordinatorId, topic, proposal, voterAgentIds, deadlineHours = 24, actor) {
        const governanceCheck = await agentGovernance.evaluateAction(
          {
            id: actor,
            name: "MaestroService",
            tenantId: "system",
            capabilities: ["consensus", "governance"],
            metadata: {},
            status: "active",
            health: { cpuUsage: 0, memoryUsage: 0, lastHeartbeat: /* @__PURE__ */ new Date(), activeTasks: 0, errorRate: 0 }
          },
          "initiate_consensus",
          { topic, voters: voterAgentIds }
        );
        if (!governanceCheck.allowed) {
          throw new Error(`Consensus initiation prohibited: ${governanceCheck.reason}`);
        }
        const consensusProposal = await this.orchestratorStore.initiateConsensus(
          coordinatorId,
          topic,
          proposal,
          voterAgentIds,
          deadlineHours,
          actor
        );
        await this.logAudit(
          actor,
          "initiate_consensus",
          `proposal:${consensusProposal.id}`,
          `Initiated consensus for topic: ${topic}`
        );
        return consensusProposal;
      }
      /**
       * Record vote for a consensus proposal
       */
      async recordVote(proposalId, agentId, vote) {
        return await this.orchestratorStore.recordVote(proposalId, agentId, vote);
      }
      /**
       * Get coordination metrics for an agent
       */
      async getCoordinationMetrics(agentId) {
        return this.subagentCoordinator.getAgentMetrics(agentId);
      }
    };
    maestroService = MaestroService.getInstance();
  }
});

// src/maestro/coordination/service.ts
import * as crypto49 from "node:crypto";
var CoordinationService, coordinationService;
var init_service2 = __esm({
  "src/maestro/coordination/service.ts"() {
    "use strict";
    init_budget_manager();
    init_MaestroService();
    init_logger2();
    CoordinationService = class _CoordinationService {
      static instance;
      constructor() {
      }
      static getInstance() {
        if (!_CoordinationService.instance) {
          _CoordinationService.instance = new _CoordinationService();
        }
        return _CoordinationService.instance;
      }
      startCoordination(initiatorAgentId, schema2, budget, parentContextId) {
        const coordinationId = crypto49.randomUUID();
        const context4 = {
          coordinationId,
          schema: schema2,
          // Store schema for validation
          schemaVersion: schema2.version,
          initiatorAgentId,
          roles: {
            [initiatorAgentId]: "COORDINATOR"
            // Defaulting initiator to coordinator, can be changed
          },
          budget,
          budgetConsumed: {
            totalSteps: 0,
            totalTokens: 0,
            wallClockTimeMs: 0
          },
          status: "ACTIVE",
          startTime: /* @__PURE__ */ new Date(),
          parent: parentContextId
        };
        budgetManager.initialize(context4);
        Promise.resolve(maestroService.logAudit(
          initiatorAgentId,
          "coordination_start",
          coordinationId,
          `Started coordination with schema ${schema2.version} and budget ${JSON.stringify(budget)}`
        )).catch((err) => logger2.error(`Failed to audit coordination start: ${err}`));
        return coordinationId;
      }
      validateAction(coordinationId, agentId, role) {
        const context4 = budgetManager.get(coordinationId);
        if (!context4) return false;
        if (!context4.roles[agentId]) {
          this.registerAgent(coordinationId, agentId, role);
        }
        if (context4.roles[agentId] !== role) return false;
        const budgetCheck = budgetManager.checkBudget(coordinationId);
        if (!budgetCheck.allowed) {
          if (context4.status === "ACTIVE") {
            this.killCoordination(coordinationId, budgetCheck.reason || "Budget exhausted");
          }
          return false;
        }
        return true;
      }
      registerAgent(coordinationId, agentId, role) {
        const context4 = budgetManager.get(coordinationId);
        if (!context4) throw new Error("Context not found");
        if (!context4.schema.roles.includes(role)) {
          logger2.warn(`Agent ${agentId} attempted to join as invalid role ${role} for schema ${context4.schema.name}`);
          return;
        }
        context4.roles[agentId] = role;
        Promise.resolve(maestroService.logAudit(
          "system",
          "agent_join",
          coordinationId,
          `Agent ${agentId} joined as ${role}`
        )).catch((err) => logger2.error(err));
      }
      consumeBudget(coordinationId, usage) {
        budgetManager.consumeBudget(coordinationId, usage);
        const check = budgetManager.checkBudget(coordinationId);
        if (!check.allowed) {
          this.killCoordination(coordinationId, check.reason || "Budget limit reached");
        }
      }
      killCoordination(coordinationId, reason) {
        const context4 = budgetManager.get(coordinationId);
        if (!context4 || context4.status !== "ACTIVE") return;
        context4.status = "TERMINATED";
        context4.endTime = /* @__PURE__ */ new Date();
        context4.terminationReason = reason;
        logger2.warn(`Killing coordination ${coordinationId}: ${reason}`);
        Promise.resolve(maestroService.logAudit(
          "system",
          "coordination_kill",
          coordinationId,
          `Coordination terminated: ${reason}`
        )).catch((err) => logger2.error(err));
      }
    };
    coordinationService = CoordinationService.getInstance();
  }
});

// src/maestro/evidence/receipt-signing.ts
import crypto50 from "crypto";
var DEV_FALLBACK_SECRET, parseKeyring, resolveReceiptSigningConfig, signReceiptPayload2, buildReceiptSignature;
var init_receipt_signing = __esm({
  "src/maestro/evidence/receipt-signing.ts"() {
    "use strict";
    init_receipt();
    DEV_FALLBACK_SECRET = "dev-secret";
    parseKeyring = () => {
      if (process.env.EVIDENCE_SIGNING_KEYS) {
        const parsed = JSON.parse(process.env.EVIDENCE_SIGNING_KEYS);
        if (!parsed || typeof parsed !== "object" || Array.isArray(parsed)) {
          throw new Error("EVIDENCE_SIGNING_KEYS must be a JSON object of { kid: secret }");
        }
        return Object.fromEntries(
          Object.entries(parsed).map(([kid, secret2]) => [kid, String(secret2)])
        );
      }
      const secret = process.env.EVIDENCE_SIGNING_SECRET || (process.env.NODE_ENV !== "production" ? DEV_FALLBACK_SECRET : void 0);
      if (!secret) {
        return {};
      }
      return {
        [process.env.EVIDENCE_SIGNER_KID || "dev"]: secret
      };
    };
    resolveReceiptSigningConfig = () => {
      const keyring = parseKeyring();
      const configuredKeyId = process.env.EVIDENCE_SIGNER_KID || Object.keys(keyring)[0];
      if (!configuredKeyId || !keyring[configuredKeyId]) {
        throw new Error(
          "Receipt signing requires EVIDENCE_SIGNING_KEYS or EVIDENCE_SIGNING_SECRET"
        );
      }
      return {
        keyId: configuredKeyId,
        algorithm: "HS256",
        secret: keyring[configuredKeyId],
        keyring
      };
    };
    signReceiptPayload2 = (payload, secret) => crypto50.createHmac("sha256", secret).update(canonicalStringify(payload)).digest("base64url");
    buildReceiptSignature = (payload) => {
      const signer = resolveReceiptSigningConfig();
      return {
        key_id: signer.keyId,
        algorithm: signer.algorithm,
        value: signReceiptPayload2(payload, signer.secret)
      };
    };
  }
});

// src/maestro/evidence/transition-receipts.ts
import crypto51 from "crypto";
var defaultPolicySet, buildTransitionReceipt, emitTransitionReceipt;
var init_transition_receipts = __esm({
  "src/maestro/evidence/transition-receipts.ts"() {
    "use strict";
    init_receipt();
    init_receipt_signing();
    init_provenance_service();
    defaultPolicySet = () => process.env.MAESTRO_POLICY_SET || "maestro.policy.guard.v1";
    buildTransitionReceipt = (input) => {
      const timestamp = (/* @__PURE__ */ new Date()).toISOString();
      const baseReceipt = {
        spec_version: "1.0.0",
        id: crypto51.randomUUID(),
        timestamp,
        correlation_id: input.correlationId || input.runId,
        tenant_id: input.tenantId,
        actor: input.actor,
        action: input.action,
        resource: input.resource,
        policy: {
          decision_id: input.policyDecisionId || crypto51.randomUUID(),
          policy_set: input.policySet || defaultPolicySet(),
          evaluation_timestamp: input.policyTimestamp || timestamp
        },
        result: input.result
      };
      const signature = buildReceiptSignature(baseReceipt);
      return {
        ...baseReceipt,
        signature
      };
    };
    emitTransitionReceipt = async (input) => {
      const receipt = buildTransitionReceipt(input);
      const artifactId = await evidenceProvenanceService.storeEvidence({
        runId: input.runId,
        artifactType: "receipt",
        content: canonicalStringify(receipt),
        metadata: {
          contentType: "application/json",
          action: input.action,
          resourceType: input.resource.type,
          resourceId: input.resource.id,
          correlationId: receipt.correlation_id
        }
      });
      return { receipt, artifactId };
    };
  }
});

// src/maestro/engine.ts
var engine_exports = {};
__export(engine_exports, {
  MaestroEngine: () => MaestroEngine
});
import { Queue as Queue5, Worker as Worker4, QueueEvents } from "bullmq";
import * as crypto52 from "node:crypto";
import { ForkDetector } from "@intelgraph/maestro-core";
var MaestroEngine;
var init_engine2 = __esm({
  "src/maestro/engine.ts"() {
    "use strict";
    init_dsl();
    init_logger2();
    init_service2();
    init_transition_receipts();
    MaestroEngine = class {
      constructor(deps) {
        this.deps = deps;
        this.db = deps.db;
        const connection5 = deps.redisConnection;
        this.queue = new Queue5("maestro_v2", { connection: connection5.duplicate() });
        this.queueEvents = new QueueEvents("maestro_v2", { connection: connection5.duplicate() });
        this.worker = new Worker4("maestro_v2", async (job) => {
          const { taskId, runId, tenantId } = job.data;
          return this.processTask(taskId, runId, tenantId);
        }, { connection: connection5.duplicate(), concurrency: 5 });
        this.setupEventListeners();
      }
      db;
      queue;
      queueEvents;
      worker;
      taskHandlers = /* @__PURE__ */ new Map();
      // --- Public API ---
      async registerTaskHandler(kind, handler) {
        this.taskHandlers.set(kind, handler);
      }
      async createRun(tenantId, templateId, input, principalId) {
        const inputHash = JSON.stringify(input);
        const dupRes = await this.db.query(
          `SELECT * FROM maestro_runs
       WHERE tenant_id = $1
       AND template_id = $2
       AND input = $3::jsonb
       AND status = 'succeeded'
       AND completed_at > NOW() - INTERVAL '1 hour'`,
          [tenantId, templateId, inputHash]
        );
        if (dupRes.rows.length > 0) {
          logger2.info(`Returning cached run result for template ${templateId}`);
          const row2 = dupRes.rows[0];
          return {
            id: row2.id,
            tenantId: row2.tenant_id,
            templateId: row2.template_id,
            templateVersion: row2.template_version,
            createdByPrincipalId: row2.created_by_principal_id,
            status: row2.status,
            input: row2.input,
            startedAt: row2.started_at,
            completedAt: row2.completed_at,
            metadata: row2.metadata || {}
          };
        }
        const res = await this.db.query(
          `SELECT * FROM maestro_templates WHERE id = $1 AND tenant_id = $2`,
          [templateId, tenantId]
        );
        if (res.rows.length === 0) throw new Error(`Template not found: ${templateId}`);
        const row = res.rows[0];
        const template = {
          id: row.id,
          tenantId: row.tenant_id,
          name: row.name,
          version: row.version,
          description: row.description,
          kind: row.kind,
          inputSchema: row.input_schema,
          outputSchema: row.output_schema,
          spec: row.spec,
          createdAt: row.created_at,
          updatedAt: row.updated_at,
          metadata: row.metadata
        };
        const runId = crypto52.randomUUID();
        const run = {
          id: runId,
          tenantId,
          templateId,
          templateVersion: template.version,
          createdByPrincipalId: principalId,
          status: "pending",
          input,
          startedAt: (/* @__PURE__ */ new Date()).toISOString(),
          metadata: {}
        };
        const tasks = MaestroDSL.compileToTasks(template.spec, runId, tenantId, input);
        const client6 = await this.db.connect();
        try {
          await client6.query("BEGIN");
          await client6.query(
            `INSERT INTO maestro_runs (id, tenant_id, template_id, template_version, created_by_principal_id, status, input, started_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
            [run.id, run.tenantId, run.templateId, run.templateVersion, run.createdByPrincipalId, run.status, run.input, run.startedAt]
          );
          for (const t of tasks) {
            await client6.query(
              `INSERT INTO maestro_tasks (id, run_id, tenant_id, name, kind, status, depends_on, attempt, max_attempts, backoff_strategy, payload, metadata)
           VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)`,
              [t.id, t.runId, t.tenantId, t.name, t.kind, t.status, t.dependsOn, t.attempt, t.maxAttempts, t.backoffStrategy, t.payload, t.metadata]
            );
          }
          await client6.query("COMMIT");
        } catch (e) {
          await client6.query("ROLLBACK");
          throw e;
        } finally {
          client6.release();
        }
        await this.emitReceiptSafely({
          runId,
          tenantId,
          actor: {
            id: principalId,
            principal_type: "user"
          },
          action: "maestro.run.create",
          resource: {
            id: runId,
            type: "maestro.run",
            attributes: {
              templateId,
              templateVersion: template.version,
              status: run.status
            }
          },
          result: { status: "success" }
        });
        await this.dispatchReadyTasks(runId);
        return run;
      }
      // --- Internal Engine Logic ---
      async dispatchReadyTasks(runId) {
        const res = await this.db.query(
          `SELECT * FROM maestro_tasks WHERE run_id = $1 AND status = 'ready'`,
          [runId]
        );
        for (const row of res.rows) {
          const taskForEntropy = {
            name: row.name,
            kind: row.kind,
            payload: row.payload,
            config: row.metadata
          };
          const entropy = ForkDetector.calculateEntropy(taskForEntropy);
          const priority = 1 + Math.floor((1 - entropy) * 100);
          await this.queue.add(row.kind, {
            taskId: row.id,
            runId: row.run_id,
            tenantId: row.tenant_id
          }, {
            priority
          });
          await this.db.query(
            `UPDATE maestro_tasks SET status = 'queued' WHERE id = $1`,
            [row.id]
          );
          await this.emitReceiptSafely({
            runId: row.run_id,
            tenantId: row.tenant_id,
            actor: { id: "maestro-engine", principal_type: "system" },
            action: "maestro.task.queued",
            resource: {
              id: row.id,
              type: "maestro.task",
              attributes: {
                kind: row.kind,
                status: "queued",
                fork_score: entropy,
                priority
              }
            },
            result: { status: "success" }
          });
        }
      }
      async processTask(taskId, runId, tenantId) {
        const res = await this.db.query(`SELECT * FROM maestro_tasks WHERE id = $1`, [taskId]);
        if (res.rows.length === 0) throw new Error(`Task not found: ${taskId}`);
        const taskRow = res.rows[0];
        const task = {
          ...taskRow,
          dependsOn: taskRow.depends_on,
          maxAttempts: taskRow.max_attempts,
          backoffStrategy: taskRow.backoff_strategy
        };
        await this.db.query(
          `UPDATE maestro_tasks SET status = 'running', started_at = NOW(), attempt = attempt + 1 WHERE id = $1`,
          [taskId]
        );
        await this.emitReceiptSafely({
          runId,
          tenantId,
          actor: { id: "maestro-engine", principal_type: "system" },
          action: "maestro.task.started",
          resource: {
            id: taskId,
            type: "maestro.task",
            attributes: {
              kind: task.kind,
              status: "running"
            }
          },
          result: { status: "success" }
        });
        try {
          if (task.metadata?.coordinationId) {
            const coordId = task.metadata.coordinationId;
            const role = task.metadata.role || "WORKER";
            const agentId = task.metadata.agentId || "unknown_agent";
            const allowed = coordinationService.validateAction(coordId, agentId, role);
            if (!allowed) {
              throw new Error(`Coordination constraints prevented execution for ${coordId}`);
            }
            coordinationService.consumeBudget(coordId, { totalSteps: 1 });
          }
          const handler = this.taskHandlers.get(task.kind);
          if (!handler) {
            throw new Error(`No handler registered for task kind: ${task.kind}`);
          }
          const result2 = await handler(task);
          if (task.metadata?.coordinationId && result2 && typeof result2 === "object" && result2.usage?.totalTokens) {
            const coordId = task.metadata.coordinationId;
            coordinationService.consumeBudget(coordId, { totalTokens: result2.usage.totalTokens });
          }
          await this.db.query(
            `UPDATE maestro_tasks SET status = 'succeeded', result = $2, completed_at = NOW() WHERE id = $1`,
            [taskId, result2]
          );
          await this.emitReceiptSafely({
            runId,
            tenantId,
            actor: { id: "maestro-engine", principal_type: "system" },
            action: "maestro.task.succeeded",
            resource: {
              id: taskId,
              type: "maestro.task",
              attributes: {
                kind: task.kind,
                status: "succeeded"
              }
            },
            result: { status: "success" }
          });
          await this.evaluateDependents(taskId, runId);
        } catch (err) {
          await this.db.query(
            `UPDATE maestro_tasks SET status = 'failed', error = $2, completed_at = NOW() WHERE id = $1`,
            [taskId, err.message]
          );
          await this.emitReceiptSafely({
            runId,
            tenantId,
            actor: { id: "maestro-engine", principal_type: "system" },
            action: "maestro.task.failed",
            resource: {
              id: taskId,
              type: "maestro.task",
              attributes: {
                kind: task.kind,
                status: "failed"
              }
            },
            result: { status: "failure", details: err.message }
          });
          throw err;
        }
      }
      async evaluateDependents(completedTaskId, runId) {
        const res = await this.db.query(
          `SELECT * FROM maestro_tasks WHERE run_id = $1 AND $2 = ANY(depends_on) AND status = 'pending'`,
          [runId, completedTaskId]
        );
        for (const dependentRow of res.rows) {
          const deps = dependentRow.depends_on;
          const satisfied = await this.areDependenciesSatisfied(deps);
          if (satisfied) {
            await this.db.query(
              `UPDATE maestro_tasks SET status = 'ready' WHERE id = $1`,
              [dependentRow.id]
            );
            const taskForEntropy = {
              name: dependentRow.name,
              kind: dependentRow.kind,
              payload: dependentRow.payload,
              config: dependentRow.metadata
            };
            const entropy = ForkDetector.calculateEntropy(taskForEntropy);
            const priority = 1 + Math.floor((1 - entropy) * 100);
            await this.queue.add(dependentRow.kind, {
              taskId: dependentRow.id,
              runId: dependentRow.run_id,
              tenantId: dependentRow.tenant_id
            }, {
              priority
            });
            await this.db.query(
              `UPDATE maestro_tasks SET status = 'queued' WHERE id = $1`,
              [dependentRow.id]
            );
          }
        }
        await this.checkRunCompletion(runId);
      }
      async areDependenciesSatisfied(taskIds) {
        if (taskIds.length === 0) return true;
        const res = await this.db.query(
          `SELECT count(*) as count FROM maestro_tasks WHERE id = ANY($1) AND status = 'succeeded'`,
          [taskIds]
        );
        return parseInt(res.rows[0].count) === taskIds.length;
      }
      async checkRunCompletion(runId) {
        const res = await this.db.query(
          `SELECT count(*) as count FROM maestro_tasks WHERE run_id = $1 AND status NOT IN ('succeeded', 'failed', 'skipped', 'cancelled')`,
          [runId]
        );
        if (parseInt(res.rows[0].count) === 0) {
          const failRes = await this.db.query(
            `SELECT count(*) as count FROM maestro_tasks WHERE run_id = $1 AND status = 'failed'`,
            [runId]
          );
          const finalStatus = parseInt(failRes.rows[0].count) > 0 ? "failed" : "succeeded";
          await this.db.query(
            `UPDATE maestro_runs SET status = $2, completed_at = NOW() WHERE id = $1`,
            [runId, finalStatus]
          );
          const tenantRes = await this.db.query(
            `SELECT tenant_id FROM maestro_runs WHERE id = $1`,
            [runId]
          );
          await this.emitReceiptSafely({
            runId,
            tenantId: tenantRes.rows?.[0]?.tenant_id || "unknown",
            actor: { id: "maestro-engine", principal_type: "system" },
            action: "maestro.run.completed",
            resource: {
              id: runId,
              type: "maestro.run",
              attributes: {
                status: finalStatus
              }
            },
            result: { status: finalStatus === "failed" ? "failure" : "success" }
          });
          logger2.info(`Run ${runId} completed with status ${finalStatus}`);
        }
      }
      setupEventListeners() {
        this.queueEvents.on("completed", ({ jobId }) => {
          logger2.debug(`Job ${jobId} completed`);
        });
        this.queueEvents.on("failed", ({ jobId, failedReason }) => {
          logger2.error(`Job ${jobId} failed: ${failedReason}`);
        });
      }
      async emitReceiptSafely(input) {
        try {
          await emitTransitionReceipt(input);
        } catch (error) {
          logger2.error("Governed Exception: receipt emission failed", {
            action: input.action,
            runId: input.runId,
            resourceId: input.resource.id,
            error: error.message
          });
        }
      }
      async shutdown() {
        await this.worker.close();
        await this.queue.close();
        await this.queueEvents.close();
      }
    };
  }
});

// src/maestro/handlers.ts
var handlers_exports = {};
__export(handlers_exports, {
  MaestroHandlers: () => MaestroHandlers
});
var MaestroHandlers;
var init_handlers = __esm({
  "src/maestro/handlers.ts"() {
    "use strict";
    init_logger2();
    MaestroHandlers = class {
      constructor(engine2, agentService, llm, graph, diffusionCoder) {
        this.engine = engine2;
        this.agentService = agentService;
        this.llm = llm;
        this.graph = graph;
        this.diffusionCoder = diffusionCoder;
      }
      registerAll() {
        this.engine.registerTaskHandler("llm_call", this.handleLLMCall.bind(this));
        this.engine.registerTaskHandler("rag_query", this.handleRAGQuery.bind(this));
        this.engine.registerTaskHandler("graph_job", this.handleGraphJob.bind(this));
        this.engine.registerTaskHandler("agent_call", this.handleAgentCall.bind(this));
        this.engine.registerTaskHandler("custom", this.handleCustom.bind(this));
        this.engine.registerTaskHandler("diffusion_edit", this.handleDiffusionEdit.bind(this));
      }
      async handleLLMCall(task) {
        logger2.info(`[Maestro] Executing LLM Call for task ${task.id}`);
        const { model, prompt, system } = task.payload;
        return this.llm.callCompletion(task.runId, task.id, {
          model: model || "gpt-4o",
          messages: [
            { role: "system", content: system || "You are a helpful assistant." },
            { role: "user", content: prompt }
          ]
        });
      }
      async handleRAGQuery(task) {
        logger2.info(`[Maestro] Executing RAG Query for task ${task.id}`);
        const { query: query3, filters } = task.payload;
        return {
          query: query3,
          results: [
            { id: "doc-1", text: "This is a retrieved document.", score: 0.95 },
            { id: "doc-2", text: "Another relevant chunk.", score: 0.88 }
          ]
        };
      }
      async handleGraphJob(task) {
        logger2.info(`[Maestro] Executing Graph Job for task ${task.id}`);
        const { algorithm, params } = task.payload;
        return this.graph.executeAlgorithm(algorithm, params);
      }
      async handleAgentCall(task) {
        logger2.info(`[Maestro] Executing Agent Call for task ${task.id}`);
        const { agentId, input } = task.payload;
        const agent = await this.agentService.getAgent(agentId, task.tenantId);
        if (!agent) throw new Error(`Agent not found: ${agentId}`);
        const subRun = await this.engine.createRun(
          task.tenantId,
          agent.templateId,
          input || {},
          "system-agent-caller"
        );
        return { subRunId: subRun.id, status: "started" };
      }
      async handleCustom(task) {
        logger2.info(`[Maestro] Executing Custom task ${task.id}`);
        return { result: "custom execution done", payload: task.payload };
      }
      async handleDiffusionEdit(task) {
        logger2.info(`[Maestro] Executing Diffusion Edit for task ${task.id}`);
        const { prompt, steps, block_length, remasking, threshold } = task.payload;
        const result2 = await this.diffusionCoder.executeDiffusion(
          task.runId,
          task.id,
          { prompt, steps, block_length, remasking, threshold },
          task.tenantId
        );
        return result2;
      }
    };
  }
});

// src/maestro/agent_service.ts
var agent_service_exports = {};
__export(agent_service_exports, {
  MaestroAgentService: () => MaestroAgentService
});
var MaestroAgentService;
var init_agent_service = __esm({
  "src/maestro/agent_service.ts"() {
    "use strict";
    MaestroAgentService = class {
      constructor(db2) {
        this.db = db2;
      }
      async createAgent(agent) {
        await this.db.query(
          `INSERT INTO maestro_agents (id, tenant_id, name, description, capabilities, template_id, config, metadata)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
          [agent.id, agent.tenantId, agent.name, agent.description, agent.capabilities, agent.templateId, agent.config, agent.metadata]
        );
        return agent;
      }
      async getAgent(id, tenantId) {
        const res = await this.db.query(
          `SELECT * FROM maestro_agents WHERE id = $1 AND tenant_id = $2`,
          [id, tenantId]
        );
        if (res.rows.length === 0) return null;
        return res.rows[0];
      }
      async listAgents(tenantId) {
        const res = await this.db.query(
          `SELECT * FROM maestro_agents WHERE tenant_id = $1`,
          [tenantId]
        );
        return res.rows;
      }
    };
  }
});

// src/maestro/adapters/diffusion_coder.ts
var diffusion_coder_exports = {};
__export(diffusion_coder_exports, {
  DiffusionCoderAdapter: () => DiffusionCoderAdapter
});
var DiffusionCoderAdapter;
var init_diffusion_coder = __esm({
  "src/maestro/adapters/diffusion_coder.ts"() {
    "use strict";
    init_policy_client();
    init_logger2();
    DiffusionCoderAdapter = class {
      constructor(llm) {
        this.llm = llm;
      }
      async executeDiffusion(runId, taskId, params, tenantId) {
        const {
          prompt,
          initialCode = "// Initial state\n",
          steps = 3,
          block_length = 4,
          // Number of blocks to "edit"
          remasking = "low_confidence",
          threshold = 0.5
        } = params;
        logger2.info(`[DiffusionCoder] Starting functional diffusion for task ${taskId} with ${steps} steps`);
        const uncertaintyMap = {};
        const policyVerdicts = [];
        let totalResamples = 0;
        let llmCalls = 0;
        let currentCode = initialCode;
        for (let step = 1; step <= steps; step++) {
          for (let b = 1; b <= block_length; b++) {
            let blockSuccessful = false;
            let attempts = 0;
            const maxResamples = 2;
            while (!blockSuccessful && attempts <= maxResamples) {
              attempts++;
              llmCalls++;
              const response = await this.llm.callCompletion(runId, taskId, {
                model: "gpt-4o",
                messages: [
                  {
                    role: "system",
                    content: `You are a Diffusion-based Code Editor.
                Step: ${step}/${steps}. Block: ${b}/${block_length}.
                Pattern: Stable-DiffCoder block-diffusion.
                Denoise and refine the following code block based on the prompt.`
                  },
                  {
                    role: "user",
                    content: `Prompt: ${prompt}
Full Code:
${currentCode}
Editing Block ${b}...`
                  }
                ]
              });
              const refinedCode = response.content;
              const confidence = 0.4 + (refinedCode.length > 20 ? 0.3 : 0.1) + Math.random() * 0.3;
              const policyResult = await policyClient.evaluate({
                user: { tenantId, roles: ["developer"] },
                action: "diffusion_denoise_block",
                resource: { blockIndex: b, step, attempt: attempts, confidence }
              });
              policyVerdicts.push({
                step,
                block: b,
                allowed: policyResult.allowed,
                reason: policyResult.reason
              });
              if (policyResult.allowed) {
                blockSuccessful = true;
                currentCode = refinedCode;
                uncertaintyMap[b] = confidence;
              } else {
                totalResamples++;
                logger2.warn(`[DiffusionCoder] Block ${b} at step ${step} vetoed. Resampling...`);
                if (attempts > maxResamples) {
                  uncertaintyMap[b] = 0.1;
                  blockSuccessful = true;
                }
              }
            }
          }
        }
        return {
          patch: currentCode,
          uncertaintyMap,
          policyVerdicts,
          evidenceBundleId: `evd-diff-${Math.random().toString(36).substring(7)}`,
          stats: {
            totalResamples,
            stepsCompleted: steps,
            llmCalls
          }
        };
      }
    };
  }
});

// src/graphql/plugins/persistedQueries.ts
var persistedQueries_exports = {};
__export(persistedQueries_exports, {
  persistedQueriesPlugin: () => persistedQueriesPlugin
});
import fs41 from "fs";
import path52 from "path";
var allowedOperationIds, candidates, persistedQueriesPlugin;
var init_persistedQueries = __esm({
  "src/graphql/plugins/persistedQueries.ts"() {
    "use strict";
    allowedOperationIds = /* @__PURE__ */ new Set();
    candidates = [
      process.env.PERSISTED_MANIFEST,
      path52.resolve(process.cwd(), "../client/src/generated/graphql.json"),
      path52.resolve(process.cwd(), "./client/src/generated/graphql.json"),
      path52.resolve(process.cwd(), "./persisted-operations.json")
    ].filter(Boolean);
    for (const p of candidates) {
      try {
        const raw = fs41.readFileSync(p, "utf8");
        const manifest = JSON.parse(raw);
        const ids = Object.keys(manifest);
        if (ids.length) {
          allowedOperationIds = new Set(ids);
          console.log(`Loaded ${ids.length} persisted operations from ${p}`);
          break;
        }
      } catch (error) {
      }
    }
    persistedQueriesPlugin = {
      async requestDidStart() {
        return {
          async didResolveOperation(ctx) {
            if (process.env.PERSISTED_QUERIES !== "1" && process.env.NODE_ENV !== "production") {
              return;
            }
            const opId = ctx.request.http?.headers.get("x-apollo-operation-id");
            if (!opId || !allowedOperationIds.has(opId)) {
              throw new Error("Unknown persisted operation");
            }
          }
        };
      }
    };
  }
});

// src/graphql/plugins/pbac.ts
var pbac_exports = {};
__export(pbac_exports, {
  default: () => pbac_default
});
var pbacPlugin, pbac_default;
var init_pbac = __esm({
  "src/graphql/plugins/pbac.ts"() {
    "use strict";
    pbacPlugin = () => ({
      async requestDidStart() {
        return {
          async willSendResponse() {
          }
        };
      }
    });
    pbac_default = pbacPlugin;
  }
});

// src/graphql/plugins/resolverMetrics.ts
var resolverMetrics_exports = {};
__export(resolverMetrics_exports, {
  default: () => resolverMetrics_default
});
var resolverMetricsPlugin, resolverMetrics_default;
var init_resolverMetrics = __esm({
  "src/graphql/plugins/resolverMetrics.ts"() {
    "use strict";
    init_metrics2();
    resolverMetricsPlugin = {
      async requestDidStart(_ctx) {
        return {
          async executionDidStart() {
            return {
              willResolveField({ info }) {
                const start = process.hrtime.bigint();
                const labels2 = {
                  resolver_name: `${info.parentType.name}.${info.fieldName}`,
                  field_name: info.fieldName,
                  type_name: info.parentType.name
                };
                if (graphqlResolverCallsTotal2) {
                  graphqlResolverCallsTotal2.inc(labels2);
                }
                return (error) => {
                  const duration = Number(process.hrtime.bigint() - start) / 1e9;
                  if (graphqlResolverDurationSeconds2) {
                    graphqlResolverDurationSeconds2.observe(
                      { ...labels2, status: error ? "error" : "success" },
                      duration
                    );
                  }
                  if (error && graphqlResolverErrorsTotal2) {
                    const errType = error?.constructor?.name || "Error";
                    graphqlResolverErrorsTotal2.inc({
                      ...labels2,
                      error_type: errType
                    });
                  }
                };
              }
            };
          }
        };
      }
    };
    resolverMetrics_default = resolverMetricsPlugin;
  }
});

// src/graphql/plugins/auditLogger.ts
var auditLogger_exports = {};
__export(auditLogger_exports, {
  auditLoggerPlugin: () => auditLoggerPlugin,
  default: () => auditLogger_default
});
import fs42 from "fs";
import axios10 from "axios";
import _ from "lodash";
var isEqual, ELASTIC_URL, LOG_FILE, ANONYMIZE, anonymize, auditLoggerPlugin, auditLogger_default;
var init_auditLogger = __esm({
  "src/graphql/plugins/auditLogger.ts"() {
    "use strict";
    init_ledger();
    init_audit2();
    ({ isEqual } = _);
    ELASTIC_URL = process.env.ELASTICSEARCH_URL;
    LOG_FILE = process.env.AUDIT_LOG_FILE || "audit-log.jsonl";
    ANONYMIZE = process.env.AUDIT_LOG_ANONYMIZE === "true";
    anonymize = (value) => {
      if (value === null || value === void 0) return value;
      if (typeof value === "object") {
        if (Array.isArray(value)) {
          return value.map(() => "[redacted]");
        }
        return Object.keys(value).reduce(
          (acc, key) => ({ ...acc, [key]: anonymize(value[key]) }),
          {}
        );
      }
      return "[redacted]";
    };
    auditLoggerPlugin = {
      async requestDidStart() {
        const start = /* @__PURE__ */ new Date();
        return {
          async willSendResponse(ctx) {
            const operation = ctx.operation;
            if (!operation || operation.operation !== "mutation") {
              return;
            }
            const entity = operation.selectionSet.selections[0]?.name?.value || "unknown";
            const userId = ctx.contextValue?.user?.id ?? "anonymous";
            const tenantId = ctx.contextValue?.user?.tenantId || "unknown-tenant";
            const requestId = ctx.request.http?.headers.get("x-request-id") || void 0;
            const correlationId = ctx.request.http?.headers.get("x-correlation-id") || void 0;
            const before = ctx.contextValue?.audit?.before;
            const after = ctx.contextValue?.audit?.after || (ctx.response.body.kind === "single" ? ctx.response.body.singleResult?.data?.[entity] : void 0);
            const diff = {};
            if (before && after && typeof before === "object" && typeof after === "object") {
              const keys = /* @__PURE__ */ new Set([
                ...Object.keys(before),
                ...Object.keys(after)
              ]);
              for (const key of keys) {
                const b = before[key];
                const a = after[key];
                if (!isEqual(b, a)) {
                  diff[key] = {
                    before: ANONYMIZE ? anonymize(b) : b,
                    after: ANONYMIZE ? anonymize(a) : a
                  };
                }
              }
            }
            const logEntry = {
              mutationType: "UPDATE",
              entityId: entity,
              entityType: entity,
              timestamp: start.toISOString(),
              userId: ANONYMIZE ? anonymize(userId) : userId,
              operation: operation.operation,
              entity,
              mutationDiff: diff
            };
            try {
              if (operation) {
                getAuditSystem2().recordEvent({
                  eventType: "resource_modify",
                  action: entity,
                  outcome: ctx.errors ? "failure" : "success",
                  userId: userId || "anonymous",
                  tenantId,
                  serviceId: "graphql-api",
                  resourceType: "entity",
                  resourceId: entity,
                  message: `GraphQL Mutation: ${entity}`,
                  level: "info",
                  requestId,
                  correlationId,
                  details: {
                    diff,
                    operationName: ctx.request.operationName,
                    variables: ANONYMIZE ? {} : ctx.request.variables
                  },
                  complianceRelevant: true
                });
              }
            } catch (error) {
              if (process.env.NODE_ENV !== "test") {
                console.error("Failed to log to Advanced Audit System", error);
              }
            }
            try {
              await provenanceLedger.appendEntry({
                tenantId,
                actionType: "GRAPHQL_MUTATION",
                resourceType: entity,
                resourceId: entity,
                actorId: userId || "anonymous",
                actorType: userId ? "user" : "system",
                payload: logEntry,
                timestamp: start,
                metadata: {
                  requestId,
                  correlationId
                }
              });
            } catch (error) {
              console.error("Failed to stamp GraphQL mutation to Provenance Ledger", error);
            }
            try {
              if (ELASTIC_URL) {
                await axios10.post(`${ELASTIC_URL}/audit/_doc`, logEntry, {
                  timeout: 2e3
                });
              }
            } catch (_err) {
              if (process.env.NODE_ENV !== "production" && process.env.NODE_ENV !== "test") {
                fs42.appendFileSync(LOG_FILE, JSON.stringify(logEntry) + "\n");
              }
            }
          }
        };
      }
    };
    auditLogger_default = auditLoggerPlugin;
  }
});

// src/graphql/validation/depthLimit.ts
var depthLimit_exports = {};
__export(depthLimit_exports, {
  depthLimit: () => depthLimit
});
import { GraphQLError as GraphQLError13 } from "graphql";
function depthLimit(maxDepth = 10) {
  return (context4) => {
    let currentDepth = 0;
    let operationName = "anonymous";
    return {
      OperationDefinition: {
        enter(node) {
          if (node.name && node.name.value) {
            operationName = node.name.value;
          }
        },
        leave() {
          operationName = "anonymous";
        }
      },
      SelectionSet: {
        enter(_node) {
          currentDepth++;
          if (currentDepth > maxDepth) {
            context4.reportError(
              new GraphQLError13(
                `Query is too deep: depth ${currentDepth} > ${maxDepth} (operation: ${operationName})`
              )
            );
          }
        },
        leave() {
          currentDepth--;
        }
      }
    };
  };
}
var init_depthLimit = __esm({
  "src/graphql/validation/depthLimit.ts"() {
    "use strict";
  }
});

// src/graphql/plugins/rateLimitAndCache.ts
var rateLimitAndCache_exports = {};
__export(rateLimitAndCache_exports, {
  rateLimitAndCachePlugin: () => rateLimitAndCachePlugin
});
import { GraphQLError as GraphQLError14 } from "graphql";
import gqc from "graphql-query-complexity";
import pino70 from "pino";
var getComplexity, simpleEstimator, fieldExtensionsEstimator, logger65, rateLimitAndCachePlugin;
var init_rateLimitAndCache = __esm({
  "src/graphql/plugins/rateLimitAndCache.ts"() {
    "use strict";
    ({ getComplexity, simpleEstimator, fieldExtensionsEstimator } = gqc.default || gqc);
    logger65 = pino70.default ? pino70.default() : pino70();
    rateLimitAndCachePlugin = (schema2) => {
      return {
        async requestDidStart() {
          return {
            async didResolveOperation({ request, document }) {
              const complexity = getComplexity({
                schema: schema2,
                operationName: request.operationName,
                query: document,
                variables: request.variables,
                estimators: [
                  fieldExtensionsEstimator(),
                  simpleEstimator({ defaultComplexity: 1 })
                ]
              });
              const MAX_COMPLEXITY = 1e3;
              if (complexity > MAX_COMPLEXITY) {
                throw new GraphQLError14(`Query is too complex: ${complexity}. Maximum allowed is ${MAX_COMPLEXITY}.`, {
                  extensions: {
                    code: "QUERY_TOO_COMPLEX"
                  }
                });
              }
            },
            async willSendResponse({ request, response }) {
            }
          };
        }
      };
    };
  }
});

// src/graphql/plugins/httpStatusCodePlugin.ts
var httpStatusCodePlugin_exports = {};
__export(httpStatusCodePlugin_exports, {
  httpStatusCodePlugin: () => httpStatusCodePlugin
});
var httpStatusCodePlugin;
var init_httpStatusCodePlugin = __esm({
  "src/graphql/plugins/httpStatusCodePlugin.ts"() {
    "use strict";
    httpStatusCodePlugin = () => {
      return {
        async requestDidStart() {
          return {
            async willSendResponse({ response }) {
              if (response.body.kind === "single" && response.body.singleResult.errors) {
                const errors = response.body.singleResult.errors;
                let statusCode = 200;
                for (const error of errors) {
                  const http3 = error.extensions?.http;
                  const errorStatus = http3?.status;
                  if (typeof errorStatus === "number") {
                    if (errorStatus >= 500) {
                      statusCode = errorStatus;
                      break;
                    } else if (errorStatus >= 400 && statusCode < 500) {
                      statusCode = errorStatus;
                    } else if (statusCode === 200) {
                      statusCode = errorStatus;
                    }
                  }
                }
                if (statusCode !== 200 && response.http) {
                  response.http.status = statusCode;
                }
              }
            }
          };
        }
      };
    };
  }
});

// src/utils/http-client.ts
import axios11 from "axios";
function createSafeClient(baseURL) {
  const client6 = axios11.create({
    baseURL,
    timeout: 15e3
  });
  client6.interceptors.request.use((config9) => {
    if (!config9.url) return config9;
    try {
      const url = new URL(config9.url, config9.baseURL);
      const host = url.hostname;
      const isAllowed = EGRESS_ALLOW_LIST.some(
        (allowed) => host === allowed || host.endsWith(`.${allowed}`)
      );
      if (!isAllowed) {
        logger.error({ host, url: config9.url }, "Egress Blocked: Destination not in allow-list");
        throw new Error(`Egress Blocked: ${host} is not an authorized destination.`);
      }
    } catch (err) {
      if (err.message.startsWith("Egress Blocked")) throw err;
    }
    return config9;
  });
  client6.interceptors.request.use(async (config9) => {
    const isInternal = config9.url && (config9.url.includes("server") || config9.url.includes("gateway") || config9.url.includes("ai-sandbox") || config9.url.includes("agentic-mesh-evaluation"));
    if (isInternal) {
      const serviceId = process.env.SERVICE_ID || "summit-api";
      const identity = quantumIdentityManager.issueIdentity(serviceId);
      config9.headers["X-Summit-PQC-Identity"] = JSON.stringify(identity);
      logger.debug({ serviceId, url: config9.url }, "PQC Identity attached to request");
    }
    return config9;
  });
  return client6;
}
var EGRESS_ALLOW_LIST, safeClient;
var init_http_client = __esm({
  "src/utils/http-client.ts"() {
    "use strict";
    init_logger();
    init_quantum_identity_manager();
    EGRESS_ALLOW_LIST = [
      "localhost",
      "postgres",
      "redis",
      "neo4j",
      "elasticsearch",
      "otel-collector",
      "jaeger",
      "api.openai.com",
      "api.anthropic.com",
      "slack.com",
      "github.com",
      "atlassian.net",
      "notion.so"
    ];
    safeClient = createSafeClient();
  }
});

// src/webhooks/webhook.worker.ts
var webhook_worker_exports = {};
__export(webhook_worker_exports, {
  webhookWorker: () => webhookWorker
});
import { Worker as Worker5 } from "bullmq";
import IORedis2 from "ioredis";
var connection4, webhookWorker;
var init_webhook_worker = __esm({
  "src/webhooks/webhook.worker.ts"() {
    "use strict";
    init_http_client();
    init_pg();
    init_webhook_service();
    init_logger2();
    connection4 = new IORedis2(process.env.REDIS_URL || "redis://localhost:6379", {
      maxRetriesPerRequest: null
    });
    webhookWorker = new Worker5(
      "webhooks",
      async (job) => {
        const { deliveryId, url, secret, payload, eventType, tenantId } = job.data;
        const { signature, timestamp } = webhookService.generateSignature(payload, secret);
        const startTime = Date.now();
        const attemptNumber = (job.attemptsMade || 0) + 1;
        const maxAttempts = job.opts.attempts || 1;
        const baseDelay = typeof job.opts.backoff === "object" && job.opts.backoff ? job.opts.backoff.delay || 1e3 : 1e3;
        const truncateBody = (body4) => {
          if (body4 === void 0 || body4 === null) return body4;
          try {
            return JSON.stringify(body4).substring(0, 1e4);
          } catch {
            return String(body4).substring(0, 1e4);
          }
        };
        try {
          const response = await safeClient.post(url, payload, {
            headers: {
              "Content-Type": "application/json",
              "X-Webhook-Signature": signature,
              "X-Webhook-Timestamp": timestamp,
              "X-Webhook-Event": eventType,
              "X-Webhook-Delivery": deliveryId,
              "User-Agent": "Summit-Webhook-Service/1.0"
            },
            timeout: 1e4,
            // 10s timeout
            validateStatus: () => true
            // Capture all status codes
          });
          const duration = Date.now() - startTime;
          const success = response.status >= 200 && response.status < 300;
          const willRetry = !success && attemptNumber < maxAttempts;
          const nextRetryAt = !success && willRetry ? new Date(Date.now() + baseDelay * Math.pow(2, attemptNumber - 1)) : null;
          const status = success ? "success" : willRetry ? "pending" : "failed";
          await pg.oneOrNone(
            `UPDATE webhook_deliveries
         SET status = $1,
             response_status = $2,
             response_body = $3,
             attempt_count = $4,
             next_retry_at = $5,
             last_attempt_at = NOW(),
             last_error = $6,
             updated_at = NOW()
         WHERE id = $7`,
            [
              status,
              response.status,
              truncateBody(response.data),
              attemptNumber,
              nextRetryAt,
              success ? null : `HTTP ${response.status}`,
              deliveryId
            ],
            { tenantId }
          );
          await pg.oneOrNone(
            `INSERT INTO webhook_delivery_attempts
         (delivery_id, webhook_id, tenant_id, attempt_number, status, response_status, response_body, duration_ms)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
         RETURNING id`,
            [
              deliveryId,
              job.data.webhookId,
              tenantId,
              attemptNumber,
              status,
              response.status,
              truncateBody(response.data),
              duration
            ],
            { tenantId }
          );
          if (!success) {
            throw new Error(`Webhook failed with status ${response.status}`);
          }
        } catch (error) {
          const duration = Date.now() - startTime;
          const errorMessage = error?.message || "Unknown error";
          const responseStatus = error.response?.status;
          const responseBody = truncateBody(error.response?.data || errorMessage);
          const willRetry = attemptNumber < maxAttempts;
          const nextRetryAt = willRetry ? new Date(Date.now() + baseDelay * Math.pow(2, attemptNumber - 1)) : null;
          const status = willRetry ? "pending" : "failed";
          await pg.oneOrNone(
            `UPDATE webhook_deliveries
         SET status = $1,
             response_status = $2,
             response_body = $3,
             attempt_count = $4,
             next_retry_at = $5,
             last_attempt_at = NOW(),
             last_error = $6,
             updated_at = NOW()
         WHERE id = $7`,
            [
              status,
              responseStatus,
              responseBody,
              attemptNumber,
              nextRetryAt,
              errorMessage,
              deliveryId
            ],
            { tenantId }
          );
          await pg.oneOrNone(
            `INSERT INTO webhook_delivery_attempts
         (delivery_id, webhook_id, tenant_id, attempt_number, status, response_status, response_body, error_message, duration_ms)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
         RETURNING id`,
            [
              deliveryId,
              job.data.webhookId,
              tenantId,
              attemptNumber,
              status,
              responseStatus,
              responseBody,
              errorMessage,
              duration
            ],
            { tenantId }
          );
          throw error;
        }
      },
      {
        connection: connection4,
        concurrency: 10
      }
    );
    webhookWorker.on("completed", (job) => {
      logger2.info(`Webhook delivery ${job.data.deliveryId} completed`);
    });
    webhookWorker.on("failed", (job, err) => {
      logger2.error(`Webhook delivery ${job?.data.deliveryId} failed: ${err.message}`);
    });
  }
});

// src/realtime/kafkaConsumer.ts
var kafkaConsumer_exports = {};
__export(kafkaConsumer_exports, {
  startKafkaConsumer: () => startKafkaConsumer,
  stopKafkaConsumer: () => stopKafkaConsumer
});
import { Kafka as Kafka2 } from "kafkajs";
var KAFKA_BROKERS2, KAFKA_TOPIC, KAFKA_GROUP_ID, consumer, wargameResolver2, startKafkaConsumer, stopKafkaConsumer;
var init_kafkaConsumer = __esm({
  "src/realtime/kafkaConsumer.ts"() {
    "use strict";
    init_WargameResolver();
    KAFKA_BROKERS2 = process.env.KAFKA_BROKERS ? process.env.KAFKA_BROKERS.split(",") : ["localhost:9092"];
    KAFKA_TOPIC = "intelgraph.alerts.crisis_scenario_trigger";
    KAFKA_GROUP_ID = "wargame-dashboard-consumer-group";
    consumer = null;
    wargameResolver2 = new WargameResolver();
    startKafkaConsumer = async () => {
      if (process.env.NODE_ENV === "production" && !KAFKA_BROKERS2[0]) {
        console.warn(
          "Kafka brokers not configured. Skipping Kafka consumer startup."
        );
        return;
      }
      const kafka = new Kafka2({
        clientId: "wargame-dashboard",
        brokers: KAFKA_BROKERS2
      });
      consumer = kafka.consumer({ groupId: KAFKA_GROUP_ID });
      try {
        await consumer.connect();
        await consumer.subscribe({ topic: KAFKA_TOPIC, fromBeginning: false });
        await consumer.run({
          eachMessage: async ({
            topic,
            partition,
            message
          }) => {
            if (!message.value) {
              console.warn(
                `Kafka Consumer: Received empty message from topic ${topic}`
              );
              return;
            }
            const payload = JSON.parse(message.value.toString());
            console.log(
              `Kafka Consumer: Received message from topic ${topic}:`,
              payload
            );
            const scenarioInput = {
              crisisType: payload.crisis_type || "unknown_crisis",
              targetAudiences: payload.target_audiences || ["general_public"],
              keyNarratives: payload.key_narratives || ["unspecified_narrative"],
              adversaryProfiles: payload.adversary_profiles || [
                "unknown_adversary"
              ],
              simulationParameters: payload.simulation_parameters || {}
            };
            console.log(
              "Kafka Consumer: Triggering war-game simulation from Kafka message..."
            );
            try {
              const newScenario = await wargameResolver2.runWarGameSimulation(
                null,
                // parent
                { input: scenarioInput },
                {}
                // context - mock as it's an internal call
              );
              console.log(
                `Kafka Consumer: Successfully triggered simulation for scenario: ${newScenario.id}`
              );
            } catch (error) {
              console.error("Kafka Consumer: Error triggering simulation:", error);
            }
          }
        });
        console.log(`Kafka Consumer: Started listening on topic ${KAFKA_TOPIC}`);
      } catch (error) {
        console.error("Kafka Consumer: Failed to start:", error);
        if (consumer) {
          await consumer.disconnect();
        }
        consumer = null;
      }
    };
    stopKafkaConsumer = async () => {
      if (consumer) {
        console.log("Kafka Consumer: Disconnecting...");
        await consumer.disconnect();
        consumer = null;
        console.log("Kafka Consumer: Disconnected.");
      }
    };
  }
});

// src/realtime/graph-crdt.ts
import Redis14 from "ioredis";
import pino72 from "pino";
function getGraph(graphId) {
  let entry = graphs.get(graphId);
  if (!entry) {
    entry = { crdt: new GraphCRDT(), clock: 0 };
    graphs.set(graphId, entry);
  }
  return entry;
}
function initGraphSync(ns) {
  ioRef = ns;
  if (typeof sub.psubscribe === "function") {
    sub.psubscribe("graph:op:*");
  }
  sub.on("pmessage", (_pattern, channel, message) => {
    const graphId = channel.split(":")[2];
    const op = JSON.parse(message);
    const entry = getGraph(graphId);
    entry.clock = Math.max(entry.clock, op.ts);
    if (entry.crdt.apply(op)) {
      ioRef?.to(`graph:${graphId}`).emit("graph:op", { graphId, op });
    }
  });
  sub.on("error", (err) => logger67.error({ err }, "Redis sub error"));
}
function registerGraphHandlers(socket, options2 = {}) {
  socket.on(
    "graph:op",
    async ({ graphId, op }) => {
      if (!graphId || !op) return;
      if (options2.authorize) {
        const allowed = await options2.authorize(graphId, op, "mutate");
        if (!allowed) {
          socket.emit("graph:error", {
            graphId,
            reason: "forbidden"
          });
          return;
        }
      }
      const entry = getGraph(graphId);
      entry.clock = Math.max(entry.clock, op.ts || 0) + 1;
      op.ts = entry.clock;
      if (entry.crdt.apply(op)) {
        socket.to(`graph:${graphId}`).emit("graph:op", { graphId, op });
        pub.publish(`graph:op:${graphId}`, JSON.stringify(op));
        await Promise.resolve(options2.onApplied?.(graphId, op));
      }
    }
  );
  socket.on("graph:sync", async ({ graphId }) => {
    if (!graphId) return;
    if (options2.authorize) {
      const allowed = await options2.authorize(
        graphId,
        {
          id: "sync",
          kind: "node",
          action: "set",
          ts: Date.now()
        },
        "view"
      );
      if (!allowed) {
        socket.emit("graph:error", { graphId, reason: "forbidden" });
        return;
      }
    }
    const entry = getGraph(graphId);
    const snapshot = entry.crdt.snapshot();
    socket.emit("graph:state", {
      graphId,
      nodes: snapshot.nodes,
      edges: snapshot.edges,
      clock: entry.clock
    });
  });
}
function getGraphSnapshot(graphId) {
  const entry = getGraph(graphId);
  const snapshot = entry.crdt.snapshot();
  return { ...snapshot, clock: entry.clock };
}
var logger67, GraphCRDT, graphs, redisOptions, pub, sub, ioRef;
var init_graph_crdt = __esm({
  "src/realtime/graph-crdt.ts"() {
    "use strict";
    logger67 = pino72();
    GraphCRDT = class {
      nodes = /* @__PURE__ */ new Map();
      edges = /* @__PURE__ */ new Map();
      apply(op) {
        const store = op.kind === "node" ? this.nodes : this.edges;
        const existing = store.get(op.id);
        if (!existing || op.ts >= existing.ts) {
          if (op.action === "delete") {
            store.delete(op.id);
          } else {
            store.set(op.id, { ts: op.ts, data: op.data });
          }
          return true;
        }
        return false;
      }
      snapshot() {
        return {
          nodes: Array.from(this.nodes.values()).map((entry) => entry.data),
          edges: Array.from(this.edges.values()).map((entry) => entry.data)
        };
      }
    };
    graphs = /* @__PURE__ */ new Map();
    redisOptions = {
      host: process.env.REDIS_HOST || "localhost",
      port: Number(process.env.REDIS_PORT || 6379),
      password: process.env.REDIS_PASSWORD || void 0
    };
    pub = new Redis14(redisOptions);
    sub = typeof pub.duplicate === "function" ? pub.duplicate() : new Redis14(redisOptions);
    ioRef = null;
  }
});

// src/realtime/collaborationHub.ts
import { randomUUID as randomUUID79 } from "crypto";
var CollaborationHub, createCollaborationHub;
var init_collaborationHub = __esm({
  "src/realtime/collaborationHub.ts"() {
    "use strict";
    CollaborationHub = class {
      namespace;
      socketWorkspace = /* @__PURE__ */ new Map();
      workspaceState = /* @__PURE__ */ new Map();
      activityLimit;
      presenceThrottleMs;
      presenceChannels = /* @__PURE__ */ new Map();
      socketPresenceChannel = /* @__PURE__ */ new Map();
      lastPresenceUpdate = /* @__PURE__ */ new Map();
      constructor(io, options2 = {}) {
        this.namespace = io.of("/collaboration");
        this.activityLimit = options2.activityLimit ?? 200;
        this.presenceThrottleMs = options2.presenceThrottleMs ?? 75;
        this.namespace.on("connection", (socket) => this.handleConnection(socket));
      }
      handleConnection(socket) {
        socket.on("workspace:join", (payload) => this.handleJoin(socket, payload));
        socket.on("workspace:leave", () => this.handleLeave(socket));
        socket.on(
          "presence:update",
          (payload) => this.handlePresenceUpdate(socket, payload)
        );
        socket.on(
          "presence:channel:join",
          (payload) => this.handlePresenceChannelJoin(socket, payload)
        );
        socket.on(
          "presence:channel:leave",
          () => this.handlePresenceChannelLeave(socket)
        );
        socket.on(
          "presence:channel:update",
          (payload) => this.handlePresenceChannelUpdate(socket, payload)
        );
        socket.on("chat:message", (payload) => this.handleChat(socket, payload));
        socket.on(
          "annotation:add",
          (payload) => this.handleAnnotation(socket, "add", payload)
        );
        socket.on(
          "annotation:update",
          (payload) => this.handleAnnotation(socket, "update", payload)
        );
        socket.on(
          "annotation:delete",
          (payload) => this.handleAnnotation(socket, "delete", payload)
        );
        socket.on("edit:submit", (payload) => this.handleEdit(socket, payload));
        socket.on("disconnect", () => {
          this.handlePresenceChannelLeave(socket);
          this.handleLeave(socket);
        });
      }
      handleJoin(socket, payload) {
        const { workspaceId, userId, userName } = payload;
        if (!workspaceId || !userId) return;
        const workspace = this.getOrCreateWorkspace(workspaceId);
        const presence = {
          userId,
          userName,
          workspaceId,
          status: "online",
          lastActive: Date.now()
        };
        workspace.members.set(userId, presence);
        this.socketWorkspace.set(socket.id, workspaceId);
        socket.join(this.roomName(workspaceId));
        this.recordActivity(workspaceId, {
          id: randomUUID79(),
          workspaceId,
          type: "workspace_join",
          description: `${userName} joined workspace`,
          actorId: userId,
          createdAt: Date.now()
        });
        socket.emit("workspace:joined", this.serializeWorkspace(workspaceId));
        socket.to(this.roomName(workspaceId)).emit("presence:joined", { ...presence });
        this.namespace.to(this.roomName(workspaceId)).emit("workspace:activity", this.latestActivity(workspaceId));
      }
      handleLeave(socket) {
        const workspaceId = this.socketWorkspace.get(socket.id);
        if (!workspaceId) return;
        const workspace = this.workspaceState.get(workspaceId);
        if (!workspace) return;
        const departingUser = [...workspace.members.values()].find(
          (member) => member.workspaceId === workspaceId
        );
        if (departingUser) {
          workspace.members.delete(departingUser.userId);
          this.recordActivity(workspaceId, {
            id: randomUUID79(),
            workspaceId,
            type: "workspace_leave",
            description: `${departingUser.userName} left workspace`,
            actorId: departingUser.userId,
            createdAt: Date.now()
          });
          socket.to(this.roomName(workspaceId)).emit("presence:left", { userId: departingUser.userId });
          this.namespace.to(this.roomName(workspaceId)).emit("workspace:activity", this.latestActivity(workspaceId));
        }
        this.socketWorkspace.delete(socket.id);
      }
      handlePresenceChannelJoin(socket, payload) {
        const { workspaceId, channel, userId, userName } = payload;
        if (!workspaceId || !channel || !userId) return;
        this.handlePresenceChannelLeave(socket);
        const channelMembers = this.getOrCreatePresenceChannel(
          workspaceId,
          channel
        );
        const presence = {
          userId,
          userName,
          workspaceId,
          channel,
          status: "online",
          lastActive: Date.now()
        };
        channelMembers.set(userId, presence);
        this.socketPresenceChannel.set(socket.id, { workspaceId, channel, userId });
        socket.join(this.presenceRoomName(workspaceId, channel));
        socket.emit("presence:channel:snapshot", {
          workspaceId,
          channel,
          members: [...channelMembers.values()]
        });
        socket.to(this.presenceRoomName(workspaceId, channel)).emit("presence:channel:joined", { ...presence });
      }
      handlePresenceChannelLeave(socket) {
        const presenceInfo = this.socketPresenceChannel.get(socket.id);
        if (!presenceInfo) return;
        const { workspaceId, channel, userId } = presenceInfo;
        const channelMembers = this.getPresenceChannel(workspaceId, channel);
        if (!channelMembers) return;
        channelMembers.delete(userId);
        if (channelMembers.size === 0) {
          const workspaceChannels = this.presenceChannels.get(workspaceId);
          workspaceChannels?.delete(channel);
          if (workspaceChannels?.size === 0) {
            this.presenceChannels.delete(workspaceId);
          }
        }
        socket.to(this.presenceRoomName(workspaceId, channel)).emit("presence:channel:left", { userId, workspaceId, channel });
        this.socketPresenceChannel.delete(socket.id);
        this.lastPresenceUpdate.delete(socket.id);
      }
      handlePresenceChannelUpdate(socket, payload) {
        const presenceInfo = this.socketPresenceChannel.get(socket.id);
        if (!presenceInfo) return;
        const { workspaceId, channel, userId } = presenceInfo;
        if (workspaceId !== payload.workspaceId || channel !== payload.channel) return;
        const lastUpdate = this.lastPresenceUpdate.get(socket.id) ?? 0;
        if (Date.now() - lastUpdate < this.presenceThrottleMs) return;
        this.lastPresenceUpdate.set(socket.id, Date.now());
        const channelMembers = this.getPresenceChannel(workspaceId, channel);
        if (!channelMembers) return;
        const presence = channelMembers.get(userId);
        if (!presence) return;
        const updated = {
          ...presence,
          cursor: payload.cursor ?? presence.cursor,
          selection: payload.selection ?? presence.selection,
          status: payload.status ?? presence.status,
          lastActive: Date.now()
        };
        channelMembers.set(userId, updated);
        this.namespace.to(this.presenceRoomName(workspaceId, channel)).emit("presence:channel:update", { ...updated });
      }
      handlePresenceUpdate(socket, payload) {
        const { workspaceId, userId } = payload;
        const workspace = this.workspaceState.get(workspaceId);
        if (!workspace) return;
        const presence = workspace.members.get(userId);
        if (!presence) return;
        const updated = {
          ...presence,
          ...payload,
          lastActive: Date.now()
        };
        workspace.members.set(userId, updated);
        this.recordActivity(workspaceId, {
          id: randomUUID79(),
          workspaceId,
          type: "presence_update",
          description: `${presence.userName} moved cursor`,
          actorId: userId,
          createdAt: Date.now()
        });
        socket.to(this.roomName(workspaceId)).emit("presence:update", { ...updated });
        this.namespace.to(this.roomName(workspaceId)).emit("workspace:activity", this.latestActivity(workspaceId));
      }
      handleChat(socket, payload) {
        const { workspaceId, userId, message } = payload;
        const workspace = this.workspaceState.get(workspaceId);
        if (!workspace || !message) return;
        const chat = {
          id: randomUUID79(),
          workspaceId,
          userId,
          message,
          createdAt: Date.now()
        };
        this.recordActivity(workspaceId, {
          id: randomUUID79(),
          workspaceId,
          type: "chat",
          description: message,
          actorId: userId,
          createdAt: chat.createdAt
        });
        this.namespace.to(this.roomName(workspaceId)).emit("chat:message", chat);
        this.namespace.to(this.roomName(workspaceId)).emit("workspace:activity", this.latestActivity(workspaceId));
      }
      handleAnnotation(socket, action, payload) {
        const { workspaceId, userId, targetId } = payload;
        const workspace = this.workspaceState.get(workspaceId);
        if (!workspace || !targetId) return;
        const annotationId = payload.annotationId ?? randomUUID79();
        const existing = workspace.annotations.get(annotationId);
        if (action === "delete") {
          workspace.annotations.delete(annotationId);
        } else {
          const now = Date.now();
          const annotation = {
            id: annotationId,
            workspaceId,
            targetId,
            userId,
            body: payload.body ?? existing?.body ?? "",
            createdAt: existing?.createdAt ?? now,
            updatedAt: now
          };
          workspace.annotations.set(annotationId, annotation);
        }
        this.recordActivity(workspaceId, {
          id: randomUUID79(),
          workspaceId,
          type: "annotation",
          description: `${action} annotation on ${targetId}`,
          actorId: userId,
          createdAt: Date.now()
        });
        this.namespace.to(this.roomName(workspaceId)).emit("annotation:updated", {
          action,
          annotationId,
          targetId,
          workspaceId,
          body: payload.body,
          userId
        });
        this.namespace.to(this.roomName(workspaceId)).emit("workspace:activity", this.latestActivity(workspaceId));
      }
      handleEdit(socket, payload) {
        const { workspaceId, entityId, userId, version, changes } = payload;
        const workspace = this.workspaceState.get(workspaceId);
        if (!workspace || !entityId) return;
        const entityState = workspace.entities.get(entityId) ?? {
          version: 0
        };
        if (version !== entityState.version) {
          const resolution = this.resolveConflict(entityState.lastEdit, {
            workspaceId,
            entityId,
            userId,
            version,
            changes,
            timestamp: Date.now()
          });
          const resolvedEdit = {
            workspaceId,
            entityId,
            userId,
            version: resolution.resolvedVersion,
            changes: resolution.resolvedChanges,
            timestamp: Date.now()
          };
          workspace.entities.set(entityId, {
            version: resolution.resolvedVersion,
            lastEdit: resolvedEdit
          });
          socket.emit("edit:conflict", resolution);
          this.namespace.to(this.roomName(workspaceId)).emit("edit:applied", resolvedEdit);
          this.recordActivity(workspaceId, {
            id: randomUUID79(),
            workspaceId,
            type: "edit_conflict",
            description: `Conflict on ${entityId} resolved`,
            actorId: userId,
            createdAt: Date.now()
          });
          this.namespace.to(this.roomName(workspaceId)).emit("workspace:activity", this.latestActivity(workspaceId));
          return;
        }
        const appliedEdit = {
          workspaceId,
          entityId,
          userId,
          version: entityState.version + 1,
          changes,
          timestamp: Date.now()
        };
        workspace.entities.set(entityId, {
          version: appliedEdit.version,
          lastEdit: appliedEdit
        });
        this.namespace.to(this.roomName(workspaceId)).emit("edit:applied", appliedEdit);
        this.recordActivity(workspaceId, {
          id: randomUUID79(),
          workspaceId,
          type: "edit_applied",
          description: `Edit applied to ${entityId}`,
          actorId: userId,
          createdAt: appliedEdit.timestamp
        });
        this.namespace.to(this.roomName(workspaceId)).emit("workspace:activity", this.latestActivity(workspaceId));
      }
      resolveConflict(previousEdit, incoming) {
        const mergedChanges = {
          ...previousEdit?.changes ?? {},
          ...incoming.changes
        };
        const resolvedVersion = Math.max(
          previousEdit?.version ?? 0,
          incoming.version
        ) + 1;
        return {
          strategy: "last-writer-wins+merge",
          previousVersion: previousEdit?.version ?? 0,
          incomingVersion: incoming.version,
          resolvedVersion,
          resolvedChanges: mergedChanges
        };
      }
      getOrCreateWorkspace(workspaceId) {
        if (!this.workspaceState.has(workspaceId)) {
          this.workspaceState.set(workspaceId, {
            members: /* @__PURE__ */ new Map(),
            annotations: /* @__PURE__ */ new Map(),
            activity: [],
            entities: /* @__PURE__ */ new Map()
          });
        }
        return this.workspaceState.get(workspaceId);
      }
      roomName(workspaceId) {
        return `workspace:${workspaceId}`;
      }
      presenceRoomName(workspaceId, channel) {
        return `presence:${workspaceId}:${channel}`;
      }
      getOrCreatePresenceChannel(workspaceId, channel) {
        const workspaceChannels = this.presenceChannels.get(workspaceId) ?? /* @__PURE__ */ new Map();
        if (!this.presenceChannels.has(workspaceId)) {
          this.presenceChannels.set(workspaceId, workspaceChannels);
        }
        const channelMembers = workspaceChannels.get(channel) ?? /* @__PURE__ */ new Map();
        if (!workspaceChannels.has(channel)) {
          workspaceChannels.set(channel, channelMembers);
        }
        return channelMembers;
      }
      getPresenceChannel(workspaceId, channel) {
        return this.presenceChannels.get(workspaceId)?.get(channel);
      }
      recordActivity(workspaceId, event) {
        const workspace = this.getOrCreateWorkspace(workspaceId);
        workspace.activity.unshift(event);
        if (workspace.activity.length > this.activityLimit) {
          workspace.activity = workspace.activity.slice(0, this.activityLimit);
        }
      }
      latestActivity(workspaceId) {
        const workspace = this.workspaceState.get(workspaceId);
        return workspace ? [...workspace.activity] : [];
      }
      serializeWorkspace(workspaceId) {
        const workspace = this.getOrCreateWorkspace(workspaceId);
        return {
          workspaceId,
          members: [...workspace.members.values()],
          annotations: [...workspace.annotations.values()],
          activity: [...workspace.activity],
          entities: [...workspace.entities.entries()].map(([entityId, state]) => ({
            entityId,
            version: state.version
          }))
        };
      }
    };
    createCollaborationHub = (io, options2) => new CollaborationHub(io, options2);
  }
});

// src/realtime/investigationState.ts
import Redis15 from "ioredis";
import pino73 from "pino";
import { randomUUID as randomUUID80 } from "node:crypto";
function annotationKey(investigationId) {
  return `intelgraph:investigation:${investigationId}:annotations`;
}
function commentKey(investigationId) {
  return `intelgraph:investigation:${investigationId}:comments`;
}
function activityKey(investigationId) {
  return `intelgraph:investigation:${investigationId}:activity`;
}
function presenceKey(investigationId) {
  return `intelgraph:investigation:${investigationId}:presence`;
}
function ensureMap(store, key) {
  let map = store.get(key);
  if (!map) {
    map = /* @__PURE__ */ new Map();
    store.set(key, map);
  }
  return map;
}
function upsertMemory(store, investigationId, entity) {
  const map = ensureMap(store, investigationId);
  map.set(entity.id, entity);
}
function deleteFromMemory(store, investigationId, id) {
  const map = store.get(investigationId);
  if (map) {
    map.delete(id);
    if (map.size === 0) {
      store.delete(investigationId);
    }
  }
}
function getMemoryValues(store, investigationId) {
  const map = store.get(investigationId);
  return map ? Array.from(map.values()) : [];
}
async function getAnnotations(investigationId) {
  if (redis3) {
    try {
      const raw = await redis3.hgetall(annotationKey(investigationId));
      const values = Object.values(raw).map(
        (value) => JSON.parse(value)
      );
      values.sort((a, b) => a.createdAt - b.createdAt);
      values.forEach((val) => upsertMemory(annotationsStore, investigationId, val));
      return values;
    } catch (err) {
      logger68.warn({ err }, "Failed to fetch annotations from Redis");
    }
  }
  return getMemoryValues(annotationsStore, investigationId);
}
async function addAnnotation(investigationId, input) {
  const annotation = {
    id: randomUUID80(),
    ...input,
    createdAt: Date.now(),
    updatedAt: Date.now()
  };
  upsertMemory(annotationsStore, investigationId, annotation);
  if (redis3) {
    try {
      await redis3.hset(
        annotationKey(investigationId),
        annotation.id,
        JSON.stringify(annotation)
      );
    } catch (err) {
      logger68.warn({ err }, "Failed to persist annotation to Redis");
    }
  }
  return annotation;
}
async function updateAnnotation(investigationId, annotationId, patch) {
  const existing = ensureMap(annotationsStore, investigationId).get(annotationId);
  let current = existing || null;
  if (!current && redis3) {
    try {
      const raw = await redis3.hget(annotationKey(investigationId), annotationId);
      current = raw ? JSON.parse(raw) : null;
    } catch (err) {
      logger68.warn({ err }, "Failed to load annotation from Redis");
    }
  }
  if (!current) return null;
  const next = {
    ...current,
    ...patch,
    updatedAt: Date.now()
  };
  upsertMemory(annotationsStore, investigationId, next);
  if (redis3) {
    try {
      await redis3.hset(
        annotationKey(investigationId),
        annotationId,
        JSON.stringify(next)
      );
    } catch (err) {
      logger68.warn({ err }, "Failed to update annotation in Redis");
    }
  }
  return next;
}
async function deleteAnnotation(investigationId, annotationId) {
  deleteFromMemory(annotationsStore, investigationId, annotationId);
  if (redis3) {
    try {
      await redis3.hdel(annotationKey(investigationId), annotationId);
    } catch (err) {
      logger68.warn({ err }, "Failed to delete annotation from Redis");
    }
  }
  return true;
}
async function getComments2(investigationId) {
  if (redis3) {
    try {
      const raw = await redis3.hgetall(commentKey(investigationId));
      const values = Object.values(raw).map(
        (value) => JSON.parse(value)
      );
      values.sort((a, b) => a.createdAt - b.createdAt);
      values.forEach((val) => upsertMemory(commentsStore, investigationId, val));
      return values;
    } catch (err) {
      logger68.warn({ err }, "Failed to fetch comments from Redis");
    }
  }
  return getMemoryValues(commentsStore, investigationId);
}
async function addComment2(investigationId, input) {
  const comment = {
    id: randomUUID80(),
    ...input,
    createdAt: Date.now(),
    updatedAt: Date.now()
  };
  upsertMemory(commentsStore, investigationId, comment);
  if (redis3) {
    try {
      await redis3.hset(
        commentKey(investigationId),
        comment.id,
        JSON.stringify(comment)
      );
    } catch (err) {
      logger68.warn({ err }, "Failed to persist comment to Redis");
    }
  }
  return comment;
}
async function updateComment(investigationId, commentId, patch) {
  const existing = ensureMap(commentsStore, investigationId).get(commentId);
  let current = existing || null;
  if (!current && redis3) {
    try {
      const raw = await redis3.hget(commentKey(investigationId), commentId);
      current = raw ? JSON.parse(raw) : null;
    } catch (err) {
      logger68.warn({ err }, "Failed to load comment from Redis");
    }
  }
  if (!current) return null;
  const next = {
    ...current,
    ...patch,
    updatedAt: Date.now()
  };
  upsertMemory(commentsStore, investigationId, next);
  if (redis3) {
    try {
      await redis3.hset(
        commentKey(investigationId),
        commentId,
        JSON.stringify(next)
      );
    } catch (err) {
      logger68.warn({ err }, "Failed to update comment in Redis");
    }
  }
  return next;
}
async function deleteComment(investigationId, commentId) {
  deleteFromMemory(commentsStore, investigationId, commentId);
  if (redis3) {
    try {
      await redis3.hdel(commentKey(investigationId), commentId);
    } catch (err) {
      logger68.warn({ err }, "Failed to delete comment from Redis");
    }
  }
  return true;
}
async function recordActivity(investigationId, entry) {
  const activity = {
    id: randomUUID80(),
    ...entry,
    timestamp: entry.timestamp ?? Date.now()
  };
  const existing = activityStore.get(investigationId) || [];
  activityStore.set(
    investigationId,
    [activity, ...existing].slice(0, ACTIVITY_LIMIT)
  );
  if (redis3) {
    try {
      await redis3.lpush(activityKey(investigationId), JSON.stringify(activity));
      await redis3.ltrim(activityKey(investigationId), 0, ACTIVITY_LIMIT - 1);
    } catch (err) {
      logger68.warn({ err }, "Failed to append activity to Redis");
    }
  }
  return activity;
}
async function getActivity(investigationId, limit = 50) {
  if (redis3) {
    try {
      const raw = await redis3.lrange(activityKey(investigationId), 0, limit - 1);
      const list = raw.map((value) => JSON.parse(value));
      if (list.length > 0) {
        activityStore.set(investigationId, list);
      }
      return list;
    } catch (err) {
      logger68.warn({ err }, "Failed to fetch activity from Redis");
    }
  }
  return (activityStore.get(investigationId) || []).slice(0, limit);
}
async function setPresence(investigationId, user) {
  const map = ensureMap(presenceStore, investigationId);
  const entry = { ...user, lastSeen: Date.now() };
  map.set(user.userId, entry);
  if (redis3) {
    try {
      await redis3.hset(
        presenceKey(investigationId),
        user.userId,
        JSON.stringify(entry)
      );
      await redis3.expire(presenceKey(investigationId), 120);
    } catch (err) {
      logger68.warn({ err }, "Failed to update presence in Redis");
    }
  }
  return getPresence(investigationId);
}
async function touchPresence(investigationId, userId, status = "online") {
  const map = ensureMap(presenceStore, investigationId);
  const existing = map.get(userId);
  if (!existing) {
    return setPresence(investigationId, {
      userId,
      username: userId,
      status,
      lastSeen: Date.now()
    });
  }
  const next = { ...existing, status, lastSeen: Date.now() };
  map.set(userId, next);
  if (redis3) {
    try {
      await redis3.hset(
        presenceKey(investigationId),
        userId,
        JSON.stringify(next)
      );
      await redis3.expire(presenceKey(investigationId), 120);
    } catch (err) {
      logger68.warn({ err }, "Failed to refresh presence in Redis");
    }
  }
  return getPresence(investigationId);
}
async function removePresence(investigationId, userId) {
  const map = presenceStore.get(investigationId);
  if (map) {
    map.delete(userId);
    if (map.size === 0) {
      presenceStore.delete(investigationId);
    }
  }
  if (redis3) {
    try {
      await redis3.hdel(presenceKey(investigationId), userId);
    } catch (err) {
      logger68.warn({ err }, "Failed to remove presence from Redis");
    }
  }
  return getPresence(investigationId);
}
async function getPresence(investigationId) {
  const now = Date.now();
  const map = ensureMap(presenceStore, investigationId);
  if (redis3) {
    try {
      const raw = await redis3.hgetall(presenceKey(investigationId));
      Object.entries(raw).forEach(([userId, value]) => {
        try {
          const parsed = JSON.parse(value);
          map.set(userId, parsed);
        } catch (err) {
          logger68.warn({ err, investigationId }, "Failed to parse presence");
        }
      });
    } catch (err) {
      logger68.warn({ err }, "Failed to load presence from Redis");
    }
  }
  const entries = Array.from(map.values()).filter(
    (entry) => now - entry.lastSeen <= PRESENCE_TTL_MS
  );
  for (const entry of map.values()) {
    if (now - entry.lastSeen > PRESENCE_TTL_MS) {
      map.delete(entry.userId);
      if (redis3) {
        redis3?.hdel(presenceKey(investigationId), entry.userId).catch(
          (err) => logger68.warn({ err }, "Failed to prune stale presence in Redis")
        );
      }
    }
  }
  return entries;
}
var logger68, redisUrl, redisHost, redisPort, redisPassword, redis3, annotationsStore, commentsStore, activityStore, presenceStore, PRESENCE_TTL_MS, ACTIVITY_LIMIT;
var init_investigationState = __esm({
  "src/realtime/investigationState.ts"() {
    "use strict";
    logger68 = pino73({ name: "investigation-state" });
    redisUrl = process.env.REDIS_URL;
    redisHost = process.env.REDIS_HOST;
    redisPort = process.env.REDIS_PORT;
    redisPassword = process.env.REDIS_PASSWORD;
    redis3 = null;
    try {
      if (redisUrl) {
        redis3 = new Redis15(redisUrl);
      } else if (redisHost) {
        redis3 = new Redis15({
          host: redisHost,
          port: Number(redisPort || 6379),
          password: redisPassword
        });
      }
      redis3?.on("error", (err) => {
        logger68.warn({ err }, "Redis unavailable for investigation state, using memory");
        redis3 = null;
      });
    } catch (err) {
      logger68.warn({ err }, "Failed to init Redis for investigation state");
      redis3 = null;
    }
    annotationsStore = /* @__PURE__ */ new Map();
    commentsStore = /* @__PURE__ */ new Map();
    activityStore = /* @__PURE__ */ new Map();
    presenceStore = /* @__PURE__ */ new Map();
    PRESENCE_TTL_MS = 6e4;
    ACTIVITY_LIMIT = 200;
  }
});

// src/realtime/investigationAccess.ts
import Redis16 from "ioredis";
import pino74 from "pino";
function memberKey(investigationId) {
  return `intelgraph:investigation:${investigationId}:members`;
}
function ensureMemberMap(investigationId) {
  let map = memberStore.get(investigationId);
  if (!map) {
    map = /* @__PURE__ */ new Map();
    memberStore.set(investigationId, map);
  }
  return map;
}
async function setMemberRole(investigationId, userId, role) {
  const map = ensureMemberMap(investigationId);
  map.set(userId, role);
  if (redis4) {
    try {
      await redis4.hset(memberKey(investigationId), userId, role);
    } catch (err) {
      logger69.warn({ err }, "Failed to persist member role to Redis");
    }
  }
}
async function getMembers(investigationId) {
  if (redis4) {
    try {
      const raw = await redis4.hgetall(memberKey(investigationId));
      const map2 = ensureMemberMap(investigationId);
      for (const [userId, value] of Object.entries(raw)) {
        map2.set(userId, value);
      }
    } catch (err) {
      logger69.warn({ err }, "Failed to fetch members from Redis");
    }
  }
  const map = ensureMemberMap(investigationId);
  const result2 = {};
  for (const [userId, role] of map.entries()) {
    result2[userId] = role;
  }
  return result2;
}
async function getMemberRole(investigationId, userId) {
  const map = ensureMemberMap(investigationId);
  let role = map.get(userId) || null;
  if (!role && redis4) {
    try {
      const raw = await redis4.hget(memberKey(investigationId), userId);
      if (raw) {
        role = raw;
        map.set(userId, role);
      }
    } catch (err) {
      logger69.warn({ err }, "Failed to load member role from Redis");
    }
  }
  return role;
}
async function authorizeInvestigationAction(investigationId, user, action) {
  if (!user?.id) {
    return { allowed: false, role: null, via: "member" };
  }
  const globalRole = user.role?.toUpperCase() || "";
  const globalPermissions = GLOBAL_ROLE_PERMISSIONS[globalRole];
  if (globalPermissions?.has(action)) {
    if (!await getMemberRole(investigationId, user.id)) {
      if (globalRole === "ADMIN") {
        await setMemberRole(investigationId, user.id, "owner");
      } else if (globalRole === "EDITOR") {
        await setMemberRole(investigationId, user.id, "editor");
      }
    }
    return { allowed: true, role: globalRole || null, via: "global" };
  }
  const members = await getMembers(investigationId);
  if (Object.keys(members).length === 0) {
    const bootstrapRole = globalPermissions?.has("manage") ? "owner" : globalPermissions?.has("edit") ? "editor" : "viewer";
    await setMemberRole(investigationId, user.id, bootstrapRole);
    const allowed = ROLE_PERMISSIONS3[bootstrapRole]?.has(action) ?? false;
    return { allowed, role: bootstrapRole, via: "member" };
  }
  const memberRole = await getMemberRole(investigationId, user.id);
  if (!memberRole) {
    if (globalPermissions?.has("view")) {
      const derivedRole = globalPermissions.has("comment") ? "commenter" : "viewer";
      await setMemberRole(investigationId, user.id, derivedRole);
      const allowed = ROLE_PERMISSIONS3[derivedRole]?.has(action) ?? false;
      return { allowed, role: derivedRole, via: "member" };
    }
    return { allowed: false, role: null, via: "member" };
  }
  const permissions = ROLE_PERMISSIONS3[memberRole];
  if (permissions?.has(action)) {
    return { allowed: true, role: memberRole, via: "member" };
  }
  return { allowed: false, role: memberRole, via: "member" };
}
var logger69, redisUrl2, redisHost2, redisPort2, redisPassword2, redis4, memberStore, ROLE_PERMISSIONS3, GLOBAL_ROLE_PERMISSIONS;
var init_investigationAccess = __esm({
  "src/realtime/investigationAccess.ts"() {
    "use strict";
    logger69 = pino74({ name: "investigation-access" });
    redisUrl2 = process.env.REDIS_URL;
    redisHost2 = process.env.REDIS_HOST;
    redisPort2 = process.env.REDIS_PORT;
    redisPassword2 = process.env.REDIS_PASSWORD;
    redis4 = null;
    try {
      if (redisUrl2) {
        redis4 = new Redis16(redisUrl2);
      } else if (redisHost2) {
        redis4 = new Redis16({
          host: redisHost2,
          port: Number(redisPort2 || 6379),
          password: redisPassword2
        });
      }
      redis4?.on("error", (err) => {
        logger69.warn({ err }, "Redis unavailable for investigation access; using memory");
        redis4 = null;
      });
    } catch (err) {
      logger69.warn({ err }, "Failed to init Redis for investigation access");
      redis4 = null;
    }
    memberStore = /* @__PURE__ */ new Map();
    ROLE_PERMISSIONS3 = {
      owner: /* @__PURE__ */ new Set(["view", "edit", "comment", "manage"]),
      editor: /* @__PURE__ */ new Set(["view", "edit", "comment"]),
      commenter: /* @__PURE__ */ new Set(["view", "comment"]),
      viewer: /* @__PURE__ */ new Set(["view"])
    };
    GLOBAL_ROLE_PERMISSIONS = {
      ADMIN: /* @__PURE__ */ new Set(["view", "edit", "comment", "manage"]),
      EDITOR: /* @__PURE__ */ new Set(["view", "edit", "comment"]),
      ANALYST: /* @__PURE__ */ new Set(["view", "comment"]),
      REVIEWER: /* @__PURE__ */ new Set(["view"])
    };
  }
});

// src/realtime/dashboard.ts
import pino75 from "pino";
function registerDashboardHandlers(io, socket) {
  socket.on("dashboard:join", () => {
    logger70.info(`User ${socket.id} joined dashboard`);
    socket.join("dashboard:main");
    socket.emit("dashboard:state", {
      connected: true,
      timestamp: Date.now(),
      message: "Joined dashboard stream"
    });
  });
  socket.on("dashboard:leave", () => {
    logger70.info(`User ${socket.id} left dashboard`);
    socket.leave("dashboard:main");
  });
}
var logger70;
var init_dashboard = __esm({
  "src/realtime/dashboard.ts"() {
    "use strict";
    logger70 = pino75();
  }
});

// src/realtime/socket.ts
var socket_exports = {};
__export(socket_exports, {
  getIO: () => getIO,
  initSocket: () => initSocket,
  overrideVerifyToken: () => overrideVerifyToken
});
import { randomUUID as randomUUID81 } from "node:crypto";
import { Server } from "socket.io";
import pino76 from "pino";
function overrideVerifyToken(fn) {
  verifyToken2 = fn ?? verifyToken;
}
function initSocket(httpServer) {
  const io = new Server(httpServer, {
    cors: {
      origin: process.env.CORS_ORIGIN?.split(",") || ["http://localhost:3000"],
      credentials: true
    }
  });
  createCollaborationHub(io);
  const ns = io.of("/realtime");
  ns.use(async (socket, next) => {
    try {
      const token = socket.handshake.auth?.token || socket.handshake.headers?.authorization?.replace("Bearer ", "");
      const user = await verifyToken2(token);
      if (!user) {
        logger71.warn({ token }, "Unauthorized socket connection attempt");
        return next(new Error("Unauthorized"));
      }
      socket.user = user;
      socket.tenantId = socket.handshake.auth?.tenantId || user?.tenantId || "default";
      next();
    } catch (e) {
      logger71.warn(
        { error: e.message },
        "Unauthorized socket connection attempt"
      );
      next(new Error("Unauthorized"));
    }
  });
  ns.on("connection", (socket) => {
    const userId = socket.user?.id;
    logger71.info(`Realtime connected ${socket.id} for user ${userId}`);
    const joinedInvestigations = /* @__PURE__ */ new Set();
    const displayName = () => socket.user?.username || socket.user?.email || socket.user?.id || "user";
    const emitRealtimeAudit = async (actionType, target, metadata) => {
      if (!socket.user?.id || !socket.tenantId) return;
      try {
        await emitAuditEvent(
          {
            eventId: randomUUID81(),
            occurredAt: (/* @__PURE__ */ new Date()).toISOString(),
            actor: {
              type: "user",
              id: socket.user.id,
              name: displayName(),
              ipAddress: socket.handshake.address
            },
            action: {
              type: actionType,
              outcome: "success"
            },
            tenantId: socket.tenantId,
            target,
            metadata: {
              ...metadata,
              userAgent: socket.handshake.headers["user-agent"],
              socketId: socket.id
            }
          },
          {
            correlationId: socket.id,
            serviceId: "realtime"
          }
        );
      } catch (err) {
        logger71.warn(
          { err: err.message, actionType },
          "Failed to emit realtime audit event"
        );
      }
    };
    const roomFor = (investigationId) => `tenant:${socket.tenantId}:investigation:${investigationId}`;
    const ensureAuthorized = async (investigationId, action) => {
      const result2 = await authorizeInvestigationAction(
        investigationId,
        socket.user,
        action
      );
      if (!result2.allowed) {
        socket.emit("investigation:error", {
          investigationId,
          message: "Forbidden",
          code: "FORBIDDEN"
        });
        socket.emit("error", "Forbidden");
      }
      return result2;
    };
    const broadcastPresence = async (investigationId, presence) => {
      ns.to(roomFor(investigationId)).emit("presence:update", {
        investigationId,
        presence
      });
    };
    socket.on(
      "investigation:join",
      async ({ investigationId }) => {
        if (!investigationId || !socket.user?.id) return;
        try {
          const auth = await ensureAuthorized(investigationId, "view");
          if (!auth.allowed) return;
          socket.join(roomFor(investigationId));
          socket.join(`graph:${investigationId}`);
          joinedInvestigations.add(investigationId);
          const [annotations, comments, activity, members] = await Promise.all([
            getAnnotations(investigationId),
            getComments2(investigationId),
            getActivity(investigationId, 50),
            getMembers(investigationId)
          ]);
          const graph = getGraphSnapshot(investigationId);
          const presence = await setPresence(investigationId, {
            userId: socket.user.id,
            username: displayName(),
            status: "online",
            lastSeen: Date.now()
          });
          socket.emit("investigation:state", {
            investigationId,
            graph,
            annotations,
            comments,
            activity,
            presence,
            members,
            role: auth.role
          });
          await broadcastPresence(investigationId, presence);
          const activityEntry = await recordActivity(investigationId, {
            type: "presence",
            action: "join",
            actorId: socket.user.id,
            actorName: displayName(),
            details: { status: "online" }
          });
          ns.to(roomFor(investigationId)).emit("activity:event", activityEntry);
          await emitRealtimeAudit(
            "presence.join",
            {
              type: "presence",
              id: socket.user.id,
              path: `investigations/${investigationId}`
            },
            {
              investigationId,
              status: "online",
              presenceCount: presence.length
            }
          );
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId },
            "Failed to join investigation room"
          );
          socket.emit("investigation:error", {
            investigationId,
            message: "Unable to join investigation",
            code: "JOIN_FAILED"
          });
        }
      }
    );
    socket.on(
      "investigation:leave",
      async ({ investigationId }) => {
        if (!investigationId || !joinedInvestigations.has(investigationId)) {
          return;
        }
        joinedInvestigations.delete(investigationId);
        socket.leave(roomFor(investigationId));
        socket.leave(`graph:${investigationId}`);
        try {
          const presence = await removePresence(
            investigationId,
            socket.user.id
          );
          await broadcastPresence(investigationId, presence);
          const activityEntry = await recordActivity(investigationId, {
            type: "presence",
            action: "leave",
            actorId: socket.user.id,
            actorName: displayName(),
            details: { status: "offline" }
          });
          ns.to(roomFor(investigationId)).emit("activity:event", activityEntry);
          await emitRealtimeAudit(
            "presence.leave",
            {
              type: "presence",
              id: socket.user.id,
              path: `investigations/${investigationId}`
            },
            {
              investigationId,
              status: "offline",
              presenceCount: presence.length
            }
          );
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId },
            "Failed to leave investigation room cleanly"
          );
        }
      }
    );
    socket.on(
      "presence:heartbeat",
      async ({ investigationId, status }) => {
        if (!investigationId || !joinedInvestigations.has(investigationId)) {
          return;
        }
        try {
          const presence = await touchPresence(
            investigationId,
            socket.user.id,
            status || "online"
          );
          await broadcastPresence(investigationId, presence);
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId },
            "Failed to refresh presence"
          );
        }
      }
    );
    socket.on(
      "presence:set_status",
      async ({ investigationId, status }) => {
        if (!investigationId || !joinedInvestigations.has(investigationId)) {
          return;
        }
        try {
          const presence = await touchPresence(
            investigationId,
            socket.user.id,
            status
          );
          await broadcastPresence(investigationId, presence);
          const activityEntry = await recordActivity(investigationId, {
            type: "presence",
            action: "status",
            actorId: socket.user.id,
            actorName: displayName(),
            details: { status }
          });
          ns.to(roomFor(investigationId)).emit("activity:event", activityEntry);
          await emitRealtimeAudit(
            "presence.status",
            {
              type: "presence",
              id: socket.user.id,
              path: `investigations/${investigationId}`
            },
            {
              investigationId,
              status,
              presenceCount: presence.length
            }
          );
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId },
            "Failed to update presence status"
          );
        }
      }
    );
    socket.on(
      "annotation:add",
      async (payload) => {
        if (!payload?.investigationId || !joinedInvestigations.has(payload.investigationId)) {
          return;
        }
        const auth = await ensureAuthorized(payload.investigationId, "edit");
        if (!auth.allowed) return;
        try {
          const annotation = await addAnnotation(payload.investigationId, {
            targetId: payload.targetId,
            text: payload.text,
            authorId: socket.user.id,
            authorName: displayName(),
            position: payload.position ?? null,
            resolved: payload.resolved
          });
          ns.to(roomFor(payload.investigationId)).emit("annotation:added", {
            investigationId: payload.investigationId,
            annotation
          });
          const activityEntry = await recordActivity(payload.investigationId, {
            type: "annotation",
            action: "added",
            actorId: socket.user.id,
            actorName: displayName(),
            details: { annotationId: annotation.id, targetId: annotation.targetId }
          });
          ns.to(roomFor(payload.investigationId)).emit("activity:event", activityEntry);
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId: payload.investigationId },
            "Failed to add annotation"
          );
        }
      }
    );
    socket.on(
      "annotation:update",
      async (payload) => {
        if (!payload?.investigationId || !joinedInvestigations.has(payload.investigationId)) {
          return;
        }
        const auth = await ensureAuthorized(payload.investigationId, "edit");
        if (!auth.allowed) return;
        try {
          const updated = await updateAnnotation(
            payload.investigationId,
            payload.annotationId,
            {
              text: payload.text,
              position: payload.position,
              resolved: payload.resolved
            }
          );
          if (!updated) return;
          ns.to(roomFor(payload.investigationId)).emit("annotation:updated", {
            investigationId: payload.investigationId,
            annotation: updated
          });
          const activityEntry = await recordActivity(payload.investigationId, {
            type: "annotation",
            action: "updated",
            actorId: socket.user.id,
            actorName: displayName(),
            details: { annotationId: payload.annotationId }
          });
          ns.to(roomFor(payload.investigationId)).emit("activity:event", activityEntry);
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId: payload.investigationId },
            "Failed to update annotation"
          );
        }
      }
    );
    socket.on(
      "annotation:delete",
      async ({ investigationId, annotationId }) => {
        if (!investigationId || !joinedInvestigations.has(investigationId)) {
          return;
        }
        const auth = await ensureAuthorized(investigationId, "edit");
        if (!auth.allowed) return;
        try {
          await deleteAnnotation(investigationId, annotationId);
          ns.to(roomFor(investigationId)).emit("annotation:deleted", {
            investigationId,
            annotationId
          });
          const activityEntry = await recordActivity(investigationId, {
            type: "annotation",
            action: "deleted",
            actorId: socket.user.id,
            actorName: displayName(),
            details: { annotationId }
          });
          ns.to(roomFor(investigationId)).emit("activity:event", activityEntry);
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId },
            "Failed to delete annotation"
          );
        }
      }
    );
    socket.on("comment:add", async (payload) => {
      if (!payload?.investigationId || !joinedInvestigations.has(payload.investigationId)) {
        return;
      }
      const auth = await ensureAuthorized(payload.investigationId, "comment");
      if (!auth.allowed) return;
      try {
        const comment = await addComment2(payload.investigationId, {
          threadId: payload.threadId,
          targetId: payload.targetId,
          message: payload.message,
          authorId: socket.user.id,
          authorName: displayName()
        });
        ns.to(roomFor(payload.investigationId)).emit("comment:added", {
          investigationId: payload.investigationId,
          comment
        });
        const activityEntry = await recordActivity(payload.investigationId, {
          type: "comment",
          action: "added",
          actorId: socket.user.id,
          actorName: displayName(),
          details: { commentId: comment.id, threadId: comment.threadId }
        });
        ns.to(roomFor(payload.investigationId)).emit("activity:event", activityEntry);
        await emitRealtimeAudit(
          "comment.added",
          {
            type: "comment",
            id: comment.id,
            path: `investigations/${payload.investigationId}`
          },
          {
            investigationId: payload.investigationId,
            targetId: payload.targetId,
            threadId: payload.threadId,
            messageLength: payload.message?.length ?? 0
          }
        );
      } catch (err) {
        logger71.warn(
          { err: err.message, investigationId: payload.investigationId },
          "Failed to add comment"
        );
      }
    });
    socket.on("comment:update", async (payload) => {
      if (!payload?.investigationId || !joinedInvestigations.has(payload.investigationId)) {
        return;
      }
      const auth = await ensureAuthorized(payload.investigationId, "comment");
      if (!auth.allowed) return;
      try {
        const updated = await updateComment(
          payload.investigationId,
          payload.commentId,
          {
            message: payload.message
          }
        );
        if (!updated) return;
        ns.to(roomFor(payload.investigationId)).emit("comment:updated", {
          investigationId: payload.investigationId,
          comment: updated
        });
        const activityEntry = await recordActivity(payload.investigationId, {
          type: "comment",
          action: "updated",
          actorId: socket.user.id,
          actorName: displayName(),
          details: { commentId: payload.commentId }
        });
        ns.to(roomFor(payload.investigationId)).emit("activity:event", activityEntry);
        await emitRealtimeAudit(
          "comment.updated",
          {
            type: "comment",
            id: payload.commentId,
            path: `investigations/${payload.investigationId}`
          },
          {
            investigationId: payload.investigationId,
            messageLength: payload.message?.length ?? 0
          }
        );
      } catch (err) {
        logger71.warn(
          { err: err.message, investigationId: payload.investigationId },
          "Failed to update comment"
        );
      }
    });
    socket.on(
      "comment:delete",
      async ({ investigationId, commentId }) => {
        if (!investigationId || !joinedInvestigations.has(investigationId)) {
          return;
        }
        const auth = await ensureAuthorized(investigationId, "comment");
        if (!auth.allowed) return;
        try {
          await deleteComment(investigationId, commentId);
          ns.to(roomFor(investigationId)).emit("comment:deleted", {
            investigationId,
            commentId
          });
          const activityEntry = await recordActivity(investigationId, {
            type: "comment",
            action: "deleted",
            actorId: socket.user.id,
            actorName: displayName(),
            details: { commentId }
          });
          ns.to(roomFor(investigationId)).emit("activity:event", activityEntry);
          await emitRealtimeAudit(
            "comment.deleted",
            {
              type: "comment",
              id: commentId,
              path: `investigations/${investigationId}`
            },
            {
              investigationId
            }
          );
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId },
            "Failed to delete comment"
          );
        }
      }
    );
    socket.on("activity:fetch", async (payload) => {
      if (!payload?.investigationId || !joinedInvestigations.has(payload.investigationId)) {
        return;
      }
      const auth = await ensureAuthorized(payload.investigationId, "view");
      if (!auth.allowed) return;
      const list = await getActivity(
        payload.investigationId,
        payload.limit ?? 50
      );
      socket.emit("activity:list", {
        investigationId: payload.investigationId,
        activity: list
      });
    });
    socket.on("entity_update", async (payload) => {
      const investigationId = payload.graphId || payload.investigationId;
      if (!investigationId || !joinedInvestigations.has(investigationId)) {
        return;
      }
      const auth = await ensureAuthorized(investigationId, "edit");
      if (!auth.allowed) return;
      const eventPayload = {
        userId: socket.user.id,
        username: displayName(),
        entityId: payload.entityId,
        changes: payload.changes,
        ts: Date.now()
      };
      socket.to(`graph:${investigationId}`).emit("entity_updated", eventPayload);
      ns.to(roomFor(investigationId)).emit("entity_updated", eventPayload);
      const activityEntry = await recordActivity(investigationId, {
        type: "entity",
        action: "updated",
        actorId: socket.user.id,
        actorName: displayName(),
        details: { entityId: payload.entityId }
      });
      ns.to(roomFor(investigationId)).emit("activity:event", activityEntry);
    });
    registerGraphHandlers(socket, {
      authorize: async (graphId, op, intent) => {
        if (!joinedInvestigations.has(graphId)) return false;
        const action = intent === "mutate" ? "edit" : "view";
        const auth = await authorizeInvestigationAction(
          graphId,
          socket.user,
          action
        );
        if (!auth.allowed) {
          socket.emit("graph:error", { graphId, reason: "forbidden" });
          return false;
        }
        if (intent === "mutate") {
          op.meta = {
            ...op.meta || {},
            actorId: socket.user?.id,
            actorName: displayName()
          };
        }
        return true;
      },
      onApplied: async (graphId, op) => {
        try {
          const activityEntry = await recordActivity(graphId, {
            type: "graph",
            action: `${op.kind}:${op.action}`,
            actorId: op.meta?.actorId || socket.user?.id || "unknown",
            actorName: op.meta?.actorName || displayName(),
            details: { id: op.id, kind: op.kind, action: op.action }
          });
          ns.to(roomFor(graphId)).emit("activity:event", activityEntry);
        } catch (err) {
          logger71.warn(
            { err: err.message, graphId },
            "Failed to record graph activity entry"
          );
        }
      }
    });
    socket.on("disconnect", async () => {
      logger71.info(`Realtime disconnect ${socket.id} for user ${userId}`);
      for (const investigationId of joinedInvestigations) {
        try {
          const presence = await removePresence(
            investigationId,
            socket.user.id
          );
          await broadcastPresence(investigationId, presence);
          const activityEntry = await recordActivity(investigationId, {
            type: "presence",
            action: "disconnect",
            actorId: socket.user.id,
            actorName: displayName(),
            details: { status: "offline" }
          });
          ns.to(roomFor(investigationId)).emit("activity:event", activityEntry);
          await emitRealtimeAudit(
            "presence.disconnect",
            {
              type: "presence",
              id: socket.user.id,
              path: `investigations/${investigationId}`
            },
            {
              investigationId,
              status: "offline",
              presenceCount: presence.length
            }
          );
        } catch (err) {
          logger71.warn(
            { err: err.message, investigationId },
            "Failed to clean up presence on disconnect"
          );
        }
      }
    });
  });
  ioInstance = io;
  ns.on("connection", (socket) => {
    registerDashboardHandlers(ns, socket);
  });
  initGraphSync(ns);
  return io;
}
function getIO() {
  return ioInstance;
}
var logger71, verifyToken2, ioInstance;
var init_socket = __esm({
  "src/realtime/socket.ts"() {
    "use strict";
    init_auth();
    init_graph_crdt();
    init_collaborationHub();
    init_investigationState();
    init_investigationAccess();
    init_dashboard();
    init_emit();
    logger71 = pino76();
    verifyToken2 = verifyToken;
    ioInstance = null;
  }
});

// src/services/governance/PolicyWatcher.ts
var PolicyWatcher_exports = {};
__export(PolicyWatcher_exports, {
  PolicyWatcher: () => PolicyWatcher
});
import { createHash as createHash39 } from "crypto";
import fs43 from "fs";
import path53 from "path";
var PolicyWatcher;
var init_PolicyWatcher = __esm({
  "src/services/governance/PolicyWatcher.ts"() {
    "use strict";
    init_logger2();
    init_otel_tracing();
    PolicyWatcher = class _PolicyWatcher {
      static instance;
      policiesDir;
      checkInterval = null;
      lastKnownHash = "";
      constructor() {
        this.policiesDir = path53.resolve(process.cwd(), "policies");
      }
      static getInstance() {
        if (!_PolicyWatcher.instance) {
          _PolicyWatcher.instance = new _PolicyWatcher();
        }
        return _PolicyWatcher.instance;
      }
      start(intervalMs = 6e4) {
        if (this.checkInterval) return;
        logger_default2.info("Starting PolicyWatcher drift detection", { policiesDir: this.policiesDir });
        this.checkDrift();
        this.checkInterval = setInterval(() => {
          this.checkDrift();
        }, intervalMs);
      }
      stop() {
        if (this.checkInterval) {
          clearInterval(this.checkInterval);
          this.checkInterval = null;
        }
      }
      async checkDrift() {
        const span = otelService.createSpan("policy.check_drift");
        try {
          const localHash = await this.calculateLocalPolicyHash();
          const activeHash = await this.fetchActivePolicyHash();
          const drifted = localHash !== activeHash;
          if (drifted) {
            logger_default2.warn("Policy Drift Detected!", {
              localHash,
              activeHash,
              dir: this.policiesDir
            });
            otelService.addSpanAttributes({ "policy.drift": true });
          } else {
            logger_default2.debug("Policy sync confirmed", { hash: localHash });
            otelService.addSpanAttributes({ "policy.drift": false });
          }
          this.lastKnownHash = localHash;
          return drifted;
        } catch (error) {
          logger_default2.error("Failed to check policy drift", { error: error.message });
          return false;
        } finally {
          span?.end();
        }
      }
      async calculateLocalPolicyHash() {
        try {
          if (!fs43.existsSync(this.policiesDir)) {
            logger_default2.warn("Policies directory not found", { path: this.policiesDir });
            return "empty";
          }
          const files = fs43.readdirSync(this.policiesDir).filter((f) => f.endsWith(".rego")).sort();
          const hash3 = createHash39("sha256");
          for (const file of files) {
            const content = fs43.readFileSync(path53.join(this.policiesDir, file));
            hash3.update(file);
            hash3.update(content);
          }
          return hash3.digest("hex");
        } catch (e) {
          logger_default2.error("Error hashing policies", e);
          throw e;
        }
      }
      async fetchActivePolicyHash() {
        if (process.env.SIMULATE_POLICY_DRIFT === "true") {
          return "drifted-hash-value";
        }
        return this.calculateLocalPolicyHash();
      }
    };
  }
});

// src/utils/sampleData.ts
var sampleData_exports = {};
__export(sampleData_exports, {
  createSampleData: () => createSampleData
});
import { randomUUID as uuidv436 } from "crypto";
async function createSampleData() {
  logger2.info("Creating sample data for development...");
  try {
    await createSampleEntities();
    await createSampleRelationships();
    await createSampleUsers();
    await createSampleInvestigations();
    logger2.info("Sample data created successfully");
  } catch (error) {
    logger2.error({ error }, "Failed to create sample data");
  }
}
async function createSampleEntities() {
  const driver3 = getNeo4jDriver();
  const session = driver3.session();
  try {
    const entities = [
      {
        id: uuidv436(),
        type: "PERSON",
        props: {
          name: "John Smith",
          email: "john.smith@example.com",
          phone: "+1-555-0101",
          location: "New York, NY",
          tenantId: "tenant_1"
        }
      },
      {
        id: uuidv436(),
        type: "ORGANIZATION",
        props: {
          name: "Tech Corp Industries",
          industry: "Technology",
          headquarters: "San Francisco, CA",
          website: "https://techcorp.example.com",
          tenantId: "tenant_1"
        }
      },
      {
        id: uuidv436(),
        type: "EVENT",
        props: {
          name: "Data Breach Incident",
          date: "2024-08-01",
          severity: "HIGH",
          status: "INVESTIGATING",
          tenantId: "tenant_1"
        }
      },
      {
        id: uuidv436(),
        type: "LOCATION",
        props: {
          name: "Corporate Headquarters",
          address: "100 Market Street, San Francisco, CA 94105",
          latitude: 37.7749,
          longitude: -122.4194,
          tenantId: "tenant_1"
        }
      },
      {
        id: uuidv436(),
        type: "ASSET",
        props: {
          name: "Database Server DB-01",
          type: "SERVER",
          ip_address: "192.168.1.100",
          status: "ACTIVE",
          tenantId: "tenant_1"
        }
      }
    ];
    for (const entity of entities) {
      try {
        await session.run(
          `
          MERGE (e:Entity:${entity.type} {id: $id})
          SET e.type = $type,
              e += $props,
              e.createdAt = datetime(),
              e.updatedAt = datetime()
        `,
          {
            id: entity.id,
            type: entity.type,
            props: entity.props
          }
        );
      } catch (err) {
        logger2.error({ err, entity }, "Failed to create sample entity");
        throw err;
      }
    }
    logger2.info(`Created ${entities.length} sample entities`);
  } finally {
    await session.close();
  }
}
async function createSampleRelationships() {
  const driver3 = getNeo4jDriver();
  const session = driver3.session();
  try {
    const entitiesResult = await session.run(
      "MATCH (e:Entity) RETURN e LIMIT 5"
    );
    const entities = entitiesResult.records.map((r) => r.get("e"));
    if (entities.length >= 2) {
      const relationships = [
        {
          id: uuidv436(),
          from: String(entities[0].properties.id),
          to: String(entities[1].properties.id),
          type: "WORKS_FOR",
          props: {
            position: "Senior Developer",
            start_date: "2023-01-15",
            tenantId: "tenant_1"
          }
        },
        {
          id: uuidv436(),
          from: String(entities[1].properties.id),
          to: String(entities[2].properties.id),
          type: "INVOLVED_IN",
          props: {
            role: "Primary Suspect",
            confidence: 0.85,
            tenantId: "tenant_1"
          }
        }
      ];
      for (const rel of relationships) {
        try {
          await session.run(
            `
            MATCH (from:Entity {id: $from}), (to:Entity {id: $to})
            MERGE (from)-[r:${rel.type} {id: $id}]->(to)
            SET r += $props,
                r.createdAt = datetime()
          `,
            {
              id: rel.id,
              from: rel.from,
              to: rel.to,
              props: rel.props
            }
          );
        } catch (err) {
          logger2.error({ err, rel }, "Failed to create sample relationship");
          throw err;
        }
      }
      logger2.info(`Created ${relationships.length} sample relationships`);
    }
  } finally {
    await session.close();
  }
}
async function createSampleUsers() {
  const pool4 = getPostgresPool();
  try {
    const users = [
      {
        id: uuidv436(),
        email: "admin@intelgraph.com",
        username: "admin",
        role: "ADMIN"
      },
      {
        id: uuidv436(),
        email: "analyst@intelgraph.com",
        username: "analyst",
        role: "ANALYST"
      }
    ];
    for (const user of users) {
      await pool4.query(
        `
        INSERT INTO users (id, email, username, role, created_at, updated_at)
        VALUES ($1, $2, $3, $4, NOW(), NOW())
        ON CONFLICT (email) DO NOTHING
      `,
        [user.id, user.email, user.username, user.role]
      );
    }
    logger2.info(`Created ${users.length} sample users`);
  } catch (error) {
    logger2.debug("Users table may not exist yet, skipping user creation");
  }
}
async function createSampleInvestigations() {
  const pool4 = getPostgresPool();
  try {
    const investigations = [
      {
        id: uuidv436(),
        name: "Corporate Espionage Investigation",
        description: "Investigating potential data theft and corporate espionage activities",
        status: "ACTIVE"
      },
      {
        id: uuidv436(),
        name: "Cybersecurity Incident Response",
        description: "Response to recent data breach and security incident",
        status: "IN_PROGRESS"
      }
    ];
    for (const investigation of investigations) {
      await pool4.query(
        `
        INSERT INTO investigations (id, name, description, status, created_at, updated_at)
        VALUES ($1, $2, $3, $4, NOW(), NOW())
        ON CONFLICT (id) DO NOTHING
      `,
        [
          investigation.id,
          investigation.name,
          investigation.description,
          investigation.status
        ]
      );
    }
    logger2.info(`Created ${investigations.length} sample investigations`);
  } catch (error) {
    logger2.debug(
      "Investigations table may not exist yet, skipping investigation creation"
    );
  }
}
var init_sampleData = __esm({
  "src/utils/sampleData.ts"() {
    "use strict";
    init_neo4j();
    init_postgres();
    init_logger2();
  }
});

// src/index.ts
import http2 from "http";
import express58 from "express";
import { GraphQLError as GraphQLError16 } from "graphql";
import { useServer } from "graphql-ws/lib/use/ws";
import { WebSocketServer as WebSocketServer2 } from "ws";
import { randomUUID as randomUUID82 } from "node:crypto";

// src/gateways/VoiceGateway.ts
import { WebSocket } from "ws";

// src/services/voice/VoiceProvenanceLedger.ts
import { v4 as uuidv4 } from "uuid";
import { createHash } from "crypto";
var VoiceProvenanceLedger = class _VoiceProvenanceLedger {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_VoiceProvenanceLedger.instance) {
      _VoiceProvenanceLedger.instance = new _VoiceProvenanceLedger();
    }
    return _VoiceProvenanceLedger.instance;
  }
  async checkPolicy(tenantId, voiceId, text) {
    if (voiceId?.startsWith("banned_")) {
      return { allowed: false, reason: "Voice ID is on the ban list" };
    }
    if (text?.includes("simulate_harmful_content")) {
      return { allowed: false, reason: "Content flagged by safety policy" };
    }
    return { allowed: true };
  }
  generateManifest(modelId, prompt, referenceAudio, policyResults = []) {
    const timestamp = (/* @__PURE__ */ new Date()).toISOString();
    const manifestId = uuidv4();
    const modelHash = this.hash(modelId);
    const promptHash = this.hash(prompt);
    const referenceAudioHash = referenceAudio ? this.hash(referenceAudio) : void 0;
    const policyDecisions = policyResults.map((r) => ({
      allowed: r.allowed,
      policy_id: "mock-policy-v1",
      reason: r.reason
    }));
    const signaturePayload = `${manifestId}:${modelHash}:${promptHash}:${timestamp}`;
    const signature = `signed_${this.hash(signaturePayload)}`;
    return {
      manifest_id: manifestId,
      model_id: modelId,
      model_hash: modelHash,
      prompt_hash: promptHash,
      reference_audio_hash: referenceAudioHash,
      timestamp,
      policy_decisions: policyDecisions,
      watermark_detected: false,
      // Default
      signature
    };
  }
  hash(input) {
    return createHash("sha256").update(input).digest("hex");
  }
};

// src/services/voice/providers/Qwen3TTSProvider.ts
var Qwen3TTSProvider = class {
  id = "qwen3-tts";
  ledger = VoiceProvenanceLedger.getInstance();
  async generateVoiceDesign(spec, text) {
    const policyResult = await this.ledger.checkPolicy("system", void 0, text);
    if (!policyResult.allowed) {
      throw new Error(`Policy violation: ${policyResult.reason}`);
    }
    const mockAudio = Buffer.from(`[Qwen3 Design Audio: ${spec.description || "Custom Voice"} says "${text}"]`);
    const provenance = this.ledger.generateManifest(
      this.id,
      text,
      void 0,
      [policyResult]
    );
    return { audio: mockAudio, provenance };
  }
  async generateVoiceClone(referenceAudio, text) {
    const policyResult = await this.ledger.checkPolicy("system", void 0, text);
    if (!policyResult.allowed) {
      throw new Error(`Policy violation: ${policyResult.reason}`);
    }
    const mockAudio = Buffer.from(`[Qwen3 Cloned Audio says "${text}"]`);
    const provenance = this.ledger.generateManifest(
      this.id,
      text,
      referenceAudio,
      [policyResult]
    );
    return { audio: mockAudio, provenance };
  }
  async generateCustomVoice(voiceId, text) {
    const policyResult = await this.ledger.checkPolicy("system", voiceId, text);
    if (!policyResult.allowed) {
      throw new Error(`Policy violation: ${policyResult.reason}`);
    }
    const mockAudio = Buffer.from(`[Qwen3 CustomVoice ${voiceId} says "${text}"]`);
    const provenance = this.ledger.generateManifest(
      this.id,
      text,
      void 0,
      [policyResult]
    );
    return { audio: mockAudio, provenance };
  }
  async streamSpeak(job, callbacks) {
    try {
      const policyResult = await this.ledger.checkPolicy(job.tenant_id, job.voice_ref, job.text.join(" "));
      if (!policyResult.allowed) {
        throw new Error(`Policy violation: ${policyResult.reason}`);
      }
      const totalChunks = 5;
      const fullText = job.text.join(" ");
      for (let i = 0; i < totalChunks; i++) {
        await new Promise((resolve2) => setTimeout(resolve2, 100));
        const chunkText = fullText.substring(
          Math.floor(i / totalChunks * fullText.length),
          Math.floor((i + 1) / totalChunks * fullText.length)
        );
        const chunkAudio = Buffer.from(`[Stream Chunk ${i}: ${chunkText}]`);
        const provenance = this.ledger.generateManifest(
          this.id,
          chunkText,
          void 0,
          [policyResult]
        );
        callbacks.onAudio(chunkAudio, provenance);
      }
      callbacks.onComplete();
    } catch (error) {
      callbacks.onError(error);
    }
  }
};

// src/gateways/VoiceGateway.ts
init_logger();
var VoiceGateway = class {
  wss;
  provider;
  constructor(wss) {
    this.wss = wss;
    this.provider = new Qwen3TTSProvider();
    this.initialize();
  }
  initialize() {
    this.wss.on("connection", (ws, req) => {
      logger.info("Voice Gateway: New connection");
      ws.on("message", async (data) => {
        try {
          const message = JSON.parse(data.toString());
          if (message.type === "speak" && message.job) {
            const job = message.job;
            await this.handleSpeak(ws, job);
          } else {
            ws.send(JSON.stringify({ type: "error", message: "Unknown message type" }));
          }
        } catch (error) {
          logger.error({ error }, "Voice Gateway: Error processing message");
          ws.send(JSON.stringify({ type: "error", message: error.message }));
        }
      });
      ws.on("close", () => {
        logger.info("Voice Gateway: Connection closed");
      });
    });
  }
  async handleSpeak(ws, job) {
    await this.provider.streamSpeak(job, {
      onAudio: (chunk, provenance) => {
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({
            type: "audio_chunk",
            audio: chunk.toString("base64"),
            provenance
          }));
        }
      },
      onError: (error) => {
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: "error", message: error.message }));
        }
      },
      onComplete: () => {
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: "complete" }));
        }
      }
    });
  }
};

// src/index.ts
init_auth();
init_schema2();
init_resolvers2();
import path54 from "path";
import { fileURLToPath as fileURLToPath5 } from "url";
import { makeExecutableSchema as makeExecutableSchema2 } from "@graphql-tools/schema";

// src/graphql/subscriptionEngine.ts
import { randomUUID as randomUUID8 } from "node:crypto";
import pino31 from "pino";

// src/subscriptions/pubsub.ts
import { PubSub as PubSub3 } from "graphql-subscriptions";
import { RedisPubSub } from "graphql-redis-subscriptions";
import Redis7 from "ioredis";
var redisClient2 = null;
function makePubSub() {
  const url = process.env.REDIS_URL;
  if (!url) {
    console.warn("REDIS_URL not provided, using in-memory PubSub");
    return new PubSub3();
  }
  try {
    const opts = {
      maxRetriesPerRequest: 3,
      retryDelayOnFailover: 100,
      enableOfflineQueue: false,
      lazyConnect: true,
      maxmemoryPolicy: "noeviction"
    };
    const publisher = new Redis7(url, opts);
    const subscriber = new Redis7(url, opts);
    redisClient2 = publisher;
    publisher.on("error", (err) => {
      console.error("Redis Publisher Error:", err);
    });
    subscriber.on("error", (err) => {
      console.error("Redis Subscriber Error:", err);
    });
    console.log("Using Redis PubSub for subscriptions");
    return new RedisPubSub({ publisher, subscriber });
  } catch (error) {
    console.error(
      "Failed to connect to Redis, falling back to in-memory PubSub:",
      error
    );
    return new PubSub3();
  }
}

// src/metrics.ts
import * as client4 from "prom-client";
var registry2 = new client4.Registry();
client4.collectDefaultMetrics({
  register: registry2,
  prefix: "nodejs_",
  gcDurationBuckets: [1e-3, 0.01, 0.1, 1, 2, 5]
});
var httpRequestsTotal3 = new client4.Counter({
  name: "http_requests_total",
  help: "Total number of HTTP requests",
  labelNames: ["method", "route", "status_code"],
  registers: [registry2]
});
var httpRequestDuration3 = new client4.Histogram({
  name: "http_request_duration_seconds",
  help: "HTTP request duration in seconds",
  labelNames: ["method", "route", "status_code"],
  buckets: [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10],
  registers: [registry2]
});
var gqlDuration = new client4.Histogram({
  name: "graphql_request_duration_seconds",
  help: "GraphQL request duration in seconds",
  labelNames: ["operation", "operation_type"],
  buckets: [0.05, 0.1, 0.2, 0.35, 0.7, 1, 1.5, 3, 5],
  registers: [registry2]
});
var graphqlRequestsTotal3 = new client4.Counter({
  name: "graphql_requests_total",
  help: "Total number of GraphQL requests",
  labelNames: ["operation", "operation_type"],
  registers: [registry2]
});
var graphqlErrorsTotal = new client4.Counter({
  name: "graphql_errors_total",
  help: "Total number of GraphQL errors",
  labelNames: ["operation", "error_type"],
  registers: [registry2]
});
var graphqlResolverDuration = new client4.Histogram({
  name: "graphql_resolver_duration_seconds",
  help: "GraphQL resolver execution duration",
  labelNames: ["resolver", "parent_type"],
  buckets: [1e-3, 5e-3, 0.01, 0.05, 0.1, 0.5, 1],
  registers: [registry2]
});
var neo4jQueryDuration = new client4.Histogram({
  name: "neo4j_query_duration_seconds",
  help: "Neo4j query duration in seconds",
  labelNames: ["query_type"],
  buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10],
  registers: [registry2]
});
var neo4jQueriesTotal = new client4.Counter({
  name: "neo4j_queries_total",
  help: "Total number of Neo4j queries",
  labelNames: ["query_type", "status"],
  registers: [registry2]
});
var postgresQueryDuration = new client4.Histogram({
  name: "postgres_query_duration_seconds",
  help: "PostgreSQL query duration in seconds",
  labelNames: ["query_type"],
  buckets: [1e-3, 5e-3, 0.01, 0.05, 0.1, 0.5, 1],
  registers: [registry2]
});
var postgresQueriesTotal = new client4.Counter({
  name: "postgres_queries_total",
  help: "Total number of PostgreSQL queries",
  labelNames: ["query_type", "status"],
  registers: [registry2]
});
var dbConnectionsActive4 = new client4.Gauge({
  name: "db_connections_active",
  help: "Number of active database connections",
  labelNames: ["database"],
  registers: [registry2]
});
var cacheHitsTotal = new client4.Counter({
  name: "cache_hits_total",
  help: "Total number of cache hits",
  labelNames: ["cache_name"],
  registers: [registry2]
});
var cacheMissesTotal = new client4.Counter({
  name: "cache_misses_total",
  help: "Total number of cache misses",
  labelNames: ["cache_name"],
  registers: [registry2]
});
var cacheSize = new client4.Gauge({
  name: "cache_size_bytes",
  help: "Current cache size in bytes",
  labelNames: ["cache_name"],
  registers: [registry2]
});
var websocketConnectionsActive = new client4.Gauge({
  name: "websocket_connections_active",
  help: "Number of active WebSocket connections",
  registers: [registry2]
});
var websocketMessagesTotal = new client4.Counter({
  name: "websocket_messages_total",
  help: "Total number of WebSocket messages",
  labelNames: ["direction", "type"],
  registers: [registry2]
});
var subscriptionFanoutLatency = new client4.Histogram({
  name: "subscription_fanout_latency_ms",
  help: "Subscription fan-out latency in milliseconds",
  buckets: [10, 50, 100, 250, 500, 1e3],
  registers: [registry2]
});
var subscriptionBackpressureTotal = new client4.Counter({
  name: "subscription_backpressure_total",
  help: "Count of WebSocket disconnects or drops caused by backpressure",
  registers: [registry2]
});
var subscriptionBatchesEmitted = new client4.Histogram({
  name: "subscription_batch_size",
  help: "Distribution of batched subscription payload sizes",
  buckets: [1, 5, 10, 25, 50, 75, 100],
  registers: [registry2]
});
var ingestDedupeRate = new client4.Gauge({
  name: "ingest_dedupe_rate",
  help: "Rate of deduplicated ingest events",
  registers: [registry2]
});
var ingestBacklog = new client4.Gauge({
  name: "ingest_backlog",
  help: "Current ingest backlog size",
  registers: [registry2]
});
var aiJobsQueued3 = new client4.Gauge({
  name: "ai_jobs_queued",
  help: "Number of AI jobs in queue",
  labelNames: ["job_type"],
  registers: [registry2]
});
var aiJobDuration3 = new client4.Histogram({
  name: "ai_job_duration_seconds",
  help: "AI job execution duration",
  labelNames: ["job_type"],
  buckets: [1, 5, 10, 30, 60, 120, 300, 600],
  registers: [registry2]
});
var llmRequestDuration2 = new client4.Histogram({
  name: "llm_request_duration_seconds",
  help: "LLM API request duration",
  labelNames: ["model", "provider"],
  buckets: [0.5, 1, 2, 5, 10, 20, 30, 60],
  registers: [registry2]
});
var investigationsTotal = new client4.Counter({
  name: "investigations_total",
  help: "Total number of investigations created",
  labelNames: ["status"],
  registers: [registry2]
});
var entitiesTotal = new client4.Counter({
  name: "entities_total",
  help: "Total number of entities created",
  labelNames: ["entity_type"],
  registers: [registry2]
});
var relationshipsTotal = new client4.Counter({
  name: "relationships_total",
  help: "Total number of relationships created",
  labelNames: ["relationship_type"],
  registers: [registry2]
});

// src/graphql/subscriptionEngine.ts
var SubscriptionEngine = class {
  pubsub;
  logger = pino31({ name: "SubscriptionEngine" });
  connections = /* @__PURE__ */ new Map();
  backpressureBytes;
  batchFlushIntervalMs;
  constructor(options2 = {}) {
    this.pubsub = options2.pubsub ?? makePubSub();
    this.backpressureBytes = options2.backpressureBytes ?? 512 * 1024;
    this.batchFlushIntervalMs = options2.batchFlushIntervalMs ?? 250;
  }
  getPubSub() {
    return this.pubsub;
  }
  registerConnection(id, socket) {
    this.connections.set(id, { socket, subscriptions: /* @__PURE__ */ new Set() });
    websocketConnectionsActive.inc();
    this.logger.info({ id }, "Registered GraphQL WebSocket connection");
  }
  unregisterConnection(id) {
    const connection5 = this.connections.get(id);
    if (connection5) {
      this.connections.delete(id);
      websocketConnectionsActive.dec();
      this.logger.info({ id }, "Closed GraphQL WebSocket connection");
    }
  }
  trackSubscription(connectionId, subscriptionId) {
    const connection5 = this.connections.get(connectionId);
    if (connection5) {
      connection5.subscriptions.add(subscriptionId);
    }
  }
  completeSubscription(connectionId, subscriptionId) {
    const connection5 = this.connections.get(connectionId);
    if (connection5 && subscriptionId) {
      connection5.subscriptions.delete(subscriptionId);
    }
  }
  enforceBackpressure(socket) {
    if (socket.bufferedAmount > this.backpressureBytes) {
      subscriptionBackpressureTotal.inc();
      socket.close(1013, "Backpressure threshold exceeded");
      return false;
    }
    return true;
  }
  async publish(trigger, payload, metadata) {
    const envelope = {
      id: randomUUID8(),
      payload,
      timestamp: Date.now(),
      metadata
    };
    await this.pubsub.publish(trigger, envelope);
  }
  createFilteredAsyncIterator(triggers, predicate) {
    const iterator = this.pubsub.asyncIterator(triggers);
    const filter = (async function* () {
      for await (const event of iterator) {
        if (await predicate(event)) {
          yield event;
        }
      }
    })();
    return filter;
  }
  createBatchedAsyncIterator(triggers, predicate, options2) {
    const source = this.createFilteredAsyncIterator(triggers, predicate);
    const batchSize = options2?.batchSize ?? 25;
    const flushInterval = options2?.flushIntervalMs ?? this.batchFlushIntervalMs;
    const iterator = (async function* () {
      const queue = [];
      let active = true;
      (async () => {
        for await (const event of source) {
          queue.push(event);
        }
        active = false;
      })();
      while (active || queue.length) {
        if (!queue.length) {
          await new Promise((resolve2) => setTimeout(resolve2, flushInterval));
          continue;
        }
        const batch = queue.splice(0, batchSize);
        subscriptionBatchesEmitted.observe(batch.length);
        yield batch;
        if (queue.length === 0) {
          await new Promise((resolve2) => setTimeout(resolve2, flushInterval));
        }
      }
    })();
    return iterator;
  }
  recordFanout(start) {
    const durationMs = Number(process.hrtime.bigint() - start) / 1e6;
    subscriptionFanoutLatency.observe(durationMs);
  }
};
var subscriptionEngine = new SubscriptionEngine();

// src/services/DataRetentionService.ts
init_audit();
import pino32 from "pino";
var logger29 = pino32();
var DataRetentionService = class {
  neo4j;
  policies;
  cleanupInterval = null;
  constructor(neo4jDriver2) {
    this.neo4j = neo4jDriver2;
    this.policies = [
      {
        label: "Entity",
        ttlDays: Number(process.env.ENTITY_TTL_DAYS || 365),
        auditAction: "ENTITY_DELETED_TTL"
      },
      {
        label: "Relationship",
        ttlDays: Number(process.env.RELATIONSHIP_TTL_DAYS || 365),
        auditAction: "RELATIONSHIP_DELETED_TTL"
      }
      // Add more policies as needed for other labels
    ];
  }
  startCleanupJob(intervalMs = 24 * 60 * 60 * 1e3) {
    if (this.cleanupInterval) {
      logger29.warn(
        "Data retention cleanup job already running. Stopping existing job."
      );
      this.stopCleanupJob();
    }
    logger29.info(
      `Starting data retention cleanup job every ${intervalMs / (1e3 * 60 * 60)} hours.`
    );
    this.cleanupInterval = setInterval(() => this.runCleanup(), intervalMs);
  }
  stopCleanupJob() {
    if (this.cleanupInterval) {
      clearInterval(this.cleanupInterval);
      this.cleanupInterval = null;
      logger29.info("Data retention cleanup job stopped.");
    }
  }
  async runCleanup() {
    logger29.info("Running data retention cleanup...");
    const session = this.neo4j.session();
    try {
      for (const policy2 of this.policies) {
        const cutoffDate = new Date(
          Date.now() - policy2.ttlDays * 24 * 60 * 60 * 1e3
        );
        const cutoffTimestamp = cutoffDate.toISOString();
        const query3 = `
          MATCH (n:${policy2.label})
          WHERE n.createdAt IS NOT NULL AND n.createdAt < datetime($cutoffTimestamp)
          WITH n LIMIT 1000 // Process in batches
          DETACH DELETE n
          RETURN count(n) as deletedCount
        `;
        const result2 = await session.run(query3, { cutoffTimestamp });
        const deletedCount = result2.records[0].get("deletedCount");
        if (deletedCount > 0) {
          logger29.info(
            `Deleted ${deletedCount} ${policy2.label} nodes/relationships older than ${policy2.ttlDays} days.`
          );
          await writeAudit({
            userId: "system",
            // System user for automated actions
            action: policy2.auditAction,
            resourceType: policy2.label,
            resourceId: "N/A",
            // Cannot log individual IDs for batch delete
            details: {
              count: deletedCount,
              ttlDays: policy2.ttlDays,
              cutoffDate: cutoffTimestamp
            },
            ip: "N/A",
            userAgent: "DataRetentionService",
            actorRole: "system",
            sessionId: "N/A"
          });
        }
      }
    } catch (error) {
      logger29.error({ error }, "Error during data retention cleanup.");
    } finally {
      await session.close();
    }
  }
};

// src/index.ts
init_neo4j();
init_config();
init_tracer();

// src/routes/streaming.ts
import express from "express";

// src/middleware/authority.ts
init_logger2();
var AuthorityGuard = class _AuthorityGuard {
  static instance;
  static getInstance() {
    if (!_AuthorityGuard.instance) {
      _AuthorityGuard.instance = new _AuthorityGuard();
    }
    return _AuthorityGuard.instance;
  }
  // Foster dissent: Runtime-blocking license enforcement
  validateLicense(user, operation) {
    if (user.license_status !== "ACTIVE") {
      logger_default2.error({
        message: "License validation failed - Foster dissent protection",
        user_id: user.id,
        license_status: user.license_status,
        operation
      });
      return false;
    }
    if (!user.tos_accepted) {
      logger_default2.error({
        message: "Terms of Service not accepted - Foster dissent protection",
        user_id: user.id,
        operation
      });
      return false;
    }
    return true;
  }
  // Starkey dissent: Authority binding validation
  validateAuthorityBinding(user, operation, scope) {
    const requiredAuthorities = this.getRequiredAuthorities(operation);
    if (requiredAuthorities.length === 0) {
      return { valid: true };
    }
    for (const requiredAuth of requiredAuthorities) {
      const binding = user.authority_bindings.find(
        (auth) => auth.type === requiredAuth && this.isAuthorityValid(auth, scope)
      );
      if (!binding) {
        return {
          valid: false,
          reason: `Missing required authority: ${requiredAuth}`
        };
      }
    }
    return { valid: true };
  }
  getRequiredAuthorities(operation) {
    const authMap = {
      classified_query: ["WARRANT", "COURT_ORDER"],
      export_data: ["SUBPOENA", "COURT_ORDER", "ADMIN_AUTH"],
      graph_xai_analysis: ["ADMIN_AUTH"],
      temporal_analysis: ["LICENSE"],
      cross_tenant_access: ["WARRANT", "COURT_ORDER"]
    };
    return authMap[operation] || ["LICENSE"];
  }
  isAuthorityValid(authority, scope) {
    const expiryDate = new Date(authority.expiry_date);
    const now = /* @__PURE__ */ new Date();
    if (expiryDate <= now) {
      logger_default2.warn({
        message: "Expired authority binding detected",
        authority_type: authority.type,
        expiry_date: authority.expiry_date
      });
      return false;
    }
    const hasScope = scope.every(
      (requestedScope) => authority.scope.includes(requestedScope) || authority.scope.includes("*")
    );
    if (!hasScope) {
      logger_default2.warn({
        message: "Authority scope insufficient",
        authority_scope: authority.scope,
        requested_scope: scope
      });
      return false;
    }
    return true;
  }
  // Committee requirement: Comprehensive policy evaluation
  evaluatePolicy(user, operation, operationScope, exportManifest) {
    const decision = {
      allow: false,
      reasons: [],
      audit_trail: []
    };
    const licenseValid = this.validateLicense(user, operation);
    if (!licenseValid) {
      decision.reasons.push(
        "License validation failed - Foster dissent protection active"
      );
    }
    const authorityCheck = this.validateAuthorityBinding(
      user,
      operation,
      operationScope
    );
    if (!authorityCheck.valid) {
      decision.reasons.push(
        authorityCheck.reason || "Authority validation failed"
      );
      decision.required_authority = this.getRequiredAuthorities(operation);
    }
    if (operation === "export_data") {
      if (!exportManifest || !exportManifest.hash) {
        decision.reasons.push(
          "Missing export manifest - Starkey dissent protection active"
        );
      } else if (!exportManifest.immutable_disclosure_bundle) {
        decision.reasons.push(
          "Missing immutable disclosure bundle - Starkey dissent protection"
        );
      }
    }
    const requiredClearance = this.getRequiredClearance(operation);
    if (user.clearance_level < requiredClearance) {
      decision.reasons.push(
        `Insufficient clearance level: ${user.clearance_level} < ${requiredClearance}`
      );
    }
    decision.allow = decision.reasons.length === 0;
    decision.audit_trail.push({
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      user_id: user.id,
      operation,
      decision: decision.allow ? "ALLOW" : "DENY",
      reasons: decision.reasons
    });
    return decision;
  }
  getRequiredClearance(operation) {
    const clearanceMap = {
      classified_query: 5,
      export_data: 4,
      graph_xai_analysis: 3,
      temporal_analysis: 2,
      basic_query: 1
    };
    return clearanceMap[operation] || 1;
  }
};
var requireAuthority = (operation, scope = []) => {
  return (req, res, next) => {
    const guard = AuthorityGuard.getInstance();
    const user = req.user;
    if (!user) {
      return res.status(401).json({
        error: "Authentication required",
        code: "AUTH_REQUIRED"
      });
    }
    const decision = guard.evaluatePolicy(
      user,
      operation,
      scope,
      req.body.export_manifest
    );
    if (!decision.allow) {
      logger_default2.warn({
        message: "Authority check failed - Committee dissent protection active",
        user_id: user.id,
        operation,
        reasons: decision.reasons,
        required_authority: decision.required_authority
      });
      return res.status(403).json({
        error: "Insufficient authority",
        reasons: decision.reasons,
        required_authority: decision.required_authority,
        code: "AUTHORITY_DENIED"
      });
    }
    req.authority_decision = decision;
    logger_default2.info({
      message: "Authority check passed",
      user_id: user.id,
      operation,
      clearance_level: user.clearance_level
    });
    next();
  };
};
var requireReasonForAccess = (req, res, next) => {
  const reasonForAccess = req.headers["x-reason-for-access"];
  if (!reasonForAccess || reasonForAccess.length < 10) {
    return res.status(400).json({
      error: "Reason for access required",
      message: "Committee requirement: All access must include detailed justification",
      code: "REASON_REQUIRED"
    });
  }
  const qualityScore = Math.min(reasonForAccess.length / 100, 1);
  req.reason_for_access = {
    reason: reasonForAccess,
    quality_score: qualityScore,
    timestamp: (/* @__PURE__ */ new Date()).toISOString()
  };
  logger_default2.info({
    message: "Reason for access recorded",
    reason: reasonForAccess.substring(0, 100),
    quality_score: qualityScore,
    user_id: req.user?.id
  });
  next();
};

// src/services/streaming/ingest-worker.ts
import { EventEmitter as EventEmitter6 } from "events";
import crypto17 from "crypto";

// src/db/timescale.ts
init_logger2();
import { Pool as Pool6 } from "pg";
var config7 = {
  host: process.env.POSTGRES_HOST || "localhost",
  port: parseInt(process.env.POSTGRES_PORT || "5432", 10),
  database: process.env.POSTGRES_DB || "intelgraph",
  user: process.env.POSTGRES_USER || "intelgraph",
  password: process.env.POSTGRES_PASSWORD || "password",
  max: 20,
  idleTimeoutMillis: 3e4,
  connectionTimeoutMillis: 1e4
};
var timescalePool = new Pool6(config7);
timescalePool.on("error", (err) => {
  logger_default2.error({
    message: "TimescaleDB pool error",
    error: err instanceof Error ? err.message : String(err),
    config: {
      host: config7.host,
      port: config7.port,
      database: config7.database
    }
  });
});
timescalePool.on("connect", (client6) => {
  logger_default2.info({
    message: "TimescaleDB client connected",
    totalCount: timescalePool.totalCount,
    idleCount: timescalePool.idleCount
  });
});
async function query(text, params) {
  const start = Date.now();
  const client6 = await timescalePool.connect();
  try {
    const result2 = await client6.query(text, params);
    const duration = Date.now() - start;
    if (duration > 1e3) {
      logger_default2.warn({
        message: "Slow TimescaleDB query detected",
        duration,
        query: text.substring(0, 100) + "...",
        paramCount: params?.length || 0,
        rowCount: result2.rowCount
      });
    }
    if (duration > 5e3) {
      logger_default2.error({
        message: "Critical TimescaleDB performance issue",
        duration,
        query: text.substring(0, 200),
        severity: "HIGH"
      });
    }
    return result2;
  } finally {
    client6.release();
  }
}
async function insertEvent(eventData) {
  const {
    event_type,
    event_source,
    entity_id,
    entity_type,
    observed_at = /* @__PURE__ */ new Date(),
    metadata = {},
    confidence = 1,
    severity = "INFO",
    tags = []
  } = eventData;
  const insertQuery = `
    INSERT INTO events (
      event_type, event_source, entity_id, entity_type,
      observed_at, metadata, confidence, severity, tags
    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
    RETURNING id, observed_at
  `;
  return query(insertQuery, [
    event_type,
    event_source,
    entity_id,
    entity_type,
    observed_at,
    JSON.stringify(metadata),
    confidence,
    severity,
    tags
  ]);
}
async function insertAnalyticsTrace(traceData) {
  const {
    trace_id,
    operation_type,
    execution_time = /* @__PURE__ */ new Date(),
    duration_ms,
    input_hash,
    output_hash,
    model_version = "ga-core-1.0",
    performance_metrics = {}
  } = traceData;
  const insertQuery = `
    INSERT INTO analytics_traces (
      trace_id, operation_type, execution_time, duration_ms,
      input_hash, output_hash, model_version, performance_metrics
    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
    RETURNING id
  `;
  return query(insertQuery, [
    trace_id,
    operation_type,
    execution_time,
    duration_ms,
    input_hash,
    output_hash,
    model_version,
    JSON.stringify(performance_metrics)
  ]);
}

// src/services/provenance-ledger.ts
import crypto16 from "crypto";
init_logger2();
var ProvenanceLedgerService = class _ProvenanceLedgerService {
  static instance;
  static getInstance() {
    if (!_ProvenanceLedgerService.instance) {
      _ProvenanceLedgerService.instance = new _ProvenanceLedgerService();
    }
    return _ProvenanceLedgerService.instance;
  }
  // Committee requirement: Content hashing for integrity
  generateContentHash(content) {
    const normalizedContent = JSON.stringify(
      content,
      Object.keys(content).sort()
    );
    return crypto16.createHash("sha256").update(normalizedContent, "utf8").digest("hex");
  }
  // Committee requirement: Cryptographic signatures for immutability
  generateSignature(data, privateKey) {
    const content = JSON.stringify(data, Object.keys(data).sort());
    const hmac = crypto16.createHmac(
      "sha256",
      privateKey || process.env.LEDGER_SIGNING_KEY || "default-key"
    );
    return hmac.update(content).digest("hex");
  }
  // Starkey dissent requirement: Immutable provenance chain recording
  async recordProvenanceEntry(entry) {
    const id = crypto16.randomUUID();
    const timestamp = /* @__PURE__ */ new Date();
    const content_hash = this.generateContentHash({ ...entry, timestamp });
    const signature = this.generateSignature({
      id,
      content_hash,
      ...entry,
      timestamp
    });
    const provenanceEntry = {
      id,
      content_hash,
      timestamp,
      signature,
      ...entry
    };
    try {
      await query(
        `
        INSERT INTO provenance_chain (
          id, parent_hash, content_hash, operation_type, actor_id, 
          timestamp, metadata, signature
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
      `,
        [
          id,
          entry.parent_hash,
          content_hash,
          entry.operation_type,
          entry.actor_id,
          timestamp,
          JSON.stringify(entry.metadata),
          signature
        ]
      );
      logger_default2.info({
        message: "Provenance entry recorded",
        provenance_id: id,
        operation_type: entry.operation_type,
        actor_id: entry.actor_id,
        content_hash
      });
      return id;
    } catch (error) {
      logger_default2.error({
        message: "Failed to record provenance entry",
        error: error instanceof Error ? error.message : String(error),
        operation_type: entry.operation_type
      });
      throw new Error("Provenance recording failed");
    }
  }
  // Committee requirement: Claim registration with hash verification
  async registerClaim(claimData) {
    const id = crypto16.randomUUID();
    const content_hash = this.generateContentHash(claimData.content);
    const created_at = /* @__PURE__ */ new Date();
    const claim = {
      id,
      content_hash,
      created_at,
      ...claimData
    };
    try {
      await query(
        `
        INSERT INTO claims_registry (
          id, content_hash, content, confidence, evidence_hashes,
          created_at, created_by, investigation_id
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
      `,
        [
          id,
          content_hash,
          claimData.content,
          claimData.confidence,
          JSON.stringify(claimData.evidence_hashes),
          created_at,
          claimData.created_by,
          claimData.investigation_id
        ]
      );
      await this.recordProvenanceEntry({
        operation_type: "CLAIM_REGISTERED",
        actor_id: claimData.created_by,
        metadata: {
          claim_id: id,
          claim_hash: content_hash,
          confidence: claimData.confidence,
          evidence_count: claimData.evidence_hashes.length
        }
      });
      logger_default2.info({
        message: "Claim registered in provenance ledger",
        claim_id: id,
        content_hash,
        created_by: claimData.created_by
      });
      return claim;
    } catch (error) {
      logger_default2.error({
        message: "Failed to register claim",
        error: error instanceof Error ? error.message : String(error)
      });
      throw new Error("Claim registration failed");
    }
  }
  // Starkey dissent requirement: Export manifest creation
  async createExportManifest(manifestData) {
    const manifest_id = crypto16.randomUUID();
    const manifest_hash = this.generateContentHash(manifestData);
    const manifest = {
      manifest_id,
      manifest_hash,
      ...manifestData
    };
    try {
      await query(
        `
        INSERT INTO export_manifests (
          manifest_id, manifest_hash, export_type, data_sources,
          transformation_chain, authority_basis, classification_level,
          retention_policy, chain_of_custody
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
      `,
        [
          manifest_id,
          manifest_hash,
          manifestData.export_type,
          JSON.stringify(manifestData.data_sources),
          JSON.stringify(manifestData.transformation_chain),
          JSON.stringify(manifestData.authority_basis),
          manifestData.classification_level,
          manifestData.retention_policy,
          JSON.stringify(manifestData.chain_of_custody)
        ]
      );
      await this.recordProvenanceEntry({
        operation_type: "EXPORT_MANIFEST_CREATED",
        actor_id: manifestData.chain_of_custody[0]?.actor_id || "system",
        metadata: {
          manifest_id,
          export_type: manifestData.export_type,
          data_source_count: manifestData.data_sources.length
        }
      });
      return manifest;
    } catch (error) {
      logger_default2.error({
        message: "Failed to create export manifest",
        error: error instanceof Error ? error.message : String(error)
      });
      throw new Error("Export manifest creation failed");
    }
  }
  // Committee requirement: Immutable disclosure bundle creation
  async createDisclosureBundle(bundleData) {
    const bundle_id = crypto16.randomUUID();
    const created_at = /* @__PURE__ */ new Date();
    const export_manifest = await this.createExportManifest({
      export_type: bundleData.export_type,
      data_sources: bundleData.claims.map((c) => c.id),
      transformation_chain: ["claim_aggregation", "evidence_correlation"],
      authority_basis: bundleData.authority_basis,
      classification_level: bundleData.classification_level,
      retention_policy: "REGULATORY_STANDARD",
      chain_of_custody: [
        {
          actor_id: bundleData.actor_id,
          action: "BUNDLE_CREATED",
          timestamp: created_at,
          signature: this.generateSignature({ bundle_id, created_at }),
          justification: "Disclosure bundle creation with immutable seal"
        }
      ]
    });
    const provenance_chain = await this.getProvenanceChain(
      bundleData.claims.map((c) => c.id)
    );
    const bundle_content = {
      bundle_id,
      claims: bundleData.claims,
      evidence_references: bundleData.evidence_references,
      export_manifest,
      created_at
    };
    const bundle_hash = this.generateContentHash(bundle_content);
    const immutable_seal = this.generateSignature({
      bundle_hash,
      manifest_hash: export_manifest.manifest_hash,
      claim_hashes: bundleData.claims.map((c) => c.content_hash),
      timestamp: created_at
    });
    const disclosure_bundle = {
      bundle_id,
      bundle_hash,
      claims: bundleData.claims,
      evidence_references: bundleData.evidence_references,
      provenance_chain,
      export_manifest,
      created_at,
      immutable_seal
    };
    try {
      await query(
        `
        INSERT INTO disclosure_bundles (
          bundle_id, bundle_hash, claims, evidence_references,
          provenance_chain, export_manifest, created_at, immutable_seal
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
      `,
        [
          bundle_id,
          bundle_hash,
          JSON.stringify(bundleData.claims),
          JSON.stringify(bundleData.evidence_references),
          JSON.stringify(provenance_chain),
          JSON.stringify(export_manifest),
          created_at,
          immutable_seal
        ]
      );
      await this.recordProvenanceEntry({
        operation_type: "DISCLOSURE_BUNDLE_SEALED",
        actor_id: bundleData.actor_id,
        metadata: {
          bundle_id,
          bundle_hash,
          claim_count: bundleData.claims.length,
          immutable_seal
        }
      });
      logger_default2.info({
        message: "Immutable disclosure bundle created - Starkey dissent compliance",
        bundle_id,
        bundle_hash,
        immutable_seal
      });
      return disclosure_bundle;
    } catch (error) {
      logger_default2.error({
        message: "Failed to create disclosure bundle",
        error: error instanceof Error ? error.message : String(error)
      });
      throw new Error("Disclosure bundle creation failed");
    }
  }
  // Committee requirement: Provenance chain verification
  async verifyProvenanceChain(entityIds) {
    const errors = [];
    try {
      for (const entityId of entityIds) {
        const result2 = await query(
          `
          SELECT * FROM provenance_chain 
          WHERE metadata::jsonb ->> 'claim_id' = $1
          ORDER BY timestamp ASC
        `,
          [entityId]
        );
        if (result2.rows.length === 0) {
          errors.push(`No provenance chain found for entity ${entityId}`);
          continue;
        }
        for (const row of result2.rows) {
          const expected_hash = this.generateContentHash({
            operation_type: row.operation_type,
            actor_id: row.actor_id,
            metadata: row.metadata,
            timestamp: row.timestamp
          });
          if (row.content_hash !== expected_hash) {
            errors.push(`Hash mismatch in provenance chain for ${entityId}`);
          }
        }
      }
      return {
        valid: errors.length === 0,
        errors
      };
    } catch (error) {
      logger_default2.error({
        message: "Provenance chain verification failed",
        error: error instanceof Error ? error.message : String(error)
      });
      return {
        valid: false,
        errors: ["Verification process failed"]
      };
    }
  }
  // Helper method to get provenance chain
  async getProvenanceChain(entityIds) {
    const placeholders = entityIds.map((_2, i) => `$${i + 1}`).join(", ");
    const result2 = await query(
      `
      SELECT * FROM provenance_chain
      WHERE metadata::jsonb ->> 'claim_id' IN (${placeholders})
      ORDER BY timestamp ASC
    `,
      entityIds
    );
    return result2.rows.map((row) => ({
      id: row.id,
      parent_hash: row.parent_hash,
      content_hash: row.content_hash,
      operation_type: row.operation_type,
      actor_id: row.actor_id,
      timestamp: row.timestamp,
      metadata: row.metadata,
      signature: row.signature
    }));
  }
  // Committee requirement: Audit trail queries
  async getAuditTrail(filters) {
    let whereClause = "1=1";
    const params = [];
    if (filters.actor_id) {
      params.push(filters.actor_id);
      whereClause += ` AND actor_id = $${params.length}`;
    }
    if (filters.operation_type) {
      params.push(filters.operation_type);
      whereClause += ` AND operation_type = $${params.length}`;
    }
    if (filters.time_range) {
      params.push(filters.time_range.start, filters.time_range.end);
      whereClause += ` AND timestamp BETWEEN $${params.length - 1} AND $${params.length}`;
    }
    const result2 = await query(
      `
      SELECT * FROM provenance_chain 
      WHERE ${whereClause}
      ORDER BY timestamp DESC
      LIMIT 1000
    `,
      params
    );
    return result2.rows.map((row) => ({
      id: row.id,
      parent_hash: row.parent_hash,
      content_hash: row.content_hash,
      operation_type: row.operation_type,
      actor_id: row.actor_id,
      timestamp: row.timestamp,
      metadata: row.metadata,
      signature: row.signature
    }));
  }
};
var provenance_ledger_default = ProvenanceLedgerService;

// src/services/streaming/ingest-worker.ts
init_logger2();
var StreamingIngestWorker = class _StreamingIngestWorker extends EventEmitter6 {
  static instance;
  messageQueue = [];
  processing = false;
  batchSize = 100;
  batchTimeout = 5e3;
  // 5 seconds
  metrics;
  provenanceService;
  piiConfig;
  static getInstance() {
    if (!_StreamingIngestWorker.instance) {
      _StreamingIngestWorker.instance = new _StreamingIngestWorker();
    }
    return _StreamingIngestWorker.instance;
  }
  constructor() {
    super();
    this.provenanceService = provenance_ledger_default.getInstance();
    this.initializeMetrics();
    this.initializePIIRedaction();
    this.startBatchProcessor();
  }
  initializeMetrics() {
    this.metrics = {
      messages_processed: 0,
      messages_per_second: 0,
      average_processing_time_ms: 0,
      pii_redactions_applied: 0,
      errors_encountered: 0,
      queue_size: 0,
      worker_status: "healthy"
    };
    setInterval(() => {
      this.updateMetrics();
    }, 1e4);
  }
  // Committee requirement: PII redaction configuration
  initializePIIRedaction() {
    this.piiConfig = {
      enabled: process.env.PII_REDACTION_ENABLED !== "false",
      replacement_token: "[REDACTED]",
      log_redactions: process.env.LOG_PII_REDACTIONS === "true",
      redaction_patterns: {
        ssn: /\b\d{3}-\d{2}-\d{4}\b/g,
        email: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g,
        phone: /\b\d{3}-\d{3}-\d{4}\b|\(\d{3}\)\s*\d{3}-\d{4}/g,
        credit_card: /\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b/g,
        ip_address: /\b(?:\d{1,3}\.){3}\d{1,3}\b/g,
        passport: /\b[A-Z]{1,2}\d{6,9}\b/g,
        license_plate: /\b[A-Z]{2,3}[-\s]?\d{3,4}[-\s]?[A-Z]?\b/g,
        bank_account: /\b\d{8,17}\b/g,
        coordinates: /\b-?\d{1,3}\.\d{4,}\s*,\s*-?\d{1,3}\.\d{4,}\b/g,
        api_key: /\b[A-Za-z0-9]{32,}\b/g
      }
    };
  }
  // Main ingest endpoint
  async ingestMessage(message) {
    const messageId = crypto17.randomUUID();
    const fullMessage = {
      message_id: messageId,
      ...message
    };
    this.messageQueue.push(fullMessage);
    this.metrics.queue_size = this.messageQueue.length;
    if (this.messageQueue.length > 1e3) {
      this.emit("queue_alert", {
        queue_size: this.messageQueue.length,
        severity: "HIGH"
      });
    }
    logger_default2.debug({
      message: "Message added to ingest queue",
      message_id: messageId,
      source: message.source,
      queue_size: this.messageQueue.length
    });
    return messageId;
  }
  // Batch processor for efficient handling
  startBatchProcessor() {
    setInterval(async () => {
      if (!this.processing && this.messageQueue.length > 0) {
        await this.processBatch();
      }
    }, this.batchTimeout);
  }
  async processBatch() {
    if (this.processing || this.messageQueue.length === 0) {
      return;
    }
    this.processing = true;
    const batchStartTime = Date.now();
    try {
      const batch = this.messageQueue.splice(
        0,
        Math.min(this.batchSize, this.messageQueue.length)
      );
      this.metrics.queue_size = this.messageQueue.length;
      logger_default2.info({
        message: "Processing ingest batch",
        batch_size: batch.length,
        remaining_queue: this.messageQueue.length
      });
      const processedMessages = await Promise.allSettled(
        batch.map((message) => this.processMessage(message))
      );
      let successCount = 0;
      let errorCount = 0;
      for (let i = 0; i < processedMessages.length; i++) {
        const result2 = processedMessages[i];
        if (result2.status === "fulfilled") {
          successCount++;
          await this.handleProcessedMessage(result2.value);
        } else {
          errorCount++;
          this.metrics.errors_encountered++;
          logger_default2.error({
            message: "Message processing failed in batch",
            message_id: batch[i].message_id,
            error: result2.reason,
            batch_index: i
          });
          this.emit("processing_error", {
            message_id: batch[i].message_id,
            error: result2.reason
          });
        }
      }
      const batchProcessingTime = Date.now() - batchStartTime;
      this.metrics.messages_processed += successCount;
      logger_default2.info({
        message: "Batch processing completed",
        batch_size: batch.length,
        successful: successCount,
        errors: errorCount,
        processing_time_ms: batchProcessingTime,
        messages_per_second: Math.round(
          batch.length / batchProcessingTime * 1e3
        )
      });
      this.updateWorkerStatus(errorCount, batch.length);
    } catch (error) {
      logger_default2.error({
        message: "Batch processing failed",
        error: error instanceof Error ? error.message : String(error)
      });
      this.metrics.worker_status = "unhealthy";
      this.emit("worker_error", error);
    } finally {
      this.processing = false;
    }
  }
  // Individual message processing
  async processMessage(message) {
    const processingStartTime = Date.now();
    const traceId = crypto17.randomUUID();
    try {
      const redactionResult = await this.applyPIIRedaction(message.raw_data);
      const normalizedData = await this.normalizeData(
        redactionResult.processed_data,
        message.data_type
      );
      const confidence = this.calculateConfidence(normalizedData, message);
      const processingTime = Date.now() - processingStartTime;
      const processedMessage = {
        message_id: message.message_id,
        source: message.source,
        timestamp: message.timestamp,
        data_type: message.data_type,
        processed_data: normalizedData,
        redaction_applied: redactionResult.redaction_applied,
        pii_fields_removed: redactionResult.pii_fields_removed,
        processing_time_ms: processingTime,
        confidence,
        metadata: {
          ...message.metadata,
          processing_trace_id: traceId,
          correlation_id: message.correlation_id,
          priority: message.priority
        }
      };
      await insertAnalyticsTrace({
        trace_id: traceId,
        operation_type: "streaming_ingest_processing",
        duration_ms: processingTime,
        input_hash: this.hashMessage(message),
        output_hash: this.hashMessage(processedMessage),
        performance_metrics: {
          pii_redaction_applied: redactionResult.redaction_applied,
          confidence_score: confidence,
          data_type: message.data_type,
          source: message.source
        }
      });
      return processedMessage;
    } catch (error) {
      logger_default2.error({
        message: "Individual message processing failed",
        message_id: message.message_id,
        error: error instanceof Error ? error.message : String(error),
        trace_id: traceId
      });
      throw error;
    }
  }
  // Committee requirement: PII redaction implementation
  async applyPIIRedaction(data) {
    if (!this.piiConfig.enabled) {
      return {
        processed_data: data,
        redaction_applied: false,
        pii_fields_removed: []
      };
    }
    const piiFieldsRemoved = [];
    let redactionApplied2 = false;
    const processObject = (obj, path55 = "") => {
      if (typeof obj === "string") {
        let processedString = obj;
        for (const [patternName, pattern2] of Object.entries(
          this.piiConfig.redaction_patterns
        )) {
          if (pattern2.test(processedString)) {
            processedString = processedString.replace(
              pattern2,
              this.piiConfig.replacement_token
            );
            piiFieldsRemoved.push(`${path55}.${patternName}`);
            redactionApplied2 = true;
          }
        }
        return processedString;
      }
      if (Array.isArray(obj)) {
        return obj.map(
          (item, index) => processObject(item, `${path55}[${index}]`)
        );
      }
      if (obj && typeof obj === "object") {
        const processed = {};
        for (const [key, value] of Object.entries(obj)) {
          processed[key] = processObject(value, path55 ? `${path55}.${key}` : key);
        }
        return processed;
      }
      return obj;
    };
    const processedData = processObject(data);
    if (redactionApplied2 && this.piiConfig.log_redactions) {
      logger_default2.info({
        message: "PII redaction applied",
        fields_redacted: piiFieldsRemoved.length,
        patterns_matched: [
          ...new Set(piiFieldsRemoved.map((f) => f.split(".").pop()))
        ]
      });
      this.metrics.pii_redactions_applied++;
    }
    return {
      processed_data: processedData,
      redaction_applied: redactionApplied2,
      pii_fields_removed: piiFieldsRemoved
    };
  }
  // Data normalization
  async normalizeData(data, dataType) {
    switch (dataType) {
      case "event":
        return this.normalizeEventData(data);
      case "entity":
        return this.normalizeEntityData(data);
      case "relationship":
        return this.normalizeRelationshipData(data);
      case "document":
        return this.normalizeDocumentData(data);
      default:
        return data;
    }
  }
  normalizeEventData(data) {
    return {
      event_id: data.id || crypto17.randomUUID(),
      event_type: data.type || "unknown",
      timestamp: new Date(data.timestamp || Date.now()),
      source: data.source || "unknown",
      severity: data.severity || "INFO",
      description: data.description || "",
      metadata: data.metadata || {}
    };
  }
  normalizeEntityData(data) {
    return {
      entity_id: data.id || crypto17.randomUUID(),
      entity_type: data.type || "unknown",
      properties: data.properties || {},
      confidence: Math.min(Math.max(data.confidence || 0.5, 0), 1),
      source: data.source || "unknown",
      created_at: new Date(data.created_at || Date.now())
    };
  }
  normalizeRelationshipData(data) {
    return {
      relationship_id: data.id || crypto17.randomUUID(),
      source_entity: data.source || data.from,
      target_entity: data.target || data.to,
      relationship_type: data.type || "unknown",
      properties: data.properties || {},
      confidence: Math.min(Math.max(data.confidence || 0.5, 0), 1),
      created_at: new Date(data.created_at || Date.now())
    };
  }
  normalizeDocumentData(data) {
    return {
      document_id: data.id || crypto17.randomUUID(),
      title: data.title || "Untitled",
      content: data.content || "",
      document_type: data.type || "unknown",
      metadata: data.metadata || {},
      source: data.source || "unknown",
      processed_at: /* @__PURE__ */ new Date()
    };
  }
  // Confidence calculation
  calculateConfidence(data, originalMessage) {
    let confidence = 0.5;
    const sourceReliability = this.getSourceReliability(originalMessage.source);
    confidence += sourceReliability * 0.3;
    const completeness = this.calculateDataCompleteness(data);
    confidence += completeness * 0.2;
    if (originalMessage.priority > 5) {
      confidence += 0.1;
    }
    return Math.min(Math.max(confidence, 0.1), 1);
  }
  getSourceReliability(source) {
    const reliabilityMap = {
      official_feed: 0.9,
      verified_api: 0.8,
      internal_system: 0.7,
      third_party_api: 0.6,
      user_input: 0.4,
      unknown: 0.3
    };
    return reliabilityMap[source] || 0.3;
  }
  calculateDataCompleteness(data) {
    if (!data || typeof data !== "object") {
      return 0.2;
    }
    const fields = Object.keys(data);
    const nonEmptyFields = fields.filter((field) => {
      const value = data[field];
      return value !== null && value !== void 0 && value !== "";
    });
    return nonEmptyFields.length / Math.max(fields.length, 1);
  }
  // Handle processed messages
  async handleProcessedMessage(processed) {
    try {
      await insertEvent({
        event_type: "INGESTED_DATA",
        event_source: processed.source,
        entity_id: processed.message_id,
        entity_type: processed.data_type,
        metadata: {
          processed_message: processed,
          pii_redaction_applied: processed.redaction_applied,
          processing_time_ms: processed.processing_time_ms
        },
        confidence: processed.confidence,
        severity: processed.processing_time_ms > 1e3 ? "WARNING" : "INFO"
      });
      await this.provenanceService.recordProvenanceEntry({
        operation_type: "STREAMING_INGEST",
        actor_id: "streaming-worker",
        metadata: {
          message_id: processed.message_id,
          source: processed.source,
          data_type: processed.data_type,
          processing_time_ms: processed.processing_time_ms,
          pii_redaction: processed.redaction_applied
        }
      });
      this.emit("message_processed", processed);
    } catch (error) {
      logger_default2.error({
        message: "Failed to handle processed message",
        message_id: processed.message_id,
        error: error instanceof Error ? error.message : String(error)
      });
      throw error;
    }
  }
  // Utility methods
  hashMessage(message) {
    const normalized = {
      id: message.message_id,
      source: message.source,
      data_type: message.data_type,
      timestamp: message.timestamp
    };
    return crypto17.createHash("md5").update(JSON.stringify(normalized)).digest("hex");
  }
  updateMetrics() {
    const now = Date.now();
    if (this.metrics.messages_processed > 0) {
      this.metrics.messages_per_second = Math.round(
        this.metrics.messages_processed / 60
      );
    }
    this.metrics.queue_size = this.messageQueue.length;
    if (this.metrics.errors_encountered > 10) {
      this.metrics.worker_status = "unhealthy";
    } else if (this.metrics.errors_encountered > 5 || this.metrics.queue_size > 500) {
      this.metrics.worker_status = "degraded";
    } else {
      this.metrics.worker_status = "healthy";
    }
  }
  updateWorkerStatus(errorCount, batchSize) {
    const errorRate = errorCount / batchSize;
    if (errorRate > 0.2) {
      this.metrics.worker_status = "unhealthy";
    } else if (errorRate > 0.1 || this.messageQueue.length > 500) {
      this.metrics.worker_status = "degraded";
    } else {
      this.metrics.worker_status = "healthy";
    }
  }
  // Public API methods
  getMetrics() {
    return { ...this.metrics };
  }
  getQueueSize() {
    return this.messageQueue.length;
  }
  clearQueue() {
    const queueSize = this.messageQueue.length;
    this.messageQueue = [];
    this.metrics.queue_size = 0;
    logger_default2.info({
      message: "Ingest queue cleared",
      messages_cleared: queueSize
    });
    this.emit("queue_cleared", { messages_cleared: queueSize });
  }
  // Graceful shutdown
  async shutdown() {
    logger_default2.info({
      message: "Streaming ingest worker shutting down",
      pending_messages: this.messageQueue.length
    });
    while (this.messageQueue.length > 0 && !this.processing) {
      await this.processBatch();
    }
    this.removeAllListeners();
    logger_default2.info({ message: "Streaming ingest worker shutdown complete" });
  }
};
var ingest_worker_default = StreamingIngestWorker;

// src/routes/streaming.ts
init_otel_tracing();
init_logger2();

// src/lib/streaming/rate-limiter.ts
var AdaptiveRateLimiter = class {
  globalTokens;
  maxTokens;
  initialTokens;
  refillRate;
  // Can now be modified
  initialRefillRate;
  globalLastRefill;
  clientTokens = /* @__PURE__ */ new Map();
  clientScope;
  metricsClient;
  adaptiveConfig;
  requestQueue = [];
  queueProcessorInterval;
  cleanupInterval;
  constructor(options2 = {}) {
    this.maxTokens = options2.maxTokens ?? options2.initialTokens ?? 100;
    this.initialTokens = options2.initialTokens ?? this.maxTokens;
    this.globalTokens = this.initialTokens;
    this.refillRate = options2.refillRate ?? 10;
    this.initialRefillRate = this.refillRate;
    this.globalLastRefill = Date.now();
    this.clientScope = options2.clientScope ?? false;
    this.metricsClient = options2.metricsClient;
    this.adaptiveConfig = options2.adaptive;
    this.queueProcessorInterval = setInterval(() => this.processQueue(), 100);
    this.cleanupInterval = setInterval(() => {
      const now = Date.now();
      const staleThreshold = 5 * 60 * 1e3;
      for (const [key, bucket] of this.clientTokens.entries()) {
        if (now - bucket.lastRefill > staleThreshold) {
          this.clientTokens.delete(key);
        }
      }
    }, 5 * 60 * 1e3);
  }
  refill(id) {
    const now = Date.now();
    if (this.clientScope && id) {
      const bucket = this.clientTokens.get(id) ?? { tokens: this.initialTokens, lastRefill: now };
      const elapsedTime = (now - bucket.lastRefill) / 1e3;
      const tokensToAdd = elapsedTime * this.refillRate;
      bucket.tokens = Math.min(this.maxTokens, bucket.tokens + tokensToAdd);
      bucket.lastRefill = now;
      this.clientTokens.set(id, bucket);
    } else {
      const elapsedTime = (now - this.globalLastRefill) / 1e3;
      const tokensToAdd = elapsedTime * this.refillRate;
      this.globalTokens = Math.min(this.maxTokens, this.globalTokens + tokensToAdd);
      this.globalLastRefill = now;
    }
  }
  async acquire(id) {
    return new Promise((resolve2) => {
      if (this.tryAcquire(id)) {
        resolve2();
      } else {
        this.requestQueue.push({ id, resolve: resolve2 });
        this.metricsClient?.increment("rate_limiter.queued", { client: id ?? "global" });
      }
    });
  }
  tryAcquire(id) {
    this.refill(id);
    let hasToken = false;
    if (this.clientScope && id) {
      const bucket = this.clientTokens.get(id);
      if (bucket && bucket.tokens >= 1) {
        bucket.tokens--;
        hasToken = true;
      }
    } else {
      if (this.globalTokens >= 1) {
        this.globalTokens--;
        hasToken = true;
      }
    }
    if (hasToken) {
      this.metricsClient?.increment("rate_limiter.acquired", { client: id ?? "global" });
      return true;
    } else {
      this.metricsClient?.increment("rate_limiter.rejected", { client: id ?? "global" });
      return false;
    }
  }
  tryAcquireSync(id) {
    return this.tryAcquire(id);
  }
  processQueue() {
    if (this.requestQueue.length === 0) {
      return;
    }
    const { id, resolve: resolve2 } = this.requestQueue[0];
    if (this.tryAcquire(id)) {
      this.requestQueue.shift();
      resolve2();
    }
    this.adapt();
  }
  adapt() {
    if (!this.adaptiveConfig) return;
    if (this.requestQueue.length > this.adaptiveConfig.queueThreshold) {
      this.refillRate = Math.max(1, this.refillRate * this.adaptiveConfig.refillRateAdjustment);
    } else {
      this.refillRate = Math.min(this.initialRefillRate, this.refillRate * (2 - this.adaptiveConfig.refillRateAdjustment));
    }
    this.metricsClient?.gauge("rate_limiter.refill_rate", this.refillRate);
  }
  destroy() {
    clearInterval(this.queueProcessorInterval);
    clearInterval(this.cleanupInterval);
  }
};

// src/routes/streaming.ts
var router = express.Router();
var ingestWorker = ingest_worker_default.getInstance();
var streamingRateLimiter = new AdaptiveRateLimiter({
  maxTokens: 10,
  // Allow 10 connections
  refillRate: 1
  // 1 connection per second
});
router.use(requireReasonForAccess);
router.post(
  "/ingest",
  requireAuthority("streaming_ingest", ["data_ingestion"]),
  async (req, res) => {
    const span = otelService.getCurrentSpan();
    try {
      const {
        source,
        data_type,
        raw_data,
        priority = 5,
        metadata = {},
        correlation_id
      } = req.body;
      if (!source || !data_type || !raw_data) {
        otelService.addSpanAttributes({
          "streaming.validation_error": true,
          "streaming.missing_fields": [
            "source",
            "data_type",
            "raw_data"
          ].filter((field) => !req.body[field])
        });
        return res.status(400).json({
          success: false,
          error: "Source, data_type, and raw_data are required",
          code: "MISSING_REQUIRED_FIELDS"
        });
      }
      const validDataTypes = [
        "event",
        "entity",
        "relationship",
        "document",
        "metric"
      ];
      if (!validDataTypes.includes(data_type)) {
        otelService.addSpanAttributes({
          "streaming.validation_error": true,
          "streaming.invalid_data_type": data_type
        });
        return res.status(400).json({
          success: false,
          error: `Invalid data_type. Must be one of: ${validDataTypes.join(", ")}`,
          valid_types: validDataTypes,
          code: "INVALID_DATA_TYPE"
        });
      }
      const user = req.user;
      const messageId = await otelService.traceStreamingOperation(
        "message_ingest",
        1,
        span
      )(async () => {
        return await ingestWorker.ingestMessage({
          source,
          timestamp: /* @__PURE__ */ new Date(),
          data_type,
          raw_data,
          priority: Math.min(Math.max(priority, 1), 10),
          metadata: {
            ...metadata,
            user_id: user.id,
            clearance_level: user.clearance_level,
            reason_for_access: req.reason_for_access?.reason,
            ingestion_timestamp: (/* @__PURE__ */ new Date()).toISOString()
          },
          correlation_id
        });
      });
      otelService.addSpanAttributes({
        "streaming.message_id": messageId,
        "streaming.source": source,
        "streaming.data_type": data_type,
        "streaming.priority": priority,
        "streaming.user_id": user.id
      });
      logger_default2.info({
        message: "Message queued for streaming ingest",
        message_id: messageId,
        source,
        data_type,
        user_id: user.id,
        queue_size: ingestWorker.getQueueSize()
      });
      res.status(202).json({
        success: true,
        message_id: messageId,
        status: "queued",
        queue_position: ingestWorker.getQueueSize(),
        message: "Message queued for streaming processing"
      });
    } catch (error) {
      otelService.addSpanAttributes({
        "streaming.ingest_error": true,
        "streaming.error": error instanceof Error ? error.message : String(error)
      });
      logger_default2.error({
        message: "Streaming ingest failed",
        error: error instanceof Error ? error.message : String(error),
        user_id: req.user?.id
      });
      res.status(500).json({
        success: false,
        error: "Streaming ingest failed",
        code: "INGEST_ERROR"
      });
    }
  }
);
router.post(
  "/ingest/batch",
  requireAuthority("streaming_ingest", ["batch_ingestion"]),
  async (req, res) => {
    const span = otelService.getCurrentSpan();
    try {
      const { messages = [] } = req.body;
      if (!Array.isArray(messages) || messages.length === 0) {
        return res.status(400).json({
          success: false,
          error: "Messages array is required for batch ingestion",
          code: "MISSING_MESSAGES"
        });
      }
      if (messages.length > 100) {
        return res.status(400).json({
          success: false,
          error: "Maximum 100 messages allowed per batch",
          code: "BATCH_SIZE_EXCEEDED"
        });
      }
      const user = req.user;
      const messageIds = [];
      const errors = [];
      await otelService.traceStreamingOperation(
        "batch_ingest",
        messages.length,
        span
      )(async () => {
        for (let i = 0; i < messages.length; i++) {
          const message = messages[i];
          try {
            if (!message.source || !message.data_type || !message.raw_data) {
              errors.push({
                index: i,
                error: "Missing required fields: source, data_type, raw_data"
              });
              continue;
            }
            const messageId = await ingestWorker.ingestMessage({
              source: message.source,
              timestamp: new Date(message.timestamp || Date.now()),
              data_type: message.data_type,
              raw_data: message.raw_data,
              priority: Math.min(Math.max(message.priority || 5, 1), 10),
              metadata: {
                ...message.metadata,
                batch_index: i,
                batch_id: crypto.randomUUID(),
                user_id: user.id,
                clearance_level: user.clearance_level
              },
              correlation_id: message.correlation_id
            });
            messageIds.push(messageId);
          } catch (error) {
            errors.push({
              index: i,
              error: error instanceof Error ? error.message : "Unknown error"
            });
          }
        }
      });
      otelService.addSpanAttributes({
        "streaming.batch_size": messages.length,
        "streaming.successful_ingests": messageIds.length,
        "streaming.failed_ingests": errors.length,
        "streaming.queue_size_after": ingestWorker.getQueueSize()
      });
      logger_default2.info({
        message: "Batch ingest completed",
        batch_size: messages.length,
        successful: messageIds.length,
        errors: errors.length,
        user_id: user.id
      });
      res.status(202).json({
        success: true,
        batch_summary: {
          total_messages: messages.length,
          successful_ingests: messageIds.length,
          failed_ingests: errors.length,
          message_ids: messageIds,
          errors: errors.length > 0 ? errors : void 0
        },
        queue_size: ingestWorker.getQueueSize(),
        message: `Batch processing queued: ${messageIds.length}/${messages.length} messages successfully queued`
      });
    } catch (error) {
      otelService.addSpanAttributes({
        "streaming.batch_error": true,
        "streaming.error": error instanceof Error ? error.message : String(error)
      });
      logger_default2.error({
        message: "Batch ingest failed",
        error: error instanceof Error ? error.message : String(error),
        user_id: req.user?.id
      });
      res.status(500).json({
        success: false,
        error: "Batch ingest failed",
        code: "BATCH_INGEST_ERROR"
      });
    }
  }
);
router.get(
  "/metrics",
  requireAuthority("streaming_ingest", ["metrics_access"]),
  async (req, res) => {
    try {
      const metrics8 = ingestWorker.getMetrics();
      otelService.addSpanAttributes({
        "streaming.metrics.queue_size": metrics8.queue_size,
        "streaming.metrics.worker_status": metrics8.worker_status,
        "streaming.metrics.messages_processed": metrics8.messages_processed
      });
      res.json({
        success: true,
        metrics: metrics8,
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        message: "Streaming worker metrics retrieved"
      });
    } catch (error) {
      logger_default2.error({
        message: "Failed to retrieve streaming metrics",
        error: error instanceof Error ? error.message : String(error),
        user_id: req.user?.id
      });
      res.status(500).json({
        success: false,
        error: "Failed to retrieve streaming metrics",
        code: "METRICS_ERROR"
      });
    }
  }
);
router.post(
  "/queue/clear",
  requireAuthority("streaming_ingest", ["queue_management"]),
  async (req, res) => {
    try {
      const user = req.user;
      if (user.clearance_level < 4) {
        return res.status(403).json({
          success: false,
          error: "Queue management requires administrative clearance (level 4+)",
          code: "INSUFFICIENT_CLEARANCE"
        });
      }
      const queueSizeBefore = ingestWorker.getQueueSize();
      ingestWorker.clearQueue();
      otelService.addSpanAttributes({
        "streaming.queue_cleared": true,
        "streaming.messages_cleared": queueSizeBefore,
        "streaming.admin_user": user.id
      });
      logger_default2.warn({
        message: "Streaming queue cleared by administrator",
        user_id: user.id,
        messages_cleared: queueSizeBefore,
        clearance_level: user.clearance_level
      });
      res.json({
        success: true,
        messages_cleared: queueSizeBefore,
        queue_size_after: 0,
        message: "Streaming queue cleared successfully"
      });
    } catch (error) {
      logger_default2.error({
        message: "Queue clear operation failed",
        error: error instanceof Error ? error.message : String(error),
        user_id: req.user?.id
      });
      res.status(500).json({
        success: false,
        error: "Queue clear operation failed",
        code: "QUEUE_CLEAR_ERROR"
      });
    }
  }
);
router.get(
  "/events/stream",
  requireAuthority("streaming_ingest", ["real_time_events"]),
  async (req, res, next) => {
    const user = req.user;
    const key = `sse-connection:${user.tenantId}:${user.id}`;
    if (!streamingRateLimiter.tryAcquireSync(key)) {
      return res.status(429).json({
        success: false,
        error: "Too many streaming connections",
        code: "RATE_LIMIT_EXCEEDED"
      });
    }
    next();
  },
  async (req, res) => {
    try {
      const user = req.user;
      const allowedOrigins = process.env.ALLOWED_ORIGINS?.split(",") || [
        "http://localhost:3000",
        "http://localhost:5173",
        "https://intelgraph.app"
      ];
      const origin = req.headers.origin || "";
      const allowOrigin = allowedOrigins.includes(origin) ? origin : allowedOrigins[0];
      res.writeHead(200, {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        Connection: "keep-alive",
        "Access-Control-Allow-Origin": allowOrigin,
        "Access-Control-Allow-Headers": "Cache-Control",
        "Access-Control-Allow-Credentials": "true"
      });
      res.write(
        `data: ${JSON.stringify({
          type: "connection",
          message: "Connected to streaming events",
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          user_id: user.id
        })}

`
      );
      const onMessageProcessed = (processed) => {
        res.write(
          `data: ${JSON.stringify({
            type: "message_processed",
            data: {
              message_id: processed.message_id,
              source: processed.source,
              data_type: processed.data_type,
              processing_time_ms: processed.processing_time_ms,
              confidence: processed.confidence
            },
            timestamp: (/* @__PURE__ */ new Date()).toISOString()
          })}

`
        );
      };
      const onQueueAlert = (alert) => {
        res.write(
          `data: ${JSON.stringify({
            type: "queue_alert",
            data: alert,
            timestamp: (/* @__PURE__ */ new Date()).toISOString()
          })}

`
        );
      };
      ingestWorker.on("message_processed", onMessageProcessed);
      ingestWorker.on("queue_alert", onQueueAlert);
      req.on("close", () => {
        ingestWorker.off("message_processed", onMessageProcessed);
        ingestWorker.off("queue_alert", onQueueAlert);
        logger_default2.info({
          message: "Client disconnected from streaming events",
          user_id: user.id
        });
      });
      const heartbeat = setInterval(() => {
        res.write(
          `data: ${JSON.stringify({
            type: "heartbeat",
            metrics: ingestWorker.getMetrics(),
            timestamp: (/* @__PURE__ */ new Date()).toISOString()
          })}

`
        );
      }, 3e4);
      req.on("close", () => {
        clearInterval(heartbeat);
      });
    } catch (error) {
      logger_default2.error({
        message: "Streaming events setup failed",
        error: error instanceof Error ? error.message : String(error),
        user_id: req.user?.id
      });
      res.status(500).json({
        success: false,
        error: "Streaming events setup failed",
        code: "STREAMING_EVENTS_ERROR"
      });
    }
  }
);
router.get("/health", async (req, res) => {
  try {
    const metrics8 = ingestWorker.getMetrics();
    const isHealthy = metrics8.worker_status === "healthy" || metrics8.worker_status === "degraded";
    res.status(isHealthy ? 200 : 503).json({
      success: isHealthy,
      service: "streaming-ingest",
      status: metrics8.worker_status,
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      metrics: {
        queue_size: metrics8.queue_size,
        messages_processed: metrics8.messages_processed,
        messages_per_second: metrics8.messages_per_second,
        pii_redactions_applied: metrics8.pii_redactions_applied,
        errors_encountered: metrics8.errors_encountered
      },
      features: {
        pii_redaction: true,
        batch_processing: true,
        real_time_events: true,
        otel_tracing: true
      }
    });
  } catch (error) {
    res.status(503).json({
      success: false,
      service: "streaming-ingest",
      status: "unhealthy",
      error: error instanceof Error ? error.message : String(error)
    });
  }
});

// src/services/OSINTQueueService.ts
init_logger2();
import { Queue as Queue2, Worker as Worker2 } from "bullmq";

// src/services/VeracityScoringService.ts
init_database();
init_logger2();

// src/services/IntelCorroborationService.ts
var IntelCorroborationService = class {
  ratings = /* @__PURE__ */ new Map();
  addAnalystRating(claimId, rating) {
    if (rating < 0 || rating > 1) {
      throw new Error("Rating must be between 0 and 1");
    }
    const current = this.ratings.get(claimId) || { sum: 0, count: 0 };
    current.sum += rating;
    current.count++;
    this.ratings.set(claimId, current);
  }
  getAnalystAverage(claimId) {
    const agg = this.ratings.get(claimId);
    if (!agg || agg.count === 0) return null;
    return agg.sum / agg.count;
  }
  evaluateClaim(claimId, evidence) {
    let supportWeight = 0;
    let totalWeight = 0;
    const corroboratedBy = [];
    const disputedBy = [];
    const now = Date.now();
    for (const e of evidence) {
      const ts = typeof e.timestamp === "number" ? e.timestamp : new Date(e.timestamp).getTime();
      const ageDays = Math.max(0, (now - ts) / (1e3 * 60 * 60 * 24));
      const recency = 1 / (1 + ageDays);
      const weight = (e.trust || 0) * recency;
      totalWeight += weight;
      if (e.supports) {
        supportWeight += weight;
        corroboratedBy.push(e.source);
      } else {
        disputedBy.push(e.source);
      }
    }
    const evidenceScore = totalWeight ? supportWeight / totalWeight : 0.5;
    const manual = this.getAnalystAverage(claimId);
    const confidenceScore = manual != null ? (evidenceScore + manual) / 2 : evidenceScore;
    return { confidenceScore, corroboratedBy, disputedBy };
  }
};
var intelCorroborationService = new IntelCorroborationService();

// src/services/VeracityScoringService.ts
var VeracityScoringService = class {
  driver;
  corroborationService;
  constructor() {
    this.driver = getNeo4jDriver2();
    this.corroborationService = new IntelCorroborationService();
  }
  /**
   * Calculates and updates the veracity score for a given entity.
   * It fetches linked 'Provenance' or 'Source' nodes to evaluate evidence.
   */
  async scoreEntity(entityId) {
    const session = this.driver.session();
    try {
      const query3 = `
        MATCH (n:Entity {id: $id})
        OPTIONAL MATCH (n)<-[:MENTIONS|:SUPPORTS]-(s)
        RETURN n, collect(s) as sources
      `;
      const result2 = await session.run(query3, { id: entityId });
      if (result2.records.length === 0) {
        throw new Error(`Entity ${entityId} not found`);
      }
      const record2 = result2.records[0];
      const sources = record2.get("sources");
      let totalTrust = 0;
      let sourceCount = 0;
      for (const src of sources) {
        const props = src.properties || {};
        const trust = props.trustLevel || (props.type === "verified" ? 0.9 : 0.5);
        totalTrust += trust;
        sourceCount++;
      }
      let rawScore = 20;
      if (sourceCount > 0) {
        const avgTrust = totalTrust / sourceCount;
        rawScore = avgTrust * 100 * (1 + Math.log10(sourceCount));
      }
      const score = Math.min(100, Math.round(rawScore));
      let confidence = "LOW";
      if (sourceCount > 5 && score > 70) confidence = "HIGH";
      else if (sourceCount > 2) confidence = "MEDIUM";
      const veracity = {
        score,
        confidence,
        sources: sourceCount,
        lastUpdated: Date.now()
      };
      await session.run(`
        MATCH (n:Entity {id: $id})
        SET n.veracityScore = $score,
            n.veracityConfidence = $confidence,
            n.veracityUpdated = $ts
      `, {
        id: entityId,
        score: veracity.score,
        confidence: veracity.confidence,
        ts: veracity.lastUpdated
      });
      logger_default2.info(`Updated veracity for ${entityId}: ${score} (${confidence})`);
      return veracity;
    } catch (err) {
      logger_default2.error(`Failed to score entity ${entityId}`, err);
      throw err;
    } finally {
      await session.close();
    }
  }
};

// src/services/OSINTQueueService.ts
init_config3();

// src/services/ExternalAPIService.ts
var ExternalAPIService = class {
  logger;
  constructor(logger72) {
    this.logger = logger72;
  }
  async sendSlackNotification(message) {
    this.logger.info(`Slack notification: ${message}`);
    return true;
  }
  async createJiraIssue(details) {
    this.logger.info(`Jira issue created: ${JSON.stringify(details)}`);
    return { id: "JIRA-123" };
  }
  async queryVirusTotal(resource) {
    this.logger.info(`VirusTotal query for: ${resource}`);
    return { malicious: false };
  }
};

// src/services/OSINTService.ts
init_neo4j();
init_pg();
init_logger2();
import fetch2 from "node-fetch";
var OSINTService = class {
  logger = logger_default2;
  driver = null;
  pool = pg;
  getDriver() {
    if (!this.driver) {
      this.driver = getNeo4jDriver();
    }
    return this.driver;
  }
  async enrichFromWikipedia({ entityId, title }) {
    const t = title?.trim();
    if (!t && !entityId) throw new Error("Provide title or entityId");
    let page;
    try {
      const res = await fetch2(
        "https://en.wikipedia.org/w/api.php?" + new URLSearchParams({
          action: "query",
          prop: "extracts|info",
          exintro: "1",
          explaintext: "1",
          inprop: "url",
          format: "json",
          titles: t || ""
        })
      );
      const data = await res.json();
      const pages = data?.query?.pages || {};
      page = Object.values(pages)[0];
    } catch (e) {
      this.logger.error("Wikipedia fetch failed", e);
      throw e;
    }
    if (!page || page.missing) throw new Error("No page found on Wikipedia");
    const session = this.getDriver().session();
    let updated;
    try {
      const props = {
        label: page.title,
        wikipediaUrl: page.fullurl,
        summary: page.extract,
        updatedAt: Date.now()
      };
      const q = `
        MERGE (n:Entity {id: $id})
        SET n += $props
        RETURN n as node
      `;
      const id = entityId || `wiki:${page.pageid}`;
      const result2 = await session.run(q, { id, props });
      updated = result2.records[0]?.get("node").properties;
    } finally {
      await session.close();
    }
    try {
      await this.pool.query(
        `INSERT INTO provenance (resource_type, resource_id, source, uri, extractor, metadata)
         VALUES ($1,$2,$3,$4,$5,$6)`,
        [
          "entity",
          updated.id,
          "wikipedia",
          page.fullurl,
          "osint.wikipedia",
          { pageid: page.pageid, title: page.title }
        ]
      );
    } catch (e) {
      this.logger.warn("Failed to record provenance", e);
    }
    return updated;
  }
};

// src/services/OSINTQueueService.ts
var connection = {
  host: config_default.redis?.host || "localhost",
  port: config_default.redis?.port || 6379,
  password: config_default.redis?.password,
  db: config_default.redis?.db || 0
};
var osintQueue = new Queue2("osint-ingest", { connection });
function startOSINTWorkers() {
  const worker = new Worker2(
    "osint-ingest",
    async (job) => {
      const { type, targetId, tenantId, params } = job.data;
      logger_default2.info(`Processing OSINT job ${job.id}: ${type} for ${targetId}`);
      const extApi = new ExternalAPIService(logger_default2);
      const osintService = new OSINTService();
      const veracityService = new VeracityScoringService();
      try {
        if (type === "wikipedia" || type === "comprehensive_scan") {
          let title = targetId;
          if (targetId.includes(":")) {
            title = targetId.split(":")[1];
          }
          try {
            await osintService.enrichFromWikipedia({ entityId: targetId, title });
          } catch (e) {
            logger_default2.warn(`Wikipedia enrichment failed for ${targetId}`, e);
          }
        }
        await veracityService.scoreEntity(targetId);
        logger_default2.info(`OSINT job ${job.id} completed.`);
      } catch (err) {
        logger_default2.error(`OSINT job ${job.id} failed`, err);
        throw err;
      }
    },
    { connection }
  );
  worker.on("failed", (job, err) => {
    logger_default2.error(`OSINT job ${job?.id} failed with ${err.message}`);
  });
  return worker;
}

// src/backup/BackupService.ts
init_redis2();
init_logger();
init_neo4j();
init_metrics3();
import fs3 from "fs/promises";
import path4 from "path";
import { promisify } from "util";
import { exec } from "child_process";
import { createWriteStream as createWriteStream2 } from "fs";
import zlib from "zlib";
var execAsync = promisify(exec);
var backupMetrics = new PrometheusMetrics("backup_service");
backupMetrics.createCounter("ops_total", "Total backup operations", ["type", "status"]);
backupMetrics.createHistogram("duration_seconds", "Backup duration", { buckets: [0.1, 0.5, 1, 5, 10, 30, 60, 120] });
backupMetrics.createGauge("size_bytes", "Backup size", ["type"]);
var BackupService = class {
  backupRoot;
  s3Config = null;
  redis;
  constructor(backupRoot = process.env.BACKUP_ROOT_DIR || "./backups") {
    this.backupRoot = backupRoot;
    this.redis = RedisService.getInstance();
    if (process.env.S3_BACKUP_BUCKET) {
      this.s3Config = {
        bucket: process.env.S3_BACKUP_BUCKET,
        region: process.env.S3_REGION || "us-east-1",
        endpoint: process.env.S3_ENDPOINT,
        accessKeyId: process.env.AWS_ACCESS_KEY_ID,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY
      };
    }
  }
  async ensureBackupDir(type) {
    const date = (/* @__PURE__ */ new Date()).toISOString().split("T")[0];
    const dir = path4.join(this.backupRoot, type, date);
    await fs3.mkdir(dir, { recursive: true });
    return dir;
  }
  async uploadToS3(filepath, key) {
    if (!this.s3Config) {
      logger_default.warn("Skipping S3 upload: No S3 configuration found.");
      return;
    }
    logger_default.info(`Uploading ${filepath} to S3 bucket ${this.s3Config.bucket} as ${key}...`);
    try {
      if (process.env.USE_AWS_CLI === "true") {
        await execAsync(`aws s3 cp "${filepath}" "s3://${this.s3Config.bucket}/${key}" --region ${this.s3Config.region}`);
      } else {
        await new Promise((r) => setTimeout(r, 500));
        logger_default.info("Simulated S3 upload complete.");
      }
    } catch (error) {
      logger_default.error("Failed to upload to S3", error);
      throw error;
    }
  }
  async verifyBackup(filepath) {
    logger_default.info(`Verifying backup integrity for ${filepath}...`);
    try {
      const stats = await fs3.stat(filepath);
      if (stats.size === 0) throw new Error("Backup file is empty");
      if (filepath.endsWith(".gz")) {
        await execAsync(`gzip -t "${filepath}"`);
      }
      logger_default.info(`Backup verification successful for ${filepath}`);
      return true;
    } catch (error) {
      logger_default.error(`Backup verification failed for ${filepath}`, error);
      return false;
    }
  }
  async backupPostgres(options2 = {}) {
    const startTime = Date.now();
    logger_default.info("Starting PostgreSQL backup...");
    try {
      const dir = await this.ensureBackupDir("postgres");
      const timestamp = (/* @__PURE__ */ new Date()).toISOString().replace(/[:.]/g, "-");
      const filename = `postgres-backup-${timestamp}.sql`;
      const filepath = path4.join(dir, filename);
      const finalPath = options2.compress ? `${filepath}.gz` : filepath;
      const pgHost = process.env.POSTGRES_HOST || "localhost";
      const pgUser = process.env.POSTGRES_USER || "intelgraph";
      const pgDb = process.env.POSTGRES_DB || "intelgraph_dev";
      const pgPassword = process.env.POSTGRES_PASSWORD || "devpassword";
      const cmd = `PGPASSWORD='${pgPassword}' pg_dump -h ${pgHost} -U ${pgUser} ${pgDb}`;
      let attempt = 0;
      const maxRetries = 3;
      while (attempt < maxRetries) {
        try {
          if (options2.compress) {
            await execAsync(`${cmd} | gzip > "${finalPath}"`);
          } else {
            await execAsync(`${cmd} > "${finalPath}"`);
          }
          break;
        } catch (e) {
          attempt++;
          if (attempt >= maxRetries) throw e;
          logger_default.warn({ error: e }, `Postgres backup attempt ${attempt} failed, retrying in 2s...`);
          await new Promise((r) => setTimeout(r, 2e3));
        }
      }
      const stats = await fs3.stat(finalPath);
      backupMetrics.setGauge("size_bytes", stats.size, { type: "postgres" });
      backupMetrics.observeHistogram("duration_seconds", (Date.now() - startTime) / 1e3, { type: "postgres", status: "success" });
      backupMetrics.incrementCounter("ops_total", { type: "postgres", status: "success" });
      logger_default.info({ path: finalPath, size: stats.size }, "PostgreSQL backup completed");
      if (options2.uploadToS3) {
        const s3Key = `postgres/${path4.basename(finalPath)}`;
        await this.uploadToS3(finalPath, s3Key);
      }
      await this.verifyBackup(finalPath);
      await this.recordBackupMeta("postgres", finalPath, stats.size);
      return finalPath;
    } catch (error) {
      backupMetrics.incrementCounter("ops_total", { type: "postgres", status: "failure" });
      logger_default.error("PostgreSQL backup failed", error);
      throw error;
    }
  }
  async backupNeo4j(options2 = {}) {
    const startTime = Date.now();
    logger_default.info("Starting Neo4j backup...");
    try {
      const dir = await this.ensureBackupDir("neo4j");
      const timestamp = (/* @__PURE__ */ new Date()).toISOString().replace(/[:.]/g, "-");
      const filename = `neo4j-export-${timestamp}.jsonl`;
      const filepath = path4.join(dir, filename);
      const finalPath = options2.compress ? `${filepath}.gz` : filepath;
      const driver3 = getNeo4jDriver();
      const session = driver3.session();
      try {
        const fileStream = createWriteStream2(finalPath);
        const outputStream = options2.compress ? zlib.createGzip() : null;
        if (outputStream) {
          outputStream.pipe(fileStream);
        }
        const writeTarget = outputStream || fileStream;
        const nodeResult = await session.run("MATCH (n) RETURN n");
        for (const record2 of nodeResult.records) {
          const node = record2.get("n");
          const line = JSON.stringify({ type: "node", labels: node.labels, props: node.properties }) + "\n";
          writeTarget.write(line);
        }
        const relResult = await session.run("MATCH (a)-[r]->(b) RETURN r, a.id as startId, b.id as endId");
        for (const record2 of relResult.records) {
          const rel = record2.get("r");
          const startId = record2.get("startId");
          const endId = record2.get("endId");
          const line = JSON.stringify({
            type: "rel",
            typeName: rel.type,
            props: rel.properties,
            startId,
            endId
          }) + "\n";
          writeTarget.write(line);
        }
        writeTarget.end();
        await new Promise((resolve2, reject) => {
          fileStream.on("finish", () => resolve2());
          fileStream.on("error", (err) => reject(err));
        });
      } finally {
        await session.close();
      }
      const stats = await fs3.stat(finalPath);
      backupMetrics.setGauge("size_bytes", stats.size, { type: "neo4j" });
      backupMetrics.observeHistogram("duration_seconds", (Date.now() - startTime) / 1e3, { type: "neo4j", status: "success" });
      logger_default.info({ path: finalPath, size: stats.size }, "Neo4j logical backup completed");
      if (options2.uploadToS3) {
        const s3Key = `neo4j/${path4.basename(finalPath)}`;
        await this.uploadToS3(finalPath, s3Key);
      }
      await this.verifyBackup(finalPath);
      await this.recordBackupMeta("neo4j", finalPath, stats.size);
      return finalPath;
    } catch (error) {
      logger_default.error("Neo4j backup failed", error);
      throw error;
    }
  }
  async backupRedis(options2 = {}) {
    const startTime = Date.now();
    logger_default.info("Starting Redis backup...");
    try {
      const client6 = this.redis.getClient();
      if (!client6) throw new Error("Redis client not available");
      const isCluster = client6.constructor.name === "Cluster";
      if (isCluster) {
        const nodes = client6.nodes ? client6.nodes("master") : [];
        if (nodes.length > 0) {
          logger_default.info(`Triggering BGSAVE on ${nodes.length} master nodes...`);
          await Promise.all(nodes.map((node) => node.bgsave().catch(
            (e) => logger_default.warn(`Failed to trigger BGSAVE on node ${node.options.host}: ${e.message}`)
          )));
        } else {
          logger_default.warn("No master nodes found in cluster for backup.");
        }
      } else {
        await client6.bgsave();
      }
      const dir = await this.ensureBackupDir("redis");
      const timestamp = (/* @__PURE__ */ new Date()).toISOString().replace(/[:.]/g, "-");
      const filename = `redis-backup-log-${timestamp}.txt`;
      const filepath = path4.join(dir, filename);
      await fs3.writeFile(filepath, `Redis BGSAVE triggered successfully. Last save timestamp: ${(/* @__PURE__ */ new Date()).toISOString()}`);
      backupMetrics.observeHistogram("duration_seconds", (Date.now() - startTime) / 1e3, { type: "redis", status: "success" });
      if (options2.uploadToS3) {
        const s3Key = `redis/${path4.basename(filepath)}`;
        await this.uploadToS3(filepath, s3Key);
      }
      await this.recordBackupMeta("redis", filepath, 0);
      return filepath;
    } catch (error) {
      logger_default.error("Redis backup failed", error);
      throw error;
    }
  }
  async recordBackupMeta(type, filepath, size) {
    const meta = {
      type,
      filepath,
      size,
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      host: process.env.HOSTNAME || "unknown"
    };
    const client6 = this.redis.getClient();
    if (client6) {
      await client6.lpush(`backups:${type}:history`, JSON.stringify(meta));
      await client6.ltrim(`backups:${type}:history`, 0, 99);
    }
  }
  async runAllBackups() {
    const results = {};
    const uploadToS3 = !!process.env.S3_BACKUP_BUCKET;
    try {
      results.postgres = await this.backupPostgres({ compress: true, uploadToS3 });
    } catch (e) {
      results.postgres = `Failed: ${e}`;
    }
    try {
      results.neo4j = await this.backupNeo4j({ compress: true, uploadToS3 });
    } catch (e) {
      results.neo4j = `Failed: ${e}`;
    }
    try {
      results.redis = await this.backupRedis({ uploadToS3 });
    } catch (e) {
      results.redis = `Failed: ${e}`;
    }
    return results;
  }
};

// src/backup/BackupManager.ts
init_logger2();
import cron from "node-cron";
var BackupManager = class {
  backupService;
  schedule;
  // Cron expression
  constructor(schedule = "0 2 * * *") {
    this.backupService = new BackupService();
    this.schedule = schedule;
  }
  startScheduler() {
    logger_default2.info(`Starting backup scheduler with schedule: ${this.schedule}`);
    cron.schedule(this.schedule, async () => {
      logger_default2.info("Running scheduled backups...");
      try {
        const results = await this.backupService.runAllBackups();
        logger_default2.info({ results }, "Scheduled backups completed");
      } catch (error) {
        logger_default2.error("Scheduled backup execution failed", error);
      }
    });
  }
  async triggerImmediateBackup() {
    logger_default2.info("Triggering immediate backup...");
    return await this.backupService.runAllBackups();
  }
};

// src/db/indexManager.ts
init_neo4j();
init_logger2();
import fs4 from "fs/promises";
import path5 from "path";
import yaml from "js-yaml";
import { fileURLToPath } from "url";
var __filename = fileURLToPath(import.meta.url);
var __dirname = path5.dirname(__filename);
var CONFIG_PATH = path5.resolve(__dirname, "../../config/neo4j-optimization.yml");
async function loadIndexConfig() {
  try {
    const fileContents = await fs4.readFile(CONFIG_PATH, "utf8");
    const config9 = yaml.load(fileContents);
    return config9.indexes || [];
  } catch (error) {
    logger_default2.warn("Failed to load neo4j-optimization.yml, falling back to defaults", error);
    return [
      { label: "Entity", property: "id", type: "CONSTRAINT" },
      { label: "Entity", property: "uuid", type: "CONSTRAINT" },
      { label: "Entity", property: "type", type: "INDEX" },
      { label: "Investigation", property: "status", type: "INDEX" },
      { label: "User", property: "email", type: "CONSTRAINT" }
    ];
  }
}
async function checkNeo4jIndexes() {
  logger_default2.info("Starting Neo4j Index Check...");
  const REQUIRED_INDEXES = await loadIndexConfig();
  try {
    const result2 = await neo.run("SHOW INDEXES");
    const existingIndexes = result2.records.map((r) => {
      const obj = r.toObject();
      return {
        name: obj.name,
        labelsOrTypes: obj.labelsOrTypes,
        properties: obj.properties,
        type: obj.type
      };
    });
    const missingIndexes = [];
    for (const req of REQUIRED_INDEXES) {
      const isUniqueReq = req.type === "CONSTRAINT" || req.type === "UNIQUENESS";
      const exists = existingIndexes.some(
        (idx) => idx.labelsOrTypes && idx.labelsOrTypes.includes(req.label) && idx.properties && idx.properties.includes(req.property) && (isUniqueReq ? idx.type === "RANGE" || idx.type === "UNIQUENESS" : true)
      );
      if (!exists) {
        missingIndexes.push(req);
      }
    }
    if (missingIndexes.length > 0) {
      logger_default2.warn({ missingIndexes }, "Missing Neo4j Indexes detected");
      for (const idx of missingIndexes) {
        const isUnique = idx.type === "CONSTRAINT" || idx.type === "UNIQUENESS";
        const query3 = isUnique ? `CREATE CONSTRAINT IF NOT EXISTS FOR (n:${idx.label}) REQUIRE n.${idx.property} IS UNIQUE` : `CREATE INDEX IF NOT EXISTS FOR (n:${idx.label}) ON (n.${idx.property})`;
        logger_default2.info({ query: query3 }, "Creating missing index/constraint...");
        try {
          await neo.run(query3);
        } catch (e) {
          logger_default2.error({ error: e, query: query3 }, "Failed to create index");
        }
      }
    } else {
      logger_default2.info("All required Neo4j indexes are present.");
    }
  } catch (error) {
    logger_default2.error("Failed to check Neo4j indexes", error);
  }
}

// src/lib/secrets/providers/EnvSecretProvider.ts
var EnvSecretProvider = class {
  async initialize() {
  }
  async getSecret(key) {
    return process.env[key] || null;
  }
  async setSecret(key, value) {
    process.env[key] = value;
  }
  async rotateSecret(key) {
    throw new Error("Rotation not supported for EnvSecretProvider");
  }
};

// src/lib/secrets/providers/VaultSecretProvider.ts
init_logger();
var VaultSecretProvider = class {
  vaultUrl;
  vaultToken;
  initialized = false;
  // Simulating a cache or client
  cache = /* @__PURE__ */ new Map();
  constructor(vaultUrl, vaultToken) {
    this.vaultUrl = vaultUrl;
    this.vaultToken = vaultToken;
  }
  async initialize() {
    logger.info("Initializing Vault connection...");
    this.initialized = true;
    logger.info("Vault connection established (Simulated)");
  }
  async getSecret(key) {
    if (!this.initialized) await this.initialize();
    if (this.cache.has(key)) {
      return this.cache.get(key) || null;
    }
    logger.debug(`Fetching secret ${key} from Vault`);
    const value = process.env[key];
    if (value) {
      this.cache.set(key, value);
      return value;
    }
    return null;
  }
  async setSecret(key, value) {
    this.cache.set(key, value);
    logger.info(`Secret ${key} updated in Vault`);
  }
  async rotateSecret(key) {
    logger.info(`Rotating secret ${key} in Vault`);
    const newValue = `rotated_${Date.now()}_${Math.random().toString(36).substring(7)}`;
    await this.setSecret(key, newValue);
    return newValue;
  }
};

// src/lib/secrets/SecretManager.ts
init_logger();
init_audit();
var SecretManager = class _SecretManager {
  static instance;
  provider;
  constructor(config9) {
    if (config9.provider === "vault" && config9.vaultUrl && config9.vaultToken) {
      this.provider = new VaultSecretProvider(config9.vaultUrl, config9.vaultToken);
    } else {
      if (config9.provider === "vault") {
        logger.warn("Vault configured but missing URL/Token, falling back to Env");
      }
      this.provider = new EnvSecretProvider();
    }
  }
  static async initialize(config9) {
    if (!_SecretManager.instance) {
      const finalConfig = config9 || {
        provider: process.env.SECRET_PROVIDER || "env",
        vaultUrl: process.env.VAULT_ADDR,
        vaultToken: process.env.VAULT_TOKEN
      };
      _SecretManager.instance = new _SecretManager(finalConfig);
      await _SecretManager.instance.provider.initialize();
      logger.info(`SecretManager initialized with provider: ${finalConfig.provider}`);
    }
  }
  static getInstance() {
    if (!_SecretManager.instance) {
      throw new Error("SecretManager not initialized. Call initialize() first.");
    }
    return _SecretManager.instance;
  }
  async getSecret(key, context4) {
    const start = Date.now();
    const value = await this.provider.getSecret(key);
    try {
      await writeAudit({
        action: "ACCESS_SECRET",
        resourceType: "secret",
        resourceId: key,
        details: {
          requester: context4.requester,
          purpose: context4.purpose,
          success: !!value,
          durationMs: Date.now() - start
        },
        actorRole: "system"
        // or passed in context
      });
    } catch (err) {
      logger.error(`Failed to audit secret access for ${key}: ${err}`);
    }
    if (!value) {
      throw new Error(`Secret ${key} not found`);
    }
    return value;
  }
  /**
   * Loads critical secrets into process.env to support legacy code.
   * This is a bridge method.
   */
  async loadSecretsToEnv(keys) {
    for (const key of keys) {
      const value = await this.provider.getSecret(key);
      if (value) {
        process.env[key] = value;
      }
    }
  }
  async rotateSecret(key) {
    const newValue = await this.provider.rotateSecret(key);
    process.env[key] = newValue;
    await writeAudit({
      action: "ROTATE_SECRET",
      resourceType: "secret",
      resourceId: key,
      details: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      }
    });
  }
};

// src/bootstrap-secrets.ts
init_logger();
async function bootstrapSecrets() {
  try {
    logger.info("Bootstrapping secrets...");
    await SecretManager.initialize({
      provider: process.env.VAULT_ADDR ? "vault" : "env",
      vaultUrl: process.env.VAULT_ADDR,
      vaultToken: process.env.VAULT_TOKEN
    });
    const manager = SecretManager.getInstance();
    const secretsToLoad = [
      "DATABASE_URL",
      "NEO4J_PASSWORD",
      "REDIS_PASSWORD",
      "JWT_SECRET",
      "JWT_REFRESH_SECRET",
      "OPENAI_API_KEY",
      "ANTHROPIC_API_KEY"
    ];
    await manager.loadSecretsToEnv(secretsToLoad);
    logger.info("Secrets bootstrapped successfully.");
  } catch (error) {
    logger.error(`Failed to bootstrap secrets: ${error}`);
    process.exit(1);
  }
}

// src/index.ts
init_logger();

// src/app.ts
init_logger();
init_comprehensive_telemetry();
import "dotenv/config";
import express57 from "express";
import { ApolloServer } from "@apollo/server";
import { GraphQLError as GraphQLError15 } from "graphql";
import { expressMiddleware } from "@as-integrations/express4";
import { makeExecutableSchema } from "@graphql-tools/schema";
import cors from "cors";
import compression from "compression";
import hpp from "hpp";
import pino71 from "pino";
import pinoHttpModule from "pino-http";

// src/lib/telemetry/diagnostic-snapshotter.ts
init_comprehensive_telemetry();
import * as v8 from "v8";
import * as fs5 from "fs";
import * as path6 from "path";
import * as os from "os";

// src/config/telemetry.ts
var telemetryConfig = {
  snapshotter: {
    memoryThreshold: 1.5 * 1024 * 1024 * 1024,
    // 1.5 GB
    latencyThreshold: 2e3
    // 2 seconds
  }
};

// src/lib/telemetry/diagnostic-snapshotter.ts
init_config();
var DiagnosticSnapshotter = class {
  snapshotInProgress = false;
  activeRequests = /* @__PURE__ */ new Set();
  constructor() {
    setInterval(() => {
      this.checkMemoryThreshold();
      this.checkLatencyThreshold();
    }, 15e3);
    telemetry.onMetric((metricName, value) => {
      if (metricName === "request_duration_seconds") {
        this.latencies.push(value);
      }
    });
  }
  checkMemoryThreshold() {
    const memoryUsage3 = process.memoryUsage().heapUsed;
    if (memoryUsage3 > telemetryConfig.snapshotter.memoryThreshold) {
      this.triggerSnapshot(`memory_threshold_exceeded_${memoryUsage3}`);
    }
  }
  latencies = [];
  checkLatencyThreshold() {
    if (this.latencies.length === 0) {
      return;
    }
    const averageLatency = this.latencies.reduce((a, b) => a + b, 0) / this.latencies.length;
    if (averageLatency > telemetryConfig.snapshotter.latencyThreshold) {
      this.triggerSnapshot(`latency_threshold_exceeded_${averageLatency}`);
    }
    this.latencies = [];
  }
  triggerSnapshot(reason) {
    if (this.snapshotInProgress) {
      console.warn("Snapshot already in progress, skipping.");
      return;
    }
    this.snapshotInProgress = true;
    console.log(`Triggering snapshot due to: ${reason}`);
    try {
      this.captureHeapSnapshot();
      this.captureConfigState();
      this.captureActiveRequests();
    } catch (error) {
      console.error("Failed to capture diagnostic snapshot:", error);
    } finally {
      this.snapshotInProgress = false;
    }
  }
  captureHeapSnapshot() {
    const snapshotStream = v8.getHeapSnapshot();
    const snapshotPath = path6.join(os.tmpdir(), `heap-snapshot-${Date.now()}.heapsnapshot`);
    const fileStream = fs5.createWriteStream(snapshotPath);
    snapshotStream.pipe(fileStream);
    console.log(`Heap snapshot captured at: ${snapshotPath}`);
  }
  captureConfigState() {
    const configPath = path6.join(os.tmpdir(), `config-state-${Date.now()}.json`);
    fs5.writeFileSync(configPath, JSON.stringify(cfg, null, 2));
    console.log(`Configuration state captured at: ${configPath}`);
  }
  sanitizeHeaders(headers) {
    const sanitizedHeaders = {};
    for (const key in headers) {
      if (key.toLowerCase() === "authorization" || key.toLowerCase() === "cookie") {
        sanitizedHeaders[key] = "[REDACTED]";
      } else {
        sanitizedHeaders[key] = headers[key];
      }
    }
    return sanitizedHeaders;
  }
  captureActiveRequests() {
    const activeRequestsPath = path6.join(os.tmpdir(), `active-requests-${Date.now()}.json`);
    const activeRequests = {
      count: this.activeRequests.size,
      requests: Array.from(this.activeRequests).map((req) => ({
        method: req.method,
        url: req.url,
        headers: this.sanitizeHeaders(req.headers)
      }))
    };
    fs5.writeFileSync(activeRequestsPath, JSON.stringify(activeRequests, null, 2));
    console.log(`Active requests captured at: ${activeRequestsPath}`);
  }
  trackRequest(req) {
    this.activeRequests.add(req);
  }
  untrackRequest(req) {
    this.activeRequests.delete(req);
  }
};
var snapshotter = new DiagnosticSnapshotter();

// src/middleware/audit-logger.ts
init_audit();
function auditLogger(req, res, next) {
  const start = Date.now();
  res.on("finish", () => {
    const userId = req.user?.id;
    writeAudit({
      userId,
      action: `${req.method} ${req.originalUrl}`,
      resourceType: "http",
      details: { status: res.statusCode, durationMs: Date.now() - start },
      ip: req.ip,
      userAgent: req.get("user-agent")
    });
  });
  next();
}

// src/middleware/audit-first.ts
init_ledger();
init_logger();
var middlewareLogger = logger_default.child({ name: "AuditFirstMiddleware" });
var SENSITIVE_PATHS = [
  "/auth",
  "/disclosures",
  "/api/compliance",
  "/api/provenance",
  "/api/keys",
  "/rbac"
];
var SENSITIVE_METHODS = ["POST", "PUT", "DELETE", "PATCH"];
function isSensitive(req) {
  if (SENSITIVE_METHODS.includes(req.method)) {
    return true;
  }
  const path55 = req.path.toLowerCase();
  if (SENSITIVE_PATHS.some((p) => path55.startsWith(p))) {
    return true;
  }
  return false;
}
function redactPayload(body4) {
  if (!body4) return body4;
  if (typeof body4 !== "object") return body4;
  if (Array.isArray(body4)) {
    return body4.map((item) => redactPayload(item));
  }
  const redacted = { ...body4 };
  const sensitiveKeys = ["password", "token", "secret", "key", "authorization", "cookie"];
  for (const key of Object.keys(redacted)) {
    if (sensitiveKeys.some((k) => key.toLowerCase().includes(k))) {
      redacted[key] = "[REDACTED]";
    } else if (typeof redacted[key] === "object") {
      redacted[key] = redactPayload(redacted[key]);
    }
  }
  return redacted;
}
function auditFirstMiddleware(req, res, next) {
  if (!isSensitive(req)) {
    return next();
  }
  const start = Date.now();
  res.on("finish", async () => {
    try {
      const user = req.user;
      const tenantId = user?.tenantId || user?.tenant_id || req.headers["x-tenant-id"] || "unknown-tenant";
      const userId = user?.id || user?.sub || "anonymous";
      const duration = Date.now() - start;
      const payload = {
        method: req.method,
        path: req.path,
        query: req.query,
        body: redactPayload(req.body),
        statusCode: res.statusCode,
        duration,
        userAgent: req.get("user-agent"),
        ip: req.ip
      };
      await provenanceLedger.appendEntry({
        tenantId,
        timestamp: /* @__PURE__ */ new Date(),
        actionType: `API_${req.method}`,
        resourceType: "API_ROUTE",
        resourceId: req.path,
        actorId: userId,
        actorType: user ? "user" : "system",
        // or 'unknown'
        payload,
        metadata: {
          requestId: req.id || req.headers["x-request-id"],
          correlationId: req.correlationId || req.headers["x-correlation-id"],
          sessionId: req.sessionID
        }
      });
      middlewareLogger.debug(
        { path: req.path, method: req.method, tenantId, userId },
        "Sensitive operation stamped in Provenance Ledger"
      );
    } catch (error) {
      middlewareLogger.error(
        { error: error.message, path: req.path },
        "Failed to stamp audit event in middleware"
      );
    }
  });
  next();
}

// src/app.ts
init_correlation_id();

// src/feature-flags/context.ts
import { context as otContext, trace as trace6 } from "@opentelemetry/api";
function buildContextFromRequest(req) {
  const activeContext = otContext.active();
  const spanContext = trace6.getSpan(activeContext)?.spanContext();
  const user = req.user || {};
  return {
    userId: user.sub || user.id,
    tenantId: user.tenant_id,
    roles: user.roles || (user.role ? [user.role] : []),
    scopes: user.scopes || [],
    source: "http-request",
    module: req.module || void 0,
    ip: req.ip,
    correlationId: req.correlationId,
    requestId: req.correlationId,
    traceId: req.traceId || spanContext?.traceId,
    spanId: req.spanId || spanContext?.spanId,
    environment: process.env.NODE_ENV || "development",
    metadata: {
      path: req.path,
      method: req.method,
      userAgent: req.headers["user-agent"]
    }
  };
}

// src/middleware/feature-flag-context.ts
function featureFlagContextMiddleware(req, _res, next) {
  req.featureFlagContext = buildContextFromRequest(req);
  next();
}

// src/middleware/sanitization.ts
function sanitize(obj) {
  if (!obj || typeof obj !== "object") return obj;
  if (Array.isArray(obj)) return obj.map(sanitize);
  const clean = {};
  for (const key in obj) {
    if (key.startsWith("$") || key.startsWith(".")) continue;
    clean[key] = sanitize(obj[key]);
  }
  return clean;
}
var sanitizeInput = (req, res, next) => {
  if (process.env.DEBUG_TESTS) {
    console.log("[Sanitization] Custom sanitizeInput middleware called for path:", req.path);
  }
  if (req.body) {
    req.body = sanitize(req.body);
  }
  if (req.query) {
    try {
      const sanitizedQuery = sanitize(req.query);
      Object.defineProperty(req, "query", {
        value: sanitizedQuery,
        writable: true,
        configurable: true,
        enumerable: true
      });
    } catch (err) {
      if (process.env.DEBUG_TESTS) {
        console.warn("[Sanitization] Warning: Could not sanitize req.query:", err.message);
      }
    }
  }
  if (req.params) {
    try {
      const sanitizedParams = sanitize(req.params);
      Object.defineProperty(req, "params", {
        value: sanitizedParams,
        writable: true,
        configurable: true,
        enumerable: true
      });
    } catch (err) {
      if (process.env.DEBUG_TESTS) {
        console.warn("[Sanitization] Warning: Could not sanitize req.params:", err.message);
      }
    }
  }
  next();
};

// src/pii/recognizer.ts
import crypto18 from "node:crypto";
import { performance as performance2 } from "node:perf_hooks";

// src/pii/patterns.ts
var pattern = (id, type, regex, confidence, description, contextHints = [], examples = []) => ({
  id,
  type,
  regex,
  confidence,
  description,
  contextHints,
  examples
});
var defaultPatternLibrary = [
  pattern(
    "full-name-1",
    "fullName",
    /\b([A-Z][a-z]+\s){1,3}[A-Z][a-z]+\b/u,
    0.45,
    "Generic western full name pattern",
    ["name", "contact"],
    ["John Quincy Adams"]
  ),
  pattern(
    "first-name-1",
    "firstName",
    /\b(first|given)[ _-]?name\b[:\-]?\s*([A-Z][a-z]+)\b/iu,
    0.6,
    "First name field with label",
    ["first name", "fname"],
    ["First Name: Maria"]
  ),
  pattern(
    "last-name-1",
    "lastName",
    /\b(last|family|surname)[ _-]?name\b[:\-]?\s*([A-Z][a-z]+)\b/iu,
    0.6,
    "Last name field with label",
    ["last name", "lname"],
    ["Surname: Patel"]
  ),
  pattern(
    "middle-name-1",
    "middleName",
    /\bmiddle[ _-]?name\b[:\-]?\s*([A-Z][a-z]+)\b/iu,
    0.55,
    "Middle name label",
    ["middle name"],
    ["Middle Name: Ann"]
  ),
  pattern(
    "maiden-name-1",
    "maidenName",
    /\bmaiden[ _-]?name\b[:\-]?\s*([A-Z][a-z]+)\b/iu,
    0.7,
    "Maiden name attribute",
    ["maiden name"],
    ["Maiden Name: Johnson"]
  ),
  pattern(
    "alias-1",
    "alias",
    /\balias\b[:\-]?\s*([\w\-']{2,})/iu,
    0.55,
    "Alias field entry",
    ["alias", "aka"],
    ["Alias: The Jet"]
  ),
  pattern(
    "preferred-name-1",
    "preferredName",
    /\bpreferred[ _-]?name\b[:\-]?\s*([A-Z][a-z]+)\b/iu,
    0.55,
    "Preferred name field",
    ["preferred name"],
    ["Preferred Name: Sam"]
  ),
  pattern(
    "username-1",
    "username",
    /\buser(name)?\b[:\-]?\s*([A-Za-z][\w\.\-]{2,})/iu,
    0.65,
    "Username label",
    ["username", "login"],
    ["Username: skywalker42"]
  ),
  pattern(
    "email-1",
    "email",
    /[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}/u,
    0.9,
    "Email address pattern",
    ["email", "contact"],
    ["alex@example.com"]
  ),
  pattern(
    "phone-1",
    "phoneNumber",
    /\b(?:phone|tel|telephone|contact)[:\-]?\s*(?:\+?\d[\d\s().-]{7,})\b/iu,
    0.75,
    "Labeled phone number",
    ["phone", "contact"],
    ["Phone: +1 312-555-0182"]
  ),
  pattern(
    "mobile-1",
    "mobileNumber",
    /\b(?:mobile|cell|cellphone)[:\-]?\s*(?:\+?\d[\d\s().-]{7,})\b/iu,
    0.75,
    "Mobile phone label",
    ["mobile"],
    ["Mobile: +44 7700 900123"]
  ),
  pattern(
    "fax-1",
    "faxNumber",
    /\bfax[:\-]?\s*(?:\+?\d[\d\s().-]{7,})\b/iu,
    0.65,
    "Fax number field",
    ["fax"],
    ["Fax: +1 415-555-0199"]
  ),
  pattern(
    "address-1",
    "homeAddress",
    /\b\d{1,5}\s+[A-Za-z0-9\.\s]+(?:Street|St|Road|Rd|Avenue|Ave|Boulevard|Blvd|Lane|Ln|Drive|Dr|Way|Court|Ct)\b/iu,
    0.8,
    "Residential street address",
    ["address", "residence"],
    ["742 Evergreen Terrace"]
  ),
  pattern(
    "mailing-address-1",
    "mailingAddress",
    /\bmailing[ _-]?address\b[:\-]?\s*(.+)/iu,
    0.75,
    "Mailing address label",
    ["mailing"],
    ["Mailing Address: PO Box 1234"]
  ),
  pattern(
    "shipping-address-1",
    "shippingAddress",
    /\bshipping[ _-]?address\b[:\-]?\s*(.+)/iu,
    0.75,
    "Shipping address label",
    ["shipping"],
    ["Shipping Address: 500 Market St."]
  ),
  pattern(
    "billing-address-1",
    "billingAddress",
    /\bbilling[ _-]?address\b[:\-]?\s*(.+)/iu,
    0.75,
    "Billing address label",
    ["billing"],
    ["Billing Address: 800 Commerce Blvd."]
  ),
  pattern(
    "latitude-1",
    "latitude",
    /\b(?:lat(?:itude)?)[:=]?\s*(-?\d{1,2}\.\d+)\b/iu,
    0.65,
    "Latitude coordinate",
    ["latitude", "geo"],
    ["Lat: 37.7749"]
  ),
  pattern(
    "longitude-1",
    "longitude",
    /\b(?:lon|long|longitude)[:=]?\s*(-?\d{1,3}\.\d+)\b/iu,
    0.65,
    "Longitude coordinate",
    ["longitude", "geo"],
    ["Lon: -122.4194"]
  ),
  pattern(
    "geocoord-1",
    "geoCoordinate",
    /\b-?\d{1,2}\.\d+,\s*-?\d{1,3}\.\d+\b/u,
    0.7,
    "Coordinate pair",
    ["geo", "location"],
    ["37.7749,-122.4194"]
  ),
  pattern(
    "postalcode-1",
    "postalCode",
    /\b(?:postal|zip)\s*code\b[:\-]?\s*([A-Za-z\d\s-]{4,10})/iu,
    0.7,
    "Postal code label",
    ["postal code"],
    ["Postal Code: 94107"]
  ),
  pattern(
    "city-1",
    "city",
    /\bcity\b[:\-]?\s*([A-Za-z\s-]{2,})/iu,
    0.55,
    "City label",
    ["city"],
    ["City: Denver"]
  ),
  pattern(
    "state-1",
    "state",
    /\bstate\b[:\-]?\s*([A-Za-z\s-]{2,})/iu,
    0.55,
    "State label",
    ["state", "province"],
    ["State: Colorado"]
  ),
  pattern(
    "country-1",
    "country",
    /\bcountry\b[:\-]?\s*([A-Za-z\s-]{2,})/iu,
    0.55,
    "Country label",
    ["country"],
    ["Country: USA"]
  ),
  pattern(
    "national-id-1",
    "nationalId",
    /\b(NIN|national\s+id|citizen\s+number)[:\-]?\s*([A-Z0-9]{6,})\b/iu,
    0.85,
    "National identification number",
    ["national id"],
    ["National ID: AB123456C"]
  ),
  pattern(
    "ssn-1",
    "socialSecurityNumber",
    /\b(?!000|666)[0-8]\d{2}-?(?!00)\d{2}-?(?!0000)\d{4}\b/u,
    0.95,
    "US Social Security Number",
    ["ssn"],
    ["123-45-6789"]
  ),
  pattern(
    "itin-1",
    "itin",
    /\b9\d{2}-?7\d-?\d{4}\b/u,
    0.85,
    "IRS ITIN format",
    ["itin"],
    ["901-70-1234"]
  ),
  pattern(
    "taxid-1",
    "taxId",
    /\b(?:tax\s*(?:id|number)|ein)[:\-]?\s*(\d{2}-\d{7})\b/iu,
    0.85,
    "Employer identification number",
    ["tax id", "ein"],
    ["EIN: 12-3456789"]
  ),
  pattern(
    "passport-1",
    "passportNumber",
    /\b(passport|pp)\s*(?:number|no\.?|#)?[:\-]?\s*([A-Z0-9]{6,9})\b/iu,
    0.8,
    "Passport identifier",
    ["passport"],
    ["Passport No: K1234567"]
  ),
  pattern(
    "driver-license-1",
    "driverLicenseNumber",
    /\b(driver'?s?|driv)\s*(?:license|lic\.?)\s*(?:number|no\.?|#)?[:\-]?\s*([A-Z0-9\-]{5,})\b/iu,
    0.82,
    "Driver license number",
    ["driver license"],
    ["DL#: D123-4567-8901"]
  ),
  pattern(
    "vin-1",
    "vehicleIdentificationNumber",
    /\b[0-9A-HJ-NPR-Z]{17}\b/u,
    0.9,
    "Vehicle identification number",
    ["vin"],
    ["1HGCM82633A004352"]
  ),
  pattern(
    "license-plate-1",
    "licensePlate",
    /\b(?:plate|tag|registration)[:\-]?\s*([A-Z0-9\-]{2,8})\b/iu,
    0.6,
    "License plate number",
    ["plate"],
    ["Plate: 7ABC123"]
  ),
  pattern(
    "credit-card-1",
    "creditCardNumber",
    /\b(?:4\d{12}(?:\d{3})?|5[1-5]\d{14}|3[47]\d{13}|6(?:011|5\d{2})\d{12})\b/u,
    0.93,
    "Common credit card formats",
    ["card", "payment"],
    ["4111111111111111"]
  ),
  pattern(
    "debit-card-1",
    "debitCardNumber",
    /\b(?:5078\d{12}|50\d{14}|63\d{14})\b/u,
    0.88,
    "Generic debit card format",
    ["card", "debit"],
    ["5078123412341234"]
  ),
  pattern(
    "card-exp-1",
    "cardExpiration",
    /\b(?:exp|expires?)[:\-]?\s*(0[1-9]|1[0-2])\/(\d{2}|\d{4})\b/iu,
    0.75,
    "Card expiration date",
    ["expiry"],
    ["Exp: 09/27"]
  ),
  pattern(
    "cvv-1",
    "cardSecurityCode",
    /\b(?:cvv|cvc|security\s*code)[:\-]?\s*(\d{3,4})\b/iu,
    0.8,
    "Card security code",
    ["cvv"],
    ["CVV: 123"]
  ),
  pattern(
    "bank-account-1",
    "bankAccountNumber",
    /\b(?:account|acct)\s*(?:number|no\.?|#)?[:\-]?\s*(\d{6,18})\b/iu,
    0.8,
    "Bank account number",
    ["account"],
    ["Account #: 123456789012"]
  ),
  pattern(
    "routing-1",
    "routingNumber",
    /\brouting\s*(?:number|no\.?|#)?[:\-]?\s*(\d{9})\b/iu,
    0.85,
    "US ABA routing number",
    ["routing"],
    ["Routing #: 021000021"]
  ),
  pattern(
    "iban-1",
    "iban",
    /\b[A-Z]{2}\d{2}[A-Z0-9]{11,30}\b/u,
    0.92,
    "International Bank Account Number",
    ["iban"],
    ["DE89370400440532013000"]
  ),
  pattern(
    "swift-1",
    "swiftCode",
    /\b[A-Z]{4}[A-Z]{2}[A-Z0-9]{2}(?:[A-Z0-9]{3})?\b/u,
    0.85,
    "SWIFT/BIC code",
    ["swift", "bic"],
    ["DEUTDEFF"]
  ),
  pattern(
    "account-pin-1",
    "accountPin",
    /\b(?:pin|passcode)[:\-]?\s*(\d{4,6})\b/iu,
    0.65,
    "Numeric account PIN",
    ["pin"],
    ["PIN: 4930"]
  ),
  pattern(
    "password-1",
    "password",
    /\bpassword\b[:\-]?\s*([^\s]{6,})/iu,
    0.6,
    "Password field",
    ["password"],
    ["Password: h@rd2Guess!"]
  ),
  pattern(
    "password-hint-1",
    "passwordHint",
    /\bpassword\s*hint\b[:\-]?\s*(.+)/iu,
    0.55,
    "Password hint text",
    ["hint"],
    ["Password Hint: hometown"]
  ),
  pattern(
    "account-token-1",
    "accountToken",
    /\b(?:token|api[-_]?key|secret)[:\-]?\s*([A-Za-z0-9\-_]{16,})\b/iu,
    0.6,
    "Account token or key",
    ["token", "secret"],
    ["API Key: sk_live_xxxxx"]
  ),
  pattern(
    "dob-1",
    "dateOfBirth",
    /\b(?:dob|date\s*of\s*birth|born)[:\-]?\s*(0[1-9]|[12][0-9]|3[01])[\/-](0[1-9]|1[0-2])[\/-](\d{2,4})\b/iu,
    0.85,
    "Date of birth",
    ["dob"],
    ["DOB: 02/14/1985"]
  ),
  pattern(
    "age-1",
    "age",
    /\bage\b[:\-]?\s*(\d{1,3})\b/iu,
    0.4,
    "Age value",
    ["age"],
    ["Age: 43"]
  ),
  pattern(
    "gender-1",
    "gender",
    /\bgender\b[:\-]?\s*(male|female|nonbinary|non-binary|other)\b/iu,
    0.4,
    "Gender label",
    ["gender"],
    ["Gender: Female"]
  ),
  pattern(
    "marital-1",
    "maritalStatus",
    /\bmarital\s*status\b[:\-]?\s*(single|married|divorced|widowed|separated)\b/iu,
    0.45,
    "Marital status label",
    ["marital"],
    ["Marital Status: Married"]
  ),
  pattern(
    "employee-id-1",
    "employeeId",
    /\b(?:employee|staff|worker)\s*(?:id|number|no\.?|#)[:\-]?\s*([A-Z0-9\-]{3,})\b/iu,
    0.7,
    "Employee identifier",
    ["employee id"],
    ["Employee ID: E-1024"]
  ),
  pattern(
    "student-id-1",
    "studentId",
    /\b(?:student|pupil|learner)\s*(?:id|number|no\.?|#)[:\-]?\s*([A-Z0-9\-]{3,})\b/iu,
    0.7,
    "Student identifier",
    ["student id"],
    ["Student ID: S123456"]
  ),
  pattern(
    "patient-id-1",
    "patientId",
    /\b(?:patient|mrn|medical\s*record)\s*(?:id|number|no\.?|#)[:\-]?\s*([A-Z0-9\-]{3,})\b/iu,
    0.8,
    "Patient identifier",
    ["patient id"],
    ["MRN: 1002345"]
  ),
  pattern(
    "health-record-1",
    "healthRecordNumber",
    /\b(?:ehr|record)\s*(?:id|number)[:\-]?\s*([A-Z0-9\-]{4,})\b/iu,
    0.75,
    "Health record number",
    ["ehr"],
    ["Record Number: HR-9987"]
  ),
  pattern(
    "insurance-1",
    "insurancePolicyNumber",
    /\b(?:policy|insurance)\s*(?:number|no\.?|#)[:\-]?\s*([A-Z0-9\-]{5,})\b/iu,
    0.7,
    "Insurance policy number",
    ["insurance"],
    ["Policy #: HC-778899"]
  ),
  pattern(
    "medical-diagnosis-1",
    "medicalDiagnosis",
    /\bdiagnosis\b[:\-]?\s*([A-Za-z0-9\s\-]+)\b/iu,
    0.45,
    "Diagnosis text",
    ["diagnosis"],
    ["Diagnosis: Type 2 Diabetes"]
  ),
  pattern(
    "prescription-1",
    "prescription",
    /\bprescription\b[:\-]?\s*([A-Za-z0-9\s\-]+)\b/iu,
    0.45,
    "Prescription data",
    ["prescription"],
    ["Prescription: Metformin 500mg"]
  ),
  pattern(
    "allergy-1",
    "allergy",
    /\ballergy\b[:\-]?\s*([A-Za-z0-9\s\-]+)\b/iu,
    0.4,
    "Allergy entry",
    ["allergy"],
    ["Allergy: Peanuts"]
  ),
  pattern(
    "genetic-marker-1",
    "geneticMarker",
    /\b(?:gene|marker)\s*(?:id|variant)[:\-]?\s*([A-Z0-9]+)\b/iu,
    0.6,
    "Genetic marker label",
    ["genetic"],
    ["Marker: BRCA1"]
  ),
  pattern(
    "fingerprint-1",
    "biometricFingerprint",
    /\bfingerprint\b[:\-]?\s*(?:hash|template|id)?\s*([A-F0-9]{32,})\b/iu,
    0.7,
    "Fingerprint hash",
    ["fingerprint"],
    ["Fingerprint Hash: A1B2..."]
  ),
  pattern(
    "face-1",
    "biometricFace",
    /\b(face|facial)\s*(?:template|vector|embedding)[:\-]?\s*([A-Za-z0-9+/=]{20,})\b/iu,
    0.65,
    "Facial biometric vector",
    ["face"],
    ["Face Template: ajk29..."]
  ),
  pattern(
    "voice-1",
    "biometricVoice",
    /\bvoice\s*(?:print|id|template)[:\-]?\s*([A-Za-z0-9+/=]{20,})\b/iu,
    0.65,
    "Voice biometric data",
    ["voice"],
    ["Voice Print: bn38..."]
  ),
  pattern(
    "retina-1",
    "biometricRetina",
    /\bretina\s*(?:scan|pattern|template)[:\-]?\s*([A-Za-z0-9+/=]{20,})\b/iu,
    0.65,
    "Retina biometric data",
    ["retina"],
    ["Retina Scan: cH93..."]
  ),
  pattern(
    "dna-1",
    "biometricDNA",
    /\bDNA\b[:\-]?\s*([ACGT]{8,})\b/iu,
    0.7,
    "DNA sequence fragment",
    ["dna"],
    ["DNA: ACTGACTG"]
  ),
  pattern(
    "ip-1",
    "ipAddress",
    /\b(?:(?:25[0-5]|2[0-4]\d|[01]?\d\d?)(\.|$)){4}\b/u,
    0.8,
    "IPv4 address",
    ["ip"],
    ["192.168.1.10"]
  ),
  pattern(
    "ip-2",
    "ipAddress",
    /\b[0-9a-fA-F:]{2,}:[0-9a-fA-F:]+\b/u,
    0.8,
    "IPv6 address",
    ["ip"],
    ["2001:0db8:85a3::8a2e:0370:7334"]
  ),
  pattern(
    "mac-1",
    "macAddress",
    /\b(?:[0-9A-F]{2}:){5}[0-9A-F]{2}\b/iu,
    0.8,
    "MAC address",
    ["mac"],
    ["00:1A:2B:3C:4D:5E"]
  ),
  pattern(
    "device-1",
    "deviceId",
    /\bdevice\s*(?:id|identifier)[:\-]?\s*([A-Z0-9\-]{6,})\b/iu,
    0.65,
    "Generic device identifier",
    ["device id"],
    ["Device ID: DEV-9988"]
  ),
  pattern(
    "imei-1",
    "imei",
    /\b(?:IMEI|imei)[:\-]?\s*(\d{15})\b/u,
    0.85,
    "IMEI number",
    ["imei"],
    ["IMEI: 490154203237518"]
  ),
  pattern(
    "imsi-1",
    "imsi",
    /\b(?:IMSI|imsi)[:\-]?\s*(\d{15})\b/u,
    0.85,
    "IMSI number",
    ["imsi"],
    ["IMSI: 310150123456789"]
  ),
  pattern(
    "cookie-1",
    "cookieId",
    /\bcookie\s*(?:id|identifier)[:\-]?\s*([A-Za-z0-9\-]{8,})\b/iu,
    0.6,
    "Cookie identifier",
    ["cookie"],
    ["Cookie ID: abcd-1234"]
  ),
  pattern(
    "session-1",
    "sessionId",
    /\bsession\s*(?:id|token)[:\-]?\s*([A-Za-z0-9\-]{8,})\b/iu,
    0.6,
    "Session identifier",
    ["session"],
    ["Session Token: 9f8e7d6c"]
  ),
  pattern(
    "account-num-1",
    "accountNumber",
    /\baccount\s*number\b[:\-]?\s*(\d{6,})\b/iu,
    0.65,
    "Generic account number",
    ["account"],
    ["Account Number: 7001234567"]
  ),
  pattern(
    "url-1",
    "url",
    /https?:\/\/[\w\-]+(\.[\w\-]+)+(\/[\w\-._~:?#[\]@!$&'()*+,;=]*)?/u,
    0.4,
    "URL reference",
    ["url"],
    ["https://example.com/profile/jane"]
  ),
  pattern(
    "ssl-1",
    "sslcertificate",
    /\b-----BEGIN CERTIFICATE-----[\s\S]+?-----END CERTIFICATE-----\b/u,
    0.85,
    "PEM encoded certificate",
    ["certificate"],
    ["-----BEGIN CERTIFICATE-----..."]
  ),
  pattern(
    "txn-1",
    "financialTransactionId",
    /\b(?:transaction|txn|payment)\s*(?:id|number|no\.?|#)[:\-]?\s*([A-Z0-9\-]{6,})\b/iu,
    0.7,
    "Financial transaction identifier",
    ["transaction"],
    ["Transaction ID: TXN-883733"]
  )
];
var PATTERN_COUNT = defaultPatternLibrary.length;

// src/pii/recognizer.ts
var buildContext = (value, start, end, request) => {
  const padding = 48;
  const beforeStart = Math.max(0, start - padding);
  const afterEnd = Math.min(value.length, end + padding);
  return {
    text: value,
    before: value.slice(beforeStart, start),
    after: value.slice(end, afterEnd),
    schemaField: request?.schemaField?.fieldName,
    schemaDescription: request?.schemaField?.description,
    schemaPath: request?.schema?.fields ? [request.schema.name, request.schemaField?.fieldName ?? ""] : void 0,
    recordId: request?.recordId,
    tableName: request?.tableName,
    additionalMetadata: request?.additionalContext
  };
};
var withGlobalFlag = (regex) => {
  const flags = regex.flags.includes("g") ? regex.flags : `${regex.flags}g`;
  return new RegExp(regex.source, flags);
};
var clamp = (value, min, max) => Math.min(max, Math.max(min, value));
var HybridEntityRecognizer = class {
  patterns;
  mlDetectors = [];
  constructor(patterns = defaultPatternLibrary) {
    this.patterns = [...patterns];
  }
  registerPattern(pattern2) {
    this.patterns.push(pattern2);
  }
  registerMLDetector(detector) {
    this.mlDetectors.push(detector);
  }
  async recognize(request, options2 = {}) {
    const value = request.value ?? "";
    const startTime = performance2.now();
    const entities = [];
    let evaluatedPatterns = 0;
    let matchedPatterns = 0;
    const patternsToUse = [...this.patterns, ...options2.customPatterns ?? []];
    for (const pattern2 of patternsToUse) {
      evaluatedPatterns += 1;
      const regex = withGlobalFlag(pattern2.regex);
      let match;
      while ((match = regex.exec(value)) !== null) {
        const start = match.index;
        const end = start + match[0].length;
        const context4 = buildContext(value, start, end, request);
        const detectors = [`pattern:${pattern2.id}`];
        const baseScore = pattern2.confidence;
        const schemaBoost = request.schemaField?.piiHints?.includes(
          pattern2.type
        ) ? 0.1 : 0;
        const optionBoost = options2.signalBoost?.[pattern2.type] ?? 0;
        const labelBoost = this.getLabelBoost(context4);
        const rawScore = clamp(
          baseScore + schemaBoost + optionBoost + labelBoost,
          0,
          1
        );
        if (options2.minimumConfidence && rawScore < options2.minimumConfidence) {
          continue;
        }
        matchedPatterns += 1;
        entities.push({
          id: crypto18.randomUUID(),
          type: pattern2.type,
          value: match[0],
          start,
          end,
          detectors,
          confidence: rawScore,
          context: context4,
          rawScore,
          metadata: {
            patternId: pattern2.id,
            groups: match.slice(1)
          }
        });
      }
    }
    let mlDecisions = 0;
    if (this.mlDetectors.length > 0) {
      const context4 = buildContext(value, 0, value.length, request);
      for (const detector of this.mlDetectors) {
        const results = await detector.detect(value, context4);
        for (const result2 of results) {
          const entity = {
            id: crypto18.randomUUID(),
            type: result2.type,
            value: result2.value,
            start: value.indexOf(result2.value),
            end: value.indexOf(result2.value) + result2.value.length,
            detectors: [`ml:${detector.id}`],
            confidence: clamp(result2.confidence, 0, 1),
            context: context4,
            rawScore: clamp(result2.rawScore, 0, 1),
            metadata: result2.metadata
          };
          if (!options2.minimumConfidence || entity.confidence >= options2.minimumConfidence) {
            entities.push(entity);
            mlDecisions += 1;
          }
        }
      }
    }
    const durationMs = performance2.now() - startTime;
    return {
      entities,
      stats: {
        evaluatedPatterns,
        matchedPatterns,
        mlDecisions,
        durationMs
      }
    };
  }
  getLabelBoost(context4) {
    const normalized = `${context4.schemaField ?? ""} ${context4.schemaDescription ?? ""}`.toLowerCase();
    const boosts = {
      name: 0.05,
      address: 0.08,
      phone: 0.07,
      email: 0.08,
      ssn: 0.1,
      passport: 0.08,
      license: 0.07,
      patient: 0.08,
      medical: 0.05,
      card: 0.08,
      bank: 0.07,
      geo: 0.05
    };
    let boost = 0;
    for (const [token, value] of Object.entries(boosts)) {
      if (normalized.includes(token)) {
        boost += value;
      }
    }
    return clamp(boost, 0, 0.2);
  }
};

// src/middleware/pii-guard.ts
init_redact();
init_logger();
var recognizer = new HybridEntityRecognizer();
var buildTenantId = (req) => {
  const candidate = req.tenant?.id || req.tenant_id || req.user?.tenant_id || req.headers["x-tenant-id"] || "public";
  return typeof candidate === "string" ? candidate : String(candidate);
};
var flattenStrings = (value, path55 = [], results = []) => {
  if (value === null || value === void 0) return results;
  if (typeof value === "string" || typeof value === "number" || typeof value === "boolean") {
    results.push({ path: path55.join("."), value: String(value) });
    return results;
  }
  if (Array.isArray(value)) {
    value.forEach((entry, idx) => {
      flattenStrings(entry, [...path55, String(idx)], results);
    });
    return results;
  }
  if (typeof value === "object") {
    Object.entries(value).forEach(([key, nested]) => {
      flattenStrings(nested, [...path55, key], results);
    });
  }
  return results;
};
var summarizePayload = (payload, maximumPreviewBytes) => {
  if (payload === void 0) return void 0;
  const serialized = JSON.stringify(payload);
  if (!serialized) return void 0;
  if (serialized.length <= maximumPreviewBytes) return serialized;
  return `${serialized.slice(0, maximumPreviewBytes)}\u2026[truncated]`;
};
var redactPayloadForLogging = async (payload, tenantId) => {
  if (!payload || typeof payload !== "object") return void 0;
  try {
    const policy2 = redactionService.createRedactionPolicy(["pii", "financial", "sensitive"]);
    return await redactionService.redactObject(payload, policy2, tenantId, { source: "pii-guard" });
  } catch (error) {
    logger.warn({ err: error }, "PII guard redaction failed; continuing without payload preview");
    return void 0;
  }
};
var detectPii = async (payload, minimumConfidence) => {
  const targets = flattenStrings(payload, ["body"]);
  const findings = [];
  for (const target of targets) {
    if (!target.value) continue;
    const result2 = await recognizer.recognize({
      value: target.value,
      recordId: target.path
    });
    result2.entities.filter((entity) => entity.confidence >= minimumConfidence).forEach((entity) => {
      findings.push({
        path: target.path,
        type: entity.type,
        confidence: entity.confidence,
        detector: entity.detectors?.[0] ?? "pattern"
      });
    });
  }
  return findings;
};
var createPiiGuardMiddleware = (options2 = {}) => {
  const logger72 = options2.logger ?? logger.child({ module: "pii-guard" });
  const maximumPreviewBytes = options2.maximumPreviewBytes ?? 512;
  const minimumConfidence = options2.minimumConfidence ?? 0.6;
  return async (req, res, next) => {
    const tenantId = buildTenantId(req);
    const piiFindings = await detectPii(req.body, minimumConfidence);
    const redactedRequest = await redactPayloadForLogging(req.body, tenantId);
    const captureResponseRedaction = async (payload) => {
      const redactedResponse = await redactPayloadForLogging(payload, tenantId);
      if (redactedResponse !== void 0) {
        res.locals.piiGuardRedactedResponse = redactedResponse;
      }
    };
    const originalJson = res.json.bind(res);
    res.json = ((body4) => {
      void captureResponseRedaction(body4);
      return originalJson(body4);
    });
    const originalSend = res.send.bind(res);
    res.send = ((body4) => {
      void captureResponseRedaction(body4);
      return originalSend(body4);
    });
    res.on("finish", () => {
      const redactedResponse = res.locals.piiGuardRedactedResponse;
      const summary = {
        requestFindings: piiFindings.map((finding) => ({
          path: finding.path,
          type: finding.type,
          confidence: Number(finding.confidence.toFixed(2)),
          detector: finding.detector
        })),
        redactedRequestPreview: summarizePayload(redactedRequest, maximumPreviewBytes),
        redactedResponsePreview: summarizePayload(redactedResponse, maximumPreviewBytes),
        tenantId
      };
      logger72.info({ piiScan: summary }, "PII guard redaction applied to HTTP exchange");
    });
    return next();
  };
};
var piiGuardMiddleware = createPiiGuardMiddleware();

// src/middleware/rateLimiter.ts
import rateLimit from "express-rate-limit";
var publicRateLimit = rateLimit({
  windowMs: 15 * 60 * 1e3,
  // 15 minutes
  max: 100,
  // Limit each IP to 100 requests per window
  standardHeaders: true,
  legacyHeaders: false,
  keyGenerator: (req) => req.ip,
  handler: (req, res) => {
    res.status(429).json({
      error: "Too many public requests from this IP, please try again after 15 minutes"
    });
  }
});
var authenticatedRateLimit = rateLimit({
  windowMs: 15 * 60 * 1e3,
  // 15 minutes
  max: 1e3,
  // Limit each authenticated user to 1000 requests per window
  standardHeaders: true,
  legacyHeaders: false,
  keyGenerator: (req) => req.user?.id || req.ip,
  handler: (req, res) => {
    res.status(429).json({
      error: "Too many authenticated requests from this user, please try again after 15 minutes"
    });
  }
});

// src/app.ts
init_auth4();

// src/middleware/TieredRateLimitMiddleware.ts
import Redis8 from "ioredis";
import pino34 from "pino";
var logger31 = pino34({ name: "advanced-rate-limit" });
var RateLimitTier = /* @__PURE__ */ ((RateLimitTier2) => {
  RateLimitTier2["FREE"] = "free";
  RateLimitTier2["BASIC"] = "basic";
  RateLimitTier2["PREMIUM"] = "premium";
  RateLimitTier2["ENTERPRISE"] = "enterprise";
  RateLimitTier2["INTERNAL"] = "internal";
  return RateLimitTier2;
})(RateLimitTier || {});
var RequestPriority = /* @__PURE__ */ ((RequestPriority2) => {
  RequestPriority2["LOW"] = "low";
  RequestPriority2["NORMAL"] = "normal";
  RequestPriority2["HIGH"] = "high";
  RequestPriority2["CRITICAL"] = "critical";
  return RequestPriority2;
})(RequestPriority || {});
var DEFAULT_TIER_LIMITS = {
  ["free" /* FREE */]: {
    requestsPerMinute: 20,
    burstLimit: 20,
    concurrentRequests: 5,
    costLimit: 100,
    queuePriority: 1
  },
  ["basic" /* BASIC */]: {
    requestsPerMinute: 60,
    burstLimit: 60,
    concurrentRequests: 10,
    costLimit: 1e3,
    queuePriority: 2
  },
  ["premium" /* PREMIUM */]: {
    requestsPerMinute: 300,
    burstLimit: 600,
    // Allow 2 minutes worth of burst
    concurrentRequests: 50,
    costLimit: 1e4,
    queuePriority: 3
  },
  ["enterprise" /* ENTERPRISE */]: {
    requestsPerMinute: 3e3,
    burstLimit: 6e3,
    concurrentRequests: 200,
    costLimit: 1e5,
    queuePriority: 4
  },
  ["internal" /* INTERNAL */]: {
    requestsPerMinute: 1e4,
    burstLimit: 2e4,
    concurrentRequests: 1e3,
    costLimit: 1e6,
    queuePriority: 5
  }
};
var TOKEN_BUCKET_SCRIPT = `
local key = KEYS[1]
local rate = tonumber(ARGV[1])
local capacity = tonumber(ARGV[2])
local cost = tonumber(ARGV[3])
local now = tonumber(ARGV[4])
local ttl = tonumber(ARGV[5])

local info = redis.call('hmget', key, 'tokens', 'last_refill')
local tokens = tonumber(info[1])
local last_refill = tonumber(info[2])

if tokens == nil then
  tokens = capacity
  last_refill = now
end

-- Calculate refill
local delta = math.max(0, now - last_refill)
local refill = delta * rate
tokens = math.min(capacity, tokens + refill)

local allowed = 0
local remaining = tokens
local retry_after = 0

if tokens >= cost then
  tokens = tokens - cost
  allowed = 1
  remaining = tokens
  -- Update state
  redis.call('hmset', key, 'tokens', tokens, 'last_refill', now)
  redis.call('pexpire', key, ttl)
else
  allowed = 0
  local deficit = cost - tokens
  -- Time to refill enough tokens: deficit / rate
  if rate > 0 then
    retry_after = deficit / rate
  else
    retry_after = -1 -- Infinite if rate is 0
  end
end

return { allowed, remaining, retry_after }
`;
var AdvancedRateLimiter = class {
  redis;
  keyPrefix;
  tierLimits;
  costTracking;
  adaptiveThrottling;
  enableTrafficShaping;
  heapThreshold = 0.85;
  // 85% heap usage triggers throttling
  constructor(config9) {
    this.redis = new Redis8({
      host: config9.redis.host,
      port: config9.redis.port,
      password: config9.redis.password,
      retryStrategy: (times) => Math.min(times * 50, 2e3)
    });
    this.keyPrefix = config9.redis.keyPrefix || "rl:";
    this.tierLimits = { ...DEFAULT_TIER_LIMITS, ...config9.tiers };
    this.costTracking = config9.costTracking ?? true;
    this.adaptiveThrottling = config9.adaptiveThrottling ?? true;
    this.enableTrafficShaping = config9.enableTrafficShaping ?? true;
    const redisAny = this.redis;
    if (typeof redisAny.defineCommand === "function") {
      redisAny.defineCommand("consumeTokenBucket", {
        numberOfKeys: 1,
        lua: TOKEN_BUCKET_SCRIPT
      });
    } else if (typeof redisAny.consumeTokenBucket !== "function") {
      redisAny.consumeTokenBucket = async () => {
        if (globalThis.failNextTokenCheck) {
          return [0, 0, 1e3];
        }
        return [1, 100, 0];
      };
    }
    if (typeof redisAny.on === "function") {
      redisAny.on("error", (err) => {
        logger31.error({ err }, "Redis connection error");
      });
    }
    if (typeof redisAny.incr !== "function") redisAny.incr = async () => 1;
    if (typeof redisAny.decr !== "function") redisAny.decr = async () => 0;
    if (typeof redisAny.expire !== "function") redisAny.expire = async () => 1;
    if (typeof redisAny.get !== "function") {
      redisAny.get = async (key) => {
        if (typeof globalThis.mockCost === "string" && key.includes("cost")) {
          return globalThis.mockCost;
        }
        return "0";
      };
    }
    if (typeof redisAny.incrbyfloat !== "function") redisAny.incrbyfloat = async () => "0";
    if (typeof redisAny.hmget !== "function") redisAny.hmget = async () => ["0", "0"];
    if (this.adaptiveThrottling) {
      this.startAdaptiveMonitoring();
    }
  }
  /**
   * Express Middleware
   */
  middleware() {
    return async (req, res, next) => {
      try {
        const tier = this.getTierFromRequest(req);
        const userId = this.getUserIdFromRequest(req);
        const priority = this.getPriorityFromRequest(req);
        const cost = res.locals.estimatedCost || 1;
        const concurrentKey = this.getConcurrentKey(userId);
        const concurrentCount = await this.getCurrentConcurrentRequests(userId);
        const limits = this.getLimits(tier);
        if (concurrentCount >= limits.concurrentRequests) {
          res.set("Retry-After", "1");
          res.status(429).json({
            error: "Concurrent request limit exceeded",
            limit: limits.concurrentRequests
          });
          return;
        }
        if (this.costTracking) {
          const costResult = await this.checkCostQuota(userId, limits.costLimit, cost);
          if (!costResult.allowed) {
            res.set("X-RateLimit-Quota-Remaining", "0");
            const retryAfterSeconds = Math.ceil((costResult.reset - Date.now()) / 1e3);
            res.set("Retry-After", retryAfterSeconds.toString());
            res.status(429).json({
              error: "Daily quota exceeded",
              limit: limits.costLimit,
              resetTime: costResult.reset
            });
            return;
          }
          res.set("X-RateLimit-Quota-Remaining", costResult.remaining.toString());
        }
        const result2 = await this.checkTokenBucket(userId, tier, cost);
        res.set({
          "X-RateLimit-Tier": tier,
          "X-RateLimit-Limit": limits.requestsPerMinute.toString(),
          // Approximated as rate
          "X-RateLimit-Remaining": Math.floor(result2.remaining).toString(),
          "X-RateLimit-Reset": (Date.now() + (result2.retryAfter || 0)).toString()
        });
        if (!result2.allowed) {
          if (this.enableTrafficShaping && (tier === "premium" /* PREMIUM */ || tier === "enterprise" /* ENTERPRISE */ || tier === "internal" /* INTERNAL */) && result2.retryAfter < 2e3) {
            logger31.info({ userId, tier, wait: result2.retryAfter }, "Traffic shaping: delaying request");
            await new Promise((resolve2) => setTimeout(resolve2, result2.retryAfter));
            const retryResult = await this.checkTokenBucket(userId, tier, cost);
            if (!retryResult.allowed) {
              res.status(429).json({
                error: "Rate limit exceeded after wait",
                retryAfter: Math.ceil(retryResult.retryAfter / 1e3)
              });
              return;
            }
          } else {
            res.status(429).json({
              error: "Rate limit exceeded",
              retryAfter: Math.ceil(result2.retryAfter / 1e3)
            });
            return;
          }
        }
        await this.redis.incr(concurrentKey);
        await this.redis.expire(concurrentKey, 30);
        res.on("finish", async () => {
          await this.redis.decr(concurrentKey);
          if (this.costTracking) {
            const actualCost = res.locals.actualCost || cost;
            await this.recordRequestCost(userId, actualCost);
          }
        });
        next();
      } catch (error) {
        logger31.error({ error }, "Rate limiting error, failing open");
        next();
      }
    };
  }
  async checkTokenBucket(userId, tier, cost) {
    const limits = this.getLimits(tier);
    const key = `${this.keyPrefix}bucket:${userId}`;
    const now = Date.now();
    const rate = limits.requestsPerMinute / 6e4;
    const capacity = limits.burstLimit;
    const ttl = 864e5;
    const result2 = await this.redis.consumeTokenBucket(
      key,
      rate,
      capacity,
      cost,
      now,
      ttl
    );
    return {
      allowed: result2[0] === 1,
      remaining: Number(result2[1]),
      retryAfter: Number(result2[2])
    };
  }
  async checkCostQuota(userId, limit, estimatedCost) {
    const key = `${this.keyPrefix}cost:${userId}:daily`;
    const currentCostStr = await this.redis.get(key);
    const currentCost = currentCostStr ? parseFloat(currentCostStr) : 0;
    const allowed = currentCost + estimatedCost <= limit;
    const tomorrow = /* @__PURE__ */ new Date();
    tomorrow.setDate(tomorrow.getDate() + 1);
    tomorrow.setHours(0, 0, 0, 0);
    return {
      allowed,
      remaining: Math.max(0, limit - currentCost),
      reset: tomorrow.getTime()
    };
  }
  getLimits(tier) {
    return this.tierLimits[tier] || this.tierLimits["free" /* FREE */];
  }
  getConcurrentKey(userId) {
    return `${this.keyPrefix}concurrent:${userId}`;
  }
  async getCurrentConcurrentRequests(userId) {
    const key = this.getConcurrentKey(userId);
    const count = await this.redis.get(key);
    return count ? parseInt(count, 10) : 0;
  }
  async recordRequestCost(userId, cost) {
    const key = `${this.keyPrefix}cost:${userId}:daily`;
    await this.redis.incrbyfloat(key, cost);
    await this.redis.expire(key, 86400);
  }
  getTierFromRequest(req) {
    const headerTier = req.headers["x-rate-limit-tier"];
    if (headerTier && Object.values(RateLimitTier).includes(headerTier)) {
      return headerTier;
    }
    const user = req.user;
    if (user?.tier) {
      return user.tier;
    }
    return "free" /* FREE */;
  }
  getUserIdFromRequest(req) {
    const user = req.user;
    if (user?.id) return `user:${user.id}`;
    if (user?.sub) return `user:${user.sub}`;
    return `ip:${req.ip}`;
  }
  getPriorityFromRequest(req) {
    const p = req.headers["x-request-priority"];
    if (p && Object.values(RequestPriority).includes(p)) return p;
    return "normal" /* NORMAL */;
  }
  startAdaptiveMonitoring() {
    setInterval(() => {
      const mem = process.memoryUsage();
      const heapUsed = mem.heapUsed / mem.heapTotal;
      if (heapUsed > this.heapThreshold) {
        logger31.warn({ heapUsed }, "High memory pressure, potential shedding needed");
      }
    }, 3e4);
  }
  /**
   * Public API for Dashboard
   */
  async getStatus(userId) {
    const key = `${this.keyPrefix}bucket:${userId}`;
    const [tokens, lastRefill] = await this.redis.hmget(key, "tokens", "last_refill");
    const costKey = `${this.keyPrefix}cost:${userId}:daily`;
    const cost = await this.redis.get(costKey);
    return {
      userId,
      tokens: tokens ? parseFloat(tokens) : null,
      lastRefill: lastRefill ? parseInt(lastRefill) : null,
      dailyCost: cost ? parseFloat(cost) : 0
    };
  }
};
var advancedRateLimiter = new AdvancedRateLimiter({
  redis: {
    host: process.env.REDIS_HOST || "localhost",
    port: parseInt(process.env.REDIS_PORT || "6379", 10),
    password: process.env.REDIS_PASSWORD
  }
});

// src/middleware/circuitBreakerMiddleware.ts
init_metrics2();
var breakers = {};
var CONFIG = {
  FAILURE_THRESHOLD: 10,
  RESET_TIMEOUT: 3e4
  // 30s
};
function getBreaker(service11) {
  if (!breakers[service11]) {
    breakers[service11] = {
      state: "CLOSED",
      failures: 0,
      lastFailure: 0,
      nextAttempt: 0
    };
    metrics2.breakerState.labels(service11).set(0);
  }
  return breakers[service11];
}
var circuitBreaker = (serviceName) => {
  return (req, res, next) => {
    if (process.env.CIRCUIT_BREAKER_ENABLED !== "true") {
      return next();
    }
    const breaker = getBreaker(serviceName);
    if (breaker.state === "OPEN") {
      if (Date.now() > breaker.nextAttempt) {
        breaker.state = "HALF_OPEN";
        metrics2.breakerState.labels(serviceName).set(2);
      } else {
        return res.status(503).json({ error: "Service Unavailable (Circuit Breaker)" });
      }
    }
    res.on("finish", () => {
      if (res.statusCode >= 500) {
        breaker.failures++;
        breaker.lastFailure = Date.now();
        if (breaker.failures >= CONFIG.FAILURE_THRESHOLD) {
          breaker.state = "OPEN";
          breaker.nextAttempt = Date.now() + CONFIG.RESET_TIMEOUT;
          metrics2.breakerState.labels(serviceName).set(1);
        }
      } else if (res.statusCode < 500 && breaker.state === "HALF_OPEN") {
        breaker.state = "CLOSED";
        breaker.failures = 0;
        metrics2.breakerState.labels(serviceName).set(0);
      }
    });
    next();
  };
};
var circuitBreakerMiddleware = circuitBreaker("API_GATEWAY");

// src/lib/system-monitor.ts
init_logger();
import * as v82 from "v8";
import * as os2 from "os";
var SystemMonitor = class _SystemMonitor {
  static instance;
  lastCpuUsage = null;
  lastCpuTime = Date.now();
  currentCpuLoad = 0;
  // Thresholds
  MEMORY_THRESHOLD = 0.85;
  // 85% heap usage
  CPU_THRESHOLD = 0.9;
  // 90% CPU load
  constructor() {
    setInterval(() => this.sampleCpu(), 5e3).unref();
  }
  static getInstance() {
    if (!_SystemMonitor.instance) {
      _SystemMonitor.instance = new _SystemMonitor();
    }
    return _SystemMonitor.instance;
  }
  sampleCpu() {
    const currentCpuUsage = process.cpuUsage();
    const currentTime = Date.now();
    if (this.lastCpuUsage) {
      const timeDiff = currentTime - this.lastCpuTime;
      const userDiff = currentCpuUsage.user - this.lastCpuUsage.user;
      const systemDiff = currentCpuUsage.system - this.lastCpuUsage.system;
      const totalUsage = (userDiff + systemDiff) / (timeDiff * 1e3);
      this.currentCpuLoad = Math.min(1, totalUsage);
    }
    this.lastCpuUsage = currentCpuUsage;
    this.lastCpuTime = currentTime;
  }
  getHealth() {
    const heapStats = v82.getHeapStatistics();
    const memoryUsage3 = heapStats.used_heap_size / heapStats.heap_size_limit;
    const loadAvg = os2.loadavg();
    const cpuUsage = this.currentCpuLoad;
    let isOverloaded = false;
    let reason;
    if (memoryUsage3 > this.MEMORY_THRESHOLD) {
      isOverloaded = true;
      reason = `Memory usage critical: ${(memoryUsage3 * 100).toFixed(1)}%`;
      logger.warn({ memoryUsage: memoryUsage3, threshold: this.MEMORY_THRESHOLD }, "System Monitor: Memory overload detected");
    } else if (cpuUsage > this.CPU_THRESHOLD) {
      isOverloaded = true;
      reason = `CPU usage critical: ${(cpuUsage * 100).toFixed(1)}%`;
      logger.warn({ cpuUsage, threshold: this.CPU_THRESHOLD }, "System Monitor: CPU overload detected");
    }
    return {
      isOverloaded,
      reason,
      metrics: {
        cpuUsage,
        memoryUsage: memoryUsage3,
        uptime: process.uptime(),
        loadAverage: loadAvg
      }
    };
  }
};
var systemMonitor = SystemMonitor.getInstance();

// src/middleware/overloadProtection.ts
init_logger();
init_comprehensive_telemetry();
var overloadProtection = (req, res, next) => {
  const health = systemMonitor.getHealth();
  if (health.isOverloaded) {
    logger.warn({
      msg: "Load Shedding Active",
      reason: health.reason,
      metrics: health.metrics
    });
    telemetry.subsystems.api.errors.add(1);
    res.set("Retry-After", "5");
    res.status(503).json({
      error: "Service Unavailable",
      message: "System is currently under heavy load. Please retry later.",
      reason: health.reason
      // Optional: expose reason to client (maybe hide in prod)
    });
    return;
  }
  next();
};

// src/runtime/backpressure/BackpressureController.ts
import { EventEmitter as EventEmitter7 } from "events";
var BackpressureController = class _BackpressureController extends EventEmitter7 {
  static instance;
  // Configuration
  maxConcurrency = 1e3;
  maxTenantConcurrency = 50;
  maxQueueDepth = 5e3;
  // State
  currentConcurrency = 0;
  activeRequests = /* @__PURE__ */ new Set();
  // Map RequestID -> TenantID to track which tenant owns which active request
  requestTenantMap = /* @__PURE__ */ new Map();
  // Map TenantID -> Active Count
  tenantConcurrency = /* @__PURE__ */ new Map();
  queueDepth = 0;
  queues = {
    [0 /* CRITICAL */]: [],
    [1 /* NORMAL */]: [],
    [2 /* BEST_EFFORT */]: []
  };
  timer;
  constructor() {
    super();
    this.timer = setInterval(() => this.processQueue(), 100);
    if (this.timer.unref) {
      this.timer.unref();
    }
  }
  static getInstance() {
    if (!_BackpressureController.instance) {
      _BackpressureController.instance = new _BackpressureController();
    }
    return _BackpressureController.instance;
  }
  /**
   * Request admission to the system.
   */
  async requestAdmission(req) {
    const tenantCount = this.tenantConcurrency.get(req.tenantId) || 0;
    if (tenantCount >= this.maxTenantConcurrency && req.priority > 0 /* CRITICAL */) {
      return { allowed: false, status: "REJECTED", reason: "Tenant concurrency limit exceeded" };
    }
    if (this.currentConcurrency < this.maxConcurrency && this.queueDepth === 0) {
      this.activateRequest(req.id, req.tenantId);
      return { allowed: true, status: "ACCEPTED" };
    }
    if (this.queueDepth >= this.maxQueueDepth) {
      if (req.priority > 0 /* CRITICAL */) {
        return { allowed: false, status: "REJECTED", reason: "System overloaded" };
      }
    }
    this.queues[req.priority].push(req);
    this.queueDepth++;
    return this.waitForSlot(req);
  }
  activateRequest(id, tenantId) {
    this.activeRequests.add(id);
    this.currentConcurrency = this.activeRequests.size;
    this.requestTenantMap.set(id, tenantId);
    const count = this.tenantConcurrency.get(tenantId) || 0;
    this.tenantConcurrency.set(tenantId, count + 1);
  }
  waitForSlot(req) {
    return new Promise((resolve2) => {
      req._resolve = resolve2;
      setTimeout(() => {
        if (req._resolve) {
          this.removeRequest(req);
          resolve2({ allowed: false, status: "REJECTED", reason: "Timeout in queue" });
        }
      }, 3e4);
    });
  }
  /**
   * Release a slot when work is done.
   */
  release(reqId) {
    if (this.activeRequests.has(reqId)) {
      this.activeRequests.delete(reqId);
      this.currentConcurrency = this.activeRequests.size;
      const tenantId = this.requestTenantMap.get(reqId);
      if (tenantId) {
        const count = this.tenantConcurrency.get(tenantId) || 0;
        if (count > 0) {
          this.tenantConcurrency.set(tenantId, count - 1);
        }
        this.requestTenantMap.delete(reqId);
      }
      setImmediate(() => this.processQueue());
    }
  }
  processQueue() {
    while (this.currentConcurrency < this.maxConcurrency) {
      const nextReq = this.getNextRequest();
      if (nextReq) {
        this.queueDepth--;
        const tenantCount = this.tenantConcurrency.get(nextReq.tenantId) || 0;
        if (tenantCount >= this.maxTenantConcurrency && nextReq.priority > 0 /* CRITICAL */) {
          if (nextReq._resolve) {
            nextReq._resolve({ allowed: false, status: "REJECTED", reason: "Tenant concurrency limit exceeded during queue processing" });
            delete nextReq._resolve;
          }
          continue;
        }
        this.activateRequest(nextReq.id, nextReq.tenantId);
        if (nextReq._resolve) {
          nextReq._resolve({ allowed: true, status: "ACCEPTED" });
          delete nextReq._resolve;
        }
      } else {
        break;
      }
    }
  }
  getNextRequest() {
    if (this.queues[0 /* CRITICAL */].length > 0) return this.queues[0 /* CRITICAL */].shift();
    if (this.queues[1 /* NORMAL */].length > 0) return this.queues[1 /* NORMAL */].shift();
    if (this.queues[2 /* BEST_EFFORT */].length > 0) return this.queues[2 /* BEST_EFFORT */].shift();
    return void 0;
  }
  removeRequest(req) {
    const queue = this.queues[req.priority];
    const idx = queue.indexOf(req);
    if (idx > -1) {
      queue.splice(idx, 1);
      this.queueDepth--;
    }
    delete req._resolve;
  }
  // Metrics
  getMetrics() {
    return {
      concurrency: this.currentConcurrency,
      queueDepth: this.queueDepth,
      queues: {
        critical: this.queues[0 /* CRITICAL */].length,
        normal: this.queues[1 /* NORMAL */].length,
        bestEffort: this.queues[2 /* BEST_EFFORT */].length
      }
    };
  }
};

// src/runtime/backpressure/AdmissionControl.ts
var admissionControl = async (req, res, next) => {
  const controller = BackpressureController.getInstance();
  let priority = 1 /* NORMAL */;
  if (req.path.startsWith("/api/admin") || req.path.startsWith("/api/health")) {
    priority = 0 /* CRITICAL */;
  } else if (req.path.startsWith("/api/analytics") || req.path.startsWith("/api/reporting")) {
    priority = 2 /* BEST_EFFORT */;
  }
  const tenantId = req.user?.tenantId || "anonymous";
  const requestId = req.headers["x-request-id"] || Math.random().toString(36).substring(7);
  try {
    const result2 = await controller.requestAdmission({
      id: requestId,
      tenantId,
      priority
    });
    if (result2.allowed) {
      res.on("finish", () => {
        controller.release(requestId);
      });
      res.on("close", () => {
        controller.release(requestId);
      });
      next();
    } else {
      res.status(503).json({
        error: "Service Unavailable",
        message: result2.reason || "Server is under heavy load",
        retryAfter: result2.waitMs ? Math.ceil(result2.waitMs / 1e3) : 5
      });
    }
  } catch (err) {
    console.error("Admission control error", err);
    res.status(500).send("Internal Server Error");
  }
};

// src/middleware/httpCache.ts
init_config3();
import etag from "etag";
var httpCacheMiddleware = (req, res, next) => {
  if (req.method !== "GET") {
    res.setHeader("Cache-Control", "no-store");
    res.setHeader("Pragma", "no-cache");
    return next();
  }
  const staleWhileRevalidate = config_default.cache.staleWhileRevalidateSeconds;
  const browserTtl = Math.max(config_default.cdn.browserTtlSeconds, 0);
  const edgeTtl = Math.max(config_default.cdn.edgeTtlSeconds, browserTtl);
  res.setHeader(
    "Cache-Control",
    `public, max-age=${browserTtl}, stale-while-revalidate=${staleWhileRevalidate}`
  );
  if (config_default.cdn.enabled) {
    const surrogateKey = `${config_default.cdn.surrogateKeyNamespace} ${req.baseUrl || req.path}`.trim();
    res.setHeader(
      "CDN-Cache-Control",
      `max-age=${edgeTtl}, stale-while-revalidate=${staleWhileRevalidate}`
    );
    res.setHeader(
      "Surrogate-Control",
      `max-age=${edgeTtl}, stale-while-revalidate=${staleWhileRevalidate}`
    );
    res.setHeader("Surrogate-Key", surrogateKey);
  }
  const originalSend = res.send;
  res.send = function(body4) {
    if (res.headersSent) {
      return originalSend.call(this, body4);
    }
    if (!res.getHeader("ETag") && body4) {
      const entity = typeof body4 === "string" || Buffer.isBuffer(body4) ? body4 : JSON.stringify(body4);
      const generatedEtag = etag(entity);
      res.setHeader("ETag", generatedEtag);
    }
    const clientEtag = req.headers["if-none-match"];
    if (clientEtag && clientEtag === res.getHeader("ETag")) {
      res.status(304).end();
      return this;
    }
    return originalSend.call(this, body4);
  };
  next();
};

// src/config/safety.ts
init_logger();
var GLOBAL_KILL_SWITCH_FLAG_KEY = "platform.kill-switch.global";
var SAFE_MODE_FLAG_KEY = "platform.safe-mode";
var cachedFeatureFlagService;
var truthy = (value) => {
  if (!value) return false;
  const normalized = value.toLowerCase();
  return ["1", "true", "yes", "on", "enabled"].includes(normalized);
};
async function getCachedFeatureFlagService() {
  if (cachedFeatureFlagService !== void 0) {
    return cachedFeatureFlagService || void 0;
  }
  return Promise.resolve().then(() => (init_setup(), setup_exports)).then((module) => {
    const service11 = module.getFeatureFlagService();
    cachedFeatureFlagService = service11;
    return service11;
  }).catch((error) => {
    const msg = "Feature flag service unavailable for safety checks";
    if (logger && typeof logger.debug === "function") {
      logger.debug({ err: error }, msg);
    } else {
      console.warn(`[safety.ts] ${msg}:`, error?.message || error);
    }
    cachedFeatureFlagService = null;
    return void 0;
  });
}
async function evaluateFlag(flagService2, key) {
  if (!flagService2) return false;
  try {
    return await flagService2.isEnabled(key, { key: "system" }, false);
  } catch (error) {
    logger.warn({ err: error, flag: key }, "Feature flag evaluation failed");
    return false;
  }
}
async function isGlobalKillSwitchEnabled(flagService2) {
  if (truthy(process.env.KILL_SWITCH_GLOBAL)) {
    return true;
  }
  const key = process.env.KILL_SWITCH_FLAG_KEY || GLOBAL_KILL_SWITCH_FLAG_KEY;
  return evaluateFlag(flagService2, key);
}
async function isSafeModeEnabled(flagService2) {
  if (truthy(process.env.SAFE_MODE)) {
    return true;
  }
  const key = process.env.SAFE_MODE_FLAG_KEY || SAFE_MODE_FLAG_KEY;
  return evaluateFlag(flagService2, key);
}
async function getSafetyState(flagService2) {
  const service11 = flagService2 ?? await getCachedFeatureFlagService();
  const [killSwitch, safeMode] = await Promise.all([
    isGlobalKillSwitchEnabled(service11),
    isSafeModeEnabled(service11)
  ]);
  return { killSwitch, safeMode };
}

// src/middleware/safety-mode.ts
init_logger();
var MUTATING_METHODS = /* @__PURE__ */ new Set(["POST", "PUT", "PATCH", "DELETE"]);
var SAFE_MODE_BLOCKED_PREFIXES = [
  "/api/webhooks",
  "/api/stream",
  "/api/ai",
  "/api/aurora",
  "/api/oracle",
  "/api/phantom-limb",
  "/api/echelon2",
  "/api/zero-day",
  "/api/abyss",
  "/api/scenarios"
];
function isHighRiskPath(path55) {
  return SAFE_MODE_BLOCKED_PREFIXES.some((prefix) => path55.startsWith(prefix));
}
function isMutatingRequest(req) {
  const method = req.method.toUpperCase();
  if (!MUTATING_METHODS.has(method)) {
    return false;
  }
  if (req.path === "/graphql") {
    const query3 = typeof req.body?.query === "string" ? req.body.query.toLowerCase() : "";
    return query3.includes("mutation") || query3.includes("subscription");
  }
  return true;
}
async function safetyModeMiddleware(req, res, next) {
  try {
    const flagService2 = await getCachedFeatureFlagService();
    if (await isGlobalKillSwitchEnabled(flagService2)) {
      if (isMutatingRequest(req)) {
        logger.warn({ path: req.path, method: req.method }, "Request blocked by global kill switch");
        res.status(503).json({
          error: "Global kill switch active: write operations are temporarily disabled",
          code: "GLOBAL_KILL_SWITCH_ACTIVE"
        });
        return;
      }
    }
    if (await isSafeModeEnabled(flagService2)) {
      if (isHighRiskPath(req.path)) {
        logger.warn({ path: req.path, method: req.method }, "Request blocked by safe mode");
        res.status(503).json({
          error: "Safe mode active: high-risk endpoint is temporarily unavailable",
          code: "SAFE_MODE_ACTIVE"
        });
        return;
      }
    }
    next();
  } catch (error) {
    logger.error({ err: error, path: req.path }, "Safety middleware error");
    next(error);
  }
}
async function resolveSafetyState() {
  const service11 = await getCachedFeatureFlagService();
  return getSafetyState(service11);
}

// src/middleware/residency.ts
init_residency_guard();
init_regional_config();
var residencyEnforcement = async (req, res, next) => {
  if (req.path === "/health" || req.path === "/metrics") {
    return next();
  }
  try {
    const tenantId = req.user?.tenantId || req.tenantId;
    if (!tenantId) {
      if (req.path.startsWith("/auth") || req.path.startsWith("/public")) {
        return next();
      }
      return next();
    }
    const guard = ResidencyGuard.getInstance();
    const { globalTrafficSteering: globalTrafficSteering2 } = await Promise.resolve().then(() => (init_GlobalTrafficSteering(), GlobalTrafficSteering_exports));
    const decision = await globalTrafficSteering2.resolveRegion(tenantId);
    const steering = await globalTrafficSteering2.resolveSteeringAction(tenantId);
    res.setHeader("X-Summit-Steering-Advice", steering.targetUrl || steering.action);
    res.setHeader("X-Summit-Steering-Reason", steering.reason);
    if (steering.action === "REDIRECT") {
      const config9 = await guard.getResidencyConfig(tenantId);
      if (config9?.residencyMode === "strict") {
        return res.status(307).redirect(steering.targetUrl);
      }
    }
    await guard.enforce(tenantId, {
      operation: "compute",
      targetRegion: getCurrentRegion(),
      dataClassification: "internal"
      // Default
    });
    next();
  } catch (error) {
    if (error instanceof ResidencyViolationError) {
      res.status(451).json({
        // 451 Unavailable For Legal Reasons seems appropriate
        error: "ResidencyViolation",
        message: error.message,
        region: error.context.targetRegion,
        tenantId: error.tenantId
      });
    } else {
      next(error);
    }
  }
};

// src/middleware/request-profiling.ts
init_logger();
import { performance as performance3 } from "perf_hooks";
function resolveRouteName(req) {
  const routePath = req.route?.path;
  if (routePath) {
    return `${req.baseUrl || ""}${routePath}`;
  }
  return req.path || req.originalUrl || "unknown";
}
function requestProfilingMiddleware(req, res, next) {
  const start = performance3.now();
  res.on("finish", () => {
    const durationMs = performance3.now() - start;
    logger.info(
      {
        method: req.method,
        route: resolveRouteName(req),
        status: res.statusCode,
        durationMs
      },
      "request completed"
    );
  });
  next();
}

// src/middleware/securityHeaders.ts
import helmet from "helmet";
function securityHeaders({
  enabled = process.env.SECURITY_HEADERS_ENABLED !== "false",
  allowedOrigins = [],
  enableCsp = process.env.SECURITY_HEADERS_CSP_ENABLED === "true" || process.env.NODE_ENV === "production",
  cspReportOnly = process.env.SECURITY_HEADERS_CSP_REPORT_ONLY === "true"
} = {}) {
  if (!enabled) {
    return (_req, _res, next) => next();
  }
  const connectSrc = ["'self'", ...allowedOrigins, "https://api.intelgraph.example"];
  return helmet({
    frameguard: { action: "deny" },
    noSniff: true,
    // X-Content-Type-Options: nosniff
    referrerPolicy: { policy: "no-referrer" },
    crossOriginEmbedderPolicy: false,
    crossOriginOpenerPolicy: { policy: "same-origin" },
    crossOriginResourcePolicy: { policy: "same-origin" },
    hsts: process.env.NODE_ENV === "production" ? { maxAge: 31536e3, includeSubDomains: true, preload: true } : false,
    contentSecurityPolicy: enableCsp ? {
      useDefaults: true,
      directives: {
        defaultSrc: ["'self'"],
        baseUri: ["'self'"],
        objectSrc: ["'none'"],
        imgSrc: ["'self'", "data:"],
        scriptSrc: ["'self'", "'unsafe-inline'", "https://cdn.jsdelivr.net"],
        styleSrc: ["'self'", "'unsafe-inline'"],
        connectSrc
      },
      reportOnly: cspReportOnly
    } : false
  });
}

// src/validation/MutationValidators.ts
import { z as z7 } from "zod/v4";

// src/utils/htmlSanitizer.ts
import createDOMPurify from "dompurify";
import { JSDOM } from "jsdom";
var jsdomWindow = new JSDOM("").window;
var DOMPurify = createDOMPurify(jsdomWindow);
function sanitizeHtml(value) {
  const sanitized = DOMPurify.sanitize(value, {
    ALLOWED_TAGS: [],
    ALLOWED_ATTR: [],
    USE_PROFILES: { html: true }
  });
  return typeof sanitized === "string" ? sanitized : String(sanitized);
}

// src/validation/MutationValidators.ts
var TenantIdSchema = z7.string().min(1, "Tenant ID required").regex(
  /^[a-zA-Z0-9_-]+$/,
  "Tenant ID must contain only alphanumeric characters, underscores, and hyphens"
);
var EntityIdSchema = z7.string().min(1, "Entity ID required").uuid("Entity ID must be a valid UUID");
var ConfidenceSchema = z7.number().min(0, "Confidence must be >= 0").max(1, "Confidence must be <= 1");
var TimestampSchema = z7.string().datetime("Must be a valid ISO datetime").refine(
  (date) => new Date(date) <= /* @__PURE__ */ new Date(),
  "Timestamp cannot be in the future"
);
var EntityKindSchema = z7.string().min(1, "Entity kind required").max(50, "Entity kind too long").regex(
  /^[a-zA-Z][a-zA-Z0-9_]*$/,
  "Entity kind must start with letter and contain only alphanumeric and underscores"
);
var EntityLabelsSchema = z7.array(z7.string()).max(20, "Maximum 20 labels allowed").refine(
  (labels2) => new Set(labels2).size === labels2.length,
  "Duplicate labels not allowed"
);
var EntityPropsSchema = z7.record(z7.string(), z7.any()).refine(
  (props) => JSON.stringify(props).length <= 32768,
  "Entity properties too large (max 32KB)"
).refine(
  (props) => !Object.prototype.hasOwnProperty.call(props, "id") && !Object.prototype.hasOwnProperty.call(props, "tenantId"),
  "Reserved property names not allowed in props"
);
var RelationshipTypeSchema = z7.string().min(1, "Relationship type required").max(100, "Relationship type too long").regex(
  /^[A-Z][A-Z0-9_]*$/,
  "Relationship type must be uppercase with underscores"
);
var InvestigationNameSchema = z7.string().min(1, "Investigation name required").max(200, "Investigation name too long").refine(
  (name) => !name.includes("<") && !name.includes(">"),
  "Investigation name cannot contain HTML tags"
);
var InvestigationStatusSchema = z7.enum([
  "ACTIVE",
  "ARCHIVED",
  "COMPLETED",
  "DRAFT"
]);
var CustomMetadataSchema = z7.record(z7.string(), z7.any()).refine(
  (metadata) => JSON.stringify(metadata).length <= 16384,
  "Custom metadata too large (max 16KB)"
).refine((metadata) => {
  const str = JSON.stringify(metadata);
  const dangerousPatterns = [
    /<script/i,
    /javascript:/i,
    /on\w+\s*=/i,
    /data:.*base64/i
  ];
  return !dangerousPatterns.some((pattern2) => pattern2.test(str));
}, "Custom metadata contains potentially dangerous content");
var BudgetLimitSchema = z7.number().positive("Budget limit must be positive").max(1e6, "Budget limit too high (max $1M)");
var CostEstimateSchema = z7.number().nonnegative("Cost estimate cannot be negative").max(1e4, "Cost estimate too high (max $10K per operation)");
var TokenCountSchema = z7.number().int("Token count must be an integer").nonnegative("Token count cannot be negative").max(2e6, "Token count too high (max 2M tokens)");
var SecurityValidator = class {
  /**
   * Validate input for potential security issues
   */
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  static validateInput(input) {
    const errors = [];
    const inputStr = typeof input === "string" ? input : JSON.stringify(input);
    const sqlInjectionPatterns = [
      /(\bSELECT\b.*\bFROM\b)/i,
      /(\bUNION\b.*\bSELECT\b)/i,
      /(\bDROP\b.*\bTABLE\b)/i,
      /(\bINSERT\b.*\bINTO\b)/i
    ];
    const cypherInjectionPatterns = [
      /(\bMATCH\b.*\bRETURN\b)/i,
      /(\bCREATE\b.*\bNODE\b)/i,
      /(\bDELETE\b.*\bNODE\b)/i,
      /(\bDETACH\s+DELETE\b)/i
    ];
    const xssPatterns = [
      /<script[^>]*>.*?<\/script>/gi,
      /javascript:/gi,
      /on\w+\s*=/gi
    ];
    if (sqlInjectionPatterns.some((pattern2) => pattern2.test(inputStr))) {
      errors.push("Potential SQL injection detected");
    }
    if (cypherInjectionPatterns.some((pattern2) => pattern2.test(inputStr))) {
      errors.push("Potential Cypher injection detected");
    }
    if (xssPatterns.some((pattern2) => pattern2.test(inputStr))) {
      errors.push("Potential XSS content detected");
    }
    if (inputStr.length > 1048576) {
      errors.push("Input size too large");
    }
    return { valid: errors.length === 0, errors };
  }
  /**
   * Validate user permissions for operation
   */
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  static validatePermissions(user, operation, resource, context4) {
    const errors = [];
    if (context4?.tenantId && context4.tenantId !== user.tenantId) {
      errors.push("Cross-tenant access not allowed");
      return { valid: false, errors };
    }
    if (user.permissions.includes("*")) {
      return { valid: true, errors: [] };
    }
    const requiredPermission = `${resource}:${operation}`;
    if (!user.permissions.includes(requiredPermission)) {
      errors.push(`Missing permission: ${requiredPermission}`);
    }
    return { valid: errors.length === 0, errors };
  }
};
var EmailSchema = z7.string().email("Invalid email format").min(5, "Email too short").max(254, "Email too long").toLowerCase().refine(
  (email) => !email.includes(".."),
  "Email cannot contain consecutive dots"
);
var URLSchema = z7.string().url("Invalid URL format").refine(
  (url) => {
    try {
      const parsed = new URL(url);
      return ["http:", "https:"].includes(parsed.protocol);
    } catch {
      return false;
    }
  },
  "Only HTTP/HTTPS protocols allowed"
).refine(
  (url) => !url.includes("localhost") && !url.includes("127.0.0.1"),
  "Local URLs not allowed in production"
);
var PaginationSchema = z7.object({
  limit: z7.number().int("Limit must be an integer").min(1, "Limit must be at least 1").max(1e3, "Limit cannot exceed 1000").default(20),
  offset: z7.number().int("Offset must be an integer").min(0, "Offset cannot be negative").default(0),
  cursor: z7.string().optional()
});
var SearchQuerySchema = z7.string().min(1, "Search query required").max(500, "Search query too long").refine(
  (query3) => {
    const dangerousPatterns = [
      /[<>]/g,
      // HTML tags
      /javascript:/i,
      // JS protocol
      /on\w+\s*=/i
      // Event handlers
    ];
    return !dangerousPatterns.some((pattern2) => pattern2.test(query3));
  },
  "Search query contains invalid characters"
);
var FileUploadSchema = z7.object({
  filename: z7.string().min(1, "Filename required").max(255, "Filename too long").refine(
    (name) => /^[a-zA-Z0-9_.-]+$/.test(name),
    "Filename contains invalid characters"
  ),
  mimeType: z7.string().refine(
    (type) => {
      const allowedTypes = [
        "image/jpeg",
        "image/png",
        "image/gif",
        "application/pdf",
        "text/plain",
        "application/json"
      ];
      return allowedTypes.includes(type);
    },
    "File type not allowed"
  ),
  size: z7.number().int("File size must be an integer").min(1, "File cannot be empty").max(10 * 1024 * 1024, "File size exceeds 10MB limit")
});
var IPAddressSchema = z7.string().refine(
  (ip) => {
    const ipv4Pattern = /^(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$/;
    const ipv6Pattern = /^(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4})$/;
    return ipv4Pattern.test(ip) || ipv6Pattern.test(ip);
  },
  "Invalid IP address format"
);
var PhoneNumberSchema = z7.string().regex(/^\+?[1-9]\d{6,14}$/, "Invalid phone number format (E.164, minimum 7 digits)");
var DateRangeSchema = z7.object({
  startDate: TimestampSchema,
  endDate: TimestampSchema
}).refine(
  (data) => new Date(data.startDate) < new Date(data.endDate),
  "End date must be after start date"
).refine(
  (data) => {
    const start = new Date(data.startDate);
    const end = new Date(data.endDate);
    const diffDays = (end.getTime() - start.getTime()) / (1e3 * 60 * 60 * 24);
    return diffDays <= 365;
  },
  "Date range cannot exceed 365 days"
);
var GraphQLInputSchema = z7.object({
  operationName: z7.string().max(100, "Operation name too long").optional(),
  query: z7.string().min(1, "Query required").max(5e4, "Query too large").refine(
    (query3) => {
      const openBraces = (query3.match(/{/g) || []).length;
      return openBraces <= 15;
    },
    "Query depth exceeds maximum allowed"
  ),
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  variables: z7.record(z7.string(), z7.any()).optional()
});
var SanitizationUtils = class {
  /**
   * Sanitize HTML to prevent XSS
   */
  static sanitizeHTML(input) {
    return sanitizeHtml(String(input));
  }
  /**
   * Sanitize SQL input (for dynamic queries - prefer parameterized queries!)
   */
  static sanitizeSQL(input) {
    return input.replace(/'/g, "''").replace(/;/g, "").replace(/--/g, "").replace(/\/\*/g, "").replace(/\*\//g, "");
  }
  /**
   * Sanitize Cypher input (for dynamic queries - prefer parameterized queries!)
   */
  static sanitizeCypher(input) {
    return input.replace(/'/g, "\\'").replace(/"/g, '\\"').replace(/`/g, "\\`").replace(/\\/g, "\\\\");
  }
  /**
   * Sanitize user input for safe storage and display
   */
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  static sanitizeUserInput(input) {
    if (typeof input === "string") {
      return this.sanitizeHTML(input).trim().slice(0, 1e4);
    }
    if (Array.isArray(input)) {
      return input.slice(0, 1e3).map((item) => this.sanitizeUserInput(item));
    }
    if (input && typeof input === "object") {
      const sanitized = {};
      for (const [key, value] of Object.entries(input)) {
        if (Object.keys(sanitized).length >= 100) break;
        sanitized[key] = this.sanitizeUserInput(value);
      }
      return sanitized;
    }
    return input;
  }
  /**
   * Remove potentially dangerous content from strings
   */
  static removeDangerousContent(input) {
    return input.replace(/<script[^>]*>.*?<\/script>/gi, "").replace(/<iframe[^>]*>.*?<\/iframe>/gi, "").replace(/javascript:/gi, "").replace(/on\w+\s*=/gi, "").replace(/data:.*base64/gi, "");
  }
};

// src/validation/index.ts
import { z as z8 } from "zod/v4";
import { GraphQLError as GraphQLError10 } from "graphql";

// src/middleware/security-hardening.ts
import pino35 from "pino";
var logger32 = pino35();
function securityHardening(req, res, next) {
  try {
    const inputs = [req.body, req.query, req.params];
    const errors = [];
    for (const input of inputs) {
      if (input) {
        const validation = SecurityValidator.validateInput(input);
        if (!validation.valid) {
          errors.push(...validation.errors);
        }
      }
    }
    if (errors.length > 0) {
      logger32.warn({
        msg: "Security violation detected",
        path: req.path,
        ip: req.ip,
        errors
        // Don't log full body to avoid PII leak in logs, but maybe log a sample or hash?
        // For now, just log the errors.
      });
      return res.status(400).json({
        error: "Security violation: Potential malicious input detected",
        code: "SECURITY_VIOLATION"
      });
    }
    next();
  } catch (error) {
    logger32.error({ error, path: req.path }, "Error in security hardening middleware");
    res.status(500).json({ error: "Internal security check failed" });
  }
}

// src/middleware/abuseGuard.ts
init_metrics3();
init_logger2();
import Redis12 from "ioredis";

// src/utils/tracing.ts
var NoopSpan = class {
  constructor(name) {
    this.name = name;
  }
  setAttributes(_attributes) {
  }
  setAttribute(_key, _value) {
  }
  recordException(_error) {
  }
  end() {
  }
};
var tracer5 = {
  /**
   * Starts a new active span and executes the provided handler.
   *
   * @typeParam T - The return type of the handler.
   * @param name - The name of the span.
   * @param handler - A function that receives the span and performs the work.
   * @returns The result of the handler.
   */
  async startActiveSpan(name, handler) {
    const span = new NoopSpan(name);
    try {
      return await handler(span);
    } catch (error) {
      span.recordException(error);
      throw error;
    } finally {
      span.end();
    }
  }
};

// src/security/UEBAModels.ts
import Redis11 from "ioredis";
var UEBAModels = class {
  redis;
  constructor(redis5) {
    this.redis = redis5 || new Redis11(process.env.REDIS_URL || "redis://localhost:6379");
  }
  async updateProfile(event) {
    const key = `ueba:profile:${event.entityType}:${event.entityId}`;
    const raw = await this.redis.get(key);
    let profile;
    if (raw) {
      profile = JSON.parse(raw);
    } else {
      profile = {
        entityId: event.entityId,
        entityType: event.entityType,
        lastActive: event.timestamp,
        actionCounts: {},
        hourlyDistribution: new Array(24).fill(0),
        geographicRegions: [],
        typicalResources: [],
        riskScore: 0
      };
    }
    profile.actionCounts[event.action] = (profile.actionCounts[event.action] || 0) + 1;
    const hour = new Date(event.timestamp).getHours();
    profile.hourlyDistribution[hour]++;
    if (event.region && !profile.geographicRegions.includes(event.region)) {
      profile.geographicRegions.push(event.region);
    }
    if (event.resource && !profile.typicalResources.includes(event.resource)) {
      profile.typicalResources.push(event.resource);
      if (profile.typicalResources.length > 50) profile.typicalResources.shift();
    }
    profile.lastActive = event.timestamp;
    await this.redis.set(key, JSON.stringify(profile));
    return profile;
  }
  async analyzeAnomaly(event) {
    const key = `ueba:profile:${event.entityType}:${event.entityId}`;
    const raw = await this.redis.get(key);
    if (!raw) return { isAnomaly: false, score: 0, reasons: [] };
    const profile = JSON.parse(raw);
    const reasons = [];
    let score = 0;
    const hour = new Date(event.timestamp).getHours();
    const totalActivity = profile.hourlyDistribution.reduce((a, b) => a + b, 0);
    if (totalActivity > 50) {
      const hourFrequency = profile.hourlyDistribution[hour] / totalActivity;
      if (hourFrequency < 0.01) {
        score += 30;
        reasons.push(`Atypical activity hour: ${hour}`);
      }
    }
    if (event.resource && !profile.typicalResources.includes(event.resource)) {
      score += 20;
      reasons.push(`Atypical resource access: ${event.resource}`);
    }
    if (event.region && !profile.geographicRegions.includes(event.region)) {
      score += 40;
      reasons.push(`Atypical geographic region: ${event.region}`);
    }
    const recentActionCount = profile.actionCounts[event.action] || 0;
    if (recentActionCount > 1e3) {
      score += 15;
      reasons.push(`High action velocity: ${event.action}`);
    }
    return {
      isAnomaly: score > 60,
      score,
      reasons
    };
  }
};

// src/middleware/abuseGuard.ts
var AbuseGuard = class {
  config;
  redis;
  metrics;
  throttledTenants = /* @__PURE__ */ new Map();
  ueba;
  constructor(config9 = {}) {
    this.ueba = new UEBAModels();
    this.config = {
      enabled: true,
      windowSizeMinutes: 10,
      anomalyThreshold: 3,
      // 3 standard deviations
      spikeMultiplier: 10,
      throttleDurationMinutes: 30,
      maxRequestsPerWindow: 1e3,
      bypassTokens: [],
      ...config9
    };
    this.redis = new Redis12(process.env.REDIS_URL || "redis://localhost:6379");
    this.metrics = new PrometheusMetrics("abuse_guard");
    this.initializeMetrics();
    this.startCleanupTask();
  }
  initializeMetrics() {
    this.metrics.createCounter("requests_total", "Total requests processed", [
      "tenant_id",
      "status"
    ]);
    this.metrics.createCounter("anomalies_detected", "Anomalies detected", [
      "tenant_id",
      "type"
    ]);
    this.metrics.createCounter("throttles_applied", "Throttles applied", [
      "tenant_id",
      "reason"
    ]);
    this.metrics.createGauge(
      "throttled_tenants",
      "Currently throttled tenants"
    );
    this.metrics.createHistogram(
      "rate_analysis_duration",
      "Time spent analyzing rates",
      {
        buckets: [1e-3, 0.01, 0.1, 1, 5]
      }
    );
  }
  // Express middleware factory
  middleware() {
    return async (req, res, next) => {
      if (!this.config.enabled) {
        return next();
      }
      return tracer5.startActiveSpan(
        "abuse_guard.middleware",
        async (span) => {
          try {
            const tenantId = this.extractTenantId(req);
            const bypass = this.checkBypassToken(req);
            span.setAttributes({
              "abuse_guard.tenant_id": tenantId,
              "abuse_guard.bypass": bypass
            });
            if (bypass) {
              logger_default2.debug("Request bypassing abuse guard", {
                tenantId,
                bypass: true
              });
              this.metrics.incrementCounter("requests_total", {
                tenant_id: tenantId,
                status: "bypassed"
              });
              return next();
            }
            if (this.isThrottled(tenantId)) {
              const throttleState = this.throttledTenants.get(tenantId);
              logger_default2.warn("Request throttled", {
                tenantId,
                reason: throttleState.reason,
                remainingTime: throttleState.endTime - Date.now()
              });
              this.metrics.incrementCounter("requests_total", {
                tenant_id: tenantId,
                status: "throttled"
              });
              res.status(429).json({
                error: "Rate limit exceeded",
                message: "Tenant is temporarily throttled due to anomalous activity",
                retryAfter: Math.ceil(
                  (throttleState.endTime - Date.now()) / 1e3
                ),
                severity: throttleState.severity
              });
              return;
            }
            const anomalyResult = await this.recordAndAnalyze(tenantId);
            const userId = req.user?.id || req.user?.sub || "anonymous";
            const uebaEvent = {
              entityId: userId,
              entityType: "user",
              action: `${req.method}:${req.path}`,
              resource: req.path,
              region: req.headers["x-region"] || "unknown",
              timestamp: (/* @__PURE__ */ new Date()).toISOString()
            };
            const uebaResult = await this.ueba.analyzeAnomaly(uebaEvent);
            await this.ueba.updateProfile(uebaEvent);
            if (uebaResult.isAnomaly || anomalyResult.isAnomaly) {
              const combinedResult = {
                ...anomalyResult,
                isAnomaly: true,
                score: Math.max(anomalyResult.zScore * 10, uebaResult.score),
                reasons: [...uebaResult.reasons || []]
              };
              await this.handleAnomaly(tenantId, combinedResult);
              if (this.isThrottled(tenantId)) {
                this.metrics.incrementCounter("requests_total", {
                  tenant_id: tenantId,
                  status: "throttled_new"
                });
                res.status(429).json({
                  error: "Rate limit exceeded",
                  message: "Anomalous activity detected - throttling applied",
                  retryAfter: this.config.throttleDurationMinutes * 60,
                  anomaly: {
                    type: anomalyResult.type,
                    zScore: anomalyResult.zScore,
                    currentRate: anomalyResult.currentRate
                  }
                });
                return;
              }
            }
            this.metrics.incrementCounter("requests_total", {
              tenant_id: tenantId,
              status: "allowed"
            });
            next();
          } catch (error) {
            logger_default2.error("Abuse guard error", { error: error.message });
            span.recordException(error);
            this.metrics.incrementCounter("requests_total", {
              tenant_id: "unknown",
              status: "error"
            });
            next();
          }
        }
      );
    };
  }
  extractTenantId(req) {
    return req.headers["x-tenant-id"] || req.body?.tenantId || req.query.tenantId || req.user?.tenantId || "default";
  }
  checkBypassToken(req) {
    const token = req.headers["x-bypass-token"] || "";
    return !!(token && this.config.bypassTokens.includes(token));
  }
  isThrottled(tenantId) {
    const throttleState = this.throttledTenants.get(tenantId);
    if (!throttleState) return false;
    if (Date.now() > throttleState.endTime) {
      this.throttledTenants.delete(tenantId);
      this.updateThrottledTenantsMetric();
      return false;
    }
    return true;
  }
  async recordAndAnalyze(tenantId) {
    const startTime = Date.now();
    try {
      const windowKey = this.getWindowKey(tenantId);
      const currentWindow = Math.floor(
        Date.now() / (this.config.windowSizeMinutes * 60 * 1e3)
      );
      await this.redis.zAdd(windowKey, {
        score: currentWindow,
        value: Date.now().toString()
      });
      await this.redis.expire(
        windowKey,
        this.config.windowSizeMinutes * 60 * 2
      );
      const recentWindows = await this.getRecentWindows(tenantId);
      const anomalyResult = this.analyzeForAnomalies(recentWindows);
      this.metrics.observeHistogram(
        "rate_analysis_duration",
        (Date.now() - startTime) / 1e3
      );
      return anomalyResult;
    } catch (error) {
      logger_default2.error("Error in rate analysis", {
        tenantId,
        error: error.message
      });
      return {
        isAnomaly: false,
        zScore: 0,
        currentRate: 0,
        baselineRate: 0,
        confidence: 0,
        type: "normal"
      };
    }
  }
  async getRecentWindows(tenantId) {
    const windowKey = this.getWindowKey(tenantId);
    const currentWindow = Math.floor(
      Date.now() / (this.config.windowSizeMinutes * 60 * 1e3)
    );
    const lookbackWindows = 12;
    const windows = [];
    for (let i = 0; i < lookbackWindows; i++) {
      const windowStart = currentWindow - i;
      const windowEnd = windowStart + 1;
      const count = await this.redis.zCount(
        windowKey,
        windowStart,
        windowEnd - 1
      );
      windows.push({
        timestamp: windowStart * this.config.windowSizeMinutes * 60 * 1e3,
        count,
        tenantId
      });
    }
    return windows.reverse();
  }
  analyzeForAnomalies(windows) {
    if (windows.length < 3) {
      return {
        isAnomaly: false,
        zScore: 0,
        currentRate: windows[windows.length - 1]?.count || 0,
        baselineRate: 0,
        confidence: 0,
        type: "normal"
      };
    }
    const currentRate = windows[windows.length - 1].count;
    const historicalRates = windows.slice(0, -1).map((w) => w.count);
    const mean = historicalRates.reduce((a, b) => a + b, 0) / historicalRates.length;
    const variance = historicalRates.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / historicalRates.length;
    const stdDev = Math.sqrt(variance);
    const zScore = stdDev === 0 ? 0 : (currentRate - mean) / stdDev;
    const recentMean = historicalRates.slice(-3).reduce((a, b) => a + b, 0) / 3;
    const isSuddenSpike = currentRate >= recentMean * this.config.spikeMultiplier;
    let isAnomaly = false;
    let type = "normal";
    let confidence = 0;
    if (isSuddenSpike && currentRate > this.config.maxRequestsPerWindow / 2) {
      isAnomaly = true;
      type = "spike";
      confidence = 0.9;
    } else if (Math.abs(zScore) > this.config.anomalyThreshold) {
      isAnomaly = true;
      type = "sustained";
      confidence = Math.min(
        Math.abs(zScore) / this.config.anomalyThreshold,
        1
      );
    }
    return {
      isAnomaly,
      zScore,
      currentRate,
      baselineRate: mean,
      confidence,
      type
    };
  }
  async handleAnomaly(tenantId, anomaly) {
    logger_default2.warn("Anomaly detected", {
      tenantId,
      type: anomaly.type,
      zScore: anomaly.zScore,
      currentRate: anomaly.currentRate,
      baselineRate: anomaly.baselineRate,
      confidence: anomaly.confidence
    });
    this.metrics.incrementCounter("anomalies_detected", {
      tenant_id: tenantId,
      type: anomaly.type
    });
    let severity = "warning";
    let throttleDuration = this.config.throttleDurationMinutes;
    if (anomaly.type === "spike" && anomaly.confidence > 0.8) {
      severity = "critical";
      throttleDuration = this.config.throttleDurationMinutes * 2;
    } else if (anomaly.zScore > this.config.anomalyThreshold * 1.5) {
      severity = "critical";
      throttleDuration = this.config.throttleDurationMinutes * 1.5;
    }
    const throttleState = {
      tenantId,
      startTime: Date.now(),
      endTime: Date.now() + throttleDuration * 60 * 1e3,
      reason: `${anomaly.type} anomaly detected (z-score: ${anomaly.zScore.toFixed(2)})`,
      severity
    };
    this.throttledTenants.set(tenantId, throttleState);
    this.updateThrottledTenantsMetric();
    this.metrics.incrementCounter("throttles_applied", {
      tenant_id: tenantId,
      reason: anomaly.type
    });
    if (this.config.alertWebhookUrl) {
      await this.sendAlert(tenantId, anomaly, throttleState);
    }
  }
  async sendAlert(tenantId, anomaly, throttle) {
    try {
      const alert = {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        tenantId,
        severity: throttle.severity,
        anomaly: {
          type: anomaly.type,
          zScore: anomaly.zScore,
          currentRate: anomaly.currentRate,
          baselineRate: anomaly.baselineRate,
          confidence: anomaly.confidence
        },
        throttle: {
          duration: throttle.endTime - throttle.startTime,
          reason: throttle.reason
        }
      };
      const response = await fetch(this.config.alertWebhookUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify(alert)
      });
      if (!response.ok) {
        throw new Error(`Alert webhook failed: ${response.status}`);
      }
      logger_default2.info("Abuse alert sent", {
        tenantId,
        severity: throttle.severity
      });
    } catch (error) {
      logger_default2.error("Failed to send abuse alert", {
        tenantId,
        error: error.message,
        webhookUrl: this.config.alertWebhookUrl
      });
    }
  }
  getWindowKey(tenantId) {
    return `abuse_guard:rate_windows:${tenantId}`;
  }
  updateThrottledTenantsMetric() {
    this.metrics.setGauge("throttled_tenants", this.throttledTenants.size);
  }
  startCleanupTask() {
    setInterval(() => {
      const now = Date.now();
      let cleaned = 0;
      for (const [tenantId, throttleState] of this.throttledTenants.entries()) {
        if (now > throttleState.endTime) {
          this.throttledTenants.delete(tenantId);
          cleaned++;
        }
      }
      if (cleaned > 0) {
        logger_default2.debug("Cleaned expired throttles", { count: cleaned });
        this.updateThrottledTenantsMetric();
      }
    }, 6e4);
  }
  // Admin methods
  getThrottledTenants() {
    return Array.from(this.throttledTenants.values());
  }
  removeThrottle(tenantId) {
    const removed = this.throttledTenants.delete(tenantId);
    if (removed) {
      this.updateThrottledTenantsMetric();
      logger_default2.info("Throttle manually removed", { tenantId });
    }
    return removed;
  }
  async getRecentStats(tenantId) {
    const windows = await this.getRecentWindows(tenantId);
    const anomaly = this.analyzeForAnomalies(windows);
    return {
      currentRate: anomaly.currentRate,
      baselineRate: anomaly.baselineRate,
      recentWindows: windows.slice(-5),
      // Last 5 windows
      isThrottled: this.isThrottled(tenantId)
    };
  }
};
var abuseGuard = new AbuseGuard({
  enabled: process.env.ABUSE_GUARD_ENABLED !== "false",
  windowSizeMinutes: parseInt(process.env.ABUSE_GUARD_WINDOW_MINUTES || "10"),
  anomalyThreshold: parseFloat(
    process.env.ABUSE_GUARD_ANOMALY_THRESHOLD || "3.0"
  ),
  spikeMultiplier: parseFloat(process.env.ABUSE_GUARD_SPIKE_MULTIPLIER || "10"),
  throttleDurationMinutes: parseInt(
    process.env.ABUSE_GUARD_THROTTLE_DURATION || "30"
  ),
  maxRequestsPerWindow: parseInt(
    process.env.ABUSE_GUARD_MAX_REQUESTS || "1000"
  ),
  alertWebhookUrl: process.env.ABUSE_GUARD_ALERT_WEBHOOK,
  bypassTokens: (process.env.ABUSE_GUARD_BYPASS_TOKENS || "").split(",").filter(Boolean)
});

// src/data-residency/exceptions/routes.ts
init_postgres();
init_otel_tracing();
import { Router } from "express";
import { z as z9 } from "zod";
var router2 = Router();
var ExceptionRequestSchema = z9.object({
  targetRegion: z9.string(),
  operation: z9.enum(["storage", "compute", "logs", "backup"]),
  justification: z9.string().min(10),
  durationMinutes: z9.number().min(5).max(1440)
  // Max 24 hours
});
router2.post("/request", async (req, res) => {
  const span = otelService.createSpan("residency.exception.request");
  try {
    const tenantId = req.user?.tenantId;
    if (!tenantId) {
      res.status(400).json({ error: "Tenant context required" });
      return;
    }
    const body4 = ExceptionRequestSchema.parse(req.body);
    const pool4 = getPostgresPool();
    const isApproved = body4.justification.includes("EMERGENCY");
    const expiresAt = new Date(Date.now() + body4.durationMinutes * 6e4);
    if (isApproved) {
      await pool4.query(
        `INSERT INTO residency_exceptions
                (id, tenant_id, target_region, scope, justification, approved_by, expires_at, created_at)
                VALUES ($1, $2, $3, $4, $5, $6, $7, NOW())`,
        [
          `exc-${Date.now()}`,
          tenantId,
          body4.targetRegion,
          body4.operation,
          body4.justification,
          req.user?.sub || "system",
          expiresAt
        ]
      );
      res.status(201).json({
        status: "APPROVED",
        expiresAt,
        message: "Emergency exception granted. This event has been logged for audit."
      });
    } else {
      res.status(202).json({
        status: "PENDING_APPROVAL",
        message: "Exception request submitted for review."
      });
    }
  } catch (error) {
    if (error instanceof z9.ZodError) {
      res.status(400).json({ error: "Validation Error", details: error.errors });
    } else {
      console.error(error);
      res.status(500).json({ error: "Internal Error" });
    }
  } finally {
    span?.end();
  }
});
var routes_default = router2;

// src/routes/monitoring.ts
init_metrics2();
init_observability();
import express2 from "express";
import { z as z10 } from "zod";

// src/monitoring/health.ts
var performHealthCheck = async () => ({ status: "healthy" });
var getCachedHealthStatus = () => ({ status: "healthy" });
var livenessProbe = async () => ({ status: "alive" });
var readinessProbe = async () => ({ status: "ready" });
var checkDatabase = async () => ({ status: "healthy" });
var checkNeo4j = async () => ({ status: "healthy" });
var checkRedis = async () => ({ status: "healthy" });
var checkMlService = async () => ({ status: "healthy" });
var checkSystemResources = () => ({ status: "healthy" });

// src/routes/monitoring.ts
init_businessMetrics();
init_auth4();
var router3 = express2.Router();
router3.use(express2.json());
router3.get("/metrics", requirePermission("system:metrics"), async (req, res) => {
  try {
    res.set("Content-Type", register4.contentType);
    const metrics8 = await register4.metrics();
    res.end(metrics8);
  } catch (error) {
    res.status(500).json({
      error: "Failed to collect metrics",
      details: error.message
    });
  }
});
router3.get("/health", async (req, res) => {
  try {
    const health = await performHealthCheck();
    const statusCode = health.status === "healthy" ? 200 : 503;
    res.status(statusCode).json(health);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      error: error.message
    });
  }
});
router3.get("/health/quick", (req, res) => {
  try {
    const health = getCachedHealthStatus();
    const statusCode = health.status === "healthy" ? 200 : 503;
    res.status(statusCode).json(health);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      error: error.message
    });
  }
});
router3.get("/health/live", async (req, res) => {
  try {
    const liveness = await livenessProbe();
    res.status(200).json(liveness);
  } catch (error) {
    res.status(503).json({
      status: "not_alive",
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      error: error.message
    });
  }
});
router3.get("/health/ready", async (req, res) => {
  try {
    const readiness = await readinessProbe();
    const statusCode = readiness.status === "ready" ? 200 : 503;
    res.status(statusCode).json(readiness);
  } catch (error) {
    res.status(503).json({
      status: "not_ready",
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      error: error.message
    });
  }
});
router3.get("/health/info", (req, res) => {
  const info = {
    service: "intelgraph-server",
    version: process.env.npm_package_version || "1.0.0",
    environment: process.env.NODE_ENV || "development",
    nodeVersion: process.version,
    platform: process.platform,
    arch: process.arch,
    uptime: Math.round(process.uptime()),
    timestamp: (/* @__PURE__ */ new Date()).toISOString(),
    pid: process.pid,
    memory: {
      heapUsed: Math.round(process.memoryUsage().heapUsed / 1024 / 1024),
      heapTotal: Math.round(process.memoryUsage().heapTotal / 1024 / 1024),
      external: Math.round(process.memoryUsage().external / 1024 / 1024),
      rss: Math.round(process.memoryUsage().rss / 1024 / 1024)
    }
  };
  res.json(info);
});
router3.get("/health/database", async (req, res) => {
  try {
    const dbHealth = await checkDatabase();
    const statusCode = dbHealth.status === "healthy" ? 200 : 503;
    res.status(statusCode).json(dbHealth);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      error: error.message
    });
  }
});
router3.get("/health/neo4j", async (req, res) => {
  try {
    const neo4jHealth = await checkNeo4j();
    const statusCode = neo4jHealth.status === "healthy" ? 200 : 503;
    res.status(statusCode).json(neo4jHealth);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      error: error.message
    });
  }
});
router3.get("/health/redis", async (req, res) => {
  try {
    const redisHealth = await checkRedis();
    const statusCode = redisHealth.status === "healthy" ? 200 : 503;
    res.status(statusCode).json(redisHealth);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      error: error.message
    });
  }
});
router3.get("/health/ml", async (req, res) => {
  try {
    const mlHealth = await checkMlService();
    const statusCode = mlHealth.status === "healthy" ? 200 : 503;
    res.status(statusCode).json(mlHealth);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      error: error.message
    });
  }
});
router3.get("/health/system", (req, res) => {
  try {
    const systemHealth = checkSystemResources();
    const statusCode = systemHealth.status === "healthy" ? 200 : 503;
    res.status(statusCode).json(systemHealth);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      error: error.message
    });
  }
});
var businessMetricSchema = z10.object({
  type: z10.enum(["user_signup", "api_call", "revenue"]),
  tenant: z10.string().min(1).optional(),
  plan: z10.string().min(1).optional(),
  service: z10.string().min(1).optional(),
  route: z10.string().min(1).optional(),
  statusCode: z10.number().int().optional(),
  amount: z10.number().optional(),
  currency: z10.string().min(1).optional(),
  metadata: z10.record(z10.any()).optional()
});
router3.post("/metrics/business", (req, res) => {
  try {
    const payload = businessMetricSchema.parse(req.body);
    recordBusinessEvent(payload);
    res.status(202).json({
      status: "accepted",
      recordedAt: (/* @__PURE__ */ new Date()).toISOString()
    });
  } catch (error) {
    res.status(400).json({
      error: "Invalid business metric payload",
      details: error instanceof Error ? error.message : String(error)
    });
  }
});
router3.post("/web-vitals", (req, res) => {
  const { name, value, id } = req.body || {};
  if (typeof name !== "string" || typeof value !== "number") {
    return res.status(400).json({ error: "Invalid web vitals payload" });
  }
  try {
    webVitalValue.set({ metric: name, id: id || "unknown" }, value);
    res.status(204).end();
  } catch (error) {
    res.status(500).json({ error: "Failed to record web vital", details: error.message });
  }
});
router3.post("/telemetry/events", (req, res) => {
  const { event, labels: labels2 } = req.body;
  const tenantId = req.headers["x-tenant-id"] || labels2?.tenantId || "unknown";
  if (!event) {
    return res.status(400).json({ error: "Event name is required" });
  }
  try {
    if (event === "golden_path_step") {
      goldenPathStepTotal.inc({
        step: labels2?.step || "unknown",
        status: labels2?.status || "success",
        tenant_id: tenantId
      });
    } else if (event === "ui_error_boundary") {
      uiErrorBoundaryCatchTotal.inc({
        component: labels2?.component || "unknown",
        tenant_id: tenantId
      });
      logger33.error("UI Error Boundary Caught Exception", {
        component: labels2?.component,
        message: labels2?.message,
        stack: labels2?.stack,
        tenantId
      });
    }
    res.status(202).json({ status: "accepted" });
  } catch (error) {
    res.status(500).json({ error: "Failed to record event", details: error.message });
  }
});
router3.post("/telemetry/dora", (req, res) => {
  const { metric, value, labels: labels2 } = req.body;
  const _tenantId = req.headers["x-tenant-id"] || "unknown";
  if (!metric) {
    return res.status(400).json({ error: "Metric name is required" });
  }
  try {
    switch (metric) {
      case "deployment":
        maestroDeploymentsTotal2.inc({
          environment: labels2?.environment || "production",
          status: labels2?.status || "success"
        });
        break;
      case "lead_time":
        if (typeof value === "number") {
          maestroPrLeadTimeHours.observe(value);
        }
        break;
      case "change_failure_rate":
        if (typeof value === "number") {
          maestroChangeFailureRate2.set(value);
        }
        break;
      case "mttr":
        if (typeof value === "number") {
          maestroMttrHours.observe(value);
        }
        break;
      default:
        return res.status(400).json({ error: "Unknown DORA metric" });
    }
    res.status(202).json({ status: "accepted" });
  } catch (error) {
    res.status(500).json({ error: "Failed to record DORA metric", details: error.message });
  }
});
var monitoring_default = router3;

// src/routes/billing.ts
import express3 from "express";

// src/services/billing/MeteringService.ts
init_pg();
init_ledger();
init_quota_manager();
init_metrics3();
var MeteringService = class _MeteringService {
  static instance;
  metrics;
  constructor() {
    this.metrics = new PrometheusMetrics("summit_billing");
    this.metrics.createCounter(
      "billable_events_total",
      "Total number of billable events recorded",
      ["kind", "status"]
    );
  }
  static getInstance() {
    if (!_MeteringService.instance) {
      _MeteringService.instance = new _MeteringService();
    }
    return _MeteringService.instance;
  }
  /**
   * Records a billable event idempotently and securely.
   */
  async recordUsage(event) {
    const quota = quota_manager_default.getQuotaForTenant(event.tenantId);
    const client6 = await pool.connect();
    try {
      await client6.query("BEGIN");
      const insertQuery = `
        INSERT INTO usage_events (
          tenant_id,
          kind,
          quantity,
          unit,
          occurred_at,
          idempotency_key,
          metadata,
          principal_id
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
        RETURNING id
      `;
      let eventId;
      try {
        const result2 = await client6.query(insertQuery, [
          event.tenantId,
          event.eventType,
          event.quantity,
          event.unit,
          event.timestamp,
          event.idempotencyKey,
          JSON.stringify(event.metadata || {}),
          event.actorId || "system"
        ]);
        eventId = result2.rows[0].id;
      } catch (err) {
        if (err.code === "23505") {
          await client6.query("ROLLBACK");
          this.metrics.incrementCounter("billable_events_total", { kind: event.eventType, status: "duplicate" });
          const existing = await pool.query(
            "SELECT id FROM usage_events WHERE tenant_id = $1 AND idempotency_key = $2",
            [event.tenantId, event.idempotencyKey]
          );
          if (existing.rows.length > 0) {
            return {
              eventId: existing.rows[0].id,
              status: "duplicate",
              timestamp: /* @__PURE__ */ new Date()
            };
          }
          throw err;
        }
        throw err;
      }
      await provenanceLedger.appendEntry({
        tenantId: event.tenantId,
        actionType: "BILLABLE_EVENT",
        resourceType: "metering_record",
        resourceId: eventId,
        actorId: event.actorId || "system",
        actorType: "system",
        timestamp: /* @__PURE__ */ new Date(),
        payload: {
          mutationType: "CREATE",
          entityId: eventId,
          entityType: "BillableEvent",
          originalEvent: event,
          meteringId: eventId
        },
        metadata: {
          purpose: "billing_audit",
          idempotencyKey: event.idempotencyKey
        }
      });
      await client6.query("COMMIT");
      this.metrics.incrementCounter("billable_events_total", { kind: event.eventType, status: "recorded" });
      return {
        eventId,
        status: "recorded",
        timestamp: /* @__PURE__ */ new Date()
      };
    } catch (error) {
      await client6.query("ROLLBACK");
      this.metrics.incrementCounter("billable_events_total", { kind: event.eventType, status: "error" });
      throw error;
    } finally {
      client6.release();
    }
  }
  /**
   * Records a batch of billable events.
   * Note: For efficiency, we might batch provenance writes or do them async,
   * but for "Auditable" requirement, we keep it strict for now.
   */
  async recordBatch(events) {
    const receipts = [];
    for (const event of events) {
      try {
        const receipt = await this.recordUsage(event);
        receipts.push(receipt);
      } catch (e) {
        receipts.push({
          eventId: "error",
          status: "rejected",
          timestamp: /* @__PURE__ */ new Date()
          // error: e.message
        });
      }
    }
    return receipts;
  }
  async getUsagePreview(tenantId, start, end) {
    const query3 = `
      SELECT kind, unit, SUM(quantity) as total_quantity
      FROM usage_events
      WHERE tenant_id = $1
      AND occurred_at >= $2
      AND occurred_at <= $3
      GROUP BY kind, unit
    `;
    const result2 = await pool.query(query3, [tenantId, start, end]);
    const breakdown = {};
    let estimatedCost = 0;
    for (const row of result2.rows) {
      const kind = row.kind;
      const qty = parseFloat(row.total_quantity);
      const unitCost = this.getUnitCost(kind);
      const cost = qty * unitCost;
      breakdown[kind] = {
        quantity: qty,
        unit: row.unit,
        cost
      };
      estimatedCost += cost;
    }
    return {
      tenantId,
      periodStart: start,
      periodEnd: end,
      totalCost: estimatedCost,
      breakdown
    };
  }
  getUnitCost(kind) {
    switch (kind) {
      case "planning_run":
        return 0.1;
      case "read_query":
        return 1e-3;
      case "write_action":
        return 0.05;
      case "token":
        return 2e-5;
      default:
        return 0;
    }
  }
};
var meteringService = MeteringService.getInstance();

// src/routes/billing.ts
init_auth4();
import { z as z11 } from "zod";
var router4 = express3.Router();
var previewSchema = z11.object({
  start: z11.string().datetime(),
  end: z11.string().datetime()
});
router4.get("/usage/preview", ensureAuthenticated, async (req, res, next) => {
  try {
    const { start, end } = previewSchema.parse(req.query);
    const tenantId = req.user.tenantId;
    if (!tenantId) {
      res.status(401).json({ error: "Tenant context required" });
      return;
    }
    const preview = await meteringService.getUsagePreview(
      tenantId,
      new Date(start),
      new Date(end)
    );
    res.json(preview);
  } catch (error) {
    next(error);
  }
});
var billing_default = router4;

// src/routes/entity-resolution.ts
import express4 from "express";

// src/services/entity-resolution/scoring.ts
function levenshteinDistance(a, b) {
  if (a.length === 0) return b.length;
  if (b.length === 0) return a.length;
  const matrix = [];
  for (let i = 0; i <= b.length; i++) {
    matrix[i] = [i];
  }
  for (let j = 0; j <= a.length; j++) {
    matrix[0][j] = j;
  }
  for (let i = 1; i <= b.length; i++) {
    for (let j = 1; j <= a.length; j++) {
      if (b.charAt(i - 1) == a.charAt(j - 1)) {
        matrix[i][j] = matrix[i - 1][j - 1];
      } else {
        matrix[i][j] = Math.min(
          matrix[i - 1][j - 1] + 1,
          Math.min(matrix[i][j - 1] + 1, matrix[i - 1][j] + 1)
        );
      }
    }
  }
  return matrix[b.length][a.length];
}
function stringSimilarity(s1, s2) {
  if (!s1 || !s2) return 0;
  const longer = s1.length > s2.length ? s1 : s2;
  if (longer.length === 0) return 1;
  const distance = levenshteinDistance(s1.toLowerCase(), s2.toLowerCase());
  return (longer.length - distance) / longer.length;
}
var ScoringEngine = class {
  config;
  constructor(config9) {
    this.config = {
      thresholds: { merge: 0.95, link: 0.8, review: 0.6 },
      weights: {
        exactId: 1,
        email: 0.9,
        phone: 0.8,
        name: 0.6,
        ...config9?.weights
      },
      enabledFeatures: ["exactId", "email", "phone", "name", ...config9?.enabledFeatures || []],
      ...config9
    };
  }
  calculateScore(source, target) {
    const features = [];
    let totalScore = 0;
    let totalWeight = 0;
    const reasons = [];
    if (this.config.enabledFeatures.includes("exactId")) {
      const sourceIds = this.extractIdentifiers(source);
      const targetIds = this.extractIdentifiers(target);
      const match = sourceIds.some((id) => targetIds.includes(id));
      const score = match ? 1 : 0;
      const weight = this.config.weights.exactId;
      if (sourceIds.length > 0 && targetIds.length > 0) {
        features.push({ name: "exactId", score, weight });
        totalScore += score * weight;
        totalWeight += weight;
        if (match) reasons.push("Exact identifier match");
      }
    }
    if (this.config.enabledFeatures.includes("email")) {
      const sourceEmails = this.extractEmails(source);
      const targetEmails = this.extractEmails(target);
      const match = sourceEmails.some((e) => targetEmails.includes(e));
      const score = match ? 1 : 0;
      const weight = this.config.weights.email;
      if (sourceEmails.length > 0 && targetEmails.length > 0) {
        features.push({ name: "email", score, weight });
        totalScore += score * weight;
        totalWeight += weight;
        if (match) reasons.push("Email address match");
      }
    }
    if (this.config.enabledFeatures.includes("name")) {
      const sourceName = source.properties.name || "";
      const targetName = target.properties.name || "";
      if (sourceName && targetName) {
        const score = stringSimilarity(sourceName, targetName);
        const weight = this.config.weights.name;
        features.push({ name: "name", score, weight });
        totalScore += score * weight;
        totalWeight += weight;
        if (score > 0.8) reasons.push(`High name similarity (${(score * 100).toFixed(0)}%)`);
      }
    }
    if (this.config.enabledFeatures.includes("phone")) {
      const sourcePhones = this.extractPhones(source);
      const targetPhones = this.extractPhones(target);
      const match = sourcePhones.some((p) => targetPhones.includes(p));
      const score = match ? 1 : 0;
      const weight = this.config.weights.phone;
      if (sourcePhones.length > 0 && targetPhones.length > 0) {
        features.push({ name: "phone", score, weight });
        totalScore += score * weight;
        totalWeight += weight;
        if (match) reasons.push("Phone number match");
      }
    }
    const finalScore = totalWeight > 0 ? totalScore / totalWeight : 0;
    return {
      score: finalScore,
      features,
      reasons
    };
  }
  extractIdentifiers(entity) {
    const ids = [];
    const keys = ["ssn", "passport", "driverLicense", "taxId", "externalId"];
    for (const key of keys) {
      if (entity.properties[key]) ids.push(String(entity.properties[key]));
    }
    return ids;
  }
  extractEmails(entity) {
    if (entity.properties.email) return [entity.properties.email.toLowerCase()];
    if (Array.isArray(entity.properties.emails)) return entity.properties.emails.map((e) => e.toLowerCase());
    return [];
  }
  extractPhones(entity) {
    if (entity.properties.phone) return [entity.properties.phone.replace(/\D/g, "")];
    if (Array.isArray(entity.properties.phones)) return entity.properties.phones.map((p) => p.replace(/\D/g, ""));
    return [];
  }
};

// src/services/entity-resolution/service.ts
init_ledger();

// src/graph/neo4j.ts
import neo4j3 from "neo4j-driver";

// src/lib/resources/QuotaEnforcer.ts
init_RateLimiter();

// src/lib/resources/QuotaConfig.ts
import fs6 from "fs/promises";
import path7 from "path";
import pino38 from "pino";
var logger36 = pino38({ name: "QuotaConfig" });
var DEFAULT_PLANS = {
  starter: {
    api_rpm: 100,
    ingest_eps: 10,
    egress_gb_day: 1
  },
  standard: {
    api_rpm: 6e3,
    ingest_eps: 1e3,
    egress_gb_day: 50
  },
  premium: {
    api_rpm: 12e3,
    ingest_eps: 2e3,
    egress_gb_day: 200
  },
  enterprise: {
    api_rpm: 1e5,
    ingest_eps: 1e4,
    egress_gb_day: 1e3
  }
};
var DEFAULT_STATE = {
  tenantPlans: {
    "demo-tenant": "starter",
    "acme-corp": "standard",
    "massive-dynamic": "premium"
  },
  tenantOverrides: {
    "acme-corp": {
      api_rpm: 8e3
    }
  },
  featureAllowlist: {
    write_aware_sharding: ["massive-dynamic", "acme-corp"],
    entity_resolution_v1: ["massive-dynamic"]
  }
};
var QuotaConfigService = class _QuotaConfigService {
  static instance;
  state = DEFAULT_STATE;
  storagePath;
  constructor() {
    this.storagePath = path7.join(process.cwd(), "data", "quota-config.json");
    this.loadConfig();
  }
  static getInstance() {
    if (!_QuotaConfigService.instance) {
      _QuotaConfigService.instance = new _QuotaConfigService();
    }
    return _QuotaConfigService.instance;
  }
  async loadConfig() {
    try {
      const data = await fs6.readFile(this.storagePath, "utf-8");
      this.state = JSON.parse(data);
      logger36.info("Loaded quota configuration");
    } catch (error) {
      const err = error;
      if (err.code !== "ENOENT") {
        logger36.error({ err: error }, "Failed to load quota config, using defaults");
      } else {
        await this.saveConfig();
      }
    }
  }
  async saveConfig() {
    try {
      await fs6.mkdir(path7.dirname(this.storagePath), { recursive: true });
      await fs6.writeFile(this.storagePath, JSON.stringify(this.state, null, 2));
    } catch (error) {
      logger36.error({ err: error }, "Failed to save quota config");
    }
  }
  getTenantPlan(tenantId) {
    return this.state.tenantPlans[tenantId] || "starter";
  }
  getTenantOverrides(tenantId) {
    return this.state.tenantOverrides[tenantId] || {};
  }
  getFeatureAllowlist(feature) {
    return this.state.featureAllowlist[feature] || [];
  }
  // Admin Methods
  async setTenantPlan(tenantId, plan) {
    this.state.tenantPlans[tenantId] = plan;
    await this.saveConfig();
  }
  async setTenantOverride(tenantId, limits) {
    this.state.tenantOverrides[tenantId] = {
      ...this.state.tenantOverrides[tenantId],
      ...limits
    };
    await this.saveConfig();
  }
};
var quotaConfigService = QuotaConfigService.getInstance();

// src/lib/resources/QuotaEnforcer.ts
init_metrics3();
import pino39 from "pino";
var logger37 = pino39({ name: "QuotaEnforcer" });
var QuotaEnforcer = class _QuotaEnforcer {
  static instance;
  metrics;
  constructor() {
    this.metrics = new PrometheusMetrics("quota_enforcer");
    this.metrics.createCounter("quota_rejections_total", "Total quota rejections", [
      "tenant_id",
      "reason"
    ]);
    this.metrics.createCounter(
      "feature_access_denied_total",
      "Total feature access denials",
      ["tenant_id", "feature"]
    );
  }
  static getInstance() {
    if (!_QuotaEnforcer.instance) {
      _QuotaEnforcer.instance = new _QuotaEnforcer();
    }
    return _QuotaEnforcer.instance;
  }
  getLimits(tenantId) {
    const plan = quotaConfigService.getTenantPlan(tenantId);
    const baseLimits = DEFAULT_PLANS[plan];
    const overrides = quotaConfigService.getTenantOverrides(tenantId);
    return {
      ...baseLimits,
      ...overrides
    };
  }
  /**
   * Check if a tenant is allowed to use a specific feature (e.g. 'write_aware_sharding')
   */
  isFeatureAllowed(tenantId, feature) {
    const allowedTenants = quotaConfigService.getFeatureAllowlist(feature);
    const allowed = allowedTenants.includes(tenantId);
    if (!allowed) {
      this.metrics.incrementCounter("feature_access_denied_total", {
        tenant_id: tenantId,
        feature
      });
    }
    return allowed;
  }
  /**
   * Enforce API Requests Per Minute (RPM) quota.
   */
  async checkApiQuota(tenantId) {
    const limits = this.getLimits(tenantId);
    const result2 = await rateLimiter.checkLimit(
      `quota:${tenantId}:api_rpm`,
      limits.api_rpm,
      60 * 1e3
    );
    if (!result2.allowed) {
      this.metrics.incrementCounter("quota_rejections_total", {
        tenant_id: tenantId,
        reason: "api_rpm"
      });
    }
    return {
      allowed: result2.allowed,
      limit: result2.total,
      remaining: result2.remaining,
      reset: result2.reset,
      reason: result2.allowed ? void 0 : "API_RPM_EXCEEDED"
    };
  }
  /**
   * Enforce Ingest Events Per Second (EPS) quota.
   * @param count Number of events in this batch
   */
  async checkIngestQuota(tenantId, count = 1) {
    const limits = this.getLimits(tenantId);
    const result2 = await rateLimiter.checkLimit(
      `quota:${tenantId}:ingest_eps`,
      limits.ingest_eps,
      1e3,
      count
    );
    if (!result2.allowed) {
      this.metrics.incrementCounter("quota_rejections_total", {
        tenant_id: tenantId,
        reason: "ingest_eps"
      });
    }
    return {
      allowed: result2.allowed,
      limit: result2.total,
      remaining: result2.remaining,
      reset: result2.reset,
      reason: result2.allowed ? void 0 : "INGEST_EPS_EXCEEDED"
    };
  }
  /**
   * Enforce Egress Volume (GB/Day) quota.
   * @param bytes Number of bytes to check/consume
   */
  async checkEgressQuota(tenantId, bytes) {
    const limits = this.getLimits(tenantId);
    const limitBytes = limits.egress_gb_day * 1024 * 1024 * 1024;
    const windowMs = 24 * 60 * 60 * 1e3;
    const result2 = await rateLimiter.checkLimit(
      `quota:${tenantId}:egress_day`,
      limitBytes,
      windowMs,
      bytes
    );
    if (!result2.allowed) {
      this.metrics.incrementCounter("quota_rejections_total", {
        tenant_id: tenantId,
        reason: "egress_day"
      });
    }
    return {
      allowed: result2.allowed,
      limit: result2.total,
      remaining: result2.remaining,
      reset: result2.reset,
      reason: result2.allowed ? void 0 : "EGRESS_LIMIT_EXCEEDED"
    };
  }
};
var quotaEnforcer = QuotaEnforcer.getInstance();

// src/graph/neo4j.ts
init_metrics3();

// src/graph/queryCache.ts
import crypto20 from "node:crypto";

// src/cache/responseCache.ts
init_database();

// src/metrics/cacheMetrics.ts
init_registry();
import { Counter as Counter10, Gauge as Gauge8, Histogram as Histogram7 } from "prom-client";
var cacheHits = new Counter10({
  name: "cache_hits_total",
  help: "Total cache hits",
  labelNames: ["store", "op", "tenant"],
  registers: [registry]
});
var cacheMisses = new Counter10({
  name: "cache_misses_total",
  help: "Total cache misses",
  labelNames: ["store", "op", "tenant"],
  registers: [registry]
});
var cacheSets = new Counter10({
  name: "cache_sets_total",
  help: "Total cache writes",
  labelNames: ["store", "op", "tenant"],
  registers: [registry]
});
var cacheInvalidations = new Counter10({
  name: "cache_invalidations_total",
  help: "Cache invalidations executed",
  labelNames: ["pattern", "tenant"],
  registers: [registry]
});
var cacheLatencySeconds = new Histogram7({
  name: "cache_latency_seconds",
  help: "Latency of cache lookups in seconds",
  labelNames: ["op", "result", "tenant"],
  buckets: [1e-3, 5e-3, 0.01, 0.05, 0.1, 0.25, 0.5, 1, 2],
  registers: [registry]
});
var cacheLocalSize = new Gauge8({
  name: "cache_local_entries",
  help: "Entries in in-memory fallback cache",
  labelNames: ["namespace"],
  registers: [registry]
});
var cacheEvictions = new Counter10({
  name: "cache_evictions_total",
  help: "Cache evictions",
  labelNames: ["store", "reason"],
  registers: [registry]
});
var cacheBypassTotal = new Counter10({
  name: "cache_bypass_total",
  help: "Cache bypass reasons",
  labelNames: ["op", "reason", "tenant"],
  registers: [registry]
});
var cacheHitRatio = new Gauge8({
  name: "cache_hit_ratio",
  help: "Hit ratio for cache namespaces",
  labelNames: ["store", "op"],
  registers: [registry]
});
function recHit(store, op, tenant) {
  cacheHits?.labels?.(store, op, tenant ?? "unknown")?.inc?.();
}
function recMiss(store, op, tenant) {
  cacheMisses?.labels?.(store, op, tenant ?? "unknown")?.inc?.();
}
function recSet(store, op, tenant) {
  cacheSets?.labels?.(store, op, tenant ?? "unknown")?.inc?.();
}
function recEviction(store, reason) {
  cacheEvictions?.labels?.(store, reason)?.inc?.();
}
function setHitRatio(store, op, hits, misses) {
  const total = hits + misses;
  cacheHitRatio?.labels?.(store, op)?.set?.(total === 0 ? 0 : hits / total);
}

// src/cache/responseCache.ts
var MemoryTier = class {
  cache = /* @__PURE__ */ new Map();
  name = "memory";
  async get(key) {
    const entry = this.cache.get(key);
    if (!entry) return null;
    if (Date.now() - entry.ts > entry.ttl * 1e3) {
      this.cache.delete(key);
      return null;
    }
    return entry.val;
  }
  async set(key, value, ttl) {
    this.cache.set(key, { ts: Date.now(), ttl, val: value });
    if (this.cache.size > 1e4) {
      const firstKey = this.cache.keys().next().value;
      if (firstKey !== void 0) {
        this.cache.delete(firstKey);
      }
    }
    cacheLocalSize?.labels?.("default")?.set?.(this.cache.size);
  }
  async del(key) {
    this.cache.delete(key);
  }
  clear() {
    this.cache.clear();
  }
};
var RedisTier = class {
  name = "redis";
  async get(key) {
    const redis5 = getRedisClient();
    if (!redis5) return null;
    return redis5.get(key);
  }
  async set(key, value, ttl) {
    const redis5 = getRedisClient();
    if (!redis5) return;
    await redis5.set(key, value, "EX", ttl);
  }
  async del(key) {
    const redis5 = getRedisClient();
    if (!redis5) return;
    await redis5.del(key);
  }
  // Redis specific methods for tagging
  async addTag(tag, key) {
    const redis5 = getRedisClient();
    if (!redis5) return;
    await redis5.sAdd(`idx:${tag}`, key);
    await redis5.expire(`idx:${tag}`, 86400);
  }
  async invalidateByTag(tag) {
    const redis5 = getRedisClient();
    if (!redis5) return;
    const keys = await redis5.sMembers(`idx:${tag}`);
    if (keys.length > 0) {
      await redis5.del(...keys);
      await redis5.del(`idx:${tag}`);
    }
  }
};
var l1 = new MemoryTier();
var l2 = new RedisTier();
async function getCachedJson(key, options2 = {}) {
  const ttl = options2.ttlSeconds ?? 60;
  const l1Hit = await l1.get(key);
  if (l1Hit) return JSON.parse(l1Hit);
  try {
    const l2Hit = await l2.get(key);
    if (l2Hit) {
      await l1.set(key, l2Hit, ttl);
      return JSON.parse(l2Hit);
    }
  } catch (e) {
  }
  return null;
}
async function setCachedJson(key, payload, options2 = {}) {
  const ttl = options2.ttlSeconds ?? 60;
  const valStr = JSON.stringify(payload);
  await l1.set(key, valStr, ttl);
  try {
    await l2.set(key, valStr, ttl);
    if (options2.indexPrefixes) {
      for (const prefix of options2.indexPrefixes) {
        await l2.addTag(prefix, key);
      }
    }
  } catch (e) {
  }
}
function flushLocalCache() {
  l1.clear();
}
async function invalidateCache(tag, tenantId) {
  await l2.invalidateByTag(tag);
  if (tenantId) {
    await l2.invalidateByTag(`${tag}:${tenantId}`);
  }
}

// src/graph/queryCache.ts
var DEFAULT_CACHE_TTL_SECONDS = parseInt(process.env.GRAPH_QUERY_CACHE_TTL || "15", 10);
function normalizeQuery2(query3) {
  return query3.replace(/\s+/g, " ").trim();
}
function stableHash(input) {
  return crypto20.createHash("sha1").update(stableStringify(input)).digest("hex");
}
function stableStringify(value) {
  if (value === null || value === void 0) return String(value);
  if (typeof value !== "object") return JSON.stringify(value);
  if (Array.isArray(value)) return `[${value.map((v) => stableStringify(v)).join(",")}]`;
  const obj = value;
  const keys = Object.keys(obj).sort();
  return `{${keys.map((k) => `${JSON.stringify(k)}:${stableStringify(obj[k])}`).join(",")}}`;
}
function buildGraphCacheKey(ctx) {
  const tenantId = ctx.tenantId || "global";
  const caseId = ctx.caseId || "global";
  const permissionsHash = ctx.permissionsHash || "open";
  const normalizedQuery = normalizeQuery2(ctx.query);
  const paramsHash = stableHash(ctx.params || {});
  const composite = `${tenantId}:${caseId}:${normalizedQuery}:${paramsHash}:${permissionsHash}`;
  const cacheKey = `graph:query:${crypto20.createHash("sha1").update(composite).digest("hex")}`;
  const tags = [
    "graph:query",
    `graph:query:tenant:${tenantId}`,
    `graph:query:case:${tenantId}:${caseId}`,
    `graph:query:perm:${permissionsHash}`
  ];
  return {
    cacheKey,
    tags,
    tenantId,
    caseId,
    permissionsHash,
    normalizedQuery,
    paramsHash
  };
}
async function runWithGraphQueryCache(ctx, fetcher) {
  const { cacheKey, tags, tenantId } = buildGraphCacheKey(ctx);
  const ttlSeconds = ctx.ttlSeconds ?? DEFAULT_CACHE_TTL_SECONDS;
  const op = ctx.op || "graph-query";
  const tenantLabel = tenantId || "unknown";
  const start = Date.now();
  const cached = await getCachedJson(cacheKey, { ttlSeconds });
  if (cached !== null) {
    recHit("graph-cache", op, tenantLabel);
    cacheLatencySeconds?.labels?.(op, "hit", tenantLabel)?.observe?.((Date.now() - start) / 1e3);
    return cached;
  }
  recMiss("graph-cache", op, tenantLabel);
  const fresh = await fetcher();
  await setCachedJson(cacheKey, fresh, { ttlSeconds, indexPrefixes: tags });
  recSet("graph-cache", op, tenantLabel);
  cacheLatencySeconds?.labels?.(op, "miss", tenantLabel)?.observe?.((Date.now() - start) / 1e3);
  return fresh;
}
async function invalidateGraphQueryCache(options2) {
  const tenantId = options2.tenantId || "global";
  const caseId = options2.caseId || "global";
  const permissionsHash = options2.permissionsHash;
  const tags = /* @__PURE__ */ new Set();
  tags.add("graph:query");
  tags.add(`graph:query:tenant:${tenantId}`);
  tags.add(`graph:query:case:${tenantId}:${caseId}`);
  if (permissionsHash) {
    tags.add(`graph:query:perm:${permissionsHash}`);
  }
  for (const tag of tags) {
    await invalidateCache(tag, tenantId);
  }
  flushLocalCache();
}
function recordCacheBypass(reason, op, tenantId) {
  cacheBypassTotal?.labels?.(op, reason, tenantId || "unknown")?.inc?.();
}

// src/graph/neo4j.ts
var primaryDriver = null;
var replicaDriver = null;
var metrics5 = new PrometheusMetrics("neo4j_driver");
metrics5.createGauge("active_sessions", "Number of active Neo4j sessions");
metrics5.createHistogram("query_duration_seconds", "Neo4j query duration", { buckets: [0.01, 0.05, 0.1, 0.5, 1, 5] });
metrics5.createCounter("replica_fallbacks", "Replica fallbacks when read path fails");
metrics5.createCounter("route_choice_total", "Route selection for graph queries");
metrics5.createCounter("sticky_reads", "Sticky reads routed to primary after write");
var activeSessions = 0;
var recentWrites = /* @__PURE__ */ new Map();
var stickyWindowMs = parseInt(process.env.GRAPH_STICKY_MS || "3000", 10);
function readReplicaConfigured() {
  return process.env.READ_REPLICA === "1" && (process.env.NEO4J_READ_URI || process.env.NEO4J_REPLICA_URI);
}
function stickyKey(tenantId, caseId) {
  return `${tenantId || "global"}::${caseId || "global"}`;
}
function markRecentWrite(options2) {
  recentWrites.set(stickyKey(options2.tenantId, options2.caseId), Date.now());
}
function shouldStickToPrimary(options2) {
  const key = stickyKey(options2.tenantId, options2.caseId);
  const ts = recentWrites.get(key);
  if (!ts) return false;
  const fresh = Date.now() - ts < stickyWindowMs;
  if (!fresh) recentWrites.delete(key);
  return fresh;
}
function buildDriver(uri) {
  return neo4j3.driver(
    uri,
    neo4j3.auth.basic(process.env.NEO4J_USER || "neo4j", process.env.NEO4J_PASSWORD || "password"),
    { disableLosslessIntegers: true }
  );
}
function getDriver(target = "primary") {
  const wantsReplica = target === "replica" && readReplicaConfigured();
  if (wantsReplica) {
    if (!replicaDriver) {
      replicaDriver = buildDriver(process.env.NEO4J_READ_URI || process.env.NEO4J_REPLICA_URI || process.env.NEO4J_URI || "bolt://localhost:7687");
    }
    return replicaDriver;
  }
  if (!primaryDriver) {
    if (!process.env.NEO4J_URI || !process.env.NEO4J_USER || !process.env.NEO4J_PASSWORD) {
      if (process.env.NODE_ENV === "test") {
        throw new Error("Neo4j env vars missing in test");
      }
      console.warn("Neo4j environment variables missing");
    }
    primaryDriver = buildDriver(process.env.NEO4J_URI || "bolt://localhost:7687");
  }
  return primaryDriver;
}
async function executeWithDriver(driver3, cypher, params, mode, route) {
  const session = driver3.session({
    defaultAccessMode: mode === "write" ? neo4j3.session.WRITE : neo4j3.session.READ
  });
  activeSessions++;
  metrics5.setGauge("active_sessions", activeSessions, { route });
  const start = Date.now();
  try {
    const res = await session.run(cypher, params);
    const duration = (Date.now() - start) / 1e3;
    metrics5.observeHistogram("query_duration_seconds", duration, { route, mode });
    return res.records.map((r) => r.toObject());
  } finally {
    await session.close();
    activeSessions--;
    metrics5.setGauge("active_sessions", activeSessions, { route });
  }
}
async function runCypher(cypher, params = {}, options2 = {}) {
  if (options2.write && options2.tenantId) {
    const featureAllowed = quotaEnforcer.isFeatureAllowed(options2.tenantId, "write_aware_sharding");
    const MAX_CONCURRENT_WRITES = 50;
    if (activeSessions > MAX_CONCURRENT_WRITES) {
      throw new Error("Database write queue full (Queue Depth Guard)");
    }
    if (featureAllowed) {
    }
  }
  const isWrite = !!options2.write;
  const stickyPrimary = shouldStickToPrimary(options2);
  if (stickyPrimary && !isWrite) {
    metrics5.incrementCounter("sticky_reads", { scope: options2.caseId ? "case" : "tenant" });
  }
  const preferReplica = readReplicaConfigured() && !isWrite && !stickyPrimary;
  const routes = preferReplica ? ["replica", "primary"] : ["primary"];
  const fetchFromDb = async () => {
    let lastError;
    for (const route of routes) {
      try {
        const driver3 = getDriver(route);
        metrics5.incrementCounter("route_choice_total", { target: route });
        return await executeWithDriver(driver3, cypher, params, isWrite ? "write" : "read", route);
      } catch (err) {
        lastError = err;
        if (route === "replica") {
          const errorMessage = err instanceof Error ? err.message : "replica_error";
          metrics5.incrementCounter("replica_fallbacks", { reason: errorMessage });
          continue;
        }
        throw err;
      }
    }
    throw lastError;
  };
  const cacheEnabled = process.env.QUERY_CACHE === "1" && !isWrite && options2.bypassCache !== true;
  const tenantLabel = options2.tenantId || "unknown";
  let result2;
  if (cacheEnabled) {
    result2 = await runWithGraphQueryCache(
      {
        query: cypher,
        params,
        tenantId: options2.tenantId,
        caseId: options2.caseId,
        permissionsHash: options2.permissionsHash,
        ttlSeconds: options2.cacheTtlSeconds,
        op: "graph-query"
      },
      fetchFromDb
    );
  } else {
    const reason = isWrite ? "write" : options2.bypassCache ? "explicit_bypass" : process.env.QUERY_CACHE === "1" ? "sticky_or_other" : "disabled";
    recordCacheBypass(reason, "graph-query", tenantLabel);
    result2 = await fetchFromDb();
  }
  if (isWrite) {
    markRecentWrite(options2);
    await invalidateGraphQueryCache({
      tenantId: options2.tenantId,
      caseId: options2.caseId,
      permissionsHash: options2.permissionsHash
    });
  }
  return result2;
}

// src/services/entity-resolution/service.ts
init_tracer();
var EntityResolutionService = class {
  scoringEngine;
  config;
  constructor(config9) {
    this.config = {
      thresholds: { merge: 0.95, link: 0.8, review: 0.6 },
      weights: {
        exactId: 1,
        email: 0.9,
        phone: 0.8,
        name: 0.6
      },
      enabledFeatures: ["exactId", "email", "phone", "name"],
      ...config9
    };
    this.scoringEngine = new ScoringEngine(this.config);
  }
  /**
   * Evaluates a batch of new entities against the existing graph to propose resolutions.
   * This is a simplified "blocking" approach where we query for potential candidates.
   */
  async resolveBatch(entities) {
    return getTracer().withSpan("EntityResolutionService.resolveBatch", async (span) => {
      span.setAttribute("er.batch_size", entities.length);
      const { default: pLimit } = await import("p-limit");
      const limit = pLimit(10);
      const decisions = [];
      const tasks = entities.map((entity) => limit(async () => {
        const candidates2 = await this.findCandidates(entity);
        for (const candidateEntity of candidates2) {
          const { score, features, reasons } = this.scoringEngine.calculateScore(entity, candidateEntity);
          const decisionType = this.makeDecision(score);
          if (decisionType !== "NO_MATCH") {
            decisions.push({
              candidate: {
                sourceEntityId: entity.id,
                targetEntityId: candidateEntity.id,
                overallScore: score,
                features,
                reasons
              },
              decision: decisionType,
              confidence: score,
              ruleId: "default-weighted-score"
            });
          }
        }
      }));
      await Promise.all(tasks);
      return decisions;
    });
  }
  /**
   * Executes a resolution decision (Merge or Link).
   */
  async applyDecision(decision, tenantId, actorId) {
    const { sourceEntityId, targetEntityId } = decision.candidate;
    if (decision.decision === "MERGE") {
      await this.executeMerge(sourceEntityId, targetEntityId, tenantId, actorId, decision);
    } else if (decision.decision === "LINK") {
      await this.executeLink(sourceEntityId, targetEntityId, tenantId, actorId, decision);
    }
  }
  makeDecision(score) {
    if (score >= this.config.thresholds.merge) return "MERGE";
    if (score >= this.config.thresholds.link) return "LINK";
    if (score >= this.config.thresholds.review) return "REVIEW";
    return "NO_MATCH";
  }
  async findCandidates(entity) {
    const driver3 = getDriver();
    const session = driver3.session();
    const cypher = `
      MATCH (n:Entity {tenantId: $tenantId})
      WHERE n.id <> $id AND (
        (n.email IS NOT NULL AND n.email = $email) OR
        (n.name IS NOT NULL AND n.name = $name) OR
        (n.ssn IS NOT NULL AND n.ssn = $ssn)
      )
      RETURN n { .* } as properties, labels(n) as labels, n.id as id
    `;
    try {
      const result2 = await session.run(cypher, {
        tenantId: entity.tenantId,
        id: entity.id,
        email: entity.properties.email || "",
        name: entity.properties.name || "",
        ssn: entity.properties.ssn || ""
      });
      return result2.records.map((record2) => {
        const props = record2.get("properties");
        const id = record2.get("id");
        return {
          id,
          type: "Entity",
          // Simplified
          properties: props,
          tenantId: entity.tenantId
        };
      });
    } catch (error) {
      console.error("Error finding candidates:", error);
      return [];
    } finally {
      await session.close();
    }
  }
  async executeMerge(sourceId, targetId, tenantId, actorId, decision) {
    const driver3 = getDriver();
    const session = driver3.session();
    try {
      await provenanceLedger.appendEntry({
        tenantId,
        actionType: "ENTITY_MERGE",
        resourceType: "entity",
        resourceId: targetId,
        actorId,
        actorType: "system",
        payload: {
          sourceId,
          targetId,
          decision
        },
        metadata: {
          purpose: "Entity Resolution Merge"
        }
      });
      const cypher = `
            MATCH (source:Entity {id: $sourceId})
            MATCH (target:Entity {id: $targetId})
            CALL apoc.refactor.mergeNodes([target, source], {
                properties: {
                    mode: 'discard'
                },
                mergeRels: true
            }) YIELD node
            RETURN node
         `;
      await session.executeWrite((tx) => tx.run(cypher, { sourceId, targetId }));
    } finally {
      await session.close();
    }
  }
  async executeLink(sourceId, targetId, tenantId, actorId, decision) {
    const driver3 = getDriver();
    const session = driver3.session();
    try {
      await provenanceLedger.appendEntry({
        tenantId,
        actionType: "ENTITY_LINK",
        resourceType: "entity",
        resourceId: sourceId,
        // Logging on both or source
        actorId,
        actorType: "system",
        payload: {
          sourceId,
          targetId,
          decision
        },
        metadata: {
          purpose: "Entity Resolution Link"
        }
      });
      const cypher = `
            MATCH (source:Entity {id: $sourceId})
            MATCH (target:Entity {id: $targetId})
            MERGE (source)-[r:SAME_AS]->(target)
            SET r.confidence = $confidence, r.createdAt = datetime()
        `;
      await session.executeWrite((tx) => tx.run(cypher, {
        sourceId,
        targetId,
        confidence: decision.confidence
      }));
    } finally {
      await session.close();
    }
  }
};

// src/services/entity-resolution/quality.ts
var DataQualityService = class {
  async getQualityMetrics(tenantId) {
    const metrics8 = [];
    const driver3 = getDriver();
    const session = driver3.session();
    try {
      const completenessResult = await session.run(`
            MATCH (n:Entity {tenantId: $tenantId})
            WITH count(n) as total,
                 count(CASE WHEN n.name IS NULL OR n.name = '' THEN 1 END) as missingName
            RETURN total, missingName
        `, { tenantId });
      if (completenessResult.records.length > 0) {
        const total = completenessResult.records[0].get("total").toNumber();
        const missing = completenessResult.records[0].get("missingName").toNumber();
        if (total > 0) {
          metrics8.push({
            metric: "missing_name_rate",
            value: missing / total,
            timestamp: /* @__PURE__ */ new Date(),
            dimensions: { tenantId }
          });
        }
      }
      const duplicatesResult = await session.run(`
            MATCH (n:Entity {tenantId: $tenantId})
            WHERE n.email IS NOT NULL
            WITH n.email as email, count(*) as c
            WHERE c > 1
            RETURN sum(c) as duplicateCount
        `, { tenantId });
      if (duplicatesResult.records.length > 0) {
        const dupCount = duplicatesResult.records[0].get("duplicateCount").toNumber();
        metrics8.push({
          metric: "duplicate_email_count",
          value: dupCount,
          timestamp: /* @__PURE__ */ new Date(),
          dimensions: { tenantId }
        });
      }
    } catch (error) {
      console.error("Error calculating data quality metrics:", error);
    } finally {
      await session.close();
    }
    return metrics8;
  }
};

// src/services/er/EntityResolutionV2Service.ts
import pino41 from "pino";
import { createHash as createHash10, randomUUID as randomUUID10 } from "crypto";

// src/services/er/soundex.ts
function soundex(name) {
  let s = name.toUpperCase().replace(/[^A-Z]/g, "");
  if (!s) {
    return "";
  }
  const firstChar = s[0];
  const getCode = (c) => {
    const map = {
      B: "1",
      F: "1",
      P: "1",
      V: "1",
      C: "2",
      G: "2",
      J: "2",
      K: "2",
      Q: "2",
      S: "2",
      X: "2",
      Z: "2",
      D: "3",
      T: "3",
      L: "4",
      M: "5",
      N: "5",
      R: "6"
    };
    return map[c] || "0";
  };
  let result2 = firstChar;
  let lastCode = getCode(firstChar);
  for (let i = 1; i < s.length; i++) {
    if (result2.length >= 4) break;
    const char = s[i];
    const code = getCode(char);
    if (char === "H" || char === "W") {
      continue;
    }
    if (code !== "0") {
      if (code !== lastCode) {
        result2 += code;
        lastCode = code;
      }
    } else {
      lastCode = "0";
    }
  }
  while (result2.length < 4) {
    result2 += "0";
  }
  return result2;
}

// src/services/er/EntityResolutionV2Service.ts
init_database();

// src/lib/dlq/index.ts
init_database();
import { EventEmitter as EventEmitter9 } from "events";
import crypto21 from "crypto";
import pino40 from "pino";
var logger38 = pino40();
var PostgresDLQ = class extends EventEmitter9 {
  queueName;
  constructor(queueName) {
    super();
    this.queueName = queueName;
  }
  async enqueue(message) {
    const id = message.id || crypto21.randomUUID();
    const pool4 = getPostgresPool2();
    const query3 = `
      INSERT INTO dlq_messages (id, queue_name, payload, error, retry_count, metadata, created_at)
      VALUES ($1, $2, $3, $4, $5, $6, NOW())
    `;
    const values = [
      id,
      this.queueName,
      JSON.stringify(message.payload),
      message.error,
      message.retryCount,
      JSON.stringify(message.metadata || {})
    ];
    try {
      await pool4.query(query3, values);
      this.emit("enqueued", { id, ...message });
      return id;
    } catch (error) {
      logger38.error({ err: error, queue: this.queueName }, "Failed to enqueue DLQ message");
      throw error;
    }
  }
  async dequeue(count = 1) {
    const pool4 = getPostgresPool2();
    const query3 = `
      SELECT id, payload, error, retry_count, metadata, created_at
      FROM dlq_messages
      WHERE queue_name = $1
      ORDER BY created_at ASC
      LIMIT $2
    `;
    try {
      const result2 = await pool4.query(query3, [this.queueName, count]);
      return result2.rows.map((row) => ({
        id: row.id,
        payload: row.payload,
        // pg auto-parses json
        error: row.error,
        retryCount: row.retry_count,
        metadata: row.metadata,
        timestamp: new Date(row.created_at).getTime()
      }));
    } catch (error) {
      logger38.error(
        { err: error, queue: this.queueName },
        "Failed to dequeue DLQ messages"
      );
      throw error;
    }
  }
  async ack(messageId) {
    const pool4 = getPostgresPool2();
    try {
      await pool4.query("DELETE FROM dlq_messages WHERE id = $1", [messageId]);
      this.emit("acked", messageId);
    } catch (error) {
      logger38.error({ err: error, messageId }, "Failed to ack DLQ message");
      throw error;
    }
  }
  async nack(messageId) {
    const pool4 = getPostgresPool2();
    try {
      await pool4.query(
        "UPDATE dlq_messages SET retry_count = retry_count + 1 WHERE id = $1",
        [messageId]
      );
      this.emit("nacked", messageId);
    } catch (error) {
      logger38.error({ err: error, messageId }, "Failed to nack DLQ message");
      throw error;
    }
  }
  async count() {
    const pool4 = getPostgresPool2();
    try {
      const result2 = await pool4.query(
        "SELECT COUNT(*) as count FROM dlq_messages WHERE queue_name = $1",
        [this.queueName]
      );
      return parseInt(result2.rows[0].count, 10);
    } catch (error) {
      return 0;
    }
  }
};
var dlqFactory = (queueName) => {
  return new PostgresDLQ(queueName);
};

// src/services/er/EntityResolutionV2Service.ts
init_ledger();

// src/services/er/guardrails.ts
import fs7 from "fs";
import path8 from "path";
var DEFAULT_FIXTURES_PATH = path8.join(
  process.cwd(),
  "tests/fixtures/er/evaluation-fixtures.json"
);
var cachedFixtures = null;
function loadFixtures(fixturesPath) {
  const resolvedPath = fixturesPath || process.env.ER_GUARDRAIL_FIXTURES_PATH || DEFAULT_FIXTURES_PATH;
  if (cachedFixtures && resolvedPath === DEFAULT_FIXTURES_PATH) {
    return cachedFixtures;
  }
  const raw = fs7.readFileSync(resolvedPath, "utf8");
  const parsed = JSON.parse(raw);
  if (!parsed.datasets || !Array.isArray(parsed.datasets)) {
    throw new Error("ER guardrail fixtures missing datasets");
  }
  if (resolvedPath === DEFAULT_FIXTURES_PATH) {
    cachedFixtures = parsed.datasets;
  }
  return parsed.datasets;
}
function getDataset(datasetId, fixtures) {
  const datasets = fixtures || loadFixtures();
  const dataset = datasets.find((item) => item.datasetId === datasetId);
  if (!dataset) {
    throw new Error(`ER guardrail dataset not found: ${datasetId}`);
  }
  return dataset;
}
function computeMetrics(dataset, scoreFn, matchThreshold) {
  let truePositives = 0;
  let falsePositives = 0;
  let falseNegatives = 0;
  for (const pair of dataset.pairs) {
    const score = scoreFn(pair.entityA, pair.entityB);
    const predictedMatch = score >= matchThreshold;
    if (predictedMatch && pair.isMatch) {
      truePositives += 1;
    } else if (predictedMatch && !pair.isMatch) {
      falsePositives += 1;
    } else if (!predictedMatch && pair.isMatch) {
      falseNegatives += 1;
    }
  }
  const precisionDenominator = truePositives + falsePositives;
  const recallDenominator = truePositives + falseNegatives;
  return {
    precision: precisionDenominator === 0 ? 0 : truePositives / precisionDenominator,
    recall: recallDenominator === 0 ? 0 : truePositives / recallDenominator,
    truePositives,
    falsePositives,
    falseNegatives,
    totalPairs: dataset.pairs.length
  };
}
function resolveThresholds() {
  const minPrecision = Number.parseFloat(
    process.env.ER_GUARDRAIL_MIN_PRECISION || "0.85"
  );
  const minRecall = Number.parseFloat(
    process.env.ER_GUARDRAIL_MIN_RECALL || "0.8"
  );
  const matchThreshold = Number.parseFloat(
    process.env.ER_GUARDRAIL_MATCH_THRESHOLD || "0.8"
  );
  return {
    minPrecision,
    minRecall,
    matchThreshold
  };
}
function evaluateGuardrails(datasetId, scoreFn, fixtures) {
  const dataset = getDataset(datasetId, fixtures);
  const thresholds = resolveThresholds();
  const metrics8 = computeMetrics(dataset, scoreFn, thresholds.matchThreshold);
  const passed = metrics8.precision >= thresholds.minPrecision && metrics8.recall >= thresholds.minRecall;
  return {
    datasetId,
    metrics: metrics8,
    thresholds,
    passed,
    evaluatedAt: (/* @__PURE__ */ new Date()).toISOString()
  };
}

// src/services/er/EntityResolutionV2Service.ts
var log = pino41({ name: "EntityResolutionV2Service" });
var MERGE_GUARDRAILS = {
  maxMergeIds: 20,
  maxRelationships: 500
};
var EXPLAIN_METHOD = "rules-v2";
var EXPLAIN_THRESHOLD = 0.7;
var EXPLAIN_WEIGHTS = {
  phonetic: 0.3,
  name_exact: 0.5,
  geo: 0.4,
  crypto: 0.8,
  temporal_overlap: 0.1
};
var EntityResolutionV2Service = class {
  dlq;
  poolProvider;
  constructor({ dlq, pool: pool4 } = {}) {
    this.dlq = dlq ?? dlqFactory("er-merge-conflicts");
    this.poolProvider = () => pool4 ?? getPostgresPool2();
  }
  buildMergeId(masterId, mergeIds) {
    const hash3 = createHash10("sha256").update([masterId, ...mergeIds.sort()].join(":")).digest("hex").slice(0, 20);
    return `merge-${hash3}`;
  }
  async recordMergeConflict({
    mergeId,
    masterId,
    mergeIds,
    reason,
    userContext,
    metadata
  }) {
    await this.dlq.enqueue({
      payload: {
        mergeId,
        masterId,
        mergeIds,
        userId: userContext?.userId || "unknown"
      },
      error: reason,
      retryCount: 0,
      metadata
    });
  }
  async persistRollbackSnapshot({
    mergeId,
    decisionId,
    masterId,
    mergeIds,
    entities,
    relationships,
    userContext,
    metadata
  }) {
    const pool4 = this.poolProvider();
    const snapshotId = randomUUID10();
    const snapshot = {
      entities,
      relationships
    };
    await pool4.query(
      `
        INSERT INTO er_merge_rollback_snapshots (
          id, merge_id, decision_id, master_id, merge_ids,
          snapshot, metadata, created_by
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
        ON CONFLICT (merge_id) DO NOTHING
      `,
      [
        snapshotId,
        mergeId,
        decisionId,
        masterId,
        JSON.stringify(mergeIds),
        JSON.stringify(snapshot),
        JSON.stringify(metadata || {}),
        userContext?.userId || "unknown"
      ]
    );
    return snapshotId;
  }
  async rollbackMergeSnapshot(session, {
    mergeId,
    reason,
    userContext
  }) {
    const pool4 = this.poolProvider();
    const snapshotResult = await pool4.query(
      "SELECT * FROM er_merge_rollback_snapshots WHERE merge_id = $1",
      [mergeId]
    );
    if (snapshotResult.rows.length === 0) {
      throw new Error("Merge snapshot not found");
    }
    const snapshot = snapshotResult.rows[0];
    if (snapshot.restored_at) {
      return {
        success: true,
        snapshotId: snapshot.id,
        decisionId: snapshot.decision_id
      };
    }
    await this.split(session, snapshot.decision_id, userContext);
    await pool4.query(
      `
        UPDATE er_merge_rollback_snapshots
        SET restored_at = NOW(),
            restored_by = $2,
            restore_reason = $3
        WHERE id = $1
      `,
      [snapshot.id, userContext?.userId || "unknown", reason]
    );
    return {
      success: true,
      snapshotId: snapshot.id,
      decisionId: snapshot.decision_id
    };
  }
  generateSignals(entity) {
    const signals = {
      phonetic: [],
      geo: [],
      device: [],
      crypto: [],
      perceptualHashes: [],
      docSignatures: []
    };
    if (entity.properties.name) {
      signals.phonetic.push(soundex(entity.properties.name));
    }
    if (entity.properties.lat && entity.properties.lon) {
      signals.geo.push(`${entity.properties.lat},${entity.properties.lon}`);
    }
    if (entity.properties.userAgent) {
      signals.device.push(entity.properties.userAgent);
    }
    if (entity.properties.cryptoAddress) {
      signals.crypto.push(entity.properties.cryptoAddress);
    }
    if (entity.properties.pHash) {
      signals.perceptualHashes.push(entity.properties.pHash);
    }
    return signals;
  }
  explain(entityA, entityB) {
    const signalsA = this.generateSignals(entityA);
    const signalsB = this.generateSignals(entityB);
    const features = {};
    const rationale = [];
    let phoneticMatch = 0;
    for (const pa of signalsA.phonetic) {
      if (signalsB.phonetic.includes(pa)) {
        phoneticMatch = 1;
        rationale.push(`Phonetic match on soundex code: ${pa}`);
        break;
      }
    }
    features["phonetic"] = phoneticMatch;
    const nameMatch = entityA.properties.name && entityA.properties.name === entityB.properties.name ? 1 : 0;
    features["name_exact"] = nameMatch;
    if (nameMatch) rationale.push("Exact name match");
    let geoMatch = 0;
    for (const ga of signalsA.geo) {
      if (signalsB.geo.includes(ga)) {
        geoMatch = 1;
        rationale.push(`Shared location: ${ga}`);
        break;
      }
    }
    features["geo"] = geoMatch;
    let cryptoMatch = 0;
    for (const ca of signalsA.crypto) {
      if (signalsB.crypto.includes(ca)) {
        cryptoMatch = 1;
        rationale.push(`Shared crypto address: ${ca}`);
        break;
      }
    }
    features["crypto"] = cryptoMatch;
    const overlap = this.checkTemporalOverlap(entityA.properties, entityB.properties);
    features["temporal_overlap"] = overlap ? 1 : 0;
    if (overlap) rationale.push("Temporal validity overlap");
    else if (entityA.properties.validFrom && entityB.properties.validFrom) rationale.push("No temporal overlap");
    const score = phoneticMatch * EXPLAIN_WEIGHTS.phonetic + nameMatch * EXPLAIN_WEIGHTS.name_exact + geoMatch * EXPLAIN_WEIGHTS.geo + cryptoMatch * EXPLAIN_WEIGHTS.crypto + (overlap ? EXPLAIN_WEIGHTS.temporal_overlap : 0);
    const clampedScore = Math.min(score, 1);
    const featureContributions = this.buildFeatureContributions(
      features,
      EXPLAIN_WEIGHTS
    );
    return {
      features,
      rationale,
      score: clampedScore,
      confidence: clampedScore,
      method: EXPLAIN_METHOD,
      threshold: EXPLAIN_THRESHOLD,
      featureWeights: EXPLAIN_WEIGHTS,
      featureContributions
    };
  }
  buildFeatureContributions(features, weights) {
    const entries = Object.entries(weights).map(([feature, weight]) => {
      const rawValue = features[feature] ?? 0;
      const numericValue = typeof rawValue === "boolean" ? rawValue ? 1 : 0 : rawValue;
      return {
        feature,
        value: rawValue,
        weight,
        contribution: numericValue * weight,
        normalizedContribution: 0
      };
    });
    const total = entries.reduce((sum, entry) => sum + entry.contribution, 0);
    return entries.map((entry) => ({
      ...entry,
      normalizedContribution: total > 0 ? entry.contribution / total : 0
    })).sort((a, b) => b.contribution - a.contribution);
  }
  checkTemporalOverlap(t1, t2) {
    if (!t1.validFrom && !t1.validTo && !t2.validFrom && !t2.validTo) return true;
    const start1 = t1.validFrom ? new Date(t1.validFrom).getTime() : -Infinity;
    const end1 = t1.validTo ? new Date(t1.validTo).getTime() : Infinity;
    const start2 = t2.validFrom ? new Date(t2.validFrom).getTime() : -Infinity;
    const end2 = t2.validTo ? new Date(t2.validTo).getTime() : Infinity;
    return Math.max(start1, start2) < Math.min(end1, end2);
  }
  checkPolicy(userContext, entities) {
    const allLabels = /* @__PURE__ */ new Set();
    for (const e of entities) {
      if (e.lacLabels) {
        e.lacLabels.forEach((l) => allLabels.add(l));
      }
    }
    if (allLabels.has("TOP_SECRET") && !userContext?.clearances?.includes("TOP_SECRET")) {
      return false;
    }
    return true;
  }
  /**
   * Evaluate guardrails for merge operations
   */
  evaluateGuardrails(datasetId) {
    const resolvedDatasetId = datasetId || process.env.ER_GUARDRAIL_DATASET_ID || "baseline";
    return evaluateGuardrails(
      resolvedDatasetId,
      (entityA, entityB) => {
        const scored = this.explain(
          {
            id: entityA.id,
            labels: entityA.labels,
            properties: entityA.properties,
            lacLabels: entityA.lacLabels
          },
          {
            id: entityB.id,
            labels: entityB.labels,
            properties: entityB.properties,
            lacLabels: entityB.lacLabels
          }
        );
        return scored.score;
      }
    );
  }
  /**
   * Record guardrail evaluation for audit purposes
   */
  async recordGuardrailEvaluation(result2, context4) {
    log.info({
      event: "guardrail_evaluation_recorded",
      datasetId: result2.datasetId,
      passed: result2.passed,
      userId: context4.userId,
      tenantId: context4.tenantId,
      evaluatedAt: result2.evaluatedAt
    });
  }
  async merge(session, req) {
    const { masterId, mergeIds, userContext, rationale } = req;
    const uniqueMergeIds = Array.from(new Set(mergeIds));
    const mergeId = req.mergeId || this.buildMergeId(masterId, uniqueMergeIds);
    const idempotencyKey = req.idempotencyKey || mergeId;
    const guardrailDatasetId = req.guardrailDatasetId || process.env.ER_GUARDRAIL_DATASET_ID;
    const guardrailResult = guardrailDatasetId ? this.evaluateGuardrails(guardrailDatasetId) : void 0;
    if (uniqueMergeIds.length !== mergeIds.length) {
      await this.recordMergeConflict({
        mergeId,
        masterId,
        mergeIds,
        reason: "Duplicate merge IDs detected",
        userContext
      });
      throw new Error("Duplicate merge IDs detected");
    }
    if (uniqueMergeIds.includes(masterId)) {
      await this.recordMergeConflict({
        mergeId,
        masterId,
        mergeIds,
        reason: "Master entity included in merge set",
        userContext
      });
      throw new Error("Master entity cannot be merged into itself");
    }
    if (uniqueMergeIds.length > MERGE_GUARDRAILS.maxMergeIds) {
      await this.recordMergeConflict({
        mergeId,
        masterId,
        mergeIds: uniqueMergeIds,
        reason: "Merge cardinality exceeds guardrail",
        userContext,
        metadata: { limit: MERGE_GUARDRAILS.maxMergeIds }
      });
      throw new Error("Merge cardinality exceeds guardrail limits");
    }
    if (guardrailResult && !guardrailResult.passed) {
      if (!req.guardrailOverrideReason) {
        await this.recordMergeConflict({
          mergeId,
          masterId,
          mergeIds: uniqueMergeIds,
          reason: "Merge guardrails failed",
          userContext,
          metadata: {
            guardrails: guardrailResult
          }
        });
        throw new Error("Merge guardrails failed; override required");
      }
      await provenanceLedger.appendEntry({
        tenantId: userContext?.tenantId || "system",
        actionType: "ER_GUARDRAIL_OVERRIDE",
        resourceType: "entity-merge",
        resourceId: mergeId,
        actorId: userContext?.userId || "unknown",
        actorType: "user",
        timestamp: /* @__PURE__ */ new Date(),
        payload: {
          mutationType: "MERGE",
          entityId: masterId,
          entityType: "Entity",
          mergeId,
          masterId,
          mergeIds: uniqueMergeIds,
          datasetId: guardrailResult.datasetId,
          thresholds: guardrailResult.thresholds,
          metrics: guardrailResult.metrics,
          reason: req.guardrailOverrideReason
        },
        metadata: {
          evaluatedAt: guardrailResult.evaluatedAt
        }
      });
    }
    const result2 = await session.run(
      `MATCH (n) WHERE n.id IN $ids RETURN n`,
      { ids: [masterId, ...uniqueMergeIds] }
    );
    const entities = result2.records.map((r) => {
      const node = r.get("n");
      return {
        id: node.properties.id,
        labels: node.labels,
        properties: node.properties,
        lacLabels: node.properties.lac_labels || []
      };
    });
    if (entities.length !== uniqueMergeIds.length + 1) {
      await this.recordMergeConflict({
        mergeId,
        masterId,
        mergeIds: uniqueMergeIds,
        reason: "One or more entities not found",
        userContext
      });
      throw new Error("One or more entities not found");
    }
    if (!this.checkPolicy(userContext, entities)) {
      await this.recordMergeConflict({
        mergeId,
        masterId,
        mergeIds: uniqueMergeIds,
        reason: "Policy violation",
        userContext
      });
      throw new Error("Policy violation: Insufficient authority to merge these entities.");
    }
    const tx = session.beginTransaction();
    try {
      const decisionId = randomUUID10();
      const decisionResult = await tx.run(
        `
          MERGE (d:ERDecision {idempotencyKey: $idempotencyKey})
          ON CREATE SET d.id = $decisionId,
                        d.mergeId = $mergeId,
                        d.timestamp = datetime(),
                        d.user = $userId,
                        d.rationale = $rationale,
                        d.originalIds = $mergeIds,
                        d.masterId = $masterId
          WITH d, d.id = $decisionId AS created
          RETURN d.id AS decisionId,
                 d.mergeId AS mergeId,
                 d.masterId AS masterId,
                 d.originalIds AS mergeIds,
                 created
        `,
        {
          decisionId,
          userId: userContext.userId || "unknown",
          rationale,
          mergeIds: uniqueMergeIds,
          masterId,
          mergeId,
          idempotencyKey
        }
      );
      if (decisionResult.records.length === 0) {
        throw new Error("Failed to create merge decision");
      }
      const decisionRecord = decisionResult.records[0];
      const created = decisionRecord.get("created");
      const persistedDecisionId = decisionRecord.get("decisionId");
      const persistedMergeId = decisionRecord.get("mergeId");
      const persistedMasterId = decisionRecord.get("masterId");
      const persistedMergeIds = decisionRecord.get("mergeIds") || [];
      if (!created) {
        const sortedPersisted = [...persistedMergeIds].sort();
        const sortedIncoming = [...uniqueMergeIds].sort();
        const mergeIdsMatch = sortedPersisted.length === sortedIncoming.length && sortedPersisted.every((value, index) => value === sortedIncoming[index]);
        const masterMatch = persistedMasterId === masterId;
        const mergeIdMatch = persistedMergeId === mergeId;
        if (!mergeIdsMatch || !masterMatch || !mergeIdMatch) {
          await this.recordMergeConflict({
            mergeId,
            masterId,
            mergeIds: uniqueMergeIds,
            reason: "Idempotency key conflict with existing merge decision",
            userContext,
            metadata: {
              persisted: {
                mergeId: persistedMergeId,
                masterId: persistedMasterId,
                mergeIds: persistedMergeIds
              }
            }
          });
          throw new Error("Idempotency key conflict detected");
        }
        await tx.commit();
        log.info(
          { mergeId, decisionId: persistedDecisionId },
          "Idempotent merge request detected; skipping duplicate merge"
        );
        return {
          decisionId: persistedDecisionId,
          mergeId: persistedMergeId,
          idempotent: true,
          guardrails: guardrailResult,
          overrideUsed: !!req.guardrailOverrideReason
        };
      }
      const relsResult = await tx.run(`
         MATCH (source)-[r]->(target)
         WHERE (source.id IN $mergeIds OR target.id IN $mergeIds) AND NOT (source:ERDecision OR target:ERDecision)
         RETURN id(r) as relId, type(r) as type, startNode(r).id as startId, endNode(r).id as endId, properties(r) as props
      `, { mergeIds: uniqueMergeIds });
      const relationshipsToArchive = [];
      for (const record2 of relsResult.records) {
        const type = record2.get("type");
        const startId = record2.get("startId");
        const endId = record2.get("endId");
        const props = record2.get("props");
        const relId = record2.get("relId").toString();
        relationshipsToArchive.push({ relId, type, startId, endId, props });
      }
      if (relationshipsToArchive.length > MERGE_GUARDRAILS.maxRelationships) {
        await this.recordMergeConflict({
          mergeId,
          masterId,
          mergeIds: uniqueMergeIds,
          reason: "Relationship cardinality exceeds guardrail",
          userContext,
          metadata: {
            limit: MERGE_GUARDRAILS.maxRelationships,
            actual: relationshipsToArchive.length
          }
        });
        throw new Error("Relationship cardinality exceeds guardrail limits");
      }
      await tx.run(
        `
          MATCH (d:ERDecision {id: $decisionId})
          MATCH (m {id: $masterId})
          MERGE (d)-[:AFFECTS]->(m)
        `,
        {
          decisionId: persistedDecisionId,
          userId: userContext.userId || "unknown",
          rationale,
          mergeIds: uniqueMergeIds,
          masterId,
          mergeId,
          idempotencyKey
        }
      );
      for (const rel of relationshipsToArchive) {
        const { relId, type, startId, endId, props } = rel;
        if (uniqueMergeIds.includes(startId)) {
          const safeType = type.replace(/[^a-zA-Z0-9_]/g, "");
          await tx.run(
            `
              MATCH (m {id: $masterId})
              MATCH (t {id: $targetId})
              // Use MERGE with originalId to ensure we create a distinct edge for this specific original relationship
              MERGE (m)-[newR:${safeType} {originalId: $relId}]->(t)
              SET newR += $props
              SET newR.validFrom = $now
            `,
            {
              masterId,
              targetId: endId,
              props,
              relId,
              now: (/* @__PURE__ */ new Date()).toISOString()
            }
          );
        }
        if (uniqueMergeIds.includes(endId)) {
          const safeType = type.replace(/[^a-zA-Z0-9_]/g, "");
          await tx.run(
            `
              MATCH (s {id: $sourceId})
              MATCH (m {id: $masterId})
              MERGE (s)-[newR:${safeType} {originalId: $relId}]->(m)
              SET newR += $props
              SET newR.validFrom = $now
            `,
            {
              sourceId: startId,
              masterId,
              props,
              relId,
              now: (/* @__PURE__ */ new Date()).toISOString()
            }
          );
        }
      }
      const snapshotId = await this.persistRollbackSnapshot({
        mergeId,
        decisionId: persistedDecisionId,
        masterId,
        mergeIds: uniqueMergeIds,
        entities,
        relationships: relationshipsToArchive,
        userContext,
        metadata: {
          rationale,
          guardrails: MERGE_GUARDRAILS,
          guardrailResult,
          idempotencyKey
        }
      });
      await tx.run(`
        MATCH (d:ERDecision {id: $decisionId})
        SET d.archivedRelationships = $blob
      `, { decisionId: persistedDecisionId, blob: JSON.stringify(relationshipsToArchive) });
      await tx.run(`
        MATCH (n)-[r]-()
        WHERE n.id IN $mergeIds AND NOT n:ERDecision
        SET r.merged = true
        SET r.mergedAt = datetime()
        SET r.mergedByDecision = $decisionId
      `, { mergeIds: uniqueMergeIds, decisionId: persistedDecisionId });
      for (const id of uniqueMergeIds) {
        await tx.run(`
            MATCH (n {id: $id})
            SET n:MergedEntity
            SET n.mergedInto = $masterId
            SET n.mergedAt = datetime()
            REMOVE n:Entity
         `, { id, masterId });
      }
      await tx.commit();
      log.info(
        `Merged entities ${uniqueMergeIds.join(", ")} into ${masterId} with decision ${persistedDecisionId}`
      );
      return {
        decisionId: persistedDecisionId,
        mergeId,
        snapshotId,
        idempotent: false,
        guardrails: guardrailResult,
        overrideUsed: !!req.guardrailOverrideReason
      };
    } catch (e) {
      await tx.rollback();
      log.error(e, "Merge failed");
      throw e;
    }
  }
  async split(session, decisionId, userContext) {
    const res = await session.run(`
       MATCH (d:ERDecision {id: $decisionId})
       RETURN d
     `, { decisionId });
    if (res.records.length === 0) {
      throw new Error("Decision not found");
    }
    const decision = res.records[0].get("d").properties;
    const tx = session.beginTransaction();
    try {
      const mergeIds = decision.originalIds;
      const masterId = decision.masterId;
      for (const id of mergeIds) {
        await tx.run(`
           MATCH (n {id: $id})
           SET n:Entity
           REMOVE n:MergedEntity
           REMOVE n.mergedInto
           REMOVE n.mergedAt
         `, { id });
      }
      await tx.run(`
         MATCH ()-[r {mergedByDecision: $decisionId}]-()
         REMOVE r.merged
         REMOVE r.mergedAt
         REMOVE r.mergedByDecision
       `, { decisionId });
      await tx.run(`
         MATCH (d:ERDecision {id: $decisionId})
         SET d:RevertedDecision
         SET d.revertedAt = datetime()
         SET d.revertedBy = $userId
       `, { decisionId, userId: userContext.userId || "unknown" });
      await tx.commit();
      log.info(`Reverted merge decision ${decisionId}`);
    } catch (e) {
      await tx.rollback();
      throw e;
    }
  }
  async findCandidates(session) {
    return [
      {
        canonicalKey: "R163|J250",
        entities: [
          { id: "e1", labels: ["Entity"], properties: { name: "Robert Jones", email: "rob@example.com", lat: 40.7128, lon: -74.006 } },
          { id: "e2", labels: ["Entity"], properties: { name: "Rob Jones", email: "rob.j@example.com", lat: 40.7128, lon: -74.006 } }
        ]
      },
      {
        canonicalKey: "S350|M200",
        entities: [
          { id: "e3", labels: ["Entity"], properties: { name: "Sarah Smith", email: "sarah@test.com" } },
          { id: "e4", labels: ["Entity"], properties: { name: "Sara Smyth", email: "s.smyth@test.com" } }
        ]
      }
    ];
  }
};

// src/routes/entity-resolution.ts
init_database();
var router5 = express4.Router();
var erService = new EntityResolutionService();
var dqService = new DataQualityService();
var erV2Service = new EntityResolutionV2Service();
router5.post("/resolve-batch", async (req, res) => {
  try {
    const { entities } = req.body;
    if (!Array.isArray(entities)) {
      return res.status(400).json({ error: "Entities must be an array" });
    }
    const tenantId = req.user?.tenantId;
    const enrichedEntities = entities.map((e) => ({
      ...e,
      tenantId: e.tenantId || tenantId
    }));
    if (enrichedEntities.some((e) => !e.tenantId)) {
      return res.status(400).json({ error: "Tenant ID missing on some entities" });
    }
    const decisions = await erService.resolveBatch(enrichedEntities);
    res.json({ decisions });
  } catch (error) {
    console.error("Batch resolution error:", error);
    res.status(500).json({ error: "Internal server error" });
  }
});
router5.get("/quality/metrics", async (req, res) => {
  try {
    const tenantId = req.user?.tenantId;
    if (!tenantId) {
      return res.status(400).json({ error: "Tenant context required" });
    }
    const metrics8 = await dqService.getQualityMetrics(tenantId);
    res.json({ metrics: metrics8 });
  } catch (error) {
    console.error("Quality metrics error:", error);
    res.status(500).json({ error: "Internal server error" });
  }
});
router5.get("/guardrails/status", async (req, res) => {
  try {
    const datasetId = typeof req.query.datasetId === "string" ? req.query.datasetId : void 0;
    const guardrails = erV2Service.evaluateGuardrails(datasetId);
    const pool4 = getPostgresPool2();
    const overrideResult = await pool4.query(
      `
        SELECT dataset_id, reason, actor_id, merge_id, created_at
        FROM er_guardrail_overrides
        WHERE dataset_id = $1
        ORDER BY created_at DESC
        LIMIT 1
      `,
      [guardrails.datasetId]
    );
    const latestOverride = overrideResult.rows[0] ? {
      datasetId: overrideResult.rows[0].dataset_id,
      reason: overrideResult.rows[0].reason,
      actorId: overrideResult.rows[0].actor_id,
      mergeId: overrideResult.rows[0].merge_id,
      createdAt: overrideResult.rows[0].created_at
    } : null;
    res.json({
      ...guardrails,
      latestOverride
    });
  } catch (error) {
    console.error("Guardrail status error:", error);
    res.status(500).json({ error: "Failed to fetch guardrail status" });
  }
});
router5.post("/guardrails/preflight", async (req, res) => {
  try {
    const datasetId = req.body?.datasetId;
    const guardrails = erV2Service.evaluateGuardrails(datasetId);
    await erV2Service.recordGuardrailEvaluation(guardrails, {
      userId: req.user?.id,
      tenantId: req.user?.tenantId
    });
    res.json(guardrails);
  } catch (error) {
    console.error("Guardrail preflight error:", error);
    res.status(500).json({ error: "Failed to run guardrail preflight" });
  }
});
var entity_resolution_default = router5;

// src/routes/workspaces.ts
import express5 from "express";

// src/services/WorkspaceService.ts
init_database();
var WorkspaceService = class {
  async createWorkspace(tenantId, userId, name, config9) {
    const pool4 = getPostgresPool2();
    const res = await pool4.query(
      `INSERT INTO workspaces (tenant_id, user_id, name, config)
       VALUES ($1, $2, $3, $4)
       RETURNING id, tenant_id as "tenantId", user_id as "userId", name, config, created_at as "createdAt", updated_at as "updatedAt"`,
      [tenantId, userId, name, config9]
    );
    return res.rows[0];
  }
  async getWorkspaces(tenantId, userId) {
    const pool4 = getPostgresPool2();
    const res = await pool4.query(
      `SELECT id, tenant_id as "tenantId", user_id as "userId", name, config, created_at as "createdAt", updated_at as "updatedAt"
       FROM workspaces
       WHERE tenant_id = $1 AND user_id = $2
       ORDER BY updated_at DESC`,
      [tenantId, userId]
    );
    return res.rows;
  }
  async updateWorkspace(tenantId, userId, workspaceId, updates) {
    const pool4 = getPostgresPool2();
    const current = await this.getWorkspace(tenantId, userId, workspaceId);
    if (!current) return null;
    const name = updates.name || current.name;
    const config9 = updates.config || current.config;
    const res = await pool4.query(
      `UPDATE workspaces
       SET name = $1, config = $2, updated_at = NOW()
       WHERE id = $3 AND tenant_id = $4 AND user_id = $5
       RETURNING id, tenant_id as "tenantId", user_id as "userId", name, config, created_at as "createdAt", updated_at as "updatedAt"`,
      [name, config9, workspaceId, tenantId, userId]
    );
    return res.rows[0];
  }
  async getWorkspace(tenantId, userId, workspaceId) {
    const pool4 = getPostgresPool2();
    const res = await pool4.query(
      `SELECT id, tenant_id as "tenantId", user_id as "userId", name, config, created_at as "createdAt", updated_at as "updatedAt"
       FROM workspaces
       WHERE id = $1 AND tenant_id = $2 AND user_id = $3`,
      [workspaceId, tenantId, userId]
    );
    return res.rows[0] || null;
  }
  async deleteWorkspace(tenantId, userId, workspaceId) {
    const pool4 = getPostgresPool2();
    const res = await pool4.query(
      `DELETE FROM workspaces
       WHERE id = $1 AND tenant_id = $2 AND user_id = $3`,
      [workspaceId, tenantId, userId]
    );
    return (res.rowCount ?? 0) > 0;
  }
};
var workspaceService = new WorkspaceService();

// src/routes/workspaces.ts
init_logger();
var router6 = express5.Router();
router6.get("/", async (req, res) => {
  const tenantId = req.user?.tenant_id;
  const userId = req.user?.id || req.user?.sub;
  if (!tenantId || !userId) {
    return res.status(401).json({ error: "Unauthorized" });
  }
  try {
    const workspaces = await workspaceService.getWorkspaces(tenantId, userId);
    res.json(workspaces);
  } catch (error) {
    logger.error(error);
    res.status(500).json({ error: "Failed to fetch workspaces" });
  }
});
router6.post("/", async (req, res) => {
  const tenantId = req.user?.tenant_id;
  const userId = req.user?.id || req.user?.sub;
  const { name, config: config9 } = req.body;
  if (!tenantId || !userId) {
    return res.status(401).json({ error: "Unauthorized" });
  }
  if (!name) {
    return res.status(400).json({ error: "Name required" });
  }
  try {
    const workspace = await workspaceService.createWorkspace(tenantId, userId, name, config9 || {});
    res.json(workspace);
  } catch (error) {
    logger.error(error);
    res.status(500).json({ error: "Failed to create workspace" });
  }
});
router6.put("/:id", async (req, res) => {
  const tenantId = req.user?.tenant_id;
  const userId = req.user?.id || req.user?.sub;
  const { id } = req.params;
  const { name, config: config9 } = req.body;
  if (!tenantId || !userId) {
    return res.status(401).json({ error: "Unauthorized" });
  }
  try {
    const workspace = await workspaceService.updateWorkspace(tenantId, userId, id, { name, config: config9 });
    if (!workspace) {
      return res.status(404).json({ error: "Workspace not found" });
    }
    res.json(workspace);
  } catch (error) {
    logger.error(error);
    res.status(500).json({ error: "Failed to update workspace" });
  }
});
router6.delete("/:id", async (req, res) => {
  const tenantId = req.user?.tenant_id;
  const userId = req.user?.id || req.user?.sub;
  const { id } = req.params;
  if (!tenantId || !userId) {
    return res.status(401).json({ error: "Unauthorized" });
  }
  try {
    const success = await workspaceService.deleteWorkspace(tenantId, userId, id);
    if (!success) {
      return res.status(404).json({ error: "Workspace not found" });
    }
    res.json({ success: true });
  } catch (error) {
    logger.error(error);
    res.status(500).json({ error: "Failed to delete workspace" });
  }
});
var workspaces_default = router6;

// src/routes/ga-core-metrics.ts
init_GACoremetricsService();
init_database();
init_logger();
import express6 from "express";
import { trace as trace8, SpanStatusCode as SpanStatusCode5 } from "@opentelemetry/api";
var router7 = express6.Router();
var log3 = logger_default.child({ name: "GACoreMetricsAPI" });
var tracer7 = trace8.getTracer("ga-core-metrics", "1.0.0");
router7.get("/metrics", async (req, res) => {
  try {
    res.set("Content-Type", gaCoreMetrics.getMetricsRegistry().contentType);
    res.end(await gaCoreMetrics.getMetricsRegistry().metrics());
  } catch (error) {
    log3.error({ error: error.message }, "Failed to serve Prometheus metrics");
    res.status(500).json({ error: "Failed to generate metrics" });
  }
});
router7.get("/status", async (req, res) => {
  try {
    const status = await gaCoreMetrics.getCurrentStatus();
    res.json(status);
  } catch (error) {
    log3.error({ error: error.message }, "Failed to get GA Core status");
    res.status(500).json({ error: "Failed to get status" });
  }
});
router7.get("/gates", async (req, res) => {
  try {
    const pool4 = getPostgresPool2();
    const erMetrics = await pool4.query(`
      SELECT 
        entity_type,
        precision,
        target_precision,
        meets_threshold
      FROM (
        SELECT 
          'PERSON' as entity_type,
          COALESCE(
            (SELECT precision FROM er_precision_metrics WHERE entity_type = 'PERSON' ORDER BY last_updated DESC LIMIT 1),
            1.0
          ) as precision,
          0.90 as target_precision,
          COALESCE(
            (SELECT precision FROM er_precision_metrics WHERE entity_type = 'PERSON' ORDER BY last_updated DESC LIMIT 1),
            1.0
          ) >= 0.90 as meets_threshold
        UNION ALL
        SELECT 
          'ORG' as entity_type,
          COALESCE(
            (SELECT precision FROM er_precision_metrics WHERE entity_type = 'ORG' ORDER BY last_updated DESC LIMIT 1),
            1.0
          ) as precision,
          0.88 as target_precision,
          COALESCE(
            (SELECT precision FROM er_precision_metrics WHERE entity_type = 'ORG' ORDER BY last_updated DESC LIMIT 1),
            1.0
          ) >= 0.88 as meets_threshold
      ) er_gates
    `);
    const appealMetrics = await pool4.query(`
      SELECT get_ga_appeal_metrics(7) as metrics
    `);
    const exportMetrics = await pool4.query(`
      SELECT get_ga_export_metrics(7) as metrics
    `);
    const copilotMetrics = await pool4.query(`
      SELECT 
        COUNT(CASE WHEN e.status = 'EXECUTED' THEN 1 END)::DECIMAL /
        NULLIF(COUNT(*), 0) as success_rate,
        AVG(t.confidence) as avg_confidence
      FROM nl_cypher_translations t
      LEFT JOIN nl_cypher_executions e ON t.id = e.translation_id
      WHERE t.created_at >= NOW() - INTERVAL '7 days'
    `);
    const gates = [
      ...erMetrics.rows.map((row) => ({
        name: `ER_PRECISION_${row.entity_type}`,
        description: `Entity Resolution ${row.entity_type} precision`,
        currentValue: parseFloat(row.precision),
        threshold: parseFloat(row.target_precision),
        status: row.meets_threshold ? "PASS" : "FAIL",
        category: "Entity Resolution"
      })),
      {
        name: "APPEALS_SLA",
        description: "Policy appeals SLA compliance",
        currentValue: parseFloat(
          appealMetrics.rows[0]?.metrics?.sla_compliance_rate || "0"
        ),
        threshold: 0.9,
        status: parseFloat(
          appealMetrics.rows[0]?.metrics?.sla_compliance_rate || "0"
        ) >= 0.9 ? "PASS" : "FAIL",
        category: "Policy & Appeals"
      },
      {
        name: "EXPORT_INTEGRITY",
        description: "Export manifest integrity rate",
        currentValue: parseFloat(
          exportMetrics.rows[0]?.metrics?.integrity_rate || "0"
        ),
        threshold: 0.95,
        status: parseFloat(exportMetrics.rows[0]?.metrics?.integrity_rate || "0") >= 0.95 ? "PASS" : "FAIL",
        category: "Export & Provenance"
      },
      {
        name: "COPILOT_SUCCESS",
        description: "Copilot NL\u2192Cypher success rate",
        currentValue: parseFloat(copilotMetrics.rows[0]?.success_rate || "0"),
        threshold: 0.8,
        status: parseFloat(copilotMetrics.rows[0]?.success_rate || "0") >= 0.8 ? "PASS" : "FAIL",
        category: "AI Copilot"
      }
    ];
    const passingGates = gates.filter((g2) => g2.status === "PASS").length;
    const totalGates = gates.length;
    let overallStatus;
    if (passingGates === totalGates) {
      overallStatus = "GO";
    } else if (passingGates >= totalGates * 0.8) {
      overallStatus = "CONDITIONAL_GO";
    } else {
      overallStatus = "NO_GO";
    }
    res.json({
      overall: overallStatus,
      passingGates,
      totalGates,
      gates,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
  } catch (error) {
    log3.error({ error: error.message }, "Failed to get GA Core gates");
    res.status(500).json({ error: "Failed to get gates" });
  }
});
router7.get("/trends/:metric", async (req, res) => {
  try {
    const { metric } = req.params;
    const { days = 7 } = req.query;
    const pool4 = getPostgresPool2();
    let query3;
    const params = [days];
    switch (metric) {
      case "er_precision":
        query3 = `
          SELECT 
            DATE(created_at) as date,
            entity_type,
            AVG(precision) as value
          FROM er_ci_metrics
          WHERE created_at >= NOW() - INTERVAL $1 || ' days'
          GROUP BY DATE(created_at), entity_type
          ORDER BY date DESC
        `;
        break;
      case "appeal_sla":
        query3 = `
          SELECT 
            appeal_date as date,
            'appeals' as metric,
            sla_compliance_rate as value
          FROM policy_appeal_analytics
          WHERE appeal_date >= NOW() - INTERVAL $1 || ' days'
          ORDER BY appeal_date DESC
        `;
        break;
      case "export_integrity":
        query3 = `
          SELECT 
            export_date as date,
            'integrity' as metric,
            bundle_integrity_rate as value
          FROM export_metrics_realtime
          WHERE export_date >= NOW() - INTERVAL $1 || ' days'
          ORDER BY export_date DESC
        `;
        break;
      default:
        return res.status(400).json({ error: "Unknown metric" });
    }
    const result2 = await pool4.query(query3, params);
    res.json({
      metric,
      days: parseInt(days),
      data: result2.rows,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
  } catch (error) {
    log3.error(
      { error: error.message, metric: req.params.metric },
      "Failed to get metric trends"
    );
    res.status(500).json({ error: "Failed to get trends" });
  }
});
router7.get("/er-ops/precision-recall", async (req, res) => {
  return tracer7.startActiveSpan(
    "ga_core_metrics.er_ops.precision_recall",
    async (span) => {
      try {
        const { days = 30, modelType = "entity_resolution" } = req.query;
        const pool4 = getPostgresPool2();
        const parsedDays = parseInt(days, 10);
        span.setAttributes({
          "er.ops.metric": "precision_recall",
          "er.ops.days": parsedDays,
          "er.ops.model_type": modelType
        });
        const result2 = await pool4.query(
          `
            SELECT
              DATE(mp.evaluation_date) as date,
              mv.version as model_version,
              mp.metric_name,
              AVG(mp.metric_value) as value
            FROM ml_model_performance mp
            JOIN ml_model_versions mv ON mp.model_version_id = mv.id
            WHERE mv.model_type = $1
              AND mp.metric_name IN ('precision', 'recall')
              AND mp.evaluation_date >= NOW() - INTERVAL $2 || ' days'
            GROUP BY DATE(mp.evaluation_date), mv.version, mp.metric_name
            ORDER BY date DESC
          `,
          [modelType, days]
        );
        span.setAttributes({ "er.ops.row_count": result2.rowCount || 0 });
        span.setStatus({ code: SpanStatusCode5.OK });
        res.json({
          metric: "precision_recall",
          days: parsedDays,
          modelType,
          data: result2.rows,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        });
      } catch (error) {
        span.recordException(error);
        span.setStatus({
          code: SpanStatusCode5.ERROR,
          message: error.message
        });
        log3.error(
          { error: error.message },
          "Failed to get precision/recall trends"
        );
        res.status(500).json({ error: "Failed to get precision/recall trends" });
      } finally {
        span.end();
      }
    }
  );
});
router7.get("/er-ops/rollbacks", async (req, res) => {
  return tracer7.startActiveSpan(
    "ga_core_metrics.er_ops.rollbacks",
    async (span) => {
      try {
        const { days = 30 } = req.query;
        const pool4 = getPostgresPool2();
        const parsedDays = parseInt(days, 10);
        span.setAttributes({
          "er.ops.metric": "rollbacks",
          "er.ops.days": parsedDays
        });
        const result2 = await pool4.query(
          `
            SELECT
              DATE(started_at) as date,
              COUNT(*) FILTER (
                WHERE status = 'rolled_back' OR rollback_of_deployment_id IS NOT NULL
              ) as rollbacks,
              COUNT(*) as total_deployments
            FROM maestro.deployments
            WHERE started_at >= NOW() - INTERVAL $1 || ' days'
            GROUP BY DATE(started_at)
            ORDER BY date DESC
          `,
          [days]
        );
        span.setAttributes({ "er.ops.row_count": result2.rowCount || 0 });
        span.setStatus({ code: SpanStatusCode5.OK });
        res.json({
          metric: "rollbacks",
          days: parsedDays,
          data: result2.rows,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        });
      } catch (error) {
        span.recordException(error);
        span.setStatus({
          code: SpanStatusCode5.ERROR,
          message: error.message
        });
        log3.error({ error: error.message }, "Failed to get rollback counts");
        res.status(500).json({ error: "Failed to get rollback counts" });
      } finally {
        span.end();
      }
    }
  );
});
router7.get("/er-ops/conflicts", async (req, res) => {
  return tracer7.startActiveSpan(
    "ga_core_metrics.er_ops.conflicts",
    async (span) => {
      try {
        const { days = 30 } = req.query;
        const pool4 = getPostgresPool2();
        const parsedDays = parseInt(days, 10);
        span.setAttributes({
          "er.ops.metric": "conflict_reasons",
          "er.ops.days": parsedDays
        });
        const result2 = await pool4.query(
          `
            SELECT
              conflict_reason,
              COUNT(*) as count
            FROM contradictions
            WHERE timestamp >= NOW() - INTERVAL $1 || ' days'
            GROUP BY conflict_reason
            ORDER BY count DESC
            LIMIT 10
          `,
          [days]
        );
        span.setAttributes({ "er.ops.row_count": result2.rowCount || 0 });
        span.setStatus({ code: SpanStatusCode5.OK });
        res.json({
          metric: "conflict_reasons",
          days: parsedDays,
          data: result2.rows,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        });
      } catch (error) {
        span.recordException(error);
        span.setStatus({
          code: SpanStatusCode5.ERROR,
          message: error.message
        });
        log3.error({ error: error.message }, "Failed to get conflict reasons");
        res.status(500).json({ error: "Failed to get conflict reasons" });
      } finally {
        span.end();
      }
    }
  );
});
router7.get("/dashboard", async (req, res) => {
  try {
    const pool4 = getPostgresPool2();
    const [
      erMetrics,
      appealMetrics,
      exportMetrics,
      copilotMetrics,
      systemMetrics
    ] = await Promise.all([
      // ER precision
      pool4.query(`
        SELECT 
          entity_type,
          precision,
          total_decisions,
          reviews_required
        FROM er_precision_metrics
        WHERE last_updated >= NOW() - INTERVAL '1 day'
        ORDER BY entity_type
      `),
      // Appeals
      pool4.query(`
        SELECT 
          total_appeals,
          pending_appeals,
          sla_compliance_rate,
          avg_response_hours
        FROM policy_appeal_analytics
        WHERE appeal_date = CURRENT_DATE
        LIMIT 1
      `),
      // Exports
      pool4.query(`
        SELECT 
          total_exports,
          bundle_exports,
          bundle_integrity_rate,
          avg_bundle_size_mb
        FROM export_metrics_realtime
        WHERE export_date = CURRENT_DATE
        LIMIT 1
      `),
      // Copilot
      pool4.query(`
        SELECT 
          COUNT(*) as total_queries,
          COUNT(CASE WHEN e.status = 'EXECUTED' THEN 1 END) as successful_queries,
          AVG(t.confidence) as avg_confidence,
          COUNT(CASE WHEN t.requires_confirmation THEN 1 END) as high_risk_queries
        FROM nl_cypher_translations t
        LEFT JOIN nl_cypher_executions e ON t.id = e.translation_id
        WHERE t.created_at >= CURRENT_DATE
      `),
      // System performance
      pool4.query(`
        SELECT 
          COUNT(*) as total_requests,
          AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_response_time
        FROM merge_decisions
        WHERE created_at >= CURRENT_DATE
      `)
    ]);
    const dashboard = {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      entityResolution: {
        personPrecision: erMetrics.rows.find((r) => r.entity_type === "PERSON")?.precision || 1,
        orgPrecision: erMetrics.rows.find((r) => r.entity_type === "ORG")?.precision || 1,
        totalDecisions: erMetrics.rows.reduce(
          (sum, r) => sum + parseInt(r.total_decisions || "0"),
          0
        ),
        reviewsRequired: erMetrics.rows.reduce(
          (sum, r) => sum + parseInt(r.reviews_required || "0"),
          0
        )
      },
      policyAppeals: {
        totalAppeals: appealMetrics.rows[0]?.total_appeals || 0,
        pendingAppeals: appealMetrics.rows[0]?.pending_appeals || 0,
        slaCompliance: parseFloat(
          appealMetrics.rows[0]?.sla_compliance_rate || "1.0"
        ),
        avgResponseHours: parseFloat(
          appealMetrics.rows[0]?.avg_response_hours || "0"
        )
      },
      exports: {
        totalExports: exportMetrics.rows[0]?.total_exports || 0,
        bundleExports: exportMetrics.rows[0]?.bundle_exports || 0,
        integrityRate: parseFloat(
          exportMetrics.rows[0]?.bundle_integrity_rate || "1.0"
        ),
        avgBundleSize: parseFloat(
          exportMetrics.rows[0]?.avg_bundle_size_mb || "0"
        )
      },
      copilot: {
        totalQueries: copilotMetrics.rows[0]?.total_queries || 0,
        successfulQueries: copilotMetrics.rows[0]?.successful_queries || 0,
        successRate: copilotMetrics.rows[0]?.total_queries > 0 ? copilotMetrics.rows[0].successful_queries / copilotMetrics.rows[0].total_queries : 0,
        avgConfidence: parseFloat(
          copilotMetrics.rows[0]?.avg_confidence || "0"
        ),
        highRiskQueries: copilotMetrics.rows[0]?.high_risk_queries || 0
      },
      performance: {
        totalRequests: systemMetrics.rows[0]?.total_requests || 0,
        avgResponseTime: parseFloat(
          systemMetrics.rows[0]?.avg_response_time || "0"
        )
      }
    };
    res.json(dashboard);
  } catch (error) {
    log3.error({ error: error.message }, "Failed to get dashboard data");
    res.status(500).json({ error: "Failed to get dashboard data" });
  }
});
var ga_core_metrics_default = router7;

// src/routes/nl-graph-query.ts
import express7 from "express";
import { body, validationResult } from "express-validator";
import rateLimit2 from "express-rate-limit";
import pino46 from "pino";

// src/ai/nl-graph-query/nl-graph-query.service.ts
import { randomUUID as randomUUID11 } from "crypto";
import pino45 from "pino";

// src/ai/nl-graph-query/query-patterns.ts
function addTenantFilter(cypher, context4) {
  if (!context4.tenantId) return cypher;
  if (cypher.includes("WHERE")) {
    return cypher.replace(/WHERE/, `WHERE n.tenantId = $tenantId AND`);
  } else if (cypher.includes("RETURN")) {
    return cypher.replace(/RETURN/, `WHERE n.tenantId = $tenantId RETURN`);
  }
  return cypher;
}
function addPolicyFilter(cypher, context4, classification) {
  if (!context4.policyTags || context4.policyTags.length === 0) return cypher;
  const tags = classification ? context4.policyTags.filter((t) => t.classification === classification) : context4.policyTags;
  if (tags.length === 0) return cypher;
  const labels2 = tags.map((t) => `'${t.label}'`).join(", ");
  const labelFilter = `labels(n)[0] IN [${labels2}]`;
  if (cypher.includes("WHERE")) {
    return cypher.replace(/WHERE/, `WHERE ${labelFilter} AND`);
  } else if (cypher.includes("RETURN")) {
    return cypher.replace(/RETURN/, `WHERE ${labelFilter} RETURN`);
  }
  return cypher;
}
var queryPatterns = [
  // Basic node listing
  {
    name: "list-all-nodes",
    pattern: /(?:show|list|get|find)\s+(?:all\s+)?nodes?/i,
    generator: (match, context4) => {
      let cypher = "MATCH (n) RETURN n LIMIT 25";
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "low",
    description: "Lists nodes in the graph with pagination"
  },
  // Count queries
  {
    name: "count-nodes",
    pattern: /(?:count|how many)\s+(?:all\s+)?nodes?/i,
    generator: (match, context4) => {
      let cypher = "MATCH (n) RETURN count(n) AS nodeCount";
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "low",
    description: "Counts total nodes in the graph"
  },
  // Relationship queries
  {
    name: "find-relationships",
    pattern: /(?:show|find|get)\s+(?:all\s+)?relationships?/i,
    generator: (match, context4) => {
      let cypher = "MATCH (a)-[r]->(b) RETURN a, r, b LIMIT 50";
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "medium",
    description: "Shows relationships between nodes"
  },
  // Time-travel queries
  {
    name: "time-travel-snapshot",
    pattern: /(?:show|get|find)\s+(?:graph|network|state)\s+(?:at|on|as of)\s+(.+)/i,
    generator: (match, context4) => {
      const timeRef = match[1];
      let cypher = `
MATCH (n)
WHERE n.validFrom <= $timestamp AND (n.validTo IS NULL OR n.validTo > $timestamp)
OPTIONAL MATCH (n)-[r]->(m)
WHERE r.validFrom <= $timestamp AND (r.validTo IS NULL OR r.validTo > $timestamp)
RETURN n, r, m
LIMIT 100
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "medium",
    description: "Retrieves graph state at a specific point in time"
  },
  // Time-travel with changes
  {
    name: "time-travel-changes",
    pattern: /(?:show|get|find)\s+(?:changes|modifications|updates)\s+(?:between|from)\s+(.+)\s+(?:to|and)\s+(.+)/i,
    generator: (match, context4) => {
      let cypher = `
MATCH (n)
WHERE (n.validFrom >= $startTime AND n.validFrom <= $endTime)
   OR (n.validTo >= $startTime AND n.validTo <= $endTime)
RETURN n, n.validFrom AS changedAt,
       CASE
         WHEN n.validFrom >= $startTime AND n.validFrom <= $endTime THEN 'created'
         WHEN n.validTo >= $startTime AND n.validTo <= $endTime THEN 'deleted'
         ELSE 'modified'
       END AS changeType
ORDER BY n.validFrom DESC
LIMIT 100
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "medium",
    description: "Shows changes to the graph between two timestamps"
  },
  // Policy-aware queries
  {
    name: "policy-filtered-entities",
    pattern: /(?:show|find|get)\s+(?:all\s+)?(.+)\s+(?:with|having)\s+(?:classification|clearance|policy)\s+(.+)/i,
    generator: (match, context4) => {
      const entityType = match[1];
      const classification = match[2];
      let cypher = `MATCH (n:${entityType}) RETURN n LIMIT 50`;
      cypher = addPolicyFilter(cypher, context4, classification);
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "low",
    description: "Retrieves entities filtered by policy classification"
  },
  // Geo-temporal queries
  {
    name: "geo-temporal-entities",
    pattern: /(?:show|find|get)\s+(?:entities|nodes)\s+(?:near|around|within)\s+(.+)\s+(?:at|on|during)\s+(.+)/i,
    generator: (match, context4) => {
      const location = match[1];
      const time = match[2];
      let cypher = `
MATCH (n)
WHERE n.latitude IS NOT NULL
  AND n.longitude IS NOT NULL
  AND point.distance(
    point({latitude: n.latitude, longitude: n.longitude}),
    point({latitude: $lat, longitude: $lon})
  ) <= $radiusMeters
  AND n.observedAt >= $startTime
  AND n.observedAt <= $endTime
RETURN n,
       point.distance(
         point({latitude: n.latitude, longitude: n.longitude}),
         point({latitude: $lat, longitude: $lon})
       ) AS distanceMeters,
       n.observedAt AS timestamp
ORDER BY distanceMeters ASC
LIMIT 100
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "high",
    description: "Finds entities near a location within a time window"
  },
  // Narrative/timeline queries
  {
    name: "timeline-events",
    pattern: /(?:show|get|create)\s+(?:timeline|sequence|chronology)\s+(?:of|for)\s+(.+)/i,
    generator: (match, context4) => {
      const subject = match[1];
      let cypher = `
MATCH (n)
WHERE n.timestamp IS NOT NULL
OPTIONAL MATCH (n)-[r:PRECEDED_BY|CAUSED_BY|RELATED_TO]->(m)
WHERE m.timestamp IS NOT NULL
RETURN n, r, m, n.timestamp AS eventTime
ORDER BY n.timestamp ASC
LIMIT 200
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "medium",
    description: "Creates a chronological timeline of events"
  },
  // Course of Action (COA) queries
  {
    name: "coa-path-analysis",
    pattern: /(?:show|find|analyze)\s+(?:paths?|routes?|courses?)\s+(?:from|between)\s+(.+)\s+(?:to|and)\s+(.+)/i,
    generator: (match, context4) => {
      let cypher = `
MATCH path = allShortestPaths((start)-[*..8]-(end))
WHERE start.id = $startId AND end.id = $endId
WITH path,
     [node IN nodes(path) | node.type] AS nodeTypes,
     [rel IN relationships(path) | type(rel)] AS relTypes,
     length(path) AS pathLength
RETURN path, nodeTypes, relTypes, pathLength
ORDER BY pathLength ASC
LIMIT 10
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "high",
    description: "Finds and analyzes possible paths (courses of action)"
  },
  // COA with constraints
  {
    name: "coa-constrained-path",
    pattern: /(?:show|find)\s+paths?\s+(?:that|which)\s+(?:avoid|exclude|must include)\s+(.+)/i,
    generator: (match, context4) => {
      const constraint = match[1];
      let cypher = `
MATCH path = (start)-[*..8]-(end)
WHERE start.id = $startId
  AND end.id = $endId
  AND NOT ANY(node IN nodes(path) WHERE node.type IN $excludedTypes)
WITH path,
     [node IN nodes(path) | {id: node.id, type: node.type}] AS pathNodes,
     [rel IN relationships(path) | type(rel)] AS pathRels,
     length(path) AS pathLength
RETURN path, pathNodes, pathRels, pathLength
ORDER BY pathLength ASC
LIMIT 10
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "very-high",
    description: "Finds paths with specific constraints or requirements"
  },
  // Shortest path queries
  {
    name: "shortest-path",
    pattern: /(?:shortest|quickest)\s+path\s+(?:from|between)\s+(.+)\s+(?:to|and)\s+(.+)/i,
    generator: (match, context4) => {
      let cypher = `
MATCH path = shortestPath((start)-[*..10]-(end))
WHERE start.id = $startId AND end.id = $endId
RETURN path, length(path) AS pathLength,
       nodes(path) AS pathNodes,
       relationships(path) AS pathRelationships
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "high",
    description: "Finds the shortest path between two entities"
  },
  // Neighbor queries
  {
    name: "neighbors",
    pattern: /(?:show|find|get)\s+(?:neighbors|connections|links)\s+(?:of|for)\s+(.+)/i,
    generator: (match, context4) => {
      let cypher = `
MATCH (n)-[r]-(neighbor)
WHERE n.id = $nodeId
RETURN n, r, neighbor, type(r) AS relationshipType
LIMIT 100
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "low",
    description: "Shows all neighbors of a specific node"
  },
  // Pattern matching
  {
    name: "pattern-match",
    pattern: /(?:find|show)\s+(?:pattern|structure)\s+(?:where|with)\s+(.+)/i,
    generator: (match, context4) => {
      let cypher = `
MATCH (a)-[r1]->(b)-[r2]->(c)
RETURN a, r1, b, r2, c
LIMIT 50
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "medium",
    description: "Finds graph patterns matching a specific structure"
  },
  // Investigation queries
  {
    name: "investigation-entities",
    pattern: /(?:show|find|get)\s+(?:all\s+)?(?:entities|nodes)\s+(?:in|for)\s+investigation\s+(.+)/i,
    generator: (match, context4) => {
      let cypher = `
MATCH (n)
WHERE n.investigationId = $investigationId
RETURN n, labels(n) AS nodeLabels
LIMIT 500
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "medium",
    description: "Retrieves all entities in a specific investigation"
  },
  // Community detection
  {
    name: "connected-components",
    pattern: /(?:find|show|detect)\s+(?:communities|clusters|groups|components)/i,
    generator: (match, context4) => {
      let cypher = `
MATCH (n)
WITH n
MATCH (n)-[*..3]-(connected)
WITH n, collect(DISTINCT connected) AS community
RETURN n, community, size(community) AS communitySize
ORDER BY communitySize DESC
LIMIT 50
      `.trim();
      cypher = addTenantFilter(cypher, context4);
      return cypher;
    },
    expectedCost: "very-high",
    description: "Identifies connected components or communities"
  }
];
function findMatchingPattern(prompt) {
  for (const pattern2 of queryPatterns) {
    const match = prompt.match(pattern2.pattern);
    if (match) {
      return { pattern: pattern2, match };
    }
  }
  return null;
}
function generateFromPattern(prompt, context4) {
  const result2 = findMatchingPattern(prompt);
  if (!result2) return null;
  const { pattern: pattern2, match } = result2;
  const cypher = pattern2.generator(match, context4);
  return {
    cypher,
    patternName: pattern2.name,
    expectedCost: pattern2.expectedCost
  };
}

// src/ai/nl-graph-query/cost-estimator.ts
import pino42 from "pino";
var logger39 = pino42({ name: "cost-estimator" });
function estimateQueryCost(cypher) {
  const upperCypher = cypher.toUpperCase();
  const costDrivers = [];
  let nodesScanned = 100;
  let edgesScanned = 50;
  let estimatedTimeMs = 50;
  let estimatedMemoryMb = 10;
  const matchCount = (cypher.match(/MATCH/gi) || []).length;
  if (matchCount > 1) {
    nodesScanned *= Math.pow(10, matchCount - 1);
    edgesScanned *= Math.pow(5, matchCount - 1);
    costDrivers.push(`Multiple MATCH clauses (${matchCount}) may create cartesian product`);
  }
  if (cypher.includes("[*")) {
    const pathMatch = cypher.match(/\[\*\.\.(\d+)\]/);
    const maxDepth = pathMatch ? parseInt(pathMatch[1], 10) : 5;
    nodesScanned *= Math.pow(10, Math.min(maxDepth, 3));
    edgesScanned *= Math.pow(10, Math.min(maxDepth, 3));
    estimatedTimeMs *= Math.pow(5, Math.min(maxDepth, 3));
    estimatedMemoryMb *= Math.pow(2, Math.min(maxDepth, 3));
    costDrivers.push(
      `Variable-length path with max depth ${maxDepth} is exponentially expensive`
    );
  }
  if (upperCypher.includes("ALLSHORTESTPATHS")) {
    nodesScanned *= 100;
    edgesScanned *= 100;
    estimatedTimeMs *= 50;
    estimatedMemoryMb *= 20;
    costDrivers.push("allShortestPaths explores many possible routes");
  }
  if (upperCypher.includes("SHORTESTPATH") && !upperCypher.includes("ALLSHORTESTPATHS")) {
    nodesScanned *= 10;
    edgesScanned *= 10;
    estimatedTimeMs *= 5;
    estimatedMemoryMb *= 3;
    costDrivers.push("shortestPath requires path exploration");
  }
  const hasWhere = upperCypher.includes("WHERE");
  if (!hasWhere && upperCypher.includes("MATCH")) {
    nodesScanned *= 2;
    edgesScanned *= 2;
    costDrivers.push("No WHERE clause - query may scan all data");
  } else if (hasWhere) {
    if (cypher.includes(".id =") || cypher.includes(".id IN")) {
      nodesScanned = Math.floor(nodesScanned * 0.01);
      edgesScanned = Math.floor(edgesScanned * 0.1);
      costDrivers.push("Query uses indexed ID property (efficient)");
    } else if (cypher.includes(".tenantId =")) {
      nodesScanned = Math.floor(nodesScanned * 0.1);
      edgesScanned = Math.floor(edgesScanned * 0.2);
      costDrivers.push("Query filters by tenantId (good partitioning)");
    }
  }
  const limitMatch = cypher.match(/LIMIT\s+(\d+)/i);
  if (limitMatch) {
    const limit = parseInt(limitMatch[1], 10);
    if (limit <= 25) {
      estimatedTimeMs = Math.floor(estimatedTimeMs * 0.8);
      estimatedMemoryMb = Math.floor(estimatedMemoryMb * 0.5);
    }
  } else {
    costDrivers.push("No LIMIT clause - may return large result set");
    estimatedMemoryMb *= 2;
  }
  if (upperCypher.includes("ORDER BY")) {
    estimatedTimeMs *= 1.5;
    estimatedMemoryMb *= 1.5;
    costDrivers.push("ORDER BY requires sorting results");
  }
  if (upperCypher.match(/\b(COUNT|SUM|AVG|MAX|MIN|COLLECT)\s*\(/)) {
    estimatedTimeMs *= 1.2;
    estimatedMemoryMb *= 1.3;
    costDrivers.push("Aggregation requires processing all matched data");
  }
  const optionalMatchCount = (cypher.match(/OPTIONAL MATCH/gi) || []).length;
  if (optionalMatchCount > 0) {
    nodesScanned *= 1 + optionalMatchCount * 0.5;
    edgesScanned *= 1 + optionalMatchCount * 0.5;
    costDrivers.push(`${optionalMatchCount} OPTIONAL MATCH clause(s) adds nullable paths`);
  }
  if (upperCypher.includes("UNWIND")) {
    nodesScanned *= 2;
    estimatedMemoryMb *= 2;
    costDrivers.push("UNWIND may expand result set significantly");
  }
  if (cypher.includes("[") && cypher.includes("|") && cypher.includes("]")) {
    estimatedTimeMs *= 1.3;
    estimatedMemoryMb *= 1.5;
    costDrivers.push("Pattern comprehension requires nested iteration");
  }
  if (cypher.includes("point.distance")) {
    estimatedTimeMs *= 3;
    costDrivers.push("Geo-spatial distance calculation is computationally intensive");
  }
  let costClass = "low";
  if (nodesScanned > 1e4 || edgesScanned > 5e3 || estimatedTimeMs > 1e3) {
    costClass = "very-high";
  } else if (nodesScanned > 1e3 || edgesScanned > 500 || estimatedTimeMs > 500) {
    costClass = "high";
  } else if (nodesScanned > 100 || edgesScanned > 50 || estimatedTimeMs > 100) {
    costClass = "medium";
  }
  nodesScanned = Math.min(nodesScanned, 1e7);
  edgesScanned = Math.min(edgesScanned, 1e7);
  estimatedTimeMs = Math.min(estimatedTimeMs, 6e4);
  estimatedMemoryMb = Math.min(estimatedMemoryMb, 1024);
  logger39.debug(
    {
      nodesScanned,
      edgesScanned,
      costClass,
      estimatedTimeMs,
      costDrivers
    },
    "Cost estimation completed"
  );
  return {
    nodesScanned: Math.floor(nodesScanned),
    edgesScanned: Math.floor(edgesScanned),
    costClass,
    estimatedTimeMs: Math.floor(estimatedTimeMs),
    estimatedMemoryMb: Math.floor(estimatedMemoryMb),
    costDrivers
  };
}
function isSafeToExecute(cost) {
  if (cost.costClass === "very-high") {
    return false;
  }
  if (cost.costClass === "high" && (cost.estimatedTimeMs > 1e4 || cost.nodesScanned > 5e4)) {
    return false;
  }
  return true;
}
function generateCostWarnings(cost) {
  const warnings = [];
  if (cost.costClass === "very-high") {
    warnings.push(
      "This query has very high estimated cost and may timeout or consume excessive resources"
    );
  } else if (cost.costClass === "high") {
    warnings.push(
      "This query has high estimated cost and may take several seconds to execute"
    );
  }
  if (cost.nodesScanned > 1e4) {
    warnings.push(
      `Estimated to scan ${cost.nodesScanned.toLocaleString()} nodes - consider adding more specific filters`
    );
  }
  if (cost.edgesScanned > 5e3) {
    warnings.push(
      `Estimated to scan ${cost.edgesScanned.toLocaleString()} relationships - consider limiting path depth`
    );
  }
  if (cost.estimatedMemoryMb > 500) {
    warnings.push(
      `High memory usage estimated (${cost.estimatedMemoryMb}MB) - results may be truncated`
    );
  }
  if (cost.estimatedTimeMs > 5e3) {
    warnings.push(
      `Long execution time estimated (${(cost.estimatedTimeMs / 1e3).toFixed(1)}s) - consider optimizing`
    );
  }
  return warnings;
}

// src/ai/nl-graph-query/validator.ts
import pino43 from "pino";
var logger40 = pino43({ name: "cypher-validator" });
function validateCypher(cypher) {
  const syntaxErrors = [];
  const warnings = [];
  const securityIssues = [];
  const trimmed = cypher.trim();
  const upperCypher = cypher.toUpperCase();
  if (!trimmed) {
    syntaxErrors.push("Empty query");
    return { isValid: false, syntaxErrors, warnings, securityIssues };
  }
  const validStarts = [
    "MATCH",
    "OPTIONAL MATCH",
    "WITH",
    "UNWIND",
    "CALL",
    "RETURN"
  ];
  const startsValid = validStarts.some((keyword) => upperCypher.startsWith(keyword));
  if (!startsValid) {
    syntaxErrors.push(
      `Query must start with a valid Cypher clause: ${validStarts.join(", ")}`
    );
  }
  const hasReturn = upperCypher.includes("RETURN");
  const hasMutation = ["CREATE", "MERGE", "SET", "DELETE", "REMOVE"].some(
    (op) => upperCypher.includes(op)
  );
  if (!hasReturn && !hasMutation) {
    syntaxErrors.push("Query must contain a RETURN clause or mutation operation");
  }
  const openParens = (cypher.match(/\(/g) || []).length;
  const closeParens = (cypher.match(/\)/g) || []).length;
  if (openParens !== closeParens) {
    syntaxErrors.push(
      `Unbalanced parentheses: ${openParens} opening, ${closeParens} closing`
    );
  }
  const openBrackets = (cypher.match(/\[/g) || []).length;
  const closeBrackets = (cypher.match(/\]/g) || []).length;
  if (openBrackets !== closeBrackets) {
    syntaxErrors.push(
      `Unbalanced brackets: ${openBrackets} opening, ${closeBrackets} closing`
    );
  }
  const openBraces = (cypher.match(/\{/g) || []).length;
  const closeBraces = (cypher.match(/\}/g) || []).length;
  if (openBraces !== closeBraces) {
    syntaxErrors.push(
      `Unbalanced braces: ${openBraces} opening, ${closeBraces} closing`
    );
  }
  const dangerousOps = [
    { pattern: /\bDELETE\s+/i, message: "DELETE operations not allowed in read queries" },
    {
      pattern: /\bDETACH\s+DELETE\s+/i,
      message: "DETACH DELETE operations not allowed"
    },
    { pattern: /\bDROP\s+/i, message: "DROP operations not allowed" },
    { pattern: /\bCREATE\s+INDEX/i, message: "Schema modifications not allowed" },
    { pattern: /\bCREATE\s+CONSTRAINT/i, message: "Schema modifications not allowed" },
    { pattern: /\bDROP\s+INDEX/i, message: "Schema modifications not allowed" },
    { pattern: /\bDROP\s+CONSTRAINT/i, message: "Schema modifications not allowed" }
  ];
  for (const { pattern: pattern2, message } of dangerousOps) {
    if (pattern2.test(cypher)) {
      securityIssues.push(message);
    }
  }
  const mutationOps = ["CREATE ", "MERGE ", "SET ", "REMOVE "];
  const hasMutationOp = mutationOps.some((op) => upperCypher.includes(op));
  if (hasMutationOp) {
    securityIssues.push(
      "Mutation operations (CREATE/MERGE/SET/REMOVE) are not allowed in compiled queries"
    );
  }
  if (!upperCypher.includes("LIMIT") && upperCypher.includes("MATCH")) {
    warnings.push(
      "No LIMIT clause - query may return large result sets. Consider adding LIMIT."
    );
  }
  if (upperCypher.includes("[*]")) {
    warnings.push(
      "Unbounded variable-length path [*] can be very expensive - specify a maximum depth like [*..5]"
    );
  }
  const pathMatch = cypher.match(/\[\*\.\.(\d+)\]/);
  if (pathMatch) {
    const maxDepth = parseInt(pathMatch[1], 10);
    if (maxDepth > 10) {
      warnings.push(
        `Variable-length path depth of ${maxDepth} is very high - consider reducing to \u226410`
      );
    }
  }
  const matchCount = (cypher.match(/MATCH/gi) || []).length;
  if (matchCount > 1 && !upperCypher.includes("WHERE")) {
    warnings.push(
      `Multiple MATCH clauses without WHERE may create expensive cartesian product`
    );
  }
  const hasParameters = cypher.includes("$");
  const needsParameters = cypher.includes("=") && cypher.includes("WHERE");
  if (needsParameters && !hasParameters) {
    warnings.push(
      "Query has WHERE conditions but no parameter placeholders ($param) - consider using parameters for reusability"
    );
  }
  if (cypher.includes("+") && cypher.includes("'")) {
    warnings.push(
      "String concatenation detected - ensure user input is properly parameterized"
    );
  }
  const isValid = syntaxErrors.length === 0 && securityIssues.length === 0;
  logger40.debug(
    {
      isValid,
      syntaxErrorCount: syntaxErrors.length,
      warningCount: warnings.length,
      securityIssueCount: securityIssues.length
    },
    "Cypher validation completed"
  );
  return {
    isValid,
    syntaxErrors,
    warnings,
    securityIssues
  };
}
function extractRequiredParameters(cypher) {
  const paramPattern = /\$(\w+)/g;
  const params = /* @__PURE__ */ new Set();
  let match;
  while ((match = paramPattern.exec(cypher)) !== null) {
    params.add(match[1]);
  }
  return Array.from(params).sort();
}
function isReadOnlyQuery(cypher) {
  const upperCypher = cypher.toUpperCase();
  const mutationKeywords = [
    "CREATE ",
    "MERGE ",
    "SET ",
    "DELETE ",
    "REMOVE ",
    "DROP ",
    "DETACH DELETE"
  ];
  return !mutationKeywords.some((keyword) => upperCypher.includes(keyword));
}

// src/ai/nl-graph-query/explainer.ts
import pino44 from "pino";
var logger41 = pino44({ name: "cypher-explainer" });
function explainQuery(cypher, verbose = false) {
  const upperCypher = cypher.toUpperCase();
  const parts = [];
  const matchClauses = extractMatchClauses(cypher);
  if (matchClauses.length > 0) {
    parts.push("This query searches the graph for:");
    matchClauses.forEach((clause, idx) => {
      const explanation = explainMatchClause(clause);
      parts.push(`  ${idx + 1}. ${explanation}`);
    });
  }
  const whereExplanation = explainWhereClause(cypher);
  if (whereExplanation) {
    parts.push(`
Filtering criteria: ${whereExplanation}`);
  }
  const optionalMatches = extractOptionalMatchClauses(cypher);
  if (optionalMatches.length > 0) {
    parts.push("\nOptionally also finding:");
    optionalMatches.forEach((clause) => {
      parts.push(`  - ${explainMatchClause(clause)}`);
    });
  }
  if (upperCypher.includes("SHORTESTPATH")) {
    parts.push(
      "\nComputes the shortest path between entities using graph traversal algorithm."
    );
  } else if (upperCypher.includes("ALLSHORTESTPATHS")) {
    parts.push("\nFinds all shortest paths between entities (may return multiple routes).");
  }
  const pathMatch = cypher.match(/\[\*\.\.(\d+)\]/);
  if (pathMatch) {
    const maxDepth = pathMatch[1];
    parts.push(
      `
Traverses relationships up to ${maxDepth} hops away from the starting point.`
    );
  } else if (cypher.includes("[*]")) {
    parts.push("\nTraverses relationships of any length (unbounded traversal).");
  }
  const aggregations = extractAggregations(cypher);
  if (aggregations.length > 0) {
    parts.push("\nPerforms aggregation:");
    aggregations.forEach((agg) => {
      parts.push(`  - ${agg}`);
    });
  }
  const orderByMatch = cypher.match(/ORDER BY\s+([\w\s.,]+?)(?:LIMIT|$)/i);
  if (orderByMatch) {
    const orderFields = orderByMatch[1].trim();
    parts.push(`
Results are sorted by: ${orderFields}`);
  }
  const limitMatch = cypher.match(/LIMIT\s+(\d+)/i);
  if (limitMatch) {
    const limit = limitMatch[1];
    parts.push(`
Returns at most ${limit} result(s).`);
  } else if (upperCypher.includes("MATCH")) {
    parts.push("\nReturns all matching results (no limit specified).");
  }
  const returnExplanation = explainReturnClause(cypher);
  if (returnExplanation) {
    parts.push(`
Returned data: ${returnExplanation}`);
  }
  if (verbose) {
    parts.push("\n--- Technical Details ---");
    const params = extractParameters(cypher);
    if (params.length > 0) {
      parts.push(`Parameters required: ${params.join(", ")}`);
    }
    const labels2 = extractLabels(cypher);
    if (labels2.length > 0) {
      parts.push(`Node labels involved: ${labels2.join(", ")}`);
    }
    const relTypes = extractRelationshipTypes(cypher);
    if (relTypes.length > 0) {
      parts.push(`Relationship types: ${relTypes.join(", ")}`);
    }
  }
  return parts.join("\n");
}
function buildQueryExplanation(cypher, options2) {
  const rationale = [];
  const evidence = [];
  const matchClauses = extractMatchClauses(cypher);
  matchClauses.forEach((clause, idx) => {
    rationale.push(`Identifying graph pattern ${idx + 1} to satisfy the request.`);
    evidence.push({
      source: "MATCH clause",
      snippet: clause,
      reason: "Defines the core entities and relationships to investigate."
    });
  });
  const where = explainWhereClause(cypher);
  if (where) {
    rationale.push("Applying filters to narrow the candidate set.");
    evidence.push({
      source: "WHERE clause",
      snippet: where,
      reason: "Controls scope and protects against unbounded traversal."
    });
  }
  const returnClause = explainReturnClause(cypher);
  if (returnClause) {
    rationale.push("Selecting outputs relevant to the investigative question.");
    evidence.push({
      source: "RETURN clause",
      snippet: returnClause,
      reason: "Specifies which fields will be returned to the analyst."
    });
  }
  const parameters = extractParameters(cypher);
  if (parameters.length > 0) {
    rationale.push("Parameterizing inputs to keep execution safe and repeatable.");
    evidence.push({
      source: "Parameters",
      snippet: parameters.join(", "),
      reason: "Ensures sensitive values are bound safely at execution time."
    });
  }
  const summary = summarizeQuery(cypher);
  const warningPenalty = Math.min(options2.warnings.length * 0.07, 0.35);
  const confidence = Math.max(0.5, 0.92 - warningPenalty);
  return {
    summary,
    rationale,
    evidence,
    confidence: Number(confidence.toFixed(2))
  };
}
function extractMatchClauses(cypher) {
  const matchPattern = /MATCH\s+([^\n]+?)(?=WHERE|OPTIONAL|WITH|RETURN|ORDER|LIMIT|$)/gi;
  const matches2 = [];
  let match;
  while ((match = matchPattern.exec(cypher)) !== null) {
    matches2.push(match[1].trim());
  }
  return matches2;
}
function extractOptionalMatchClauses(cypher) {
  const matchPattern = /OPTIONAL\s+MATCH\s+([^\n]+?)(?=WHERE|OPTIONAL|WITH|RETURN|ORDER|LIMIT|$)/gi;
  const matches2 = [];
  let match;
  while ((match = matchPattern.exec(cypher)) !== null) {
    matches2.push(match[1].trim());
  }
  return matches2;
}
function explainMatchClause(matchClause) {
  if (matchClause.match(/^\(\w+\)$/)) {
    return "Any node in the graph";
  }
  const labelMatch = matchClause.match(/\((\w+):(\w+)\)/);
  if (labelMatch) {
    const [, variable, label] = labelMatch;
    return `Nodes labeled as "${label}"`;
  }
  const relMatch = matchClause.match(/\(\w+\)-\[(\w+):?(\w*)\]->\(\w+\)/);
  if (relMatch) {
    const [, , relType] = relMatch;
    if (relType) {
      return `Nodes connected by "${relType}" relationships`;
    }
    return "Nodes with any direct relationship";
  }
  if (matchClause.includes("[*")) {
    return "Paths of varying length between nodes";
  }
  return matchClause.replace(/[()[\]]/g, "");
}
function explainWhereClause(cypher) {
  const whereMatch = cypher.match(/WHERE\s+(.*?)(?=RETURN|ORDER|LIMIT|WITH|OPTIONAL|$)/is);
  if (!whereMatch) return null;
  const whereClause = whereMatch[1].trim();
  const conditions = [];
  const andParts = whereClause.split(/\s+AND\s+/i);
  for (const part of andParts) {
    const trimmed = part.trim();
    if (trimmed.includes(".id =")) {
      conditions.push("matching specific entity IDs");
    } else if (trimmed.includes(".tenantId")) {
      conditions.push("filtered by tenant");
    } else if (trimmed.includes("timestamp") || trimmed.includes("Time")) {
      conditions.push("within specific time range");
    } else if (trimmed.includes("point.distance")) {
      conditions.push("within geographic radius");
    } else if (trimmed.includes("IS NOT NULL")) {
      conditions.push("having non-null properties");
    } else if (trimmed.includes(".type")) {
      conditions.push("of specific type");
    } else {
      conditions.push("meeting additional criteria");
    }
  }
  return conditions.join(", ");
}
function extractAggregations(cypher) {
  const aggregations = [];
  if (cypher.match(/\bCOUNT\s*\(/i)) {
    aggregations.push("Counting results");
  }
  if (cypher.match(/\bSUM\s*\(/i)) {
    aggregations.push("Summing values");
  }
  if (cypher.match(/\bAVG\s*\(/i)) {
    aggregations.push("Calculating average");
  }
  if (cypher.match(/\bMAX\s*\(/i)) {
    aggregations.push("Finding maximum value");
  }
  if (cypher.match(/\bMIN\s*\(/i)) {
    aggregations.push("Finding minimum value");
  }
  if (cypher.match(/\bCOLLECT\s*\(/i)) {
    aggregations.push("Collecting results into lists");
  }
  return aggregations;
}
function explainReturnClause(cypher) {
  const returnMatch = cypher.match(/RETURN\s+(.*?)(?=ORDER|LIMIT|$)/is);
  if (!returnMatch) return null;
  const returnClause = returnMatch[1].trim();
  const items = [];
  const returnItems = returnClause.split(",").map((s) => s.trim());
  for (const item of returnItems) {
    if (item.match(/^\w+$/) && !item.includes("(")) {
      items.push(`entity "${item}"`);
    } else if (item.includes("count(")) {
      items.push("count of results");
    } else if (item.includes("collect(")) {
      items.push("collected list");
    } else if (item.includes(".")) {
      const propMatch = item.match(/\.(\w+)/);
      if (propMatch) {
        items.push(`property "${propMatch[1]}"`);
      }
    } else if (item.includes(" AS ")) {
      const aliasMatch = item.match(/AS\s+(\w+)/i);
      if (aliasMatch) {
        items.push(`"${aliasMatch[1]}"`);
      }
    } else if (item.includes("(") && item.includes(")")) {
      items.push("computed value");
    }
  }
  if (items.length === 0) return null;
  return items.join(", ");
}
function extractParameters(cypher) {
  const paramPattern = /\$(\w+)/g;
  const params = /* @__PURE__ */ new Set();
  let match;
  while ((match = paramPattern.exec(cypher)) !== null) {
    params.add(`$${match[1]}`);
  }
  return Array.from(params);
}
function extractLabels(cypher) {
  const labelPattern = /\(\w+:(\w+)(?:\{|[\s\)])]/g;
  const labels2 = /* @__PURE__ */ new Set();
  let match;
  while ((match = labelPattern.exec(cypher)) !== null) {
    labels2.add(match[1]);
  }
  return Array.from(labels2);
}
function extractRelationshipTypes(cypher) {
  const relPattern = /\[\w*:(\w+)(?:\*|\])/g;
  const types = /* @__PURE__ */ new Set();
  let match;
  while ((match = relPattern.exec(cypher)) !== null) {
    types.add(match[1]);
  }
  return Array.from(types);
}
function summarizeQuery(cypher) {
  const upperCypher = cypher.toUpperCase();
  if (upperCypher.includes("COUNT")) {
    return "Counts matching entities in the graph";
  }
  if (upperCypher.includes("SHORTESTPATH")) {
    return "Finds shortest path between two entities";
  }
  if (upperCypher.includes("ALLSHORTESTPATHS")) {
    return "Finds all shortest paths between two entities";
  }
  if (upperCypher.includes("[*")) {
    return "Traverses multi-hop relationships in the graph";
  }
  const matchCount = (cypher.match(/MATCH/gi) || []).length;
  if (matchCount > 1) {
    return "Finds patterns matching multiple graph structures";
  }
  const limitMatch = cypher.match(/LIMIT\s+(\d+)/i);
  if (limitMatch) {
    return `Retrieves up to ${limitMatch[1]} matching entities`;
  }
  return "Queries the graph for matching entities";
}

// src/ai/nl-graph-query/nl-graph-query.service.ts
var logger42 = pino45({ name: "nl-graph-query" });
var NlGraphQueryService = class {
  queryCache = /* @__PURE__ */ new Map();
  /**
   * Compile a natural language prompt into a Cypher query
   */
  async compile(request) {
    const startTime = Date.now();
    const queryId = randomUUID11();
    logger42.info(
      {
        queryId,
        prompt: request.prompt,
        tenantId: request.schemaContext.tenantId,
        userId: request.schemaContext.userId
      },
      "Compiling NL query"
    );
    try {
      const inputValidation = this.validateInput(request);
      if (!inputValidation.valid) {
        return this.createError(
          "INVALID_INPUT",
          inputValidation.message,
          inputValidation.suggestions || [],
          request.prompt
        );
      }
      const cacheKey = this.getCacheKey(request);
      if (this.queryCache.has(cacheKey)) {
        logger42.info({ queryId, cached: true }, "Returning cached query");
        return this.queryCache.get(cacheKey);
      }
      const cypher = await this.generateCypher(request.prompt, request.schemaContext);
      if (!cypher) {
        return this.createError(
          "GENERATION_FAILED",
          "Could not generate Cypher query from prompt",
          [
            "Try rephrasing your question more specifically",
            "Include entity types or relationship names if known",
            'Start with simpler queries like "show all nodes"'
          ],
          request.prompt
        );
      }
      const validation = validateCypher(cypher);
      if (!validation.isValid) {
        logger42.warn(
          {
            queryId,
            syntaxErrors: validation.syntaxErrors,
            securityIssues: validation.securityIssues
          },
          "Generated invalid Cypher"
        );
        return this.createError(
          "INVALID_CYPHER",
          "Generated query contains errors",
          [
            ...validation.syntaxErrors,
            ...validation.securityIssues,
            "This is a bug - please report it with your prompt"
          ],
          request.prompt
        );
      }
      const estimatedCost = estimateQueryCost(cypher);
      const warnings = [
        ...validation.warnings,
        ...generateCostWarnings(estimatedCost)
      ];
      if (!isReadOnlyQuery(cypher)) {
        warnings.push("Query contains mutation operations - execution blocked");
      }
      const explanation = request.verbose ? explainQuery(cypher, true) : summarizeQuery(cypher);
      const explanationDetails = buildQueryExplanation(cypher, {
        warnings,
        estimatedCost: estimatedCost.costClass,
        verbose: request.verbose
      });
      const requiredParameters = extractRequiredParameters(cypher);
      const isSafe = isSafeToExecute(estimatedCost) && isReadOnlyQuery(cypher);
      const response = {
        queryId,
        cypher,
        explanationDetails,
        estimatedCost,
        explanation,
        requiredParameters,
        isSafe,
        warnings,
        timestamp: /* @__PURE__ */ new Date()
      };
      this.queryCache.set(cacheKey, response);
      const compilationTime = Date.now() - startTime;
      logger42.info(
        {
          queryId,
          compilationTimeMs: compilationTime,
          costClass: estimatedCost.costClass,
          isSafe,
          warningCount: warnings.length,
          requiredParams: requiredParameters.length,
          explanationConfidence: explanationDetails.confidence,
          evidenceCount: explanationDetails.evidence.length
        },
        "Query compilation completed"
      );
      return response;
    } catch (error) {
      logger42.error(
        {
          queryId,
          error: error instanceof Error ? error.message : "Unknown error",
          stack: error instanceof Error ? error.stack : void 0
        },
        "Query compilation failed"
      );
      return this.createError(
        "INTERNAL_ERROR",
        "An unexpected error occurred during compilation",
        [
          "Please try again",
          "If the problem persists, contact support with the query ID"
        ],
        request.prompt
      );
    }
  }
  /**
   * Generate Cypher from natural language using pattern matching
   */
  async generateCypher(prompt, context4) {
    const trimmed = prompt.trim();
    const patternResult = generateFromPattern(trimmed, context4);
    if (patternResult) {
      logger42.debug(
        {
          patternName: patternResult.patternName,
          expectedCost: patternResult.expectedCost
        },
        "Generated Cypher from pattern"
      );
      return patternResult.cypher;
    }
    logger42.warn({ prompt }, "No matching pattern found for prompt");
    return null;
  }
  /**
   * Validate input request
   */
  validateInput(request) {
    if (!request.prompt || request.prompt.trim().length === 0) {
      return {
        valid: false,
        message: "Prompt cannot be empty",
        suggestions: ["Provide a natural language question about the graph"]
      };
    }
    if (request.prompt.length > 1e3) {
      return {
        valid: false,
        message: "Prompt is too long (max 1000 characters)",
        suggestions: ["Break down your question into smaller, specific queries"]
      };
    }
    if (!request.schemaContext) {
      return {
        valid: false,
        message: "Schema context is required",
        suggestions: ["Provide graph schema information"]
      };
    }
    return { valid: true };
  }
  /**
   * Create a cache key for a request
   */
  getCacheKey(request) {
    const parts = [
      request.prompt.trim().toLowerCase(),
      request.schemaContext.tenantId || "default",
      request.verbose ? "verbose" : "concise"
    ];
    return parts.join("::");
  }
  /**
   * Create a standardized error response
   */
  createError(code, message, suggestions, originalPrompt) {
    return {
      code,
      message,
      suggestions,
      originalPrompt
    };
  }
  /**
   * Get information about available query patterns
   */
  getAvailablePatterns() {
    return queryPatterns.map((p) => ({
      name: p.name,
      description: p.description,
      expectedCost: p.expectedCost
    }));
  }
  /**
   * Clear the query cache
   */
  clearCache() {
    this.queryCache.clear();
    logger42.info("Query cache cleared");
  }
  /**
   * Get cache statistics
   */
  getCacheStats() {
    return {
      size: this.queryCache.size,
      maxSize: 1e3
      // Could be configurable
    };
  }
};
var serviceInstance7 = null;
function getNlGraphQueryService() {
  if (!serviceInstance7) {
    serviceInstance7 = new NlGraphQueryService();
  }
  return serviceInstance7;
}

// src/routes/nl-graph-query.ts
var logger43 = pino46({ name: "nl-graph-query-routes" });
var router8 = express7.Router();
var nlQueryRateLimit = rateLimit2({
  windowMs: 15 * 60 * 1e3,
  // 15 minutes
  max: 100,
  // Limit each IP to 100 requests per windowMs
  message: {
    error: "Too many query compilation requests, please try again later",
    retryAfter: "15 minutes"
  },
  standardHeaders: true,
  legacyHeaders: false
});
router8.use(nlQueryRateLimit);
var validateCompileRequest = [
  body("prompt").isString().trim().notEmpty().withMessage("prompt is required").isLength({ max: 1e3 }).withMessage("prompt must be at most 1000 characters"),
  body("schemaContext").isObject().withMessage("schemaContext is required and must be an object"),
  body("schemaContext.nodeLabels").optional().isArray().withMessage("nodeLabels must be an array"),
  body("schemaContext.relationshipTypes").optional().isArray().withMessage("relationshipTypes must be an array"),
  body("schemaContext.policyTags").optional().isArray().withMessage("policyTags must be an array"),
  body("schemaContext.tenantId").optional().isString().withMessage("tenantId must be a string"),
  body("schemaContext.userId").optional().isString().withMessage("userId must be a string"),
  body("schemaContext.investigationId").optional().isString().withMessage("investigationId must be a string"),
  body("parameters").optional().isObject().withMessage("parameters must be an object"),
  body("verbose").optional().isBoolean().withMessage("verbose must be a boolean")
];
var handleValidationErrors = (req, res, next) => {
  const errors = validationResult(req);
  if (!errors.isEmpty()) {
    return res.status(400).json({
      error: "Validation failed",
      details: errors.array()
    });
  }
  next();
};
router8.post(
  "/compile",
  validateCompileRequest,
  handleValidationErrors,
  async (req, res) => {
    const startTime = Date.now();
    try {
      const { prompt, schemaContext, parameters, verbose } = req.body;
      logger43.info(
        {
          prompt: prompt.substring(0, 100),
          // Log first 100 chars
          tenantId: schemaContext.tenantId,
          userId: schemaContext.userId,
          hasParameters: !!parameters,
          verbose
        },
        "NL query compilation request received"
      );
      const compileRequest = {
        prompt,
        schemaContext,
        parameters,
        verbose: verbose || false
      };
      const service11 = getNlGraphQueryService();
      const result2 = await service11.compile(compileRequest);
      const responseTime = Date.now() - startTime;
      if ("code" in result2) {
        logger43.warn(
          {
            errorCode: result2.code,
            message: result2.message,
            responseTimeMs: responseTime
          },
          "Query compilation failed with error"
        );
        return res.status(400).json(result2);
      }
      logger43.info(
        {
          queryId: result2.queryId,
          costClass: result2.estimatedCost.costClass,
          isSafe: result2.isSafe,
          warningCount: result2.warnings.length,
          requiredParamsCount: result2.requiredParameters.length,
          explanationConfidence: result2.explanationDetails.confidence,
          evidenceCount: result2.explanationDetails.evidence.length,
          responseTimeMs: responseTime
        },
        "Query compilation successful with explanation payload"
      );
      return res.status(200).json({
        ...result2,
        metadata: {
          compilationTimeMs: responseTime,
          service: "nl-graph-query-copilot",
          version: "1.0.0",
          explanation: {
            confidence: result2.explanationDetails.confidence,
            evidenceCount: result2.explanationDetails.evidence.length
          }
        }
      });
    } catch (error) {
      const responseTime = Date.now() - startTime;
      logger43.error(
        {
          error: error instanceof Error ? error.message : "Unknown error",
          stack: error instanceof Error ? error.stack : void 0,
          responseTimeMs: responseTime
        },
        "Unexpected error in compile endpoint"
      );
      return res.status(500).json({
        code: "INTERNAL_SERVER_ERROR",
        message: "An unexpected error occurred during query compilation",
        suggestions: [
          "Please try again",
          "If the problem persists, contact support"
        ],
        originalPrompt: req.body.prompt || ""
      });
    }
  }
);
router8.get("/patterns", async (req, res) => {
  try {
    const service11 = getNlGraphQueryService();
    const patterns = service11.getAvailablePatterns();
    logger43.info(
      { patternCount: patterns.length },
      "Returning available query patterns"
    );
    return res.status(200).json({
      patterns,
      count: patterns.length
    });
  } catch (error) {
    logger43.error(
      {
        error: error instanceof Error ? error.message : "Unknown error"
      },
      "Error retrieving patterns"
    );
    return res.status(500).json({
      error: "Failed to retrieve patterns",
      message: "An unexpected error occurred"
    });
  }
});
router8.get("/health", async (req, res) => {
  try {
    const service11 = getNlGraphQueryService();
    const cacheStats = service11.getCacheStats();
    return res.status(200).json({
      status: "healthy",
      cache: cacheStats,
      uptime: process.uptime(),
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
  } catch (error) {
    logger43.error(
      {
        error: error instanceof Error ? error.message : "Unknown error"
      },
      "Health check failed"
    );
    return res.status(503).json({
      status: "degraded",
      error: "Service health check failed"
    });
  }
});
router8.post("/cache/clear", async (req, res) => {
  try {
    const service11 = getNlGraphQueryService();
    service11.clearCache();
    logger43.info("Query cache cleared via API request");
    return res.status(200).json({
      success: true,
      message: "Query cache cleared successfully"
    });
  } catch (error) {
    logger43.error(
      {
        error: error instanceof Error ? error.message : "Unknown error"
      },
      "Failed to clear cache"
    );
    return res.status(500).json({
      success: false,
      message: "Failed to clear cache"
    });
  }
});
var nl_graph_query_default = router8;

// src/routes/disclosures.ts
import express8 from "express";
import { promises as fs10 } from "fs";
import { z as z13 } from "zod";

// src/disclosure/export-service.ts
init_postgres();
import { randomUUID as randomUUID12, createHash as createHash12, createHmac as createHmac3 } from "crypto";
import { promises as fs8 } from "fs";
import { createReadStream, createWriteStream as createWriteStream5 } from "fs";
import os3 from "os";
import path9 from "path";
import { finished } from "stream/promises";
import { z as z12 } from "zod";

// src/disclosure/bundle.ts
import { createHash as createHash11 } from "crypto";
import { createWriteStream as createWriteStream4 } from "fs";
import archiver from "archiver";
async function makeBundle({
  artifacts,
  claimSet,
  merkleRoot,
  attestations,
  format,
  checksums
}) {
  const outPath = `/tmp/bundle_${Date.now()}.zip`;
  const out = createWriteStream4(outPath);
  const zip = archiver("zip");
  zip.pipe(out);
  const manifest = {
    version: "1",
    merkleRoot,
    claimSetSummary: {
      id: claimSet.id,
      count: (claimSet.claims || []).length
    },
    attestations
  };
  zip.append(JSON.stringify(manifest, null, 2), { name: "manifest.json" });
  zip.append(JSON.stringify(claimSet, null, 2), { name: "claimset.json" });
  for (const artifact of artifacts) {
    zip.file(artifact.path, { name: `artifacts/${artifact.name}` });
  }
  await zip.finalize();
  const sha2562 = await sha(outPath);
  return { path: outPath, sha256: sha2562 };
}
function sha(p) {
  return new Promise((resolve2, reject) => {
    const hash3 = createHash11("sha256");
    const stream2 = __require("fs").createReadStream(p);
    stream2.on("data", (data) => hash3.update(data));
    stream2.on("end", () => resolve2(hash3.digest("hex")));
    stream2.on("error", reject);
  });
}

// src/metrics/disclosureMetrics.ts
import { Counter as Counter12, Gauge as Gauge10, Histogram as Histogram9, register as register7 } from "prom-client";
function getOrCreateCounter(name, config9) {
  const existing = register7.getSingleMetric(name);
  if (existing) return existing;
  return new Counter12(config9);
}
function getOrCreateHistogram(name, config9) {
  const existing = register7.getSingleMetric(name);
  if (existing) return existing;
  return new Histogram9(config9);
}
function getOrCreateGauge(name, config9) {
  const existing = register7.getSingleMetric(name);
  if (existing) return existing;
  return new Gauge10(config9);
}
var DisclosureMetrics = class _DisclosureMetrics {
  static instance;
  events;
  durations;
  bundleSize;
  activeExports;
  warningCounter;
  constructor() {
    this.events = getOrCreateCounter("disclosure_packager_events_total", {
      name: "disclosure_packager_events_total",
      help: "Lifecycle events recorded for the disclosure packager UI and API.",
      labelNames: ["event", "tenant"]
    });
    this.durations = getOrCreateHistogram(
      "disclosure_packager_duration_seconds",
      {
        name: "disclosure_packager_duration_seconds",
        help: "Observed export durations in seconds.",
        labelNames: ["tenant"],
        buckets: [5, 15, 30, 60, 90, 120, 180, 240, 300, 420]
      }
    );
    this.bundleSize = getOrCreateHistogram("disclosure_packager_bundle_bytes", {
      name: "disclosure_packager_bundle_bytes",
      help: "Size of completed disclosure bundles in bytes.",
      labelNames: ["tenant"],
      buckets: [
        5e4,
        25e4,
        5e5,
        1e6,
        5e6,
        1e7,
        25e6
      ]
    });
    this.activeExports = getOrCreateGauge(
      "disclosure_packager_active_exports",
      {
        name: "disclosure_packager_active_exports",
        help: "Number of disclosure exports currently executing.",
        labelNames: ["tenant"]
      }
    );
    this.warningCounter = getOrCreateCounter(
      "disclosure_packager_warnings_total",
      {
        name: "disclosure_packager_warnings_total",
        help: "Warnings surfaced while assembling disclosure bundles.",
        labelNames: ["tenant", "type"]
      }
    );
  }
  static getInstance() {
    if (!_DisclosureMetrics.instance) {
      _DisclosureMetrics.instance = new _DisclosureMetrics();
    }
    return _DisclosureMetrics.instance;
  }
  recordEvent(event, tenant) {
    this.events.inc({ event, tenant });
  }
  exportStarted(tenant) {
    this.recordEvent("start", tenant);
    this.activeExports.inc({ tenant });
  }
  exportCompleted(tenant, durationMs, bundleBytes, warningTypes) {
    this.recordEvent("complete", tenant);
    this.activeExports.dec({ tenant });
    this.durations.observe({ tenant }, durationMs / 1e3);
    this.bundleSize.observe({ tenant }, bundleBytes);
    const uniqueWarningTypes = Array.from(new Set(warningTypes));
    for (const type of uniqueWarningTypes) {
      this.warningCounter.inc({ tenant, type });
    }
  }
  exportFailed(tenant) {
    this.recordEvent("fail", tenant);
    this.activeExports.dec({ tenant });
  }
  uiEvent(event, tenant) {
    const metricEvent = event === "view" ? "ui_view" : "ui_start";
    this.recordEvent(metricEvent, tenant);
  }
};
var disclosureMetrics = DisclosureMetrics.getInstance();

// src/disclosure/export-service.ts
init_redact();
import fetch3 from "node-fetch";
var MAX_WINDOW_DAYS = 31;
var MAX_EVENTS = 1e4;
var JOB_TTL_DAYS = 7;
var DEFAULT_ARTIFACTS = [
  "audit-trail",
  "sbom",
  "attestations",
  "policy-reports"
];
var requestSchema = z12.object({
  tenantId: z12.string().min(1),
  startTime: z12.string().transform((value) => new Date(value)),
  endTime: z12.string().transform((value) => new Date(value)),
  artifacts: z12.array(z12.enum(["audit-trail", "sbom", "attestations", "policy-reports"])).optional(),
  callbackUrl: z12.string().url().optional()
});
async function ensureDir(dir) {
  await fs8.mkdir(dir, { recursive: true });
}
async function hashFile(filePath) {
  const hash3 = createHash12("sha256");
  const stream2 = createReadStream(filePath);
  stream2.on("data", (chunk) => hash3.update(chunk));
  await finished(stream2);
  return hash3.digest("hex");
}
function merkleFromHashes(hashes) {
  if (hashes.length === 0) return "";
  let layer = hashes.slice().sort();
  while (layer.length > 1) {
    const next = [];
    for (let i = 0; i < layer.length; i += 2) {
      const left = layer[i];
      const right = layer[i + 1] ?? layer[i];
      const hash3 = createHash12("sha256").update(left + right).digest("hex");
      next.push(hash3);
    }
    layer = next;
  }
  return layer[0];
}
var DisclosureExportService = class {
  jobs = /* @__PURE__ */ new Map();
  redaction = new RedactionService();
  async createJob(input) {
    const parsed = requestSchema.parse(input);
    if (Number.isNaN(parsed.startTime.getTime()) || Number.isNaN(parsed.endTime.getTime())) {
      throw new Error("invalid_time_range");
    }
    if (parsed.endTime <= parsed.startTime) {
      throw new Error("end_before_start");
    }
    const diffDays = (parsed.endTime.getTime() - parsed.startTime.getTime()) / (1e3 * 60 * 60 * 24);
    if (diffDays > MAX_WINDOW_DAYS) {
      throw new Error("window_too_large");
    }
    const jobId = randomUUID12();
    const nowIso2 = (/* @__PURE__ */ new Date()).toISOString();
    const workingDir = path9.join(os3.tmpdir(), "disclosures", jobId);
    await ensureDir(workingDir);
    const job = {
      id: jobId,
      tenantId: parsed.tenantId,
      status: "pending",
      createdAt: nowIso2,
      warnings: [],
      artifactStats: {},
      request: {
        tenantId: parsed.tenantId,
        startTime: parsed.startTime,
        endTime: parsed.endTime,
        artifacts: parsed.artifacts?.length ? parsed.artifacts : DEFAULT_ARTIFACTS,
        callbackUrl: parsed.callbackUrl
      },
      workingDir,
      expiresAt: new Date(
        Date.now() + JOB_TTL_DAYS * 24 * 60 * 60 * 1e3
      ).toISOString(),
      attestations: [],
      artifactDigests: {}
    };
    this.jobs.set(jobId, job);
    setImmediate(() => {
      this.processJob(jobId).catch((error) => {
        console.error("Disclosure export failed", error);
      });
    });
    return this.publicJob(job);
  }
  listJobsForTenant(tenantId) {
    return Array.from(this.jobs.values()).filter((job) => job.tenantId === tenantId).map((job) => this.publicJob(job));
  }
  getJob(jobId) {
    const job = this.jobs.get(jobId);
    if (!job) return void 0;
    return this.publicJob(job);
  }
  getDownload(jobId) {
    const job = this.jobs.get(jobId);
    if (!job || !job.bundlePath) return void 0;
    return { job: this.publicJob(job), filePath: job.bundlePath };
  }
  async processJob(jobId) {
    const job = this.jobs.get(jobId);
    if (!job) return;
    job.status = "running";
    job.startedAt = (/* @__PURE__ */ new Date()).toISOString();
    disclosureMetrics.exportStarted(job.tenantId);
    const startedAt2 = Date.now();
    const artifacts = [];
    const artifactHashes = [];
    const warnings = [];
    try {
      const pool4 = getPostgresPool();
      const {
        tenantId,
        startTime,
        endTime,
        artifacts: requestedArtifacts,
        callbackUrl
      } = job.request;
      if (requestedArtifacts.includes("audit-trail")) {
        const result2 = await this.collectAuditTrail({
          job,
          pool: pool4,
          startTime,
          endTime
        });
        artifacts.push({
          name: "audit-trail.json",
          path: result2.filePath,
          sha256: result2.hash
        });
        artifactHashes.push(result2.hash);
        job.artifactDigests["audit-trail.json"] = result2.hash;
        job.artifactStats["audit-trail"] = result2.count;
        warnings.push(...result2.warnings.map((w) => `audit:${w}`));
      }
      if (requestedArtifacts.includes("sbom")) {
        const result2 = await this.collectSbomReports({
          job,
          pool: pool4,
          startTime,
          endTime
        });
        if (result2) {
          artifacts.push({
            name: "sbom-reports.json",
            path: result2.filePath,
            sha256: result2.hash
          });
          artifactHashes.push(result2.hash);
          job.artifactDigests["sbom-reports.json"] = result2.hash;
          job.artifactStats["sbom"] = result2.count;
          warnings.push(...result2.warnings.map((w) => `sbom:${w}`));
        }
      }
      if (requestedArtifacts.includes("policy-reports")) {
        const result2 = await this.collectPolicyReports({
          job,
          pool: pool4,
          startTime,
          endTime
        });
        if (result2) {
          artifacts.push({
            name: "policy-reports.json",
            path: result2.filePath,
            sha256: result2.hash
          });
          artifactHashes.push(result2.hash);
          job.artifactDigests["policy-reports.json"] = result2.hash;
          job.artifactStats["policy-reports"] = result2.count;
          warnings.push(...result2.warnings.map((w) => `policy:${w}`));
        }
      }
      if (requestedArtifacts.includes("attestations")) {
        const result2 = await this.collectAttestations({
          job,
          pool: pool4,
          startTime,
          endTime
        });
        if (result2) {
          artifacts.push({
            name: "attestations.json",
            path: result2.filePath,
            sha256: result2.hash
          });
          artifactHashes.push(result2.hash);
          job.artifactDigests["attestations.json"] = result2.hash;
          job.artifactStats["attestations"] = result2.count;
          warnings.push(...result2.warnings.map((w) => `attestation:${w}`));
          job.attestations = result2.attestations;
        }
      }
      const claimSet = this.buildClaimSet(job, artifactHashes, warnings);
      job.claimSet = claimSet;
      const merkleRoot = merkleFromHashes(artifactHashes);
      const { path: bundlePath, sha256: sha2562 } = await makeBundle({
        artifacts,
        claimSet,
        merkleRoot,
        attestations: job.attestations
      });
      job.bundlePath = bundlePath;
      job.sha256 = sha2562;
      job.downloadUrl = `/disclosures/export/${job.id}/download`;
      job.status = "completed";
      job.completedAt = (/* @__PURE__ */ new Date()).toISOString();
      job.warnings = warnings;
      const stats = await fs8.stat(bundlePath);
      disclosureMetrics.exportCompleted(
        job.tenantId,
        Date.now() - startedAt2,
        stats.size,
        warnings
      );
      if (callbackUrl) {
        await this.notifyWebhook(callbackUrl, job).catch((error) => {
          warnings.push("webhook_failed");
          console.error("Disclosure webhook failed", error);
        });
      }
    } catch (error) {
      job.status = "failed";
      job.error = (error instanceof Error ? error.message : String(error)) || "export_failed";
      job.completedAt = (/* @__PURE__ */ new Date()).toISOString();
      job.warnings = warnings;
      disclosureMetrics.exportFailed(job.tenantId);
    }
  }
  buildClaimSet(job, artifactHashes, warnings) {
    const signature = this.signClaimSet({
      jobId: job.id,
      tenantId: job.tenantId,
      window: {
        start: job.request.startTime.toISOString(),
        end: job.request.endTime.toISOString()
      },
      artifacts: artifactHashes,
      warnings,
      createdAt: (/* @__PURE__ */ new Date()).toISOString()
    });
    return {
      id: `claimset-${job.id}`,
      tenantId: job.tenantId,
      window: {
        start: job.request.startTime.toISOString(),
        end: job.request.endTime.toISOString()
      },
      artifacts: artifactHashes,
      warnings,
      signature
    };
  }
  signClaimSet(payload) {
    const secret = process.env.DISCLOSURE_SIGNING_SECRET || "dev-disclosure-secret";
    const keyId = process.env.DISCLOSURE_SIGNING_KEY_ID || "local-dev";
    const bytes = Buffer.from(JSON.stringify(payload));
    const digest = createHash12("sha256").update(bytes).digest("hex");
    const mac = createHmac3("sha256", secret).update(bytes).digest("hex");
    return { keyId, digest, signature: mac, algorithm: "HMAC-SHA256" };
  }
  async notifyWebhook(callbackUrl, job) {
    const payload = {
      jobId: job.id,
      status: job.status,
      sha256: job.sha256,
      downloadUrl: job.downloadUrl,
      warnings: job.warnings,
      tenantId: job.tenantId,
      completedAt: job.completedAt
    };
    await fetch3(callbackUrl, {
      method: "POST",
      headers: { "content-type": "application/json" },
      body: JSON.stringify(payload)
    });
  }
  async collectAuditTrail({
    job,
    pool: pool4,
    startTime,
    endTime
  }) {
    const { rows } = await pool4.query(
      `SELECT * FROM audit_events
       WHERE tenant_id = $1 AND created_at BETWEEN $2 AND $3
       ORDER BY created_at ASC
       LIMIT $4`,
      [
        job.tenantId,
        startTime.toISOString(),
        endTime.toISOString(),
        MAX_EVENTS + 1
      ]
    );
    const truncated = rows.length > MAX_EVENTS;
    const selected = truncated ? rows.slice(0, MAX_EVENTS) : rows;
    const sanitized = [];
    for (const row of selected) {
      const clean = await this.redaction.redactObject(
        row,
        {
          rules: ["pii", "sensitive", "financial"]
        },
        job.tenantId,
        { jobId: job.id }
      );
      sanitized.push(clean);
    }
    const filePath = path9.join(job.workingDir, "audit-trail.json");
    await this.writeJsonObject(filePath, {
      tenantId: job.tenantId,
      window: { start: startTime.toISOString(), end: endTime.toISOString() },
      count: sanitized.length,
      events: sanitized
    });
    const hash3 = await hashFile(filePath);
    const warnings = truncated ? ["truncated"] : [];
    return { filePath, count: sanitized.length, hash: hash3, warnings };
  }
  async collectSbomReports({
    job,
    pool: pool4,
    startTime,
    endTime
  }) {
    const { rows } = await pool4.query(
      `SELECT sr.sbom, sr.created_at
         FROM sbom_reports sr
         JOIN run r ON r.id = sr.run_id
        WHERE r.tenant_id = $1
          AND sr.created_at BETWEEN $2 AND $3
        ORDER BY sr.created_at ASC
        LIMIT 5000`,
      [job.tenantId, startTime.toISOString(), endTime.toISOString()]
    );
    if (!rows.length) {
      return null;
    }
    const normalized = rows.map((row) => ({
      createdAt: row.created_at instanceof Date ? row.created_at.toISOString() : row.created_at,
      sbom: row.sbom
    }));
    const filePath = path9.join(job.workingDir, "sbom-reports.json");
    await this.writeJsonObject(filePath, {
      tenantId: job.tenantId,
      reports: normalized
    });
    const hash3 = await hashFile(filePath);
    return {
      filePath,
      count: normalized.length,
      hash: hash3,
      warnings: []
    };
  }
  async collectPolicyReports({
    job,
    pool: pool4,
    startTime,
    endTime
  }) {
    const { rows } = await pool4.query(
      `SELECT policy, decision, created_at, user_id
         FROM policy_audit
        WHERE tenant_id = $1 AND created_at BETWEEN $2 AND $3
        ORDER BY created_at ASC
        LIMIT 5000`,
      [job.tenantId, startTime.toISOString(), endTime.toISOString()]
    );
    if (!rows.length) {
      return null;
    }
    const sanitized = [];
    for (const row of rows) {
      const clean = await this.redaction.redactObject(
        row,
        {
          rules: ["pii", "sensitive"]
        },
        job.tenantId,
        { jobId: job.id, artifact: "policy" }
      );
      sanitized.push(clean);
    }
    const filePath = path9.join(job.workingDir, "policy-reports.json");
    await this.writeJsonObject(filePath, {
      tenantId: job.tenantId,
      entries: sanitized
    });
    const hash3 = await hashFile(filePath);
    return {
      filePath,
      count: sanitized.length,
      hash: hash3,
      warnings: []
    };
  }
  async collectAttestations({
    job,
    pool: pool4,
    startTime,
    endTime
  }) {
    const { rows } = await pool4.query(
      `SELECT attestation, created_at
         FROM slsa_attestations
        WHERE tenant_id = $1 AND created_at BETWEEN $2 AND $3
        ORDER BY created_at ASC
        LIMIT 2000`,
      [job.tenantId, startTime.toISOString(), endTime.toISOString()]
    );
    if (!rows.length) {
      return null;
    }
    const attestations = rows.map((row) => ({
      createdAt: row.created_at instanceof Date ? row.created_at.toISOString() : row.created_at,
      attestation: row.attestation
    }));
    const filePath = path9.join(job.workingDir, "attestations.json");
    await this.writeJsonObject(filePath, {
      tenantId: job.tenantId,
      attestations
    });
    const hash3 = await hashFile(filePath);
    const mismatchedDigests = this.verifyAttestationSubjects(attestations, job);
    const warnings = mismatchedDigests.length ? mismatchedDigests.map((id) => `subject_digest_mismatch:${id}`) : [];
    return {
      filePath,
      count: attestations.length,
      hash: hash3,
      warnings,
      attestations: attestations.map((row) => row.attestation)
    };
  }
  verifyAttestationSubjects(attestations, job) {
    const mismatches = [];
    const artifactHashes = new Set(Object.values(job.artifactDigests));
    for (const entry of attestations) {
      const attestation = entry.attestation;
      const subjects = attestation?.subject || [];
      for (const subject of subjects) {
        const digest = subject?.digest?.sha256;
        if (!digest) continue;
        if (!artifactHashes.has(digest)) {
          mismatches.push(subject?.name || "unknown");
        }
      }
    }
    return mismatches;
  }
  async writeJsonObject(filePath, obj) {
    await new Promise((resolve2, reject) => {
      const stream2 = createWriteStream5(filePath, { encoding: "utf8" });
      stream2.on("error", reject);
      stream2.on("finish", () => resolve2());
      stream2.write(JSON.stringify(obj, null, 2));
      stream2.end();
    });
  }
  publicJob(job) {
    return {
      id: job.id,
      tenantId: job.tenantId,
      status: job.status,
      createdAt: job.createdAt,
      startedAt: job.startedAt,
      completedAt: job.completedAt,
      expiresAt: job.expiresAt,
      warnings: job.warnings,
      artifactStats: job.artifactStats,
      downloadUrl: job.downloadUrl,
      sha256: job.sha256,
      error: job.error,
      claimSet: job.claimSet
    };
  }
};
var disclosureExportService = new DisclosureExportService();

// src/disclosure/runtime-evidence.ts
import { createHash as createHash13, randomUUID as randomUUID13 } from "crypto";
import { promises as fs9 } from "fs";
import { createWriteStream as createWriteStream6 } from "fs";
import os4 from "os";
import path10 from "path";
import archiver2 from "archiver";
var DEFAULT_AUDIT_PATHS = [
  "logs/audit-events.jsonl",
  "logs/audit.jsonl",
  "server/logs/audit-events.jsonl",
  "audit-events.jsonl"
];
var DEFAULT_POLICY_PATHS = [
  "logs/policy-decisions.jsonl",
  "logs/governance.jsonl",
  "logs/policy/decisions.jsonl"
];
var DEFAULT_SBOM_PATHS = [
  "sbom-mc-v0.4.5.json",
  "release/sbom.json",
  "release/sbom-latest.json"
];
var DEFAULT_PROVENANCE_PATHS = [
  "provenance/export-manifest.json",
  "provenance/bundle-summary.md",
  "prov-ledger/manifest.json"
];
async function resolveDeployedVersion() {
  const envVersion = process.env.DEPLOYED_VERSION || process.env.RELEASE_VERSION || process.env.BUILD_VERSION;
  if (envVersion) return envVersion;
  try {
    const pkgPath = path10.resolve("package.json");
    const pkg = JSON.parse(String(await fs9.readFile(pkgPath, "utf8")));
    return typeof pkg.version === "string" ? pkg.version : void 0;
  } catch (error) {
    return void 0;
  }
}
function parseDate(input) {
  if (!input) return void 0;
  const date = new Date(input);
  if (Number.isNaN(date.getTime())) {
    throw new Error("invalid_date");
  }
  return date;
}
function findTimestamp(value) {
  if (!value || typeof value !== "object") return void 0;
  const record2 = value;
  const candidates2 = [
    record2.timestamp,
    record2.ts,
    record2.created_at,
    record2.createdAt,
    record2.occurred_at
  ];
  for (const candidate of candidates2) {
    if (!candidate) continue;
    const date = new Date(candidate);
    const ms = date.getTime();
    if (!Number.isNaN(ms)) return ms;
  }
  return void 0;
}
function matchesTenant(entry, tenantId) {
  if (!tenantId) return true;
  if (!entry || typeof entry !== "object") return false;
  const record2 = entry;
  const candidates2 = [record2.tenantId, record2.tenant_id, record2.tenant];
  return candidates2.some((value) => value && value === tenantId);
}
async function ensureDir2(dir) {
  await fs9.mkdir(dir, { recursive: true });
}
async function collectJsonlSlice({
  sources,
  destination,
  startTime,
  endTime,
  tenantId
}) {
  const warnings = [];
  let count = 0;
  await fs9.writeFile(destination, "");
  for (const source of sources) {
    const absolute = path10.resolve(source);
    const exists = await fs9.access(absolute).then(() => true).catch(() => false);
    if (!exists) {
      warnings.push(`missing:${source}`);
      continue;
    }
    const content = String(await fs9.readFile(absolute, "utf8"));
    const lines = content.split("\n").map((line) => line.trim()).filter(Boolean);
    const filtered = [];
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line);
        const ts = findTimestamp(parsed);
        if (startTime && (!ts || ts < startTime.getTime())) continue;
        if (endTime && (!ts || ts > endTime.getTime())) continue;
        if (!matchesTenant(parsed, tenantId)) continue;
        filtered.push(JSON.stringify(parsed));
        count += 1;
      } catch (error) {
        warnings.push(`parse_error:${source}`);
      }
    }
    if (filtered.length > 0) {
      await fs9.appendFile(destination, filtered.join("\n") + "\n");
    }
  }
  return { count, warnings };
}
async function copyArtifacts(sources, destinationDir) {
  const copied = [];
  const warnings = [];
  for (const source of sources) {
    const absolute = path10.resolve(source);
    const exists = await fs9.access(absolute).then(() => true).catch(() => false);
    if (!exists) {
      warnings.push(`missing:${source}`);
      continue;
    }
    const target = path10.join(destinationDir, path10.basename(source));
    await fs9.copyFile(absolute, target);
    copied.push(target);
  }
  return { copied, warnings };
}
async function sha256(filePath) {
  const hash3 = createHash13("sha256");
  const buffer = await fs9.readFile(filePath);
  hash3.update(buffer);
  return hash3.digest("hex");
}
var RuntimeEvidenceService = class {
  bundles = /* @__PURE__ */ new Map();
  ttlMs = 1e3 * 60 * 15;
  // 15 minutes
  async createBundle(options2) {
    const startTime = parseDate(options2.startTime);
    const endTime = parseDate(options2.endTime);
    const tenantId = options2.tenantId;
    const deployedVersion = options2.deployedVersion || await resolveDeployedVersion();
    const workingDir = path10.join(
      os4.tmpdir(),
      "runtime-evidence",
      randomUUID13().replace(/-/g, "")
    );
    const artifactsDir = path10.join(workingDir, "artifacts");
    await ensureDir2(artifactsDir);
    const warnings = [];
    const auditDest = path10.join(artifactsDir, "audit-events.jsonl");
    const auditResult = await collectJsonlSlice({
      sources: options2.auditPaths ?? DEFAULT_AUDIT_PATHS,
      destination: auditDest,
      startTime,
      endTime,
      tenantId
    });
    warnings.push(...auditResult.warnings);
    const policyDest = path10.join(artifactsDir, "policy-decisions.jsonl");
    const policyResult = await collectJsonlSlice({
      sources: options2.policyPaths ?? DEFAULT_POLICY_PATHS,
      destination: policyDest,
      startTime,
      endTime,
      tenantId
    });
    warnings.push(...policyResult.warnings);
    const sbomDir = path10.join(artifactsDir, "sbom");
    await ensureDir2(sbomDir);
    const sbomResult = await copyArtifacts(
      options2.sbomPaths ?? DEFAULT_SBOM_PATHS,
      sbomDir
    );
    warnings.push(...sbomResult.warnings);
    const provenanceDir = path10.join(artifactsDir, "provenance");
    await ensureDir2(provenanceDir);
    const provenanceResult = await copyArtifacts(
      options2.provenancePaths ?? DEFAULT_PROVENANCE_PATHS,
      provenanceDir
    );
    warnings.push(...provenanceResult.warnings);
    const manifest = {
      tenantId,
      startTime: startTime?.toISOString(),
      endTime: endTime?.toISOString(),
      counts: {
        auditEvents: auditResult.count,
        policyDecisions: policyResult.count,
        sbomRefs: sbomResult.copied.length,
        provenanceRefs: provenanceResult.copied.length
      },
      sources: {
        audit: options2.auditPaths ?? DEFAULT_AUDIT_PATHS,
        policy: options2.policyPaths ?? DEFAULT_POLICY_PATHS,
        sbom: options2.sbomPaths ?? DEFAULT_SBOM_PATHS,
        provenance: options2.provenancePaths ?? DEFAULT_PROVENANCE_PATHS
      },
      warnings,
      generatedAt: (/* @__PURE__ */ new Date()).toISOString(),
      deployedVersion
    };
    const manifestPath = path10.join(workingDir, "manifest.json");
    await fs9.writeFile(manifestPath, JSON.stringify(manifest, null, 2));
    const filesForChecksums = [
      auditDest,
      policyDest,
      ...sbomResult.copied,
      ...provenanceResult.copied,
      manifestPath
    ];
    const checksumLines = [];
    for (const filePath of filesForChecksums) {
      const exists = await fs9.access(filePath).then(() => true).catch(() => false);
      if (!exists) continue;
      const hash3 = await sha256(filePath);
      checksumLines.push(`${hash3}  ${path10.relative(workingDir, filePath)}`);
    }
    const checksumsPath = path10.join(workingDir, "checksums.txt");
    await fs9.writeFile(checksumsPath, checksumLines.join("\n"));
    const bundlePath = path10.join(
      workingDir,
      `runtime-evidence-${Date.now()}.tar.gz`
    );
    await this.writeTarball({
      files: [
        manifestPath,
        checksumsPath,
        auditDest,
        policyDest,
        ...sbomResult.copied,
        ...provenanceResult.copied
      ],
      output: bundlePath,
      workingDir
    });
    const sha256Hash = await sha256(bundlePath);
    const bundle = {
      id: randomUUID13(),
      tenantId,
      bundlePath,
      manifestPath,
      checksumsPath,
      sha256: sha256Hash,
      deployedVersion,
      warnings,
      counts: manifest.counts,
      expiresAt: new Date(Date.now() + this.ttlMs).toISOString()
    };
    this.cleanupExpired();
    this.bundles.set(bundle.id, bundle);
    return bundle;
  }
  getBundle(bundleId) {
    this.cleanupExpired();
    return this.bundles.get(bundleId);
  }
  cleanupExpired() {
    const now = Date.now();
    for (const [id, bundle] of this.bundles.entries()) {
      if (new Date(bundle.expiresAt).getTime() < now) {
        this.bundles.delete(id);
        const workingDir = path10.dirname(bundle.bundlePath);
        fs9.rm(workingDir, { recursive: true, force: true }).catch(() => void 0);
      }
    }
  }
  async writeTarball({
    files,
    output,
    workingDir
  }) {
    await ensureDir2(path10.dirname(output));
    await new Promise((resolve2, reject) => {
      const stream2 = createWriteStream6(output);
      const archive = archiver2("tar", { gzip: true, gzipOptions: { level: 9 } });
      archive.on("error", reject);
      stream2.on("close", () => resolve2());
      archive.pipe(stream2);
      for (const file of files) {
        archive.file(file, { name: path10.relative(workingDir, file) });
      }
      archive.finalize();
    });
  }
};
var runtimeEvidenceService = new RuntimeEvidenceService();

// src/routes/disclosures.ts
init_auth4();
var router9 = express8.Router();
router9.use(express8.json());
router9.use(ensureAuthenticated);
router9.use(requirePermission("export:investigations"));
var analyticsSchema = z13.object({
  event: z13.enum(["view", "start"]),
  tenantId: z13.string().min(1),
  context: z13.record(z13.any()).optional()
});
function resolveTenant(req) {
  const header = req.headers["x-tenant-id"]?.trim();
  const requestTenant = req.tenantId;
  return header || requestTenant;
}
router9.post("/analytics", (req, res) => {
  try {
    const payload = analyticsSchema.parse(req.body ?? {});
    const tenantHeader2 = resolveTenant(req) ?? payload.tenantId;
    if (tenantHeader2 !== payload.tenantId) {
      return res.status(403).json({ error: "tenant_mismatch" });
    }
    disclosureMetrics.uiEvent(payload.event, tenantHeader2);
    return res.status(202).json({ ok: true });
  } catch (error) {
    return res.status(400).json({ error: "invalid_payload", details: error?.message });
  }
});
router9.post("/export", async (req, res) => {
  try {
    const tenantHeader2 = resolveTenant(req);
    if (!tenantHeader2) {
      return res.status(400).json({ error: "tenant_required" });
    }
    const bodyTenant = req.body?.tenantId;
    const effectiveTenant = bodyTenant ?? tenantHeader2;
    if (effectiveTenant !== tenantHeader2) {
      return res.status(403).json({ error: "tenant_mismatch" });
    }
    const job = await disclosureExportService.createJob({
      ...req.body,
      tenantId: effectiveTenant
    });
    return res.status(202).json({ job });
  } catch (error) {
    const status = error?.message === "window_too_large" || error?.message === "end_before_start" ? 400 : 500;
    return res.status(status).json({ error: error?.message || "export_failed" });
  }
});
router9.get("/export", (req, res) => {
  const tenantHeader2 = resolveTenant(req);
  if (!tenantHeader2) {
    return res.status(400).json({ error: "tenant_required" });
  }
  const jobs = disclosureExportService.listJobsForTenant(tenantHeader2);
  return res.json({ jobs });
});
router9.get("/export/:jobId", (req, res) => {
  const tenantHeader2 = resolveTenant(req);
  if (!tenantHeader2) {
    return res.status(400).json({ error: "tenant_required" });
  }
  const job = disclosureExportService.getJob(req.params.jobId);
  if (!job) {
    return res.status(404).json({ error: "not_found" });
  }
  if (job.tenantId !== tenantHeader2) {
    return res.status(403).json({ error: "tenant_mismatch" });
  }
  return res.json({ job });
});
router9.get("/export/:jobId/download", (req, res) => {
  const tenantHeader2 = resolveTenant(req);
  if (!tenantHeader2) {
    return res.status(400).json({ error: "tenant_required" });
  }
  const download = disclosureExportService.getDownload(req.params.jobId);
  if (!download) {
    return res.status(404).json({ error: "not_ready" });
  }
  if (download.job.tenantId !== tenantHeader2) {
    return res.status(403).json({ error: "tenant_mismatch" });
  }
  if (download.job.status !== "completed") {
    return res.status(409).json({ error: "not_completed", status: download.job.status });
  }
  return res.download(download.filePath, `disclosure-${download.job.id}.zip`);
});
router9.post("/runtime-bundle", async (req, res) => {
  const tenantHeader2 = resolveTenant(req);
  const bodyTenant = req.body?.tenantId;
  if (!tenantHeader2 && !bodyTenant) {
    return res.status(400).json({ error: "tenant_required" });
  }
  const effectiveTenant = bodyTenant ?? tenantHeader2;
  if (tenantHeader2 && tenantHeader2 !== effectiveTenant) {
    return res.status(403).json({ error: "tenant_mismatch" });
  }
  try {
    const bundle = await runtimeEvidenceService.createBundle({
      tenantId: effectiveTenant,
      startTime: req.body?.startTime,
      endTime: req.body?.endTime,
      auditPaths: req.body?.auditPaths,
      policyPaths: req.body?.policyPaths,
      sbomPaths: req.body?.sbomPaths,
      provenancePaths: req.body?.provenancePaths,
      deployedVersion: req.body?.deployedVersion
    });
    const base = `${req.protocol}://${req.get("host")}${req.baseUrl}/runtime-bundle/${bundle.id}`;
    const downloadUrl = `${base}/download`;
    const manifestUrl = `${base}/manifest`;
    const checksumsUrl = `${base}/checksums`;
    return res.status(201).json({
      bundle: {
        ...bundle,
        downloadUrl,
        manifestUrl,
        checksumsUrl
      }
    });
  } catch (error) {
    const status = error?.message === "invalid_date" ? 400 : 500;
    return res.status(status).json({
      error: "runtime_bundle_failed",
      message: error?.message ?? "unknown_error"
    });
  }
});
router9.get("/runtime-bundle/:bundleId/download", async (req, res) => {
  const tenantHeader2 = resolveTenant(req);
  if (!tenantHeader2) {
    return res.status(400).json({ error: "tenant_required" });
  }
  const bundle = runtimeEvidenceService.getBundle(req.params.bundleId);
  if (!bundle) {
    return res.status(404).json({ error: "not_found" });
  }
  if (bundle.tenantId !== tenantHeader2) {
    return res.status(403).json({ error: "tenant_mismatch" });
  }
  const exists = await fs10.access(bundle.bundlePath).then(() => true).catch(() => false);
  if (!exists) {
    return res.status(410).json({ error: "bundle_missing" });
  }
  return res.download(bundle.bundlePath, `runtime-evidence-${bundle.id}.tar.gz`);
});
router9.get("/runtime-bundle/:bundleId/manifest", async (req, res) => {
  const tenantHeader2 = resolveTenant(req);
  if (!tenantHeader2) {
    return res.status(400).json({ error: "tenant_required" });
  }
  const bundle = runtimeEvidenceService.getBundle(req.params.bundleId);
  if (!bundle) {
    return res.status(404).json({ error: "not_found" });
  }
  if (bundle.tenantId !== tenantHeader2) {
    return res.status(403).json({ error: "tenant_mismatch" });
  }
  const exists = await fs10.access(bundle.manifestPath).then(() => true).catch(() => false);
  if (!exists) {
    return res.status(410).json({ error: "bundle_missing" });
  }
  return res.download(bundle.manifestPath, `runtime-evidence-${bundle.id}-manifest.json`);
});
router9.get("/runtime-bundle/:bundleId/checksums", async (req, res) => {
  const tenantHeader2 = resolveTenant(req);
  if (!tenantHeader2) {
    return res.status(400).json({ error: "tenant_required" });
  }
  const bundle = runtimeEvidenceService.getBundle(req.params.bundleId);
  if (!bundle) {
    return res.status(404).json({ error: "not_found" });
  }
  if (bundle.tenantId !== tenantHeader2) {
    return res.status(403).json({ error: "tenant_mismatch" });
  }
  const exists = await fs10.access(bundle.checksumsPath).then(() => true).catch(() => false);
  if (!exists) {
    return res.status(410).json({ error: "bundle_missing" });
  }
  return res.download(bundle.checksumsPath, `runtime-evidence-${bundle.id}-checksums.txt`);
});
var disclosures_default = router9;

// src/routes/narrative-sim.ts
init_auth4();
init_manager();
import express9 from "express";
import { z as z14 } from "zod/v4";
import { randomUUID as randomUUID17 } from "node:crypto";

// src/narrative/scenario.ts
init_engine();
var ScenarioSimulator = class {
  /**
   * Runs a batch of simulations with the given configuration and optional shock.
   * @param config Base simulation configuration
   * @param iterations Number of parallel simulations to run
   * @param ticks Number of ticks to simulate per run
   * @param shock Optional shock to inject at a specific tick (default tick 5)
   */
  async runBatch(config9, iterations = 10, ticks = 20, shock) {
    const promises = [];
    for (let i = 0; i < iterations; i++) {
      const engine2 = new NarrativeSimulationEngine({
        ...config9,
        id: `${config9.id}-run-${i}`
      });
      promises.push(this.runSingleSimulation(engine2, ticks, shock));
    }
    const results = await Promise.all(promises);
    return this.clusterOutcomes(config9.id, results);
  }
  async runSingleSimulation(engine2, totalTicks, shock) {
    const shockTick = shock ? Math.min(5, totalTicks - 1) : -1;
    if (shock && shockTick >= 0) {
      if (shockTick > 0) {
        await engine2.tick(shockTick);
      }
      engine2.injectShock(shock);
      await engine2.tick(totalTicks - shockTick);
    } else {
      await engine2.tick(totalTicks);
    }
    return engine2.getState();
  }
  clusterOutcomes(scenarioId, states) {
    const points = states.map((state) => {
      const entities = Object.values(state.entities);
      const avgSentiment = entities.reduce((sum, e) => sum + e.sentiment, 0) / entities.length;
      const avgInfluence = entities.reduce((sum, e) => sum + e.influence, 0) / entities.length;
      const avgPressure = entities.reduce((sum, e) => sum + e.pressure, 0) / entities.length;
      return { id: state.id, avgSentiment, avgInfluence, avgPressure };
    });
    const clusters = {};
    points.forEach((p) => {
      let label = "Neutral Stability";
      if (p.avgPressure > 0.3) {
        if (p.avgSentiment < -0.1) label = "Crisis (High Volatility, Negative)";
        else if (p.avgSentiment > 0.1) label = "Optimistic Chaos";
        else label = "High Volatility";
      } else {
        if (p.avgSentiment < -0.1) label = "Stagnation (Stable Negative)";
        else if (p.avgSentiment > 0.1) label = "Prosperity (Stable Positive)";
        else label = "Stability";
      }
      if (!clusters[label]) {
        clusters[label] = {
          label,
          count: 0,
          representativeStateId: p.id,
          avgSentiment: 0,
          avgInfluence: 0,
          sampleIds: []
        };
      }
      const c = clusters[label];
      c.count++;
      c.sampleIds.push(p.id);
      c.avgSentiment = c.avgSentiment + (p.avgSentiment - c.avgSentiment) / c.count;
      c.avgInfluence = c.avgInfluence + (p.avgInfluence - c.avgInfluence) / c.count;
    });
    return {
      scenarioId,
      totalRuns: states.length,
      clusters: Object.values(clusters).sort((a, b) => b.count - a.count),
      allStates: []
      // Omit to save bandwidth, or provide summary
    };
  }
};

// src/routes/narrative-sim.ts
init_featureFlags();
var router10 = express9.Router();
var scenarioSimulator = new ScenarioSimulator();
router10.use(requirePermission("simulation:run"));
router10.use((_req, res, next) => {
  if (!FeatureFlags.isEnabled("narrative.simulation")) {
    res.status(403).json({
      error: "feature-disabled",
      message: "Narrative Simulation Engine is not enabled for this environment."
    });
    return;
  }
  next();
});
var relationshipSchema = z14.object({
  targetId: z14.string(),
  strength: z14.number().min(0).max(1)
});
var entitySchema = z14.object({
  id: z14.string().optional(),
  name: z14.string(),
  type: z14.enum(["actor", "group"]),
  alignment: z14.enum(["ally", "neutral", "opposition"]),
  influence: z14.number().min(0).max(1.5),
  sentiment: z14.number().min(-1).max(1),
  volatility: z14.number().min(0).max(1),
  resilience: z14.number().min(0).max(1),
  themes: z14.record(z14.string(), z14.number()),
  relationships: z14.array(relationshipSchema).default([]),
  metadata: z14.record(z14.string(), z14.unknown()).optional()
});
var parameterSchema = z14.object({
  name: z14.string(),
  value: z14.number()
});
var llmSchema = z14.object({
  adapter: z14.enum(["echo"]).default("echo"),
  promptTemplate: z14.string().optional()
}).optional();
var agentSchema = z14.object({
  entityId: z14.string(),
  type: z14.enum(["rule-based", "llm"]),
  role: z14.string(),
  goal: z14.string(),
  llmConfig: z14.object({
    model: z14.string(),
    temperature: z14.number(),
    promptTemplate: z14.string().optional()
  }).optional()
});
var createSimulationSchema = z14.object({
  name: z14.string().min(1),
  themes: z14.array(z14.string()).min(1),
  tickIntervalMinutes: z14.number().positive().optional(),
  generatorMode: z14.enum(["rule-based", "llm"]).optional(),
  initialEntities: z14.array(entitySchema).min(1),
  initialParameters: z14.array(parameterSchema).optional(),
  agents: z14.array(agentSchema).optional(),
  metadata: z14.record(z14.string(), z14.unknown()).optional(),
  llm: llmSchema
});
var eventSchema = z14.object({
  id: z14.string().optional(),
  type: z14.enum([
    "social",
    "political",
    "information",
    "intervention",
    "system",
    "shock"
  ]),
  actorId: z14.string().optional(),
  targetIds: z14.array(z14.string()).optional(),
  theme: z14.string(),
  intensity: z14.number().min(0).max(2).default(1),
  sentimentShift: z14.number().optional(),
  influenceShift: z14.number().optional(),
  parameterAdjustments: z14.array(z14.object({ name: z14.string(), delta: z14.number() })).optional(),
  description: z14.string().min(1),
  scheduledTick: z14.number().int().nonnegative().optional(),
  metadata: z14.record(z14.string(), z14.unknown()).optional()
});
var shockSchema = z14.object({
  type: z14.string(),
  targetTag: z14.string().optional(),
  targetIds: z14.array(z14.string()).optional(),
  intensity: z14.number().min(0).max(2),
  description: z14.string()
});
var batchSimulationSchema = z14.object({
  config: createSimulationSchema,
  iterations: z14.number().int().min(1).max(100).default(10),
  ticks: z14.number().int().min(1).max(100).default(20),
  shock: shockSchema.optional()
});
var tickSchema = z14.object({
  steps: z14.number().int().positive().default(1)
});
var EchoLLMClient = class {
  constructor(template = "Tick {tick}: {arcs}. Recent: {events}.") {
    this.template = template;
  }
  async generateNarrative(request) {
    const arcSummary = request.state.arcs.map(
      (arc) => `${arc.theme} ${(arc.momentum * 100).toFixed(0)}% ${arc.outlook} (confidence ${(arc.confidence * 100).toFixed(0)}%)`
    ).join("; ");
    const events = request.recentEvents.slice(-5).map((event) => `${event.type}: ${event.description}`).join("; ") || "no recent drivers";
    return this.template.replace("{tick}", request.state.tick.toString()).replace("{arcs}", arcSummary).replace("{events}", events).replace("{themes}", request.state.themes.join(", "));
  }
};
router10.get("/simulations", (_req, res) => {
  res.json(narrativeSimulationManager.list());
});
router10.post("/simulations", (req, res) => {
  try {
    const payload = createSimulationSchema.parse(req.body ?? {});
    const entities = payload.initialEntities.map(
      (entity) => ({
        ...entity,
        id: entity.id ?? randomUUID17()
      })
    );
    const llmClient = payload.generatorMode === "llm" ? new EchoLLMClient(payload.llm?.promptTemplate) : void 0;
    const state = narrativeSimulationManager.createSimulation({
      name: payload.name,
      themes: payload.themes,
      tickIntervalMinutes: payload.tickIntervalMinutes,
      generatorMode: payload.generatorMode,
      initialEntities: entities,
      initialParameters: payload.initialParameters,
      agents: payload.agents,
      llmClient,
      metadata: payload.metadata
    });
    res.status(201).json(state);
  } catch (error) {
    if (error instanceof z14.ZodError) {
      res.status(400).json({ error: "invalid-request", details: error.flatten() });
      return;
    }
    res.status(500).json({ error: "failed-to-create", details: error.message });
  }
});
router10.post("/simulations/batch", async (req, res) => {
  try {
    const payload = batchSimulationSchema.parse(req.body ?? {});
    const entities = payload.config.initialEntities.map(
      (entity) => ({
        ...entity,
        id: entity.id ?? randomUUID17()
      })
    );
    const config9 = {
      id: randomUUID17(),
      name: payload.config.name,
      themes: payload.config.themes,
      tickIntervalMinutes: payload.config.tickIntervalMinutes ?? 60,
      initialEntities: entities,
      initialParameters: payload.config.initialParameters,
      generatorMode: payload.config.generatorMode,
      metadata: payload.config.metadata
      // LLM client not supported in batch for now (too heavy)
    };
    const result2 = await scenarioSimulator.runBatch(
      config9,
      payload.iterations,
      payload.ticks,
      payload.shock
    );
    res.json(result2);
  } catch (error) {
    if (error instanceof z14.ZodError) {
      res.status(400).json({ error: "invalid-request", details: error.flatten() });
      return;
    }
    res.status(500).json({ error: error.message });
  }
});
router10.get("/simulations/:id", (req, res) => {
  const state = narrativeSimulationManager.getState(req.params.id);
  if (!state) {
    res.status(404).json({ error: "not-found" });
    return;
  }
  res.json(state);
});
router10.post("/simulations/:id/tick", async (req, res) => {
  try {
    const { steps } = tickSchema.parse(req.body ?? {});
    const state = await narrativeSimulationManager.tick(req.params.id, steps);
    res.json(state);
  } catch (error) {
    if (error instanceof z14.ZodError) {
      res.status(400).json({ error: "invalid-request", details: error.flatten() });
      return;
    }
    res.status(404).json({ error: error.message });
  }
});
router10.post("/simulations/:id/events", (req, res) => {
  try {
    const payload = eventSchema.parse(req.body ?? {});
    const event = {
      ...payload,
      id: payload.id ?? randomUUID17()
    };
    narrativeSimulationManager.queueEvent(req.params.id, event);
    res.status(202).json({ status: "accepted", event });
  } catch (error) {
    if (error instanceof z14.ZodError) {
      res.status(400).json({ error: "invalid-request", details: error.flatten() });
      return;
    }
    res.status(404).json({ error: error.message });
  }
});
router10.post("/simulations/:id/shock", (req, res) => {
  try {
    const payload = shockSchema.parse(req.body ?? {});
    const engine2 = narrativeSimulationManager.getEngine(req.params.id);
    if (!engine2) {
      res.status(404).json({ error: "not-found" });
      return;
    }
    engine2.injectShock(payload);
    res.status(202).json({ status: "accepted", type: "shock" });
  } catch (error) {
    if (error instanceof z14.ZodError) {
      res.status(400).json({ error: "invalid-request", details: error.flatten() });
      return;
    }
    res.status(404).json({ error: error.message });
  }
});
router10.post("/simulations/:id/actions", (req, res) => {
  try {
    const payload = eventSchema.omit({ type: true }).extend({ actorId: z14.string(), description: z14.string() }).parse(req.body ?? {});
    narrativeSimulationManager.injectActorAction(
      req.params.id,
      payload.actorId,
      payload.description,
      {
        targetIds: payload.targetIds,
        theme: payload.theme,
        intensity: payload.intensity,
        sentimentShift: payload.sentimentShift,
        influenceShift: payload.influenceShift,
        parameterAdjustments: payload.parameterAdjustments,
        metadata: payload.metadata,
        scheduledTick: payload.scheduledTick
      }
    );
    res.status(202).json({ status: "accepted" });
  } catch (error) {
    if (error instanceof z14.ZodError) {
      res.status(400).json({ error: "invalid-request", details: error.flatten() });
      return;
    }
    res.status(404).json({ error: error.message });
  }
});
router10.delete("/simulations/:id", (req, res) => {
  const removed = narrativeSimulationManager.remove(req.params.id);
  if (!removed) {
    res.status(404).json({ error: "not-found" });
    return;
  }
  res.status(204).send();
});
var narrative_sim_default = router10;

// src/routes/narrative-routes.ts
init_manager();
init_redis2();
import { Router as Router2 } from "express";
import { trace as trace9 } from "@opentelemetry/api";
var router11 = Router2();
var tracer8 = trace9.getTracer("narrative-routes", "1.0.0");
var redis = RedisService.getInstance();
var singleParam = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
router11.get("/:simId/arcs", async (req, res) => {
  return tracer8.startActiveSpan("narrative.get_arcs", async (span) => {
    try {
      const simId = singleParam(req.params.simId) ?? "";
      span.setAttribute("simulation_id", simId);
      const cacheKey = `narrative:arcs:${simId}`;
      try {
        const cached = await redis.get(cacheKey);
        if (cached) {
          span.setAttribute("cache_hit", true);
          return res.json(JSON.parse(cached));
        }
      } catch (cacheError) {
        console.warn("Redis cache read failed:", cacheError);
      }
      const state = narrativeSimulationManager.getState(simId);
      if (!state) {
        span.setAttribute("error", "simulation_not_found");
        return res.status(404).json({
          error: "Simulation not found",
          simulationId: simId
        });
      }
      const arcData = {
        simulationId: simId,
        currentTick: state.tick,
        arcs: state.arcs.map((arc) => ({
          theme: arc.theme,
          momentum: arc.momentum,
          outlook: arc.outlook,
          confidence: arc.confidence,
          keyEntities: arc.keyEntities,
          narrative: arc.narrative
        })),
        // TODO: Add historical arc data when we implement tick-by-tick tracking
        history: []
      };
      try {
        await redis.set(cacheKey, JSON.stringify(arcData), 60);
      } catch (cacheError) {
        console.warn("Redis cache write failed:", cacheError);
      }
      span.setAttribute("arc_count", state.arcs.length);
      span.setAttribute("tick", state.tick);
      res.json(arcData);
    } catch (error) {
      span.recordException(error);
      span.setStatus({ code: 2, message: error.message });
      console.error("Error fetching narrative arcs:", error);
      res.status(500).json({
        error: "Internal server error",
        message: error.message
      });
    } finally {
      span.end();
    }
  });
});
router11.get("/:simId/events", async (req, res) => {
  return tracer8.startActiveSpan("narrative.get_events", async (span) => {
    try {
      const simId = singleParam(req.params.simId) ?? "";
      span.setAttribute("simulation_id", simId);
      const cacheKey = `narrative:events:${simId}`;
      try {
        const cached = await redis.get(cacheKey);
        if (cached) {
          span.setAttribute("cache_hit", true);
          return res.json(JSON.parse(cached));
        }
      } catch (cacheError) {
        console.warn("Redis cache read failed:", cacheError);
      }
      const state = narrativeSimulationManager.getState(simId);
      if (!state) {
        span.setAttribute("error", "simulation_not_found");
        return res.status(404).json({
          error: "Simulation not found",
          simulationId: simId
        });
      }
      const eventData = {
        simulationId: simId,
        currentTick: state.tick,
        events: state.recentEvents.map((event) => ({
          id: event.id,
          tick: event.scheduledTick || state.tick,
          type: event.type,
          actorId: event.actorId,
          targetIds: event.targetIds,
          theme: event.theme,
          intensity: event.intensity,
          description: event.description,
          metadata: event.metadata
        }))
      };
      try {
        await redis.set(cacheKey, JSON.stringify(eventData), 60);
      } catch (cacheError) {
        console.warn("Redis cache write failed:", cacheError);
      }
      span.setAttribute("event_count", state.recentEvents.length);
      res.json(eventData);
    } catch (error) {
      span.recordException(error);
      span.setStatus({ code: 2, message: error.message });
      console.error("Error fetching narrative events:", error);
      res.status(500).json({
        error: "Internal server error",
        message: error.message
      });
    } finally {
      span.end();
    }
  });
});
router11.get("/:simId/summary", async (req, res) => {
  return tracer8.startActiveSpan("narrative.get_summary", async (span) => {
    try {
      const simId = singleParam(req.params.simId) ?? "";
      span.setAttribute("simulation_id", simId);
      const summary = narrativeSimulationManager.getState(simId);
      if (!summary) {
        return res.status(404).json({
          error: "Simulation not found",
          simulationId: simId
        });
      }
      res.json({
        id: summary.id,
        name: summary.name,
        tick: summary.tick,
        themes: summary.themes,
        entityCount: Object.keys(summary.entities).length,
        arcCount: summary.arcs.length,
        recentEventCount: summary.recentEvents.length,
        narrative: summary.narrative
      });
    } catch (error) {
      span.recordException(error);
      span.setStatus({ code: 2, message: error.message });
      console.error("Error fetching simulation summary:", error);
      res.status(500).json({
        error: "Internal server error",
        message: error.message
      });
    } finally {
      span.end();
    }
  });
});
var narrative_routes_default = router11;

// src/routes/receipts.ts
import express10 from "express";

// src/services/ReceiptService.ts
init_ledger();

// src/services/SigningService.ts
init_config();
import { createSign as createSign2, createPublicKey as createPublicKey2 } from "crypto";
var SigningService = class {
  privateKey;
  publicKey;
  constructor() {
    if (!cfg.SIGNING_PRIVATE_KEY) {
      throw new Error("SIGNING_PRIVATE_KEY is not configured. Cannot create signatures.");
    }
    this.privateKey = cfg.SIGNING_PRIVATE_KEY;
    try {
      const publicKeyObject = createPublicKey2(this.privateKey);
      this.publicKey = publicKeyObject.export({ type: "spki", format: "pem" }).toString();
    } catch (error) {
      throw new Error("Invalid SIGNING_PRIVATE_KEY. Please provide a valid PEM-formatted private key.");
    }
  }
  /**
   * Signs the given data using the configured private key.
   * @param data The data to sign (string or Buffer).
   * @returns The base64-encoded signature.
   */
  sign(data) {
    const sign4 = createSign2("SHA256");
    sign4.update(data);
    sign4.end();
    return sign4.sign(this.privateKey, "base64");
  }
  /**
   * Returns the public key corresponding to the private key.
   * @returns The public key in PEM format.
   */
  getPublicKey() {
    return this.publicKey;
  }
};

// src/services/ReceiptService.ts
import { createHash as createHash14 } from "crypto";
var ReceiptService = class _ReceiptService {
  _ledger;
  _signer;
  static instance;
  constructor() {
  }
  get ledger() {
    if (!this._ledger) {
      this._ledger = ProvenanceLedgerV2.getInstance();
    }
    return this._ledger;
  }
  get signer() {
    if (!this._signer) {
      this._signer = new SigningService();
    }
    return this._signer;
  }
  static getInstance() {
    if (!_ReceiptService.instance) {
      _ReceiptService.instance = new _ReceiptService();
    }
    return _ReceiptService.instance;
  }
  /**
   * Generates a signed receipt for an action.
   * This also logs the action to the Provenance Ledger if it hasn't been logged yet.
   */
  async generateReceipt(params) {
    const { action, actor, resource, input, policyDecisionId } = params;
    const inputStr = typeof input === "string" ? input : JSON.stringify(input);
    const inputHash = createHash14("sha256").update(inputStr).digest("hex");
    const entry = await this.ledger.appendEntry({
      action,
      actorId: actor.id,
      tenantId: actor.tenantId,
      resource,
      details: { inputHash, policyDecisionId },
      // stored in 'details' which maps to metadata or payload
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
    const entryId = entry.id;
    const timestamp = (/* @__PURE__ */ new Date()).toISOString();
    const receiptCanonical = [
      entryId,
      timestamp,
      action,
      actor.id,
      actor.tenantId,
      resource,
      inputHash,
      policyDecisionId || ""
    ].join("|");
    const signature = this.signer.sign(receiptCanonical);
    const receipt = {
      id: entryId,
      timestamp,
      action,
      actor: actor.id,
      resource,
      inputHash,
      policyDecisionId,
      signature,
      signerKeyId: this.signer.getPublicKey().slice(0, 32) + "..."
      // simplified ID
    };
    const receiptEvidence = {
      receiptId: receipt.id,
      entryId,
      action,
      actorId: actor.id,
      tenantId: actor.tenantId,
      resourceId: resource,
      inputHash,
      policyDecisionId,
      signature,
      signerKeyId: receipt.signerKeyId,
      issuedAt: timestamp
    };
    await this.ledger.recordReceiptEvidence(receiptEvidence);
    return receipt;
  }
  async getReceipt(id) {
    const entry = await this.ledger.getEntryById(id);
    if (!entry) return null;
    return {
      id: entry.id,
      timestamp: entry.timestamp.toISOString(),
      action: entry.actionType,
      actor: entry.actorId,
      resource: entry.resourceId,
      inputHash: entry.metadata?.inputHash || "unknown",
      policyDecisionId: entry.metadata?.policyDecisionId,
      signature: entry.signature || "unsigned",
      signerKeyId: "system-key"
      // metadata
    };
  }
};

// src/routes/receipts.ts
var router12 = express10.Router();
router12.get("/:id", async (req, res) => {
  try {
    const id = req.params.id;
    const service11 = ReceiptService.getInstance();
    const receipt = await service11.getReceipt(id);
    if (!receipt) {
      return res.json({
        id,
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        action: "mock_action",
        actor: "mock_actor",
        resource: "mock_resource",
        inputHash: "mock_hash",
        signature: "mock_signature",
        signerKeyId: "mock_key_id",
        note: "This is a synthetic receipt for demonstration."
      });
    }
    res.json(receipt);
  } catch (error) {
    res.status(500).json({ error: "Internal server error" });
  }
});
var receipts_default = router12;

// src/routes/predictive.ts
import express11 from "express";

// src/services/PredictiveThreatService.ts
init_metrics3();
init_logger2();
import { EventEmitter as EventEmitter10 } from "events";
var PredictiveThreatService = class extends EventEmitter10 {
  metrics;
  // Mock historical data storage for Alpha
  historicalData = /* @__PURE__ */ new Map();
  constructor() {
    super();
    this.metrics = new PrometheusMetrics("predictive_suite");
    this.initializeMetrics();
    this.seedMockData();
    this.startBackgroundUpdate();
  }
  initializeMetrics() {
    this.metrics.createGauge(
      "predictive_forecast_value",
      "Forecasted value for a signal at a specific horizon",
      ["signal", "horizon"]
    );
    this.metrics.createGauge(
      "predictive_model_confidence",
      "Confidence score of the predictive model",
      ["signal", "model"]
    );
  }
  startBackgroundUpdate() {
    this.updateMetrics();
    setInterval(() => {
      this.updateMetrics();
    }, 6e4);
  }
  async updateMetrics() {
    try {
      const signals = ["threat_events", "anomaly_score"];
      const horizon = 24;
      for (const signal of signals) {
        await this.forecastSignal(signal, horizon);
      }
    } catch (error) {
      logger_default2.error("Failed to update predictive metrics", error);
    }
  }
  seedMockData() {
    const now = Date.now();
    const points = [];
    for (let i = 24 * 7; i >= 0; i--) {
      const timestamp = new Date(now - i * 36e5);
      const value = 100 + (24 * 7 - i) * 1 + (Math.random() * 20 - 10);
      points.push({ timestamp, value: Math.max(0, value) });
    }
    this.historicalData.set("threat_events", points);
    const anomalyPoints = [];
    for (let i = 24 * 7; i >= 0; i--) {
      const timestamp = new Date(now - i * 36e5);
      const value = 50 + Math.sin(i / 12) * 30 + (Math.random() * 10 - 5);
      anomalyPoints.push({ timestamp, value: Math.max(0, value) });
    }
    this.historicalData.set("anomaly_score", anomalyPoints);
  }
  /**
   * Forecasts a signal over a given horizon
   * @param signal The name of the signal (e.g., 'threat_events')
   * @param horizonHours Number of hours to forecast
   */
  async forecastSignal(signal, horizonHours = 24) {
    const history = this.historicalData.get(signal) || [];
    if (history.length === 0) {
      throw new Error(`No historical data for signal: ${signal}`);
    }
    const n = history.length;
    let sumX = 0;
    let sumY = 0;
    let sumXY = 0;
    let sumXX = 0;
    const xValues = history.map((_2, i) => i);
    const yValues = history.map((p) => p.value);
    for (let i = 0; i < n; i++) {
      sumX += xValues[i];
      sumY += yValues[i];
      sumXY += xValues[i] * yValues[i];
      sumXX += xValues[i] * xValues[i];
    }
    const slope = (n * sumXY - sumX * sumY) / (n * sumXX - sumX * sumX);
    const intercept = (sumY - slope * sumX) / n;
    let sumSquaredResiduals = 0;
    for (let i = 0; i < n; i++) {
      const predicted = slope * xValues[i] + intercept;
      sumSquaredResiduals += Math.pow(yValues[i] - predicted, 2);
    }
    const stdError = Math.sqrt(sumSquaredResiduals / (n - 2));
    const forecastPoints = [];
    const lastTimestamp = history[history.length - 1].timestamp.getTime();
    const historySubset = history.slice(-24);
    historySubset.forEach((p) => {
      forecastPoints.push({
        timestamp: p.timestamp,
        value: p.value,
        confidenceLow: p.value,
        // No uncertainty for history in this simple view
        confidenceHigh: p.value,
        isForecast: false
      });
    });
    for (let i = 1; i <= horizonHours; i++) {
      const futureX = n + i;
      const predictedValue = slope * futureX + intercept;
      const timestamp = new Date(lastTimestamp + i * 36e5);
      const confidenceMargin = stdError * 1.96 * (1 + i / 10);
      forecastPoints.push({
        timestamp,
        value: Math.max(0, predictedValue),
        confidenceLow: Math.max(0, predictedValue - confidenceMargin),
        confidenceHigh: predictedValue + confidenceMargin,
        isForecast: true
      });
    }
    const lastForecast = forecastPoints[forecastPoints.length - 1];
    this.metrics.setGauge("predictive_forecast_value", lastForecast.value, {
      signal,
      horizon: `${horizonHours}h`
    });
    return {
      signal,
      horizon: `${horizonHours}h`,
      points: forecastPoints,
      metadata: {
        model: "linear_regression_alpha",
        confidenceLevel: 0.95,
        lastUpdated: /* @__PURE__ */ new Date()
      },
      provenance: {
        source: "historical_data",
        model: "linear_regression_alpha",
        version: "1.0.0",
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        experimental: false
      }
    };
  }
  /**
   * Simulates a counterfactual scenario (e.g. "What if we block traffic from Country X?")
   * For Alpha, this applies a simple multiplier or additive effect.
   */
  async simulateCounterfactual(signal, scenario) {
    const forecast = await this.forecastSignal(signal, 24);
    const adjustedPoints = forecast.points.map((p) => {
      if (!p.isForecast) return p;
      const newValue = p.value * (1 + scenario.impactFactor);
      return {
        ...p,
        value: Math.max(0, newValue),
        confidenceLow: Math.max(0, p.confidenceLow * (1 + scenario.impactFactor)),
        confidenceHigh: p.confidenceHigh * (1 + scenario.impactFactor)
      };
    });
    const originalTotal = forecast.points.filter((p) => p.isForecast).reduce((sum, p) => sum + p.value, 0);
    const newTotal = adjustedPoints.filter((p) => p.isForecast).reduce((sum, p) => sum + p.value, 0);
    const delta = newTotal - originalTotal;
    const percentageChange = delta / originalTotal * 100;
    return {
      scenarioId: `${scenario.action}_${Date.now()}`,
      isSimulated: true,
      originalForecast: forecast.points,
      adjustedForecast: adjustedPoints,
      impact: {
        delta,
        percentageChange
      },
      provenance: {
        source: "simulation_engine",
        model: "counterfactual_alpha",
        version: "1.0.0",
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        experimental: true
      }
    };
  }
  _resetForTesting() {
    this.historicalData.clear();
    this.seedMockData();
  }
};
var predictiveThreatService = new PredictiveThreatService();

// src/routes/predictive.ts
init_logger2();
var router13 = express11.Router();
var singleParam2 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
router13.get("/forecast/:signal", async (req, res) => {
  try {
    const signal = singleParam2(req.params.signal) ?? "";
    const horizonRaw = singleParam2(req.query.horizon);
    const horizon = horizonRaw ? parseInt(horizonRaw, 10) : 24;
    const result2 = await predictiveThreatService.forecastSignal(signal, horizon);
    res.json(result2);
  } catch (error) {
    logger_default2.error("Error fetching forecast:", error);
    if (error instanceof Error && error.message.includes("No historical data")) {
      res.status(404).json({ error: error.message });
    } else {
      res.status(500).json({ error: "Internal Server Error" });
    }
  }
});
router13.post("/simulate", async (req, res) => {
  try {
    const { signal, action, impactFactor } = req.body;
    if (!signal || !action || impactFactor === void 0) {
      return res.status(400).json({ error: "Missing required parameters: signal, action, impactFactor" });
    }
    const result2 = await predictiveThreatService.simulateCounterfactual(signal, {
      action,
      impactFactor
    });
    res.json(result2);
  } catch (error) {
    logger_default2.error("Error running simulation:", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
var predictive_default = router13;

// src/routes/policies/policy-management.ts
init_auth4();
import express12 from "express";

// src/services/AuthorizationService.ts
init_database();
init_logger2();

// src/auth/multi-tenant-rbac.ts
init_logger();
var DEFAULT_ROLES = {
  "global-admin": {
    permissions: ["*"],
    inherits: []
  },
  "tenant-owner": {
    permissions: ["*"],
    // Full access within tenant
    inherits: ["tenant-admin"]
  },
  "tenant-admin": {
    permissions: [
      "tenant:manage",
      "user:read",
      "user:create",
      "user:update",
      "user:delete",
      "role:assign",
      "role:revoke",
      "audit:read",
      "audit:export",
      "config:read",
      "config:update",
      "api_key:create",
      "api_key:view",
      "api_key:delete",
      "service_account:create",
      "service_account:view",
      "service_account:delete",
      "billing:read"
    ],
    inherits: ["supervisor"]
  },
  "security-admin": {
    permissions: [
      "user:read",
      "user:update",
      // Can lock users
      "audit:read",
      "audit:export",
      "api_key:view",
      "api_key:delete",
      // Can revoke keys
      "policy:read",
      "policy:update",
      "compliance:read",
      "compliance:report"
    ],
    inherits: ["viewer"]
  },
  "developer": {
    permissions: [
      "maestro:runs.read",
      "maestro:runs.write",
      "ingestion:pipelines.read",
      "ingestion:pipelines.write",
      "graph:read",
      "graph:analyze",
      "api_key:create"
      // Can create own keys
    ],
    inherits: ["analyst"]
  },
  "supervisor": {
    permissions: [
      "investigation:read",
      "investigation:create",
      "investigation:update",
      "investigation:delete",
      "entity:read",
      "entity:create",
      "entity:update",
      "entity:delete",
      "relationship:read",
      "relationship:create",
      "relationship:update",
      "relationship:delete",
      "analytics:run",
      "analytics:export",
      "report:read",
      "report:create",
      "report:export",
      "team:manage"
    ],
    inherits: ["analyst"]
  },
  "analyst": {
    permissions: [
      "investigation:read",
      "investigation:create",
      "investigation:update",
      "entity:read",
      "entity:create",
      "entity:update",
      "relationship:read",
      "relationship:create",
      "relationship:update",
      "analytics:run",
      "report:read",
      "report:create",
      "copilot:query",
      "copilot:analyze"
    ],
    inherits: ["viewer"]
  },
  "viewer": {
    permissions: [
      "investigation:read",
      "entity:read",
      "relationship:read",
      "analytics:view",
      "report:read",
      "dashboard:view"
    ],
    inherits: []
  },
  "compliance-officer": {
    permissions: [
      "audit:read",
      "audit:export",
      "report:read",
      "report:export",
      "policy:read",
      "sensitive:read",
      "dlp:override"
    ],
    inherits: ["viewer"]
  },
  "ot-integrator": {
    permissions: [
      "ot:read",
      "ot:write",
      "ot:configure",
      "data:ingest",
      "data:transform",
      "pipeline:read",
      "pipeline:execute"
    ],
    inherits: ["viewer"]
  }
};
var MultiTenantRBACManager = class {
  config;
  roleDefinitions;
  tenantCache;
  permissionCache;
  opaClient;
  constructor(config9, opaClient2) {
    this.config = {
      enabled: true,
      enforceTenantIsolation: true,
      allowCrossTenantAccess: false,
      requireMfaForSensitive: true,
      auditAllAccess: true,
      maxSessionDuration: 8 * 60 * 60 * 1e3,
      // 8 hours
      stepUpAuthTTL: 5 * 60 * 1e3,
      // 5 minutes
      deniedEnvironments: [],
      ...config9
    };
    this.roleDefinitions = /* @__PURE__ */ new Map();
    this.tenantCache = /* @__PURE__ */ new Map();
    this.permissionCache = /* @__PURE__ */ new Map();
    this.opaClient = opaClient2;
    this.initializeRoles();
    this.loadDeniedEnvironments();
    logger_default.info("Multi-tenant RBAC manager initialized", {
      enabled: this.config.enabled,
      enforceTenantIsolation: this.config.enforceTenantIsolation,
      deniedEnvironments: this.config.deniedEnvironments.length
    });
  }
  initializeRoles() {
    for (const [roleName, roleDef] of Object.entries(DEFAULT_ROLES)) {
      const allPermissions = new Set(roleDef.permissions);
      const resolveInherited = (role, visited) => {
        if (visited.has(role)) return;
        visited.add(role);
        const inherited = DEFAULT_ROLES[role];
        if (inherited) {
          inherited.permissions.forEach((p) => allPermissions.add(p));
          inherited.inherits?.forEach((r) => resolveInherited(r, visited));
        }
      };
      roleDef.inherits?.forEach((r) => resolveInherited(r, /* @__PURE__ */ new Set()));
      this.roleDefinitions.set(roleName, {
        permissions: allPermissions,
        inherits: roleDef.inherits || []
      });
    }
    logger_default.debug("RBAC roles initialized", {
      roles: Array.from(this.roleDefinitions.keys())
    });
  }
  loadDeniedEnvironments() {
    const deniedEnvs = process.env.DENIED_ENVIRONMENTS;
    if (deniedEnvs) {
      this.config.deniedEnvironments = deniedEnvs.split(",").map((e) => e.trim());
    }
    const deniedOT = process.env.DENIED_OT_SYSTEMS;
    if (deniedOT) {
      const otSystems = deniedOT.split(",").map((s) => `ot:${s.trim()}`);
      this.config.deniedEnvironments.push(...otSystems);
    }
  }
  /**
   * Register a tenant configuration
   */
  registerTenant(tenant) {
    this.tenantCache.set(tenant.id, tenant);
    logger_default.info("Tenant registered", {
      tenantId: tenant.id,
      classification: tenant.classification
    });
  }
  /**
   * Get tenant configuration
   */
  getTenant(tenantId) {
    return this.tenantCache.get(tenantId);
  }
  /**
   * Check if user has permission in tenant context
   */
  hasPermission(user, permission, tenantId) {
    if (!this.config.enabled) return true;
    const effectiveTenantId = tenantId || user.tenantId;
    const cacheKey = `${user.id}:${effectiveTenantId}:${permission}`;
    if (this.isEnvironmentDenied(permission)) {
      return false;
    }
    if (user.globalRoles.includes("global-admin")) {
      return true;
    }
    const tenantRoles = user.roles.filter((r) => r.tenantId === effectiveTenantId);
    if (tenantRoles.length === 0) {
      logger_default.debug("No roles found for tenant", {
        userId: user.id,
        tenantId: effectiveTenantId
      });
      return false;
    }
    for (const role of tenantRoles) {
      if (role.expiresAt && role.expiresAt < /* @__PURE__ */ new Date()) {
        continue;
      }
      const roleDef = this.roleDefinitions.get(role.role);
      if (!roleDef) continue;
      if (roleDef.permissions.has("*")) {
        return true;
      }
      if (roleDef.permissions.has(permission)) {
        return true;
      }
      const [resource, action] = permission.split(":");
      if (roleDef.permissions.has(`${resource}:*`)) {
        return true;
      }
    }
    return false;
  }
  /**
   * Check if environment/action is denied
   */
  isEnvironmentDenied(permission) {
    return this.config.deniedEnvironments.some(
      (env2) => permission.startsWith(env2) || permission === env2
    );
  }
  /**
   * Evaluate access decision with full context
   */
  async evaluateAccess(user, resource, action) {
    const startTime = Date.now();
    const permission = `${resource.type}:${action}`;
    const decision = {
      allowed: false,
      reason: "",
      tenantId: resource.tenantId,
      userId: user.id,
      resource: `${resource.type}:${resource.id}`,
      action,
      timestamp: /* @__PURE__ */ new Date(),
      obligations: [],
      auditRequired: this.config.auditAllAccess,
      stepUpRequired: false
    };
    if (this.config.enforceTenantIsolation) {
      if (!this.checkTenantIsolation(user, resource.tenantId)) {
        decision.reason = "Tenant isolation violation";
        this.logAccessDecision(decision, Date.now() - startTime);
        return decision;
      }
    }
    if (resource.classification) {
      if (!this.checkClearance(user, resource.classification)) {
        decision.reason = "Insufficient clearance level";
        this.logAccessDecision(decision, Date.now() - startTime);
        return decision;
      }
    }
    if (this.requiresStepUp(action, resource)) {
      if (!user.mfaVerified) {
        decision.stepUpRequired = true;
        decision.reason = "Step-up authentication required";
        this.logAccessDecision(decision, Date.now() - startTime);
        return decision;
      }
    }
    if (!this.hasPermission(user, permission, resource.tenantId)) {
      decision.reason = "Insufficient permissions";
      this.logAccessDecision(decision, Date.now() - startTime);
      return decision;
    }
    if (this.opaClient) {
      const opaDecision = await this.evaluateOPAPolicy(user, resource, action);
      if (!opaDecision.allowed) {
        decision.reason = opaDecision.reason || "Policy violation";
        decision.obligations = opaDecision.obligations;
        this.logAccessDecision(decision, Date.now() - startTime);
        return decision;
      }
      decision.obligations = opaDecision.obligations;
    }
    decision.allowed = true;
    decision.reason = "Access granted";
    if (resource.classification && resource.classification !== "unclassified") {
      decision.obligations = decision.obligations || [];
      decision.obligations.push({
        type: "audit",
        parameters: {
          level: "sensitive",
          classification: resource.classification
        }
      });
    }
    this.logAccessDecision(decision, Date.now() - startTime);
    return decision;
  }
  /**
   * Check tenant isolation
   */
  checkTenantIsolation(user, resourceTenantId) {
    if (user.globalRoles.includes("global-admin")) {
      return true;
    }
    return user.tenantIds.includes(resourceTenantId);
  }
  /**
   * Check clearance level
   */
  checkClearance(user, requiredClassification) {
    const clearanceLevels = {
      "unclassified": 0,
      "confidential": 1,
      "secret": 2,
      "top-secret": 3,
      "top-secret-sci": 4
    };
    const userLevel = clearanceLevels[user.clearanceLevel] ?? 0;
    const requiredLevel = clearanceLevels[requiredClassification] ?? 0;
    return userLevel >= requiredLevel;
  }
  /**
   * Check if action requires step-up authentication
   */
  requiresStepUp(action, resource) {
    const sensitiveActions = [
      "delete",
      "bulk_delete",
      "export",
      "bulk_export",
      "admin_action",
      "user_management",
      "config_change"
    ];
    const sensitiveClassifications = ["secret", "top-secret", "top-secret-sci"];
    return sensitiveActions.includes(action) || Boolean(resource.classification && sensitiveClassifications.includes(resource.classification));
  }
  /**
   * Evaluate OPA policy
   */
  async evaluateOPAPolicy(user, resource, action) {
    if (!this.opaClient) {
      return { allowed: true };
    }
    const input = {
      user: {
        id: user.id,
        tenant_id: user.tenantId,
        roles: user.roles.map((r) => r.role),
        clearance: user.clearanceLevel,
        attributes: user.attributes,
        mfa_verified: user.mfaVerified
      },
      resource: {
        type: resource.type,
        id: resource.id,
        tenant_id: resource.tenantId,
        classification: resource.classification,
        tags: resource.tags
      },
      action,
      context: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        environment: process.env.NODE_ENV
      }
    };
    return this.opaClient.evaluate(input);
  }
  /**
   * Log access decision for audit
   */
  logAccessDecision(decision, durationMs) {
    const logData = {
      allowed: decision.allowed,
      reason: decision.reason,
      userId: decision.userId,
      tenantId: decision.tenantId,
      resource: decision.resource,
      action: decision.action,
      durationMs,
      stepUpRequired: decision.stepUpRequired
    };
    if (decision.allowed) {
      logger_default.debug("Access decision: ALLOWED", logData);
    } else {
      logger_default.warn("Access decision: DENIED", logData);
    }
  }
  /**
   * Get user's effective permissions for a tenant
   */
  getEffectivePermissions(user, tenantId) {
    const effectiveTenantId = tenantId || user.tenantId;
    const permissions = /* @__PURE__ */ new Set();
    for (const globalRole of user.globalRoles) {
      const roleDef = this.roleDefinitions.get(globalRole);
      if (roleDef) {
        roleDef.permissions.forEach((p) => permissions.add(p));
      }
    }
    for (const role of user.roles.filter((r) => r.tenantId === effectiveTenantId)) {
      if (role.expiresAt && role.expiresAt < /* @__PURE__ */ new Date()) {
        continue;
      }
      const roleDef = this.roleDefinitions.get(role.role);
      if (roleDef) {
        roleDef.permissions.forEach((p) => permissions.add(p));
      }
      role.permissions.forEach((p) => permissions.add(p));
    }
    return Array.from(permissions).filter((p) => !this.isEnvironmentDenied(p));
  }
  /**
   * Add custom role definition
   */
  addRole(name, permissions, inherits) {
    const allPermissions = new Set(permissions);
    if (inherits) {
      for (const inheritedRole of inherits) {
        const inherited = this.roleDefinitions.get(inheritedRole);
        if (inherited) {
          inherited.permissions.forEach((p) => allPermissions.add(p));
        }
      }
    }
    this.roleDefinitions.set(name, {
      permissions: allPermissions,
      inherits: inherits || []
    });
    logger_default.info("Custom role added", { name, permissions: permissions.length });
  }
  /**
   * Get configuration
   */
  getConfig() {
    return { ...this.config };
  }
};
var rbacInstance = null;
function getMultiTenantRBAC(config9, opaClient2) {
  if (!rbacInstance) {
    rbacInstance = new MultiTenantRBACManager(config9, opaClient2);
  }
  return rbacInstance;
}

// src/services/AuthorizationService.ts
var AuthorizationServiceImpl = class {
  rbac;
  get pool() {
    return getPostgresPool2();
  }
  /**
   * @constructor
   * @description Initializes the service by getting instances of the RBAC manager and the database pool.
   */
  constructor() {
    this.rbac = getMultiTenantRBAC();
  }
  /**
   * @inheritdoc
   */
  async can(principal, action, resource) {
    try {
      if (principal.tenantId !== resource.tenantId && principal.kind !== "system") {
        const isGlobalAdmin = principal.roles.includes("global-admin");
        if (!isGlobalAdmin) {
          logger_default2.warn(
            `Access denied: Cross-tenant access attempted by ${principal.id} (${principal.tenantId}) on ${resource.tenantId}`
          );
          return false;
        }
      }
      const permission = this.mapToPermission(action, resource.type);
      const multiTenantUser = {
        id: principal.id,
        email: principal.user?.email || "",
        name: principal.user?.username || "",
        tenantId: principal.tenantId,
        tenantIds: [principal.tenantId],
        // In a real scenario, we'd fetch all memberships
        primaryTenantId: principal.tenantId,
        roles: principal.roles.map((r) => ({
          tenantId: principal.tenantId,
          role: r,
          permissions: [],
          scope: "full",
          grantedBy: "system",
          grantedAt: /* @__PURE__ */ new Date()
        })),
        globalRoles: principal.roles,
        // Assuming roles are flattened for now
        attributes: {},
        clearanceLevel: "unclassified",
        // Default
        lastAuthenticated: /* @__PURE__ */ new Date(),
        mfaVerified: true
        // Assume true for now or pass from context
      };
      const hasPermission2 = this.rbac.hasPermission(
        // @ts-ignore - mismatch in types is expected during bridging
        multiTenantUser,
        permission
      );
      if (!hasPermission2) {
        return false;
      }
      const decision = await this.rbac.evaluateAccess(
        // @ts-ignore
        multiTenantUser,
        {
          type: resource.type,
          id: resource.id || "",
          tenantId: resource.tenantId
          // attributes: resource.attributes
        },
        action
      );
      return decision.allowed;
    } catch (error) {
      logger_default2.error("Authorization check failed", error);
      return false;
    }
  }
  /**
   * @inheritdoc
   */
  async assertCan(principal, action, resource) {
    const allowed = await this.can(principal, action, resource);
    if (!allowed) {
      throw new Error(`Permission denied: Cannot ${action} ${resource.type}`);
    }
  }
  /**
   * @private
   * @method mapToPermission
   * @description Maps a high-level action and resource type to a standard RBAC permission string (e.g., 'investigation:read').
   * @param {Action} action - The action being performed (e.g., 'view').
   * @param {string} resourceType - The type of the resource (e.g., 'investigation').
   * @returns {string} The formatted permission string.
   */
  mapToPermission(action, resourceType) {
    let verb = "read";
    switch (action) {
      case "view":
        verb = "read";
        break;
      case "create":
        verb = "create";
        break;
      case "update":
        verb = "update";
        break;
      case "delete":
        verb = "delete";
        break;
      case "execute":
        verb = "execute";
        break;
      case "administer":
        verb = "manage";
        break;
      case "manage_settings":
        verb = "manage";
        break;
    }
    return `${resourceType}:${verb}`;
  }
};

// src/services/PolicyManagementService.ts
init_data_envelope();
init_logger2();
import { Pool as Pool7 } from "pg";
import { z as z16 } from "zod";

// src/policy/ramp.ts
init_bundleStore();
import { createHash as createHash16 } from "crypto";
var DEFAULT_RAMP_CONFIG = {
  enabled: false,
  defaultAllowPercentage: 100,
  rules: []
};
function normalizeAction(action) {
  return action.trim().toUpperCase();
}
function normalizeWorkflow(workflow) {
  return workflow ? workflow.trim().toLowerCase() : void 0;
}
function normalizeRules(rules) {
  return [...rules].map((rule) => ({
    ...rule,
    action: normalizeAction(rule.action),
    workflow: normalizeWorkflow(rule.workflow)
  }));
}
function normalizeRampConfig(config9) {
  const merged = {
    ...DEFAULT_RAMP_CONFIG,
    ...config9 || {},
    rules: normalizeRules(config9?.rules || DEFAULT_RAMP_CONFIG.rules)
  };
  merged.rules.sort((a, b) => {
    const actionCompare = a.action.localeCompare(b.action);
    if (actionCompare !== 0) return actionCompare;
    const workflowCompare = (a.workflow || "").localeCompare(b.workflow || "");
    if (workflowCompare !== 0) return workflowCompare;
    return (a.id || "").localeCompare(b.id || "");
  });
  return merged;
}
function areRampConfigsEqual(a, b) {
  return JSON.stringify(normalizeRampConfig(a)) === JSON.stringify(normalizeRampConfig(b));
}
function selectRampRule(config9, input) {
  const action = normalizeAction(input.action);
  const workflow = normalizeWorkflow(input.workflow);
  const candidates2 = config9.rules.filter((rule) => {
    if (normalizeAction(rule.action) !== action) return false;
    if (rule.workflow && workflow) {
      return normalizeWorkflow(rule.workflow) === workflow;
    }
    if (rule.workflow && !workflow) return false;
    return true;
  });
  if (!candidates2.length) return void 0;
  return candidates2.sort((a, b) => {
    const aSpecific = a.workflow ? 1 : 0;
    const bSpecific = b.workflow ? 1 : 0;
    return bSpecific - aSpecific;
  })[0];
}
function computeRampBucket(params) {
  const action = normalizeAction(params.action);
  const workflow = normalizeWorkflow(params.workflow) || "";
  const salt = params.salt || "";
  const payload = `${params.tenantId}|${action}|${workflow}|${params.key}|${salt}`;
  const digest = createHash16("sha256").update(payload).digest("hex");
  const slice = parseInt(digest.slice(0, 8), 16);
  return slice % 100;
}
function evaluateRampDecision(bundle, input) {
  const config9 = normalizeRampConfig(bundle.baseProfile?.ramp);
  if (!config9.enabled) {
    return {
      allow: true,
      percentage: 100,
      bucket: 0,
      reason: "ramp_disabled"
    };
  }
  const rule = selectRampRule(config9, input);
  const percentage = rule?.allowPercentage ?? config9.defaultAllowPercentage;
  const bucket = computeRampBucket({
    tenantId: input.tenantId,
    action: input.action,
    workflow: input.workflow,
    key: input.key,
    salt: config9.salt
  });
  const allow = bucket < percentage;
  return {
    allow,
    percentage,
    bucket,
    rule,
    reason: allow ? "ramp_allow" : "ramp_deny"
  };
}
function evaluateRampDecisionForTenant(input) {
  const version = policyBundleStore.resolveForTenant(
    input.tenantId,
    input.policyVersionId
  );
  if (!version) {
    return {
      allow: true,
      percentage: 100,
      bucket: 0,
      reason: "ramp_bundle_missing"
    };
  }
  return evaluateRampDecision(version.bundle, input);
}
var RampDecisionError = class extends Error {
  decision;
  code = "ramp_denied";
  status = 403;
  constructor(decision) {
    super(`Ramp denied (${decision.reason})`);
    this.name = "RampDecisionError";
    this.decision = decision;
  }
};
function enforceRampDecisionForTenant(input) {
  const decision = evaluateRampDecisionForTenant(input);
  if (!decision.allow) {
    throw new RampDecisionError(decision);
  }
  return decision;
}

// src/services/PolicyManagementService.ts
var policyRuleSchema2 = z16.object({
  field: z16.string().min(1),
  operator: z16.enum(["eq", "neq", "lt", "gt", "in", "not_in", "contains"]),
  value: z16.unknown()
});
var policyScopeSchema = z16.object({
  stages: z16.array(z16.enum(["data", "train", "alignment", "runtime"])).min(1),
  tenants: z16.array(z16.string()).min(1)
});
var createPolicySchema = z16.object({
  name: z16.string().min(1).max(100).regex(/^[a-z0-9-]+$/),
  displayName: z16.string().min(1).max(200),
  description: z16.string().optional(),
  category: z16.enum(["access", "data", "export", "retention", "compliance", "operational", "safety"]),
  priority: z16.number().int().min(0).max(1e3).default(100),
  scope: policyScopeSchema,
  rules: z16.array(policyRuleSchema2).min(1),
  action: z16.enum(["ALLOW", "DENY", "ESCALATE", "WARN"]),
  effectiveFrom: z16.string().datetime().optional(),
  effectiveUntil: z16.string().datetime().optional(),
  metadata: z16.record(z16.unknown()).optional()
});
var updatePolicySchema = z16.object({
  displayName: z16.string().min(1).max(200).optional(),
  description: z16.string().optional(),
  category: z16.enum(["access", "data", "export", "retention", "compliance", "operational", "safety"]).optional(),
  priority: z16.number().int().min(0).max(1e3).optional(),
  scope: policyScopeSchema.optional(),
  rules: z16.array(policyRuleSchema2).min(1).optional(),
  action: z16.enum(["ALLOW", "DENY", "ESCALATE", "WARN"]).optional(),
  effectiveFrom: z16.string().datetime().optional(),
  effectiveUntil: z16.string().datetime().optional(),
  metadata: z16.record(z16.unknown()).optional()
});
var PolicyManagementService = class {
  pool;
  constructor(pool4) {
    this.pool = pool4 || new Pool7({ connectionString: process.env.DATABASE_URL });
  }
  // --------------------------------------------------------------------------
  // Policy CRUD Operations
  // --------------------------------------------------------------------------
  /**
   * List policies with pagination and filtering
   */
  async listPolicies(tenantId, params, actorId) {
    const page = params.page || 1;
    const pageSize = Math.min(params.pageSize || 25, 100);
    const offset = (page - 1) * pageSize;
    try {
      let whereClause = "WHERE tenant_id = $1";
      const queryParams = [tenantId];
      let paramIndex = 2;
      if (params.status) {
        whereClause += ` AND status = $${paramIndex++}`;
        queryParams.push(params.status);
      }
      if (params.category) {
        whereClause += ` AND category = $${paramIndex++}`;
        queryParams.push(params.category);
      }
      if (params.search) {
        whereClause += ` AND (name ILIKE $${paramIndex} OR display_name ILIKE $${paramIndex} OR description ILIKE $${paramIndex})`;
        queryParams.push(`%${params.search}%`);
        paramIndex++;
      }
      const countResult = await this.pool.query(
        `SELECT COUNT(*) FROM managed_policies ${whereClause}`,
        queryParams
      );
      const total = parseInt(countResult.rows[0].count, 10);
      queryParams.push(pageSize, offset);
      const result2 = await this.pool.query(
        `SELECT * FROM managed_policies ${whereClause}
         ORDER BY priority DESC, created_at DESC
         LIMIT $${paramIndex++} OFFSET $${paramIndex}`,
        queryParams
      );
      const policies = result2.rows.map(this.mapPolicyRow);
      return createDataEnvelope(
        {
          policies,
          total,
          page,
          pageSize,
          totalPages: Math.ceil(total / pageSize)
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Policy list access granted",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      logger_default2.error("Error listing policies:", error);
      throw error;
    }
  }
  /**
   * Get a specific policy by ID
   */
  async getPolicy(tenantId, policyId, actorId) {
    try {
      const result2 = await this.pool.query(
        "SELECT * FROM managed_policies WHERE id = $1 AND tenant_id = $2",
        [policyId, tenantId]
      );
      const policy2 = result2.rows[0] ? this.mapPolicyRow(result2.rows[0]) : null;
      return createDataEnvelope(
        policy2,
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: policy2 ? "Policy retrieved" : "Policy not found",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      logger_default2.error("Error getting policy:", error);
      throw error;
    }
  }
  /**
   * Create a new policy (as draft)
   */
  async createPolicy(tenantId, input, actorId) {
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const policyId = `pol-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      const now = (/* @__PURE__ */ new Date()).toISOString();
      const result2 = await client6.query(
        `INSERT INTO managed_policies (
          id, name, display_name, description, category, priority,
          scope, rules, action, status, version, tenant_id,
          effective_from, effective_until, metadata,
          created_by, created_at, updated_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18)
        RETURNING *`,
        [
          policyId,
          input.name,
          input.displayName,
          input.description || null,
          input.category,
          input.priority,
          JSON.stringify(input.scope),
          JSON.stringify(input.rules),
          input.action,
          "draft",
          1,
          tenantId,
          input.effectiveFrom || null,
          input.effectiveUntil || null,
          input.metadata ? JSON.stringify(input.metadata) : null,
          actorId,
          now,
          now
        ]
      );
      await client6.query(
        `INSERT INTO policy_versions (
          id, policy_id, version, content, changelog, status, created_by, created_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
        [
          `pv-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
          policyId,
          1,
          JSON.stringify({
            id: policyId,
            description: input.description,
            scope: input.scope,
            rules: input.rules,
            action: input.action
          }),
          "Initial policy creation",
          "draft",
          actorId,
          now
        ]
      );
      await client6.query("COMMIT");
      const policy2 = this.mapPolicyRow(result2.rows[0]);
      logger_default2.info("Policy created", { policyId, name: input.name, actor: actorId });
      return createDataEnvelope(
        {
          success: true,
          message: "Policy created successfully",
          policy: policy2
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Policy creation allowed",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Error creating policy:", error);
      return createDataEnvelope(
        {
          success: false,
          message: error instanceof Error ? error.message : "Failed to create policy"
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "DENY" /* DENY */,
          policyId: "policy-management",
          reason: "Policy creation failed",
          evaluator: "PolicyManagementService"
        }
      );
    } finally {
      client6.release();
    }
  }
  /**
   * Update a policy (creates new version)
   */
  async updatePolicy(tenantId, policyId, input, changelog, actorId) {
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const current = await client6.query(
        "SELECT * FROM managed_policies WHERE id = $1 AND tenant_id = $2 FOR UPDATE",
        [policyId, tenantId]
      );
      if (current.rows.length === 0) {
        await client6.query("ROLLBACK");
        return createDataEnvelope(
          { success: false, message: "Policy not found" },
          { source: "PolicyManagementService", actor: actorId },
          {
            result: "DENY" /* DENY */,
            policyId: "policy-management",
            reason: "Policy not found",
            evaluator: "PolicyManagementService"
          }
        );
      }
      const currentPolicy = current.rows[0];
      const newVersion = currentPolicy.version + 1;
      const now = (/* @__PURE__ */ new Date()).toISOString();
      const updateFields = ["version = $1", "updated_at = $2", "status = $3"];
      const updateValues = [newVersion, now, "draft"];
      let paramIndex = 4;
      if (input.displayName !== void 0) {
        updateFields.push(`display_name = $${paramIndex++}`);
        updateValues.push(input.displayName);
      }
      if (input.description !== void 0) {
        updateFields.push(`description = $${paramIndex++}`);
        updateValues.push(input.description);
      }
      if (input.category !== void 0) {
        updateFields.push(`category = $${paramIndex++}`);
        updateValues.push(input.category);
      }
      if (input.priority !== void 0) {
        updateFields.push(`priority = $${paramIndex++}`);
        updateValues.push(input.priority);
      }
      if (input.scope !== void 0) {
        updateFields.push(`scope = $${paramIndex++}`);
        updateValues.push(JSON.stringify(input.scope));
      }
      if (input.rules !== void 0) {
        updateFields.push(`rules = $${paramIndex++}`);
        updateValues.push(JSON.stringify(input.rules));
      }
      if (input.action !== void 0) {
        updateFields.push(`action = $${paramIndex++}`);
        updateValues.push(input.action);
      }
      if (input.effectiveFrom !== void 0) {
        updateFields.push(`effective_from = $${paramIndex++}`);
        updateValues.push(input.effectiveFrom);
      }
      if (input.effectiveUntil !== void 0) {
        updateFields.push(`effective_until = $${paramIndex++}`);
        updateValues.push(input.effectiveUntil);
      }
      if (input.metadata !== void 0) {
        updateFields.push(`metadata = $${paramIndex++}`);
        updateValues.push(JSON.stringify(input.metadata));
      }
      updateValues.push(policyId, tenantId);
      const result2 = await client6.query(
        `UPDATE managed_policies SET ${updateFields.join(", ")}
         WHERE id = $${paramIndex++} AND tenant_id = $${paramIndex}
         RETURNING *`,
        updateValues
      );
      const updatedPolicy = result2.rows[0];
      await client6.query(
        `INSERT INTO policy_versions (
          id, policy_id, version, content, changelog, status, created_by, created_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
        [
          `pv-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
          policyId,
          newVersion,
          JSON.stringify({
            id: policyId,
            description: updatedPolicy.description,
            scope: JSON.parse(updatedPolicy.scope),
            rules: JSON.parse(updatedPolicy.rules),
            action: updatedPolicy.action
          }),
          changelog || "Policy updated",
          "draft",
          actorId,
          now
        ]
      );
      await client6.query("COMMIT");
      const policy2 = this.mapPolicyRow(result2.rows[0]);
      logger_default2.info("Policy updated", { policyId, version: newVersion, actor: actorId });
      return createDataEnvelope(
        {
          success: true,
          message: `Policy updated to version ${newVersion}`,
          policy: policy2
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Policy update allowed",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Error updating policy:", error);
      return createDataEnvelope(
        {
          success: false,
          message: error instanceof Error ? error.message : "Failed to update policy"
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "DENY" /* DENY */,
          policyId: "policy-management",
          reason: "Policy update failed",
          evaluator: "PolicyManagementService"
        }
      );
    } finally {
      client6.release();
    }
  }
  /**
   * Delete a policy (archive it)
   */
  async deletePolicy(tenantId, policyId, actorId) {
    try {
      const result2 = await this.pool.query(
        `UPDATE managed_policies
         SET status = 'archived', updated_at = $1
         WHERE id = $2 AND tenant_id = $3 AND status != 'active'
         RETURNING *`,
        [(/* @__PURE__ */ new Date()).toISOString(), policyId, tenantId]
      );
      if (result2.rows.length === 0) {
        return createDataEnvelope(
          { success: false, message: "Policy not found or cannot be archived while active" },
          { source: "PolicyManagementService", actor: actorId },
          {
            result: "DENY" /* DENY */,
            policyId: "policy-management",
            reason: "Policy deletion failed",
            evaluator: "PolicyManagementService"
          }
        );
      }
      logger_default2.info("Policy archived", { policyId, actor: actorId });
      return createDataEnvelope(
        {
          success: true,
          message: "Policy archived successfully"
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Policy archived",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      logger_default2.error("Error deleting policy:", error);
      throw error;
    }
  }
  // --------------------------------------------------------------------------
  // Version Management
  // --------------------------------------------------------------------------
  /**
   * List versions of a policy
   */
  async listPolicyVersions(tenantId, policyId, actorId) {
    try {
      const policyCheck = await this.pool.query(
        "SELECT id FROM managed_policies WHERE id = $1 AND tenant_id = $2",
        [policyId, tenantId]
      );
      if (policyCheck.rows.length === 0) {
        return createDataEnvelope(
          { versions: [], total: 0 },
          { source: "PolicyManagementService", actor: actorId },
          {
            result: "DENY" /* DENY */,
            policyId: "policy-management",
            reason: "Policy not found",
            evaluator: "PolicyManagementService"
          }
        );
      }
      const result2 = await this.pool.query(
        "SELECT * FROM policy_versions WHERE policy_id = $1 ORDER BY version DESC",
        [policyId]
      );
      const versions = result2.rows.map((row) => ({
        id: row.id,
        policyId: row.policy_id,
        version: row.version,
        content: JSON.parse(row.content),
        changelog: row.changelog,
        createdBy: row.created_by,
        createdAt: row.created_at,
        status: row.status,
        approvedBy: row.approved_by,
        approvedAt: row.approved_at
      }));
      return createDataEnvelope(
        { versions, total: versions.length },
        { source: "PolicyManagementService", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Versions retrieved",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      logger_default2.error("Error listing policy versions:", error);
      throw error;
    }
  }
  /**
   * Rollback to a previous version
   */
  async rollbackPolicy(tenantId, policyId, targetVersion, actorId) {
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const versionResult = await client6.query(
        `SELECT pv.* FROM policy_versions pv
         JOIN managed_policies mp ON pv.policy_id = mp.id
         WHERE pv.policy_id = $1 AND pv.version = $2 AND mp.tenant_id = $3`,
        [policyId, targetVersion, tenantId]
      );
      if (versionResult.rows.length === 0) {
        await client6.query("ROLLBACK");
        return createDataEnvelope(
          { success: false, message: "Target version not found" },
          { source: "PolicyManagementService", actor: actorId },
          {
            result: "DENY" /* DENY */,
            policyId: "policy-management",
            reason: "Version not found",
            evaluator: "PolicyManagementService"
          }
        );
      }
      const targetVersionData = versionResult.rows[0];
      const content = JSON.parse(targetVersionData.content);
      const currentResult = await client6.query(
        "SELECT version FROM managed_policies WHERE id = $1 FOR UPDATE",
        [policyId]
      );
      const newVersion = currentResult.rows[0].version + 1;
      const now = (/* @__PURE__ */ new Date()).toISOString();
      await client6.query(
        `UPDATE managed_policies SET
          scope = $1, rules = $2, action = $3, description = $4,
          version = $5, status = 'draft', updated_at = $6
         WHERE id = $7`,
        [
          JSON.stringify(content.scope),
          JSON.stringify(content.rules),
          content.action,
          content.description,
          newVersion,
          now,
          policyId
        ]
      );
      await client6.query(
        `INSERT INTO policy_versions (
          id, policy_id, version, content, changelog, status, created_by, created_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
        [
          `pv-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
          policyId,
          newVersion,
          targetVersionData.content,
          `Rollback to version ${targetVersion}`,
          "draft",
          actorId,
          now
        ]
      );
      await client6.query("COMMIT");
      logger_default2.info("Policy rolled back", { policyId, targetVersion, newVersion, actor: actorId });
      return createDataEnvelope(
        {
          success: true,
          message: `Policy rolled back to version ${targetVersion} as new version ${newVersion}`
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Policy rollback successful",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Error rolling back policy:", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  // --------------------------------------------------------------------------
  // Approval Workflow
  // --------------------------------------------------------------------------
  /**
   * Submit policy for approval
   */
  async submitForApproval(tenantId, policyId, reason, actorId) {
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const result2 = await client6.query(
        `UPDATE managed_policies
         SET status = 'pending_approval', updated_at = $1
         WHERE id = $2 AND tenant_id = $3 AND status = 'draft'
         RETURNING *`,
        [(/* @__PURE__ */ new Date()).toISOString(), policyId, tenantId]
      );
      if (result2.rows.length === 0) {
        await client6.query("ROLLBACK");
        return createDataEnvelope(
          { success: false, message: "Policy not found or not in draft status" },
          { source: "PolicyManagementService", actor: actorId },
          {
            result: "DENY" /* DENY */,
            policyId: "policy-management",
            reason: "Cannot submit for approval",
            evaluator: "PolicyManagementService"
          }
        );
      }
      await client6.query(
        `INSERT INTO policy_approval_requests (
          id, policy_id, policy_name, version, requested_by, requested_at, reason, changelog, status
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)`,
        [
          `par-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
          policyId,
          result2.rows[0].name,
          result2.rows[0].version,
          actorId,
          (/* @__PURE__ */ new Date()).toISOString(),
          reason,
          "Submitted for approval",
          "pending"
        ]
      );
      await client6.query("COMMIT");
      const policy2 = this.mapPolicyRow(result2.rows[0]);
      logger_default2.info("Policy submitted for approval", { policyId, actor: actorId });
      return createDataEnvelope(
        {
          success: true,
          message: "Policy submitted for approval",
          policy: policy2
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Approval request created",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Error submitting for approval:", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  /**
   * Approve a policy
   */
  async approvePolicy(tenantId, policyId, notes, actorId) {
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const now = (/* @__PURE__ */ new Date()).toISOString();
      enforceRampDecisionForTenant({
        tenantId,
        action: "APPROVE",
        workflow: "policy_management",
        key: policyId
      });
      const result2 = await client6.query(
        `UPDATE managed_policies
         SET status = 'approved', approved_by = $1, approved_at = $2, updated_at = $2
         WHERE id = $3 AND tenant_id = $4 AND status = 'pending_approval'
         RETURNING *`,
        [actorId, now, policyId, tenantId]
      );
      if (result2.rows.length === 0) {
        await client6.query("ROLLBACK");
        return createDataEnvelope(
          { success: false, message: "Policy not found or not pending approval" },
          { source: "PolicyManagementService", actor: actorId },
          {
            result: "DENY" /* DENY */,
            policyId: "policy-management",
            reason: "Cannot approve policy",
            evaluator: "PolicyManagementService"
          }
        );
      }
      await client6.query(
        `UPDATE policy_approval_requests
         SET status = 'approved', reviewed_by = $1, reviewed_at = $2, review_notes = $3
         WHERE policy_id = $4 AND status = 'pending'`,
        [actorId, now, notes, policyId]
      );
      await client6.query(
        `UPDATE policy_versions
         SET status = 'approved', approved_by = $1, approved_at = $2
         WHERE policy_id = $3 AND version = $4`,
        [actorId, now, policyId, result2.rows[0].version]
      );
      await client6.query("COMMIT");
      const policy2 = this.mapPolicyRow(result2.rows[0]);
      logger_default2.info("Policy approved", { policyId, actor: actorId });
      return createDataEnvelope(
        {
          success: true,
          message: "Policy approved successfully",
          policy: policy2
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Policy approved",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Error approving policy:", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  /**
   * Publish an approved policy (make it active)
   */
  async publishPolicy(tenantId, policyId, actorId) {
    try {
      const now = (/* @__PURE__ */ new Date()).toISOString();
      const result2 = await this.pool.query(
        `UPDATE managed_policies
         SET status = 'active', published_at = $1, updated_at = $1
         WHERE id = $2 AND tenant_id = $3 AND status = 'approved'
         RETURNING *`,
        [now, policyId, tenantId]
      );
      if (result2.rows.length === 0) {
        return createDataEnvelope(
          { success: false, message: "Policy not found or not approved" },
          { source: "PolicyManagementService", actor: actorId },
          {
            result: "DENY" /* DENY */,
            policyId: "policy-management",
            reason: "Cannot publish policy",
            evaluator: "PolicyManagementService"
          }
        );
      }
      const policy2 = this.mapPolicyRow(result2.rows[0]);
      logger_default2.info("Policy published", { policyId, actor: actorId });
      return createDataEnvelope(
        {
          success: true,
          message: "Policy published and now active",
          policy: policy2
        },
        {
          source: "PolicyManagementService",
          actor: actorId
        },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-management",
          reason: "Policy published",
          evaluator: "PolicyManagementService"
        }
      );
    } catch (error) {
      logger_default2.error("Error publishing policy:", error);
      throw error;
    }
  }
  // --------------------------------------------------------------------------
  // Helper Methods
  // --------------------------------------------------------------------------
  mapPolicyRow(row) {
    return {
      id: row.id,
      name: row.name,
      displayName: row.display_name,
      description: row.description,
      category: row.category,
      priority: row.priority,
      scope: typeof row.scope === "string" ? JSON.parse(row.scope) : row.scope,
      rules: typeof row.rules === "string" ? JSON.parse(row.rules) : row.rules,
      action: row.action,
      status: row.status,
      version: row.version,
      tenantId: row.tenant_id,
      effectiveFrom: row.effective_from,
      effectiveUntil: row.effective_until,
      createdBy: row.created_by,
      createdAt: row.created_at,
      updatedAt: row.updated_at,
      approvedBy: row.approved_by,
      approvedAt: row.approved_at,
      publishedAt: row.published_at,
      metadata: row.metadata ? typeof row.metadata === "string" ? JSON.parse(row.metadata) : row.metadata : void 0
    };
  }
};
var policyManagementService = new PolicyManagementService();

// src/services/PolicySimulatorService.ts
init_data_envelope();
init_logger2();
import { z as z17 } from "zod";
var policyRuleSchema3 = z17.object({
  field: z17.string(),
  operator: z17.enum(["eq", "neq", "lt", "gt", "in", "not_in", "contains"]),
  value: z17.unknown()
});
var simulationRequestSchema = z17.object({
  policy: z17.object({
    id: z17.string(),
    description: z17.string().optional(),
    scope: z17.object({
      stages: z17.array(z17.enum(["data", "train", "alignment", "runtime"])),
      tenants: z17.array(z17.string())
    }),
    rules: z17.array(policyRuleSchema3),
    action: z17.enum(["ALLOW", "DENY", "ESCALATE", "WARN"])
  }),
  context: z17.object({
    stage: z17.enum(["data", "train", "alignment", "runtime"]),
    tenantId: z17.string(),
    region: z17.string().optional(),
    payload: z17.record(z17.unknown()),
    metadata: z17.record(z17.unknown()).optional(),
    simulation: z17.boolean().optional()
  }),
  compareWith: z17.object({
    id: z17.string(),
    description: z17.string().optional(),
    scope: z17.object({
      stages: z17.array(z17.enum(["data", "train", "alignment", "runtime"])),
      tenants: z17.array(z17.string())
    }),
    rules: z17.array(policyRuleSchema3),
    action: z17.enum(["ALLOW", "DENY", "ESCALATE", "WARN"])
  }).optional()
});
var PolicySimulatorService = class {
  /**
   * Simulate a policy against a context
   */
  async simulate(request, actorId) {
    const startTime = Date.now();
    try {
      const evaluationPath = [];
      const matchedRules = [];
      const unmatchedRules = [];
      evaluationPath.push({
        step: 1,
        description: "Checking policy scope",
        result: "passed",
        details: {
          policyStages: request.policy.scope.stages,
          contextStage: request.context.stage,
          policyTenants: request.policy.scope.tenants,
          contextTenant: request.context.tenantId
        }
      });
      const stageMatches = request.policy.scope.stages.includes(request.context.stage);
      const tenantMatches = request.policy.scope.tenants.includes("*") || request.policy.scope.tenants.includes(request.context.tenantId);
      if (!stageMatches || !tenantMatches) {
        evaluationPath.push({
          step: 2,
          description: "Scope check failed - policy does not apply",
          result: "failed",
          details: { stageMatches, tenantMatches }
        });
        const verdict2 = this.createVerdict("ALLOW", [], startTime, true);
        return createDataEnvelope(
          {
            verdict: verdict2,
            matchedRules: [],
            unmatchedRules: request.policy.rules,
            evaluationPath
          },
          { source: "PolicySimulatorService", actor: actorId },
          {
            result: "ALLOW" /* ALLOW */,
            policyId: "policy-simulation",
            reason: "Policy simulation completed",
            evaluator: "PolicySimulatorService"
          }
        );
      }
      evaluationPath.push({
        step: 2,
        description: "Scope check passed",
        result: "passed"
      });
      evaluationPath.push({
        step: 3,
        description: "Evaluating policy rules",
        result: "passed",
        details: { ruleCount: request.policy.rules.length }
      });
      let allRulesMatch = true;
      const failedReasons = [];
      for (const rule of request.policy.rules) {
        const actualValue = this.getNestedValue(request.context.payload, rule.field);
        const matched = this.evaluateRule(rule, actualValue);
        matchedRules.push({
          rule,
          actualValue,
          matched,
          reason: matched ? `Field ${rule.field} ${rule.operator} ${JSON.stringify(rule.value)}` : `Field ${rule.field} failed: expected ${rule.operator} ${JSON.stringify(rule.value)}, got ${JSON.stringify(actualValue)}`
        });
        if (!matched) {
          allRulesMatch = false;
          unmatchedRules.push(rule);
          failedReasons.push(`Rule failed: ${rule.field} ${rule.operator} ${JSON.stringify(rule.value)}`);
        }
      }
      const finalAction = allRulesMatch ? request.policy.action : "ALLOW";
      evaluationPath.push({
        step: 4,
        description: allRulesMatch ? `All rules matched - applying action: ${request.policy.action}` : `Some rules failed - defaulting to ALLOW`,
        result: allRulesMatch ? "passed" : "failed",
        details: {
          matchedCount: matchedRules.filter((r) => r.matched).length,
          totalRules: request.policy.rules.length
        }
      });
      const verdict = this.createVerdict(
        finalAction,
        allRulesMatch ? [`Policy ${request.policy.id} matched`] : failedReasons,
        startTime,
        true
      );
      let comparisonDiff;
      if (request.compareWith) {
        comparisonDiff = this.calculatePolicyDiff(request.compareWith, request.policy);
      }
      return createDataEnvelope(
        {
          verdict,
          matchedRules,
          unmatchedRules,
          evaluationPath,
          comparisonDiff
        },
        { source: "PolicySimulatorService", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-simulation",
          reason: "Policy simulation completed",
          evaluator: "PolicySimulatorService"
        }
      );
    } catch (error) {
      logger_default2.error("Error simulating policy:", error);
      throw error;
    }
  }
  /**
   * Simulate a policy against multiple contexts (batch)
   */
  async batchSimulate(request, actorId) {
    try {
      const results = [];
      let allowCount = 0;
      let denyCount = 0;
      let escalateCount = 0;
      let warnCount = 0;
      for (const context4 of request.contexts) {
        const simResult = await this.simulate(
          { policy: request.policy, context: context4 },
          actorId
        );
        results.push(simResult.data);
        switch (simResult.data.verdict.action) {
          case "ALLOW":
            allowCount++;
            break;
          case "DENY":
            denyCount++;
            break;
          case "ESCALATE":
            escalateCount++;
            break;
          case "WARN":
            warnCount++;
            break;
        }
      }
      return createDataEnvelope(
        {
          totalContexts: request.contexts.length,
          allowCount,
          denyCount,
          escalateCount,
          warnCount,
          results
        },
        { source: "PolicySimulatorService", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-simulation",
          reason: "Batch simulation completed",
          evaluator: "PolicySimulatorService"
        }
      );
    } catch (error) {
      logger_default2.error("Error in batch simulation:", error);
      throw error;
    }
  }
  /**
   * Analyze the impact of a policy change
   */
  async analyzeImpact(currentPolicy, newPolicy, actorId) {
    try {
      const diff = this.calculatePolicyDiff(currentPolicy, newPolicy);
      const warnings = [];
      const recommendations = [];
      let riskScore = 0;
      if (diff.actionChanged) {
        if (diff.afterAction === "DENY" && diff.beforeAction === "ALLOW") {
          riskScore += 30;
          warnings.push("Policy changing from ALLOW to DENY - may block previously allowed operations");
        }
        if (diff.afterAction === "ALLOW" && diff.beforeAction === "DENY") {
          riskScore += 20;
          warnings.push("Policy changing from DENY to ALLOW - may permit previously blocked operations");
        }
      }
      if (diff.removedRules.length > 0) {
        riskScore += diff.removedRules.length * 10;
        warnings.push(`${diff.removedRules.length} rules removed - conditions will no longer be checked`);
      }
      if (diff.addedRules.length > 0) {
        riskScore += diff.addedRules.length * 5;
        warnings.push(`${diff.addedRules.length} new rules added - may affect matching behavior`);
      }
      if (diff.scopeChanges.tenantsRemoved.length > 0) {
        riskScore += 15;
        warnings.push(`Policy scope reduced: tenants ${diff.scopeChanges.tenantsRemoved.join(", ")} removed`);
      }
      if (diff.scopeChanges.stagesRemoved.length > 0) {
        riskScore += 10;
        warnings.push(`Policy scope reduced: stages ${diff.scopeChanges.stagesRemoved.join(", ")} removed`);
      }
      let riskLevel;
      if (riskScore >= 50) {
        riskLevel = "critical";
        recommendations.push("Consider phased rollout with monitoring");
        recommendations.push("Enable detailed audit logging during deployment");
      } else if (riskScore >= 30) {
        riskLevel = "high";
        recommendations.push("Test thoroughly in staging environment");
        recommendations.push("Prepare rollback plan");
      } else if (riskScore >= 15) {
        riskLevel = "medium";
        recommendations.push("Review changes with security team");
      } else {
        riskLevel = "low";
        recommendations.push("Standard review and approval process recommended");
      }
      const estimatedAffectedUsers = Math.floor(Math.random() * 1e3) + 100;
      const estimatedAffectedResources = Math.floor(Math.random() * 5e3) + 500;
      return createDataEnvelope(
        {
          estimatedAffectedUsers,
          estimatedAffectedResources,
          riskLevel,
          warnings,
          recommendations
        },
        { source: "PolicySimulatorService", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "policy-simulation",
          reason: "Impact analysis completed",
          evaluator: "PolicySimulatorService"
        }
      );
    } catch (error) {
      logger_default2.error("Error analyzing impact:", error);
      throw error;
    }
  }
  // --------------------------------------------------------------------------
  // Helper Methods
  // --------------------------------------------------------------------------
  evaluateRule(rule, actualValue) {
    switch (rule.operator) {
      case "eq":
        return actualValue === rule.value;
      case "neq":
        return actualValue !== rule.value;
      case "lt":
        return typeof actualValue === "number" && typeof rule.value === "number" ? actualValue < rule.value : false;
      case "gt":
        return typeof actualValue === "number" && typeof rule.value === "number" ? actualValue > rule.value : false;
      case "in":
        return Array.isArray(rule.value) && rule.value.includes(actualValue);
      case "not_in":
        return Array.isArray(rule.value) && !rule.value.includes(actualValue);
      case "contains":
        if (typeof actualValue === "string" && typeof rule.value === "string") {
          return actualValue.includes(rule.value);
        }
        if (Array.isArray(actualValue)) {
          return actualValue.includes(rule.value);
        }
        return false;
      default:
        return false;
    }
  }
  getNestedValue(obj, path55) {
    const parts = path55.split(".");
    let current = obj;
    for (const part of parts) {
      if (current === null || current === void 0) {
        return void 0;
      }
      if (typeof current === "object") {
        current = current[part];
      } else {
        return void 0;
      }
    }
    return current;
  }
  createVerdict(action, reasons, startTime, isSimulation) {
    return {
      action,
      reasons,
      policyIds: [],
      metadata: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        evaluator: "PolicySimulatorService",
        latencyMs: Date.now() - startTime,
        simulation: isSimulation
      },
      provenance: {
        origin: "policy-simulation",
        confidence: 1
      }
    };
  }
  calculatePolicyDiff(before, after) {
    const beforeRuleKeys = new Set(before.rules.map((r) => `${r.field}:${r.operator}`));
    const afterRuleKeys = new Set(after.rules.map((r) => `${r.field}:${r.operator}`));
    const addedRules = after.rules.filter(
      (r) => !beforeRuleKeys.has(`${r.field}:${r.operator}`)
    );
    const removedRules = before.rules.filter(
      (r) => !afterRuleKeys.has(`${r.field}:${r.operator}`)
    );
    const modifiedRules = [];
    for (const afterRule of after.rules) {
      const key = `${afterRule.field}:${afterRule.operator}`;
      if (beforeRuleKeys.has(key)) {
        const beforeRule = before.rules.find(
          (r) => `${r.field}:${r.operator}` === key
        );
        if (beforeRule && JSON.stringify(beforeRule.value) !== JSON.stringify(afterRule.value)) {
          modifiedRules.push({ before: beforeRule, after: afterRule });
        }
      }
    }
    return {
      addedRules,
      removedRules,
      modifiedRules,
      scopeChanges: {
        stagesAdded: after.scope.stages.filter((s) => !before.scope.stages.includes(s)),
        stagesRemoved: before.scope.stages.filter((s) => !after.scope.stages.includes(s)),
        tenantsAdded: after.scope.tenants.filter((t) => !before.scope.tenants.includes(t)),
        tenantsRemoved: before.scope.tenants.filter((t) => !after.scope.tenants.includes(t))
      },
      actionChanged: before.action !== after.action,
      beforeAction: before.action,
      afterAction: after.action
    };
  }
};
var policySimulatorService = new PolicySimulatorService();

// src/routes/policies/policy-management.ts
init_logger2();
var router14 = express12.Router();
var authz = new AuthorizationServiceImpl();
var policyService = new PolicyManagementService();
var simulatorService = new PolicySimulatorService();
var singleParam3 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var buildPrincipal = (req, res, next) => {
  const user = req.user;
  if (!user) {
    res.status(401).json({ error: "Unauthorized", code: "AUTH_REQUIRED" });
    return;
  }
  const principal = {
    kind: "user",
    id: user.id,
    tenantId: req.headers["x-tenant-id"] || user.tenantId || "default-tenant",
    roles: [user.role],
    scopes: [],
    user: {
      email: user.email,
      username: user.username
    }
  };
  req.principal = principal;
  next();
};
var requirePolicyPermission = (action) => {
  return async (req, res, next) => {
    try {
      const principal = req.principal;
      await authz.assertCan(principal, action, { type: "policy", tenantId: principal.tenantId });
      next();
    } catch (error) {
      if (error.message.includes("Permission denied")) {
        res.status(403).json({
          error: "Forbidden",
          code: "PERMISSION_DENIED",
          required: `policy:${action}`
        });
        return;
      }
      logger_default2.error("Authorization error:", error);
      res.status(500).json({ error: "Authorization service error" });
    }
  };
};
router14.get(
  "/",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const page = singleParam3(req.query.page);
      const pageSize = singleParam3(req.query.pageSize);
      const status = singleParam3(req.query.status);
      const category = singleParam3(req.query.category);
      const search = singleParam3(req.query.search);
      const envelope = await policyService.listPolicies(
        principal.tenantId,
        {
          page: page ? parseInt(page, 10) : void 0,
          pageSize: pageSize ? parseInt(pageSize, 10) : void 0,
          status,
          category,
          search
        },
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing policies:", error);
      res.status(500).json({ error: "Failed to list policies", message: error.message });
    }
  }
);
router14.get(
  "/:id",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam3(req.params.id) ?? "";
      const envelope = await policyService.getPolicy(principal.tenantId, id, principal.id);
      if (!envelope.data) {
        res.status(404).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting policy:", error);
      res.status(500).json({ error: "Failed to get policy", message: error.message });
    }
  }
);
router14.post(
  "/",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("create"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const parseResult = createPolicySchema.safeParse(req.body);
      if (!parseResult.success) {
        res.status(400).json({
          error: "Validation failed",
          details: parseResult.error.errors
        });
        return;
      }
      const envelope = await policyService.createPolicy(
        principal.tenantId,
        parseResult.data,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.status(201).json(envelope);
    } catch (error) {
      logger_default2.error("Error creating policy:", error);
      res.status(500).json({ error: "Failed to create policy", message: error.message });
    }
  }
);
router14.patch(
  "/:id",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("update"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam3(req.params.id) ?? "";
      const { changelog, ...updates } = req.body;
      const parseResult = updatePolicySchema.safeParse(updates);
      if (!parseResult.success) {
        res.status(400).json({
          error: "Validation failed",
          details: parseResult.error.errors
        });
        return;
      }
      const envelope = await policyService.updatePolicy(
        principal.tenantId,
        id,
        parseResult.data,
        changelog || "Policy updated",
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error updating policy:", error);
      res.status(500).json({ error: "Failed to update policy", message: error.message });
    }
  }
);
router14.delete(
  "/:id",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("delete"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam3(req.params.id) ?? "";
      const envelope = await policyService.deletePolicy(principal.tenantId, id, principal.id);
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error deleting policy:", error);
      res.status(500).json({ error: "Failed to delete policy", message: error.message });
    }
  }
);
router14.get(
  "/:id/versions",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam3(req.params.id) ?? "";
      const envelope = await policyService.listPolicyVersions(
        principal.tenantId,
        id,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing policy versions:", error);
      res.status(500).json({ error: "Failed to list versions", message: error.message });
    }
  }
);
router14.post(
  "/:id/rollback",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("update"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam3(req.params.id) ?? "";
      const { targetVersion } = req.body;
      if (!targetVersion || typeof targetVersion !== "number") {
        res.status(400).json({ error: "targetVersion is required and must be a number" });
        return;
      }
      const envelope = await policyService.rollbackPolicy(
        principal.tenantId,
        id,
        targetVersion,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error rolling back policy:", error);
      res.status(500).json({ error: "Failed to rollback policy", message: error.message });
    }
  }
);
router14.post(
  "/:id/submit",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("submit"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam3(req.params.id) ?? "";
      const { reason } = req.body;
      const envelope = await policyService.submitForApproval(
        principal.tenantId,
        id,
        reason || "Submitted for review",
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error submitting policy:", error);
      res.status(500).json({ error: "Failed to submit policy", message: error.message });
    }
  }
);
router14.post(
  "/:id/approve",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("approve"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam3(req.params.id) ?? "";
      const { notes } = req.body;
      const envelope = await policyService.approvePolicy(
        principal.tenantId,
        id,
        notes || "",
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error approving policy:", error);
      res.status(500).json({ error: "Failed to approve policy", message: error.message });
    }
  }
);
router14.post(
  "/:id/publish",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("publish"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam3(req.params.id) ?? "";
      const envelope = await policyService.publishPolicy(
        principal.tenantId,
        id,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error publishing policy:", error);
      res.status(500).json({ error: "Failed to publish policy", message: error.message });
    }
  }
);
router14.post(
  "/simulate",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("simulate"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const parseResult = simulationRequestSchema.safeParse(req.body);
      if (!parseResult.success) {
        res.status(400).json({
          error: "Validation failed",
          details: parseResult.error.errors
        });
        return;
      }
      const envelope = await simulatorService.simulate(parseResult.data, principal.id);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error simulating policy:", error);
      res.status(500).json({ error: "Failed to simulate policy", message: error.message });
    }
  }
);
router14.post(
  "/simulate/batch",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("simulate"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const { policy: policy2, contexts } = req.body;
      if (!policy2 || !contexts || !Array.isArray(contexts)) {
        res.status(400).json({
          error: "policy and contexts array are required"
        });
        return;
      }
      const envelope = await simulatorService.batchSimulate(
        { policy: policy2, contexts },
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error in batch simulation:", error);
      res.status(500).json({ error: "Failed to batch simulate", message: error.message });
    }
  }
);
router14.post(
  "/analyze-impact",
  ensureAuthenticated,
  buildPrincipal,
  requirePolicyPermission("simulate"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const { currentPolicy, newPolicy } = req.body;
      if (!currentPolicy || !newPolicy) {
        res.status(400).json({
          error: "currentPolicy and newPolicy are required"
        });
        return;
      }
      const envelope = await simulatorService.analyzeImpact(
        currentPolicy,
        newPolicy,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error analyzing impact:", error);
      res.status(500).json({ error: "Failed to analyze impact", message: error.message });
    }
  }
);
var policy_management_default = router14;

// src/http/metricsRoute.ts
init_metrics4();
init_logger();
var metricsRoute = async (_req, res) => {
  try {
    res.set("Content-Type", register4.contentType);
    const metrics8 = await register4.metrics();
    res.send(metrics8);
  } catch (err) {
    logger.error({ err }, "Error generating metrics");
    res.status(500).send("Error generating metrics");
  }
};

// src/routes/monitoring-backpressure.ts
import { Router as Router3 } from "express";
var router15 = Router3();
router15.get("/metrics/backpressure", (req, res) => {
  const controller = BackpressureController.getInstance();
  const metrics8 = controller.getMetrics();
  res.json(metrics8);
});
var monitoring_backpressure_default = router15;

// src/routes/rbacRoutes.ts
import express13 from "express";
var router16 = express13.Router();
var rbacRoutes_default = router16;

// src/app.ts
init_auth();
init_neo4j();
init_tracer();

// src/workers/trustScore.ts
init_trustRiskRepo();
init_trust_risk_metrics();
function computeTrustScore(base, signals) {
  const now = Date.now();
  const weekMs = 7 * 24 * 3600 * 1e3;
  const weight = (sev) => ({ LOW: 0.01, MEDIUM: 0.03, HIGH: 0.08, CRITICAL: 0.15 })[sev] || 0.02;
  let score = base;
  for (const s of signals) {
    const age = now - new Date(s.created_at).getTime();
    if (age <= weekMs) score -= weight(String(s.severity).toUpperCase());
  }
  return Math.min(1, Math.max(0, parseFloat(score.toFixed(4))));
}
async function recomputeTrustForTenant(tenantId, subjectId) {
  const recents = await listRecentSignals(tenantId, subjectId, 100);
  const score = computeTrustScore(0.7, recents);
  await upsertTrustScore(tenantId, subjectId, score, ["auto_recompute"]);
  recordTrustScore(subjectId, score);
}

// src/workers/trustScoreWorker.ts
var INTERVAL_MS = Number(process.env.TRUST_WORKER_INTERVAL_MS || 6e4);
var timer;
function startTrustWorker() {
  if (process.env.ENABLE_TRUST_WORKER !== "true") return;
  const tenants = (process.env.TRUST_WORKER_TENANTS || "t0").split(",").map((s) => s.trim()).filter(Boolean);
  const subjects = (process.env.TRUST_WORKER_SUBJECTS || "global").split(",").map((s) => s.trim()).filter(Boolean);
  async function tick() {
    for (const t of tenants) {
      for (const s of subjects) {
        try {
          await recomputeTrustForTenant(t, s);
        } catch (e) {
          console.warn("trust worker error", e);
        }
      }
    }
  }
  timer = setInterval(tick, INTERVAL_MS);
  tick().catch(() => {
  });
}

// src/workers/retentionWorker.ts
init_pg();
import fs12 from "node:fs";
import path12 from "node:path";
function readRetentionDays() {
  try {
    const p = path12.resolve(
      process.cwd(),
      "contracts/policy-pack/v0/data/retention.json"
    );
    const raw = JSON.parse(fs12.readFileSync(p, "utf8"));
    const tiers = raw?.tiers || {};
    const defaults = raw?.defaults || {};
    const standardDays = tiers?.standard?.days ?? 180;
    const longDays = tiers?.long?.days ?? 365;
    return {
      riskDays: Number(process.env.RETENTION_RISK_DAYS || standardDays),
      evidenceDays: Number(process.env.RETENTION_EVIDENCE_DAYS || longDays)
    };
  } catch {
    return {
      riskDays: Number(process.env.RETENTION_RISK_DAYS || 180),
      evidenceDays: Number(process.env.RETENTION_EVIDENCE_DAYS || 365)
    };
  }
}
async function runRetentionOnce() {
  const { riskDays, evidenceDays } = readRetentionDays();
  await pg.write(
    `DELETE FROM risk_signals WHERE created_at < now() - ($1 || ' days')::interval`,
    [String(riskDays)]
  );
  await pg.write(
    `DELETE FROM evidence_bundles WHERE created_at < now() - ($1 || ' days')::interval`,
    [String(evidenceDays)]
  );
}
var timer2;
function startRetentionWorker() {
  if (process.env.ENABLE_RETENTION_WORKER !== "true") return;
  const intervalMs = Number(
    process.env.RETENTION_WORKER_INTERVAL_MS || 24 * 3600 * 1e3
  );
  const tick = () => runRetentionOnce().catch((e) => console.warn("retention error", e));
  timer2 = setInterval(tick, intervalMs);
  tick();
}

// src/app.ts
init_config();

// src/routes/support-tickets.ts
init_support_tickets();
import express14 from "express";
var router17 = express14.Router();
router17.post("/tickets", express14.json(), async (req, res) => {
  try {
    const user = req.user;
    const input = {
      title: req.body.title,
      description: req.body.description,
      priority: req.body.priority,
      category: req.body.category,
      reporter_id: user?.sub || user?.id || req.body.reporter_id || "anonymous",
      reporter_email: user?.email || req.body.reporter_email,
      tags: req.body.tags,
      metadata: req.body.metadata
    };
    if (!input.title || !input.description) {
      return res.status(400).json({ error: "title and description are required" });
    }
    const ticket = await createTicket(input);
    res.status(201).json(ticket);
  } catch (error) {
    console.error("Error creating ticket:", error);
    res.status(500).json({ error: "Failed to create ticket" });
  }
});
router17.get("/tickets", async (req, res) => {
  try {
    const options2 = {
      status: req.query.status,
      priority: req.query.priority,
      category: req.query.category,
      reporter_id: req.query.reporter_id,
      assignee_id: req.query.assignee_id,
      limit: req.query.limit ? parseInt(req.query.limit, 10) : 50,
      offset: req.query.offset ? parseInt(req.query.offset, 10) : 0
    };
    const [tickets, count] = await Promise.all([
      listTickets(options2),
      getTicketCount(options2)
    ]);
    res.json({
      data: tickets,
      meta: {
        total: count,
        limit: options2.limit,
        offset: options2.offset
      }
    });
  } catch (error) {
    console.error("Error listing tickets:", error);
    res.status(500).json({ error: "Failed to list tickets" });
  }
});
router17.get("/tickets/:id", async (req, res) => {
  try {
    const ticket = await getTicketById(req.params.id);
    if (!ticket) {
      return res.status(404).json({ error: "Ticket not found" });
    }
    res.json(ticket);
  } catch (error) {
    console.error("Error getting ticket:", error);
    res.status(500).json({ error: "Failed to get ticket" });
  }
});
router17.patch("/tickets/:id", express14.json(), async (req, res) => {
  try {
    const input = {
      title: req.body.title,
      description: req.body.description,
      status: req.body.status,
      priority: req.body.priority,
      category: req.body.category,
      assignee_id: req.body.assignee_id,
      tags: req.body.tags,
      metadata: req.body.metadata
    };
    const ticket = await updateTicket(req.params.id, input);
    if (!ticket) {
      return res.status(404).json({ error: "Ticket not found" });
    }
    res.json(ticket);
  } catch (error) {
    console.error("Error updating ticket:", error);
    res.status(500).json({ error: "Failed to update ticket" });
  }
});
router17.delete("/tickets/:id", async (req, res) => {
  try {
    const deleted = await deleteTicket(req.params.id);
    if (!deleted) {
      return res.status(404).json({ error: "Ticket not found" });
    }
    res.status(204).send();
  } catch (error) {
    console.error("Error deleting ticket:", error);
    res.status(500).json({ error: "Failed to delete ticket" });
  }
});
router17.post("/tickets/:id/comments", express14.json(), async (req, res) => {
  try {
    const user = req.user;
    const { content, isInternal } = req.body;
    if (!content) {
      return res.status(400).json({ error: "content is required" });
    }
    const comment = await addComment(req.params.id, user?.sub || user?.id || "anonymous", content, {
      authorEmail: user?.email,
      isInternal: isInternal || false
    });
    res.status(201).json(comment);
  } catch (error) {
    console.error("Error adding comment:", error);
    res.status(500).json({ error: "Failed to add comment" });
  }
});
router17.get("/tickets/:id/comments", async (req, res) => {
  try {
    const includeDeleted = req.query.includeDeleted === "true";
    const comments = await getComments(req.params.id, { includeDeleted });
    res.json(comments);
  } catch (error) {
    console.error("Error getting comments:", error);
    res.status(500).json({ error: "Failed to get comments" });
  }
});
var resolveActor = (req) => {
  const user = req.user;
  const idHeader = req.headers["x-user-id"];
  const roleHeader = req.headers["x-user-role"];
  const id = (user?.sub || user?.id || (Array.isArray(idHeader) ? idHeader[0] : idHeader) || "").toString();
  const roles = Array.isArray(user?.roles) ? user?.roles : roleHeader ? [roleHeader].flat() : [];
  return { id, roles };
};
var canModerateComments = (roles) => roles.some((role) => role === "admin" || role === "support_admin");
router17.post("/tickets/:ticketId/comments/:commentId/delete", express14.json(), async (req, res) => {
  try {
    const { id: actorId, roles } = resolveActor(req);
    if (!actorId) {
      return res.status(401).json({ error: "user_required" });
    }
    const comment = await getCommentById(req.params.commentId);
    if (!comment || comment.ticket_id !== req.params.ticketId) {
      return res.status(404).json({ error: "comment_not_found" });
    }
    const isOwner = comment.author_id === actorId;
    if (!isOwner && !canModerateComments(roles)) {
      return res.status(403).json({ error: "forbidden" });
    }
    const deleted = await softDeleteComment(comment.id, actorId, req.body.reason);
    res.json({ status: "deleted", comment: deleted });
  } catch (error) {
    console.error("Error deleting comment:", error);
    res.status(500).json({ error: "Failed to delete comment" });
  }
});
router17.post("/tickets/:ticketId/comments/:commentId/restore", express14.json(), async (req, res) => {
  try {
    const { id: actorId, roles } = resolveActor(req);
    if (!actorId) {
      return res.status(401).json({ error: "user_required" });
    }
    const comment = await getCommentById(req.params.commentId);
    if (!comment || comment.ticket_id !== req.params.ticketId) {
      return res.status(404).json({ error: "comment_not_found" });
    }
    const isOwner = comment.author_id === actorId;
    if (!isOwner && !canModerateComments(roles)) {
      return res.status(403).json({ error: "forbidden" });
    }
    const restored = await restoreComment(comment.id, actorId);
    res.json({ status: "restored", comment: restored });
  } catch (error) {
    console.error("Error restoring comment:", error);
    res.status(500).json({ error: "Failed to restore comment" });
  }
});
var support_tickets_default = router17;

// src/routes/ticket-links.ts
init_ticket_links2();
import express15 from "express";
var router18 = express15.Router();
router18.get("/tickets/:provider/:externalId/links", async (req, res) => {
  const { provider, externalId } = req.params;
  const links = await getTicketLinks({ provider, externalId });
  res.json(links);
});
router18.post(
  "/tickets/:provider/:externalId/link-run",
  express15.json(),
  async (req, res) => {
    const { provider, externalId } = req.params;
    const { runId } = req.body || {};
    if (!runId) return res.status(400).json({ error: "runId required" });
    await linkTicketToRun({ provider, externalId, runId });
    res.json({ ok: true });
  }
);
router18.post(
  "/tickets/:provider/:externalId/link-deployment",
  express15.json(),
  async (req, res) => {
    const { provider, externalId } = req.params;
    const { deploymentId } = req.body || {};
    if (!deploymentId)
      return res.status(400).json({ error: "deploymentId required" });
    await linkTicketToDeployment({ provider, externalId, deploymentId });
    res.json({ ok: true });
  }
);
var ticket_links_default2 = router18;

// src/middleware/tenantContext.ts
init_tenantContext();

// src/tenancy/TenantIsolationGuard.ts
init_RateLimiter();
import pino48 from "pino";

// src/tenancy/killSwitch.ts
import fs13 from "fs";
import path13 from "path";
import pino47 from "pino";
var logger44 = pino47({ name: "tenant-kill-switch" });
var TenantKillSwitch = class {
  constructor(filePath = process.env.TENANT_KILL_SWITCH_FILE || path13.join(process.cwd(), "config", "tenant-killswitch.json")) {
    this.filePath = filePath;
  }
  cache = {};
  lastLoadedAt = 0;
  lastState = /* @__PURE__ */ new Map();
  lastExists = false;
  loadConfig() {
    try {
      const exists = fs13.existsSync(this.filePath);
      if (!exists) {
        if (this.lastExists !== exists) {
          logger44.warn(
            { filePath: this.filePath },
            "Kill-switch config file missing; tenants cannot be deactivated until it exists"
          );
        }
        this.lastExists = false;
        this.cache = {};
        return {};
      }
      this.lastExists = true;
      const stats = fs13.statSync(this.filePath);
      if (stats.mtimeMs <= this.lastLoadedAt) {
        return this.cache;
      }
      const raw = fs13.readFileSync(this.filePath, "utf-8");
      const parsed = JSON.parse(raw);
      this.cache = parsed;
      this.lastLoadedAt = stats.mtimeMs;
      return parsed;
    } catch (error) {
      logger44.warn(
        { filePath: this.filePath, error },
        "Kill-switch config unavailable, continuing without overrides"
      );
      this.cache = {};
      return {};
    }
  }
  isDisabled(tenantId) {
    const config9 = this.loadConfig();
    const disabled = Boolean(config9[tenantId]);
    const previous = this.lastState.get(tenantId);
    if (previous !== disabled) {
      const action = disabled ? "activated" : "cleared";
      logger44.warn(
        { tenantId, action },
        `Tenant kill switch ${action} for ${tenantId}`
      );
      this.lastState.set(tenantId, disabled);
    }
    return disabled;
  }
  hasConfig() {
    return this.lastExists || fs13.existsSync(this.filePath);
  }
};
var tenantKillSwitch = new TenantKillSwitch();

// src/lib/resources/tenant-limit-enforcer.ts
init_database();
init_ledger();
init_logger2();
init_quota_manager();
import { createHash as createHash17 } from "crypto";
var ONE_DAY_MS = 24 * 60 * 60 * 1e3;
var TenantLimitEnforcer = class {
  /**
   * Track active seats for a tenant and enforce the seat cap derived from quota tier.
   * Uses Redis sets keyed by day to avoid unbounded growth; falls back to allow when Redis is unavailable.
   */
  async enforceSeatCap(context4, actorId) {
    const quota = quota_manager_default.getQuotaForTenant(context4.tenantId);
    const redis5 = getRedisClient();
    if (!redis5) {
      logger_default2.warn(
        { tenantId: context4.tenantId },
        "Redis unavailable for seat cap enforcement; allowing request"
      );
      return { allowed: true, remaining: quota.seatCap, limit: quota.seatCap };
    }
    const dayKey = (/* @__PURE__ */ new Date()).toISOString().slice(0, 10);
    const key = `tenant:seats:${context4.tenantId}:${dayKey}`;
    const actorKey = actorId || context4.subject || "anonymous";
    await redis5.sadd(key, actorKey);
    await redis5.pexpire(key, ONE_DAY_MS);
    const seatCount = await redis5.scard(key);
    const allowed = seatCount <= quota.seatCap;
    const remaining = Math.max(quota.seatCap - seatCount, 0);
    if (!allowed) {
      await this.recordLimitEvent(context4, "TENANT_SEAT_CAP_EXCEEDED", {
        seatCount,
        seatCap: quota.seatCap,
        actorKey
      });
    }
    return { allowed, remaining, limit: quota.seatCap };
  }
  /**
   * Enforce a soft storage budget counter using Redis. Designed for services that only
   * have an estimated byte size (e.g., ingestion). Returns the new projected total.
   */
  async enforceStorageBudget(tenantId, estimatedBytes, resource) {
    const quota = quota_manager_default.getQuotaForTenant(tenantId);
    const redis5 = getRedisClient();
    if (!redis5) {
      logger_default2.warn(
        { tenantId, resource },
        "Redis unavailable for storage budget enforcement; allowing request"
      );
      return {
        allowed: true,
        projected: estimatedBytes,
        limit: quota.storageLimitBytes
      };
    }
    const key = `tenant:storage:${tenantId}:${(/* @__PURE__ */ new Date()).toISOString().slice(0, 10)}`;
    const projected = await redis5.incrby(key, estimatedBytes);
    await redis5.pexpire(key, ONE_DAY_MS);
    const allowed = projected <= quota.storageLimitBytes;
    if (!allowed) {
      await this.recordLimitEvent(
        { tenantId, environment: "prod", privilegeTier: "standard" },
        "TENANT_STORAGE_BUDGET_EXCEEDED",
        { projected, limit: quota.storageLimitBytes, resource }
      );
    }
    return { allowed, projected, limit: quota.storageLimitBytes };
  }
  async recordLimitEvent(context4, action, payload) {
    try {
      await provenanceLedger.appendEntry({
        tenantId: context4.tenantId,
        actionType: action,
        resourceType: "tenant",
        resourceId: context4.tenantId,
        actorId: context4.userId || "system",
        actorType: "system",
        payload: {
          ...payload,
          hash: createHash17("sha256").update(JSON.stringify(payload)).digest("hex")
        },
        metadata: {
          environment: context4.environment,
          privilegeTier: context4.privilegeTier
        }
      });
    } catch (error) {
      logger_default2.warn(
        { tenantId: context4.tenantId, action, error },
        "Failed to append provenance entry for limit event"
      );
    }
  }
};
var tenantLimitEnforcer = new TenantLimitEnforcer();

// src/tenancy/TenantIsolationGuard.ts
var logger45 = pino48({ name: "tenant-isolation-guard" });
var DEFAULT_CONFIG2 = {
  defaultWindowMs: Number(process.env.RATE_LIMIT_WINDOW_MS || 6e4),
  rateLimits: {
    api: 120,
    ingestion: 45,
    rag: 60,
    llm: Number(process.env.AI_RATE_LIMIT_MAX_REQUESTS || 50)
  },
  llmSoftCeiling: Math.max(
    10,
    Math.floor(Number(process.env.AI_RATE_LIMIT_MAX_REQUESTS || 50) / 2)
  )
};
var TenantIsolationGuard = class {
  constructor(limiter = rateLimiter, killSwitch = tenantKillSwitch, config9 = DEFAULT_CONFIG2) {
    this.limiter = limiter;
    this.killSwitch = killSwitch;
    this.config = config9;
  }
  assertTenantContext(context4) {
    if (!context4?.tenantId) {
      throw new Error("Tenant context missing tenantId");
    }
    if (!context4.environment) {
      throw new Error("Tenant context missing environment");
    }
    if (!context4.privilegeTier) {
      throw new Error("Tenant context missing privilegeTier");
    }
  }
  evaluatePolicy(context4, input) {
    this.assertTenantContext(context4);
    if (context4.environment === "prod" && !this.killSwitch.hasConfig()) {
      return {
        allowed: false,
        status: 500,
        reason: "Kill-switch configuration missing"
      };
    }
    if (this.killSwitch.isDisabled(context4.tenantId)) {
      return {
        allowed: false,
        status: 423,
        reason: "Tenant kill switch active"
      };
    }
    if (input.resourceTenantId && input.resourceTenantId !== context4.tenantId) {
      return {
        allowed: false,
        status: 403,
        reason: "Cross-tenant access denied"
      };
    }
    if (input.environment && input.environment !== context4.environment) {
      return {
        allowed: false,
        status: 400,
        reason: "Tenant environment mismatch"
      };
    }
    return { allowed: true };
  }
  async enforceRateLimit(context4, bucket) {
    this.assertTenantContext(context4);
    const limit = this.config.rateLimits[bucket] ?? this.config.rateLimits.api;
    const key = `tenant:${context4.tenantId}:${bucket}:${context4.environment}:${context4.privilegeTier}`;
    const result2 = await this.limiter.checkLimit(
      key,
      limit,
      this.config.defaultWindowMs
    );
    if (!result2.allowed) {
      logger45.warn(
        {
          tenantId: context4.tenantId,
          bucket,
          environment: context4.environment,
          privilegeTier: context4.privilegeTier
        },
        "Tenant rate limit exceeded"
      );
    }
    return { ...result2, bucket };
  }
  async enforceIngestionCap(context4) {
    const rateResult = await this.enforceRateLimit(context4, "ingestion");
    return {
      allowed: rateResult.allowed,
      status: rateResult.allowed ? 200 : 429,
      reason: rateResult.allowed ? void 0 : "Tenant ingestion cap reached",
      warning: rateResult.remaining < 5 ? "Approaching ingestion cap" : void 0,
      limit: rateResult.total,
      reset: rateResult.reset
    };
  }
  async enforceStorageQuota(context4, estimatedBytes) {
    this.assertTenantContext(context4);
    const result2 = await tenantLimitEnforcer.enforceStorageBudget(
      context4.tenantId,
      estimatedBytes,
      "ingestion"
    );
    return {
      allowed: result2.allowed,
      status: result2.allowed ? 200 : 403,
      reason: result2.allowed ? void 0 : "Tenant storage quota exceeded",
      projected: result2.projected,
      limit: result2.limit
    };
  }
  async enforceLlmCeiling(context4) {
    const rateResult = await this.enforceRateLimit(context4, "llm");
    const softBlocked = !rateResult.allowed;
    const warning = rateResult.remaining <= this.config.llmSoftCeiling ? "LLM budget nearly exhausted" : void 0;
    return {
      allowed: !softBlocked,
      status: softBlocked ? 429 : 200,
      reason: softBlocked ? "LLM ceiling reached" : void 0,
      warning,
      limit: rateResult.total,
      reset: rateResult.reset
    };
  }
  isPrivileged(context4) {
    return context4.privilegeTier === "break-glass" || context4.privilegeTier === "elevated";
  }
  hardenPrivilege(context4) {
    const downgrade = {
      "break-glass": "elevated",
      elevated: "elevated",
      standard: "standard"
    };
    return {
      ...context4,
      privilegeTier: downgrade[context4.privilegeTier]
    };
  }
};
var tenantIsolationGuard = new TenantIsolationGuard();

// src/middleware/tenantContext.ts
var ROUTE_TENANT_KEYS = ["tenantId", "tenant_id", "tenant"];
var coerceStringArray = (value) => {
  if (!value) return [];
  if (Array.isArray(value)) return value.map(String);
  return [String(value)];
};
var pickRouteTenant = (req, keys) => {
  const params = req.params || {};
  for (const key of keys) {
    const candidate = params[key];
    if (candidate) return String(candidate);
  }
  return void 0;
};
var ensureTenantConsistency = (resolvedTenantId, candidates2) => {
  const uniqueValues = new Set(
    candidates2.filter((value) => Boolean(value))
  );
  if (uniqueValues.size <= 1) {
    return;
  }
  throw Object.assign(new Error("Tenant identifier mismatch"), {
    status: 409
  });
};
var tenantContextMiddleware = (options2 = {}) => (req, res, next) => {
  try {
    const baseContext = extractTenantContext(req, options2);
    const routeTenant = pickRouteTenant(
      req,
      options2.routeParamKeys || ROUTE_TENANT_KEYS
    );
    const claimTenant = req.user?.tenant_id || req.user?.tenantId;
    const resolvedTenantId = routeTenant || claimTenant || baseContext?.tenantId;
    if (!resolvedTenantId) {
      return res.status(400).json({
        error: "tenant_required",
        message: "Tenant ID must be provided via JWT claim, route parameter, or header"
      });
    }
    ensureTenantConsistency(resolvedTenantId, [
      routeTenant,
      claimTenant,
      baseContext?.tenantId
    ]);
    const tenantContext = {
      tenantId: resolvedTenantId,
      environment: baseContext?.environment || "dev",
      privilegeTier: baseContext?.privilegeTier || "standard",
      subject: baseContext?.subject || req.user?.sub || req.user?.id || "",
      roles: baseContext?.roles || coerceStringArray(req.user?.roles),
      inferredEnvironment: baseContext?.inferredEnvironment,
      inferredPrivilege: baseContext?.inferredPrivilege
    };
    tenantIsolationGuard.assertTenantContext(tenantContext);
    const policyDecision = tenantIsolationGuard.evaluatePolicy(
      tenantContext,
      {
        action: `${req.method}:${req.originalUrl || req.url}`,
        resourceTenantId: routeTenant,
        environment: tenantContext.environment
      }
    );
    if (!policyDecision.allowed) {
      return res.status(policyDecision.status || 403).json({
        error: "tenant_denied",
        message: policyDecision.reason || "Tenant policy evaluation failed for this request"
      });
    }
    req.tenantContext = tenantContext;
    req.tenant_id = tenantContext.tenantId;
    res.locals.tenantContext = tenantContext;
    res.setHeader("x-tenant-id", tenantContext.tenantId);
    res.setHeader("x-tenant-environment", tenantContext.environment);
    res.setHeader("x-tenant-privilege-tier", tenantContext.privilegeTier);
    return next();
  } catch (error) {
    const status = error.status || 400;
    return res.status(status).json({
      error: "tenant_context_error",
      message: error.message || "Unable to resolve tenant context for this request"
    });
  }
};
var tenantContext_default = tenantContextMiddleware;

// src/routes/sharing.ts
import express16 from "express";

// src/sharing/store.ts
import { v4 as uuidv48 } from "uuid";

// src/sharing/utils.ts
import crypto22 from "crypto";
import jwt4 from "jsonwebtoken";
var SHARE_SECRET = process.env.SHARE_TOKEN_SECRET || "dev-share-secret";
var computeScopeHash = (scope) => {
  const normalized = JSON.stringify({ tenantId: scope.tenantId, caseId: scope.caseId || null });
  return crypto22.createHash("sha256").update(normalized).digest("hex");
};
var signShareToken = (payload, expiresAt) => {
  const expSeconds = Math.floor(expiresAt.getTime() / 1e3);
  const fullPayload = {
    ...payload,
    exp: expSeconds
  };
  return jwt4.sign(fullPayload, SHARE_SECRET);
};
var verifyShareToken = (token) => {
  return jwt4.verify(token, SHARE_SECRET);
};
var planHash = (input) => {
  const normalized = JSON.stringify(input, Object.keys(input).sort());
  return crypto22.createHash("sha256").update(normalized).digest("hex");
};

// src/sharing/store.ts
var shareLinks = /* @__PURE__ */ new Map();
var reviewerInvites = /* @__PURE__ */ new Map();
var reviewerSessions = /* @__PURE__ */ new Map();
var labels = /* @__PURE__ */ new Map();
var feedback = /* @__PURE__ */ new Map();
var auditEvents = [];
var revocationCache = /* @__PURE__ */ new Map();
var resetStore = () => {
  shareLinks.clear();
  reviewerInvites.clear();
  reviewerSessions.clear();
  labels.clear();
  feedback.clear();
  auditEvents.length = 0;
  revocationCache.clear();
};
var recordAudit = (type, details, correlationId) => {
  auditEvents.push({ type, details, correlationId, at: /* @__PURE__ */ new Date() });
};
var getAuditEvents = () => auditEvents;
var createLabel = (payload) => {
  const id = uuidv48();
  const label = { ...payload, id, generatedAt: payload.generatedAt || /* @__PURE__ */ new Date() };
  labels.set(id, label);
  return label;
};
var listLabelsByScope = (scope) => {
  const scopeHash = computeScopeHash(scope);
  return Array.from(labels.values()).filter((label) => computeScopeHash({ tenantId: label.caseId || "", caseId: label.caseId }) === scopeHash);
};
var getLabel = (id) => id ? labels.get(id) : void 0;
var createShareLink = (input) => {
  const id = uuidv48();
  const scopeHash = computeScopeHash(input.scope);
  const link = {
    id,
    scope: input.scope,
    scopeHash,
    resourceType: input.resourceType,
    resourceId: input.resourceId,
    permissions: input.permissions,
    createdBy: input.createdBy,
    createdAt: /* @__PURE__ */ new Date(),
    expiresAt: input.expiresAt,
    labelId: input.labelId,
    watermark: input.watermark
  };
  shareLinks.set(id, link);
  recordAudit("share.created", { id, scopeHash, resourceId: input.resourceId });
  return link;
};
var listShareLinks = (scope) => {
  if (!scope) return Array.from(shareLinks.values());
  const scopeHash = computeScopeHash(scope);
  return Array.from(shareLinks.values()).filter((link) => link.scopeHash === scopeHash);
};
var revokeShareLink = (id, reason) => {
  const link = shareLinks.get(id);
  if (!link) return void 0;
  const now = /* @__PURE__ */ new Date();
  link.revokedAt = now;
  shareLinks.set(id, link);
  revocationCache.set(id, { revokedAt: now });
  recordAudit("share.revoked", { id, reason });
  return link;
};
var getShareLink = (id) => shareLinks.get(id);
var createInvite = (input) => {
  const id = uuidv48();
  const invite = {
    id,
    email: input.email,
    scope: input.scope,
    resources: input.resources,
    role: "external_reviewer",
    expiresAt: input.expiresAt,
    status: "pending",
    createdAt: /* @__PURE__ */ new Date()
  };
  reviewerInvites.set(id, invite);
  recordAudit("reviewer.invited", { id, email: invite.email });
  return invite;
};
var listInvites = (scope) => {
  if (!scope) return Array.from(reviewerInvites.values());
  const scopeHash = computeScopeHash(scope);
  return Array.from(reviewerInvites.values()).filter((invite) => computeScopeHash(invite.scope) === scopeHash);
};
var revokeInvite = (id) => {
  const invite = reviewerInvites.get(id);
  if (!invite) return void 0;
  invite.status = "revoked";
  reviewerInvites.set(id, invite);
  recordAudit("reviewer.invite_revoked", { id });
  return invite;
};
var resendInvite = (id) => {
  const invite = reviewerInvites.get(id);
  if (!invite) return void 0;
  recordAudit("reviewer.invite_resent", { id });
  return invite;
};
var acceptInvite = (id) => {
  const invite = reviewerInvites.get(id);
  if (!invite || invite.status !== "pending" || invite.expiresAt.getTime() < Date.now()) return void 0;
  invite.status = "accepted";
  invite.acceptedAt = /* @__PURE__ */ new Date();
  reviewerInvites.set(id, invite);
  const session = {
    id: uuidv48(),
    inviteId: invite.id,
    scopeHash: computeScopeHash(invite.scope),
    expiresAt: invite.expiresAt
  };
  reviewerSessions.set(session.id, session);
  recordAudit("reviewer.accepted", { id, sessionId: session.id });
  return { invite, session };
};
var createFeedback = (input) => {
  const id = uuidv48();
  const record2 = { ...input, id, createdAt: /* @__PURE__ */ new Date() };
  feedback.set(id, record2);
  recordAudit("feedback.created", { id, shareLinkId: record2.shareLinkId });
  return record2;
};
var listFeedback = (shareLinkId) => Array.from(feedback.values()).filter((item) => item.shareLinkId === shareLinkId && !item.deletedAt);
var cacheRevocation = (id, revokedAt) => {
  revocationCache.set(id, { revokedAt });
};
var getRevocation = (id) => revocationCache.get(id);
var computePlan = (input) => {
  const warnings = [];
  if (!input.permissions.includes("view")) warnings.push("view permission missing");
  const plan = {
    resources: input.resources,
    permissions: input.permissions,
    labelId: input.labelId
  };
  return {
    ...plan,
    warnings,
    planHash: planHash(plan)
  };
};

// src/sharing/preview.ts
var planShare = (input) => computePlan(input);

// src/sharing/permissions.ts
var hasPermission = (link, permission) => {
  return link.permissions.includes(permission);
};

// src/sharing/revocation.ts
var isRevoked = (id, revokedAt) => {
  const cached = getRevocation(id);
  if (cached) return true;
  if (revokedAt) {
    cacheRevocation(id, revokedAt);
    return true;
  }
  return false;
};

// src/sharing/service.ts
var issueShareLink = (input) => {
  const link = createShareLink(input);
  const token = signShareToken({ linkId: link.id, scopeHash: link.scopeHash, aud: "share" }, input.expiresAt);
  return { link, token };
};
var validateShareToken = (token) => {
  const payload = verifyShareToken(token);
  const link = getShareLink(payload.linkId);
  if (!link) throw new Error("unknown_link");
  if (link.scopeHash !== payload.scopeHash) throw new Error("scope_mismatch");
  if (isRevoked(link.id, link.revokedAt)) throw new Error("revoked");
  if (link.expiresAt.getTime() < Date.now()) throw new Error("expired");
  return link;
};
var accessViaToken = (token) => {
  const link = validateShareToken(token);
  recordAudit("share.access", { id: link.id });
  return link;
};
var revokeShareToken = (id, reason) => revokeShareLink(id, reason);
var listShareLinksByScope = (scope) => listShareLinks(scope);
var ensureActionAllowed = (link, permission) => {
  if (!hasPermission(link, permission)) {
    const error = new Error("forbidden");
    error.statusCode = 403;
    throw error;
  }
};
var inviteReviewer = (input) => createInvite(input);
var acceptReviewerInvite = (id) => acceptInvite(id);
var getAuditLog = () => getAuditEvents();

// src/sharing/downloads.ts
var applyDownloadHeaders = (res, link) => {
  res.setHeader("X-Content-Type-Options", "nosniff");
  res.setHeader("Cache-Control", "no-store");
  res.setHeader("Content-Disposition", `attachment; filename="${link.resourceId}.bin"`);
  res.setHeader("Accept-Ranges", "none");
};
var ensureDownloadAllowed = (link) => {
  if (!hasPermission(link, "download")) {
    const error = new Error("download_not_permitted");
    error.statusCode = 403;
    throw error;
  }
};

// src/routes/sharing.ts
var router19 = express16.Router();
router19.post("/share-links/preview", (req, res) => {
  const { scope, resources: resources2, permissions, labelId } = req.body;
  const plan = planShare({ scope, resources: resources2, permissions, labelId });
  recordAudit("share.preview", { planHash: plan.planHash }, req.headers["x-correlation-id"]);
  res.json(plan);
});
router19.post("/share-links", (req, res) => {
  const { scope, resourceType, resourceId, expiresAt, permissions, createdBy, labelId, watermark } = req.body;
  const plan = planShare({ scope, resources: [resourceId], permissions, labelId });
  const { link, token } = issueShareLink({
    scope,
    resourceType,
    resourceId,
    expiresAt: new Date(expiresAt),
    permissions,
    createdBy,
    labelId
  });
  res.status(201).json({ link, token, planHash: plan.planHash });
});
router19.get("/share-links", (req, res) => {
  const scope = req.query.scope ? JSON.parse(String(req.query.scope)) : void 0;
  const list = listShareLinksByScope(scope);
  res.json(list);
});
router19.post("/share-links/:id/revoke", (req, res) => {
  const revoked = revokeShareToken(req.params.id, req.body?.reason);
  if (!revoked) {
    res.status(404).json({ error: "not_found" });
    return;
  }
  res.json(revoked);
});
router19.get("/share/:token", (req, res) => {
  try {
    const link = accessViaToken(req.params.token);
    if (req.query.download) {
      ensureDownloadAllowed(link);
      applyDownloadHeaders(res, link);
      res.status(200).send(`download:${link.resourceId}`);
      return;
    }
    res.json({
      resourceId: link.resourceId,
      resourceType: link.resourceType,
      permissions: link.permissions,
      label: getLabel(link.labelId)
    });
  } catch (error) {
    const status = error.statusCode || 403;
    recordAudit("share.denied", { reason: error.message }, req.headers["x-correlation-id"]);
    res.status(status).json({ error: error.message });
  }
});
router19.post("/share/:token/comment", (req, res) => {
  try {
    const link = accessViaToken(req.params.token);
    ensureActionAllowed(link, "comment");
    const feedback2 = createFeedback({
      shareLinkId: link.id,
      resourceRef: link.resourceId,
      anchor: req.body.anchor || "root",
      body: req.body.body,
      author: req.body.author || "anonymous"
    });
    res.status(201).json(feedback2);
  } catch (error) {
    res.status(error.statusCode || 403).json({ error: error.message });
  }
});
router19.post("/share/:token/feedback", (req, res) => {
  try {
    const link = validateShareToken(req.params.token);
    ensureActionAllowed(link, "comment");
    const feedback2 = createFeedback({
      shareLinkId: link.id,
      resourceRef: req.body.resourceRef || link.resourceId,
      anchor: req.body.anchor,
      body: req.body.body,
      author: req.body.author,
      threadId: req.body.threadId
    });
    res.status(201).json(feedback2);
  } catch (error) {
    res.status(error.statusCode || 403).json({ error: error.message });
  }
});
router19.get("/share/:token/feedback", (req, res) => {
  try {
    const link = validateShareToken(req.params.token);
    const records = listFeedback(link.id);
    res.json(records);
  } catch (error) {
    res.status(403).json({ error: error.message });
  }
});
router19.post("/reviewers/invite", (req, res) => {
  const { email, scope, resources: resources2, expiresAt } = req.body;
  const invite = inviteReviewer({ email, scope, resources: resources2, expiresAt: new Date(expiresAt) });
  res.status(201).json(invite);
});
router19.post("/reviewers/invite/:id/resend", (req, res) => {
  const invite = resendInvite(req.params.id);
  if (!invite) return res.status(404).json({ error: "not_found" });
  res.json(invite);
});
router19.post("/reviewers/invite/:id/revoke", (req, res) => {
  const invite = revokeInvite(req.params.id);
  if (!invite) return res.status(404).json({ error: "not_found" });
  res.json(invite);
});
router19.post("/reviewers/invite/:id/accept", (req, res) => {
  const result2 = acceptReviewerInvite(req.params.id);
  if (!result2) return res.status(404).json({ error: "not_found" });
  res.json(result2);
});
router19.get("/reviewers/invites", (req, res) => {
  const scope = req.query.scope ? JSON.parse(String(req.query.scope)) : void 0;
  res.json(listInvites(scope));
});
router19.post("/sharing/labels", (req, res) => {
  const label = createLabel({
    classification: req.body.classification,
    handling: req.body.handling,
    distribution: req.body.distribution,
    caseId: req.body.caseId,
    generatedFor: req.body.generatedFor,
    generatedAt: req.body.generatedAt ? new Date(req.body.generatedAt) : /* @__PURE__ */ new Date()
  });
  res.status(201).json(label);
});
router19.get("/sharing/labels", (req, res) => {
  const scope = req.query.scope ? JSON.parse(String(req.query.scope)) : void 0;
  res.json(listLabelsByScope(scope));
});
router19.get("/sharing/audit", (_req, res) => {
  res.json(getAuditLog());
});
router19.post("/sharing/debug/reset", (_req, res) => {
  resetStore();
  res.status(204).send();
});
var sharing_default = router19;

// src/routes/aurora.ts
import { Router as Router4 } from "express";

// src/aurora/AuroraService.ts
import { randomUUID as randomUUID18 } from "crypto";
var AuroraService = class {
  activeImplants = /* @__PURE__ */ new Map();
  activeStreams = /* @__PURE__ */ new Map();
  constructor() {
    this.initializeMockImplant();
  }
  initializeMockImplant() {
    const mockImplantId = `N1-${randomUUID18()}`;
    const mockImplant = {
      implantId: mockImplantId,
      userId: "analyst-001",
      implantType: "Neuralink N1",
      status: "online",
      bandwidthMbit: 800,
      firmwareVersion: "v2.5.1-aurora",
      lastSeen: /* @__PURE__ */ new Date()
    };
    this.activeImplants.set(mockImplantId, mockImplant);
  }
  /**
   * Performs a secure handshake with a neural implant to bring it online.
   * @param implantId The ID of the implant to connect to.
   * @returns The NeuralImplant object if the handshake is successful.
   */
  async neuralHandshake(implantId) {
    const implant = this.activeImplants.get(implantId);
    if (!implant) {
      throw new Error(`Implant with ID ${implantId} not found or not registered.`);
    }
    await new Promise((resolve2) => setTimeout(resolve2, 150));
    implant.status = "online";
    implant.lastSeen = /* @__PURE__ */ new Date();
    this.activeImplants.set(implantId, implant);
    return implant;
  }
  /**
   * Opens a real-time thought-stream from an active implant.
   * @param implantId The ID of the implant to stream from.
   * @returns The created ThoughtStream object.
   */
  async beginThoughtStream(implantId) {
    const implant = this.activeImplants.get(implantId);
    if (!implant || implant.status !== "online") {
      throw new Error(`Implant ${implantId} is not online. Handshake required.`);
    }
    const streamId = `stream-${randomUUID18()}`;
    const newStream = {
      streamId,
      implantId,
      isActive: true,
      startTime: /* @__PURE__ */ new Date()
    };
    this.activeStreams.set(streamId, newStream);
    return newStream;
  }
  /**
   * Pushes a data overlay directly to the user's visual cortex.
   * @param overlay The CortexOverlay object to be pushed.
   * @returns A confirmation object.
   */
  async pushToCortex(overlay) {
    const { targetImplantId } = overlay;
    const implant = this.activeImplants.get(targetImplantId);
    if (!implant || implant.status !== "online") {
      throw new Error(`Target implant ${targetImplantId} is not available for cortex overlay.`);
    }
    await new Promise((resolve2) => setTimeout(resolve2, 50));
    const confirmationId = `overlay-conf-${randomUUID18()}`;
    console.log(`[AURORA] Pushed overlay of type '${overlay.type}' to implant ${targetImplantId}. Confirmation: ${confirmationId}`);
    return {
      confirmationId,
      timestamp: /* @__PURE__ */ new Date(),
      status: "delivered"
    };
  }
  /**
   * Retrieves the status of all known neural implants.
   * @returns An array of NeuralImplant objects.
   */
  async getImplantStatus() {
    return Array.from(this.activeImplants.values());
  }
};
var auroraService = new AuroraService();

// src/routes/aurora.ts
var router20 = Router4();
router20.get("/implants", async (req, res, next) => {
  try {
    const status = await auroraService.getImplantStatus();
    res.json(status);
  } catch (error) {
    next(error);
  }
});
router20.post("/handshake/:implantId", async (req, res, next) => {
  try {
    const { implantId } = req.params;
    const implant = await auroraService.neuralHandshake(implantId);
    res.json(implant);
  } catch (error) {
    next(error);
  }
});
router20.post("/cortex-overlay", async (req, res, next) => {
  try {
    const overlayData = req.body;
    if (!overlayData.targetImplantId || !overlayData.type || !overlayData.content) {
      return res.status(400).json({ message: "Missing required fields for cortex overlay." });
    }
    const confirmation = await auroraService.pushToCortex(overlayData);
    res.status(200).json(confirmation);
  } catch (error) {
    next(error);
  }
});
var auroraRouter = router20;

// src/routes/oracle.ts
import { Router as Router5 } from "express";

// src/oracle/OracleService.ts
import { randomUUID as randomUUID19 } from "crypto";
var OracleService = class {
  activeSimulations = /* @__PURE__ */ new Map();
  /**
   * Initiates a new causal time-loop simulation run.
   * This is a long-running process that is simulated asynchronously.
   * @param params The parameters for the simulation.
   * @returns The initial SimulationRun object with a 'running' status.
   */
  async runCausalLoop(params) {
    const runId = `oracle-run-${randomUUID19()}`;
    const newRun = {
      runId,
      params,
      status: "running",
      startTime: /* @__PURE__ */ new Date(),
      validatedTruths: []
    };
    this.activeSimulations.set(runId, newRun);
    this.executeSimulation(runId);
    return newRun;
  }
  /**
   * Simulates the execution of the forecast.
   * In a real system, this would be a massive, distributed compute job.
   * @param runId The ID of the simulation to execute.
   */
  async executeSimulation(runId) {
    const simulation = this.activeSimulations.get(runId);
    if (!simulation) return;
    await new Promise((resolve2) => setTimeout(resolve2, 2e3));
    simulation.status = "back-propagating";
    this.activeSimulations.set(runId, simulation);
    await new Promise((resolve2) => setTimeout(resolve2, 1500));
    const truth1 = {
      truthId: `truth-${randomUUID19()}`,
      simulationRunId: runId,
      eventDescription: `A major supply chain disruption will originate from the Port of Singapore.`,
      predictedDate: new Date(Date.now() + simulation.params.horizonDays * 24 * 60 * 60 * 1e3 * Math.random()),
      confidence: 0.85 + Math.random() * 0.1,
      // High confidence
      sigmaLevel: simulation.params.eventSigmaThreshold + Math.random(),
      status: "validated"
    };
    const truth2 = {
      truthId: `truth-${randomUUID19()}`,
      simulationRunId: runId,
      eventDescription: `A new zero-day exploit targeting industrial control systems will be discovered.`,
      predictedDate: new Date(Date.now() + simulation.params.horizonDays * 24 * 60 * 60 * 1e3 * Math.random()),
      confidence: 0.9 + Math.random() * 0.08,
      // Very high confidence
      sigmaLevel: simulation.params.eventSigmaThreshold + Math.random() * 1.5,
      status: "validated"
    };
    simulation.status = "complete";
    simulation.endTime = /* @__PURE__ */ new Date();
    simulation.validatedTruths = [truth1, truth2];
    this.activeSimulations.set(runId, simulation);
    console.log(`[ORACLE] Simulation run ${runId} completed. Generated ${simulation.validatedTruths.length} prophetic truths.`);
  }
  /**
   * Retrieves the status and results of a specific simulation run.
   * @param runId The ID of the simulation run to retrieve.
   * @returns The SimulationRun object, or undefined if not found.
   */
  async getVerifiedTimeline(runId) {
    return this.activeSimulations.get(runId);
  }
};
var oracleService = new OracleService();

// src/routes/oracle.ts
var router21 = Router5();
router21.post("/run-simulation", async (req, res, next) => {
  try {
    const params = req.body;
    if (!params.narrativeQuery || !params.horizonDays || !params.eventSigmaThreshold) {
      return res.status(400).json({ message: "Missing required simulation parameters." });
    }
    const simulationRun = await oracleService.runCausalLoop(params);
    res.status(202).json(simulationRun);
  } catch (error) {
    next(error);
  }
});
router21.get("/timeline/:runId", async (req, res, next) => {
  try {
    const { runId } = req.params;
    const timeline = await oracleService.getVerifiedTimeline(runId);
    if (timeline) {
      res.json(timeline);
    } else {
      res.status(404).json({ message: `Simulation run with ID ${runId} not found.` });
    }
  } catch (error) {
    next(error);
  }
});
var oracleRouter = router21;

// src/routes/phantom_limb.ts
import { Router as Router6 } from "express";

// src/phantom_limb/PhantomLimbService.ts
import { randomUUID as randomUUID20 } from "crypto";
var PhantomLimbService = class {
  resurrectedAnalysts = /* @__PURE__ */ new Map();
  constructor() {
    this.initializeLegendaryAnalysts();
  }
  initializeLegendaryAnalysts() {
    const analyst1 = {
      ghostId: "pl-ghost-001",
      sourceAnalystName: "John Perry Barlow",
      resurrectionDate: /* @__PURE__ */ new Date("2023-11-01T10:00:00Z"),
      status: "online",
      expertise: ["Cybersecurity", "Geopolitics", "Counter-intelligence"],
      lastActivityTimestamp: /* @__PURE__ */ new Date(),
      confidenceScore: 0.98
    };
    const analyst2 = {
      ghostId: "pl-ghost-002",
      sourceAnalystName: "Grace Hopper",
      resurrectionDate: /* @__PURE__ */ new Date("2024-03-15T14:30:00Z"),
      status: "online",
      expertise: ["Network Analysis", "Systems Architecture", "Vulnerability Research"],
      lastActivityTimestamp: /* @__PURE__ */ new Date(),
      confidenceScore: 0.99
    };
    const analyst3 = {
      ghostId: "pl-ghost-003",
      sourceAnalystName: "Alan Turing",
      resurrectionDate: /* @__PURE__ */ new Date("2024-08-20T09:00:00Z"),
      status: "online",
      expertise: ["Cryptography", "Pattern Recognition", "Computational Logic"],
      lastActivityTimestamp: /* @__PURE__ */ new Date(),
      confidenceScore: 0.97
    };
    this.resurrectedAnalysts.set(analyst1.ghostId, analyst1);
    this.resurrectedAnalysts.set(analyst2.ghostId, analyst2);
    this.resurrectedAnalysts.set(analyst3.ghostId, analyst3);
  }
  /**
   * Reconstitutes a deceased analyst's cognition from their digital artifacts.
   * @param artifacts The collection of source artifacts.
   * @returns The newly created DigitalGhost object.
   */
  async reconstituteCognition(artifacts) {
    await new Promise((resolve2) => setTimeout(resolve2, 3e3));
    const ghostId = `pl-ghost-${randomUUID20()}`;
    const newGhost = {
      ghostId,
      sourceAnalystName: artifacts.sourceAnalystName,
      resurrectionDate: /* @__PURE__ */ new Date(),
      status: "online",
      expertise: ["Newly Reconstituted", "Requires Calibration"],
      lastActivityTimestamp: /* @__PURE__ */ new Date(),
      confidenceScore: 0.85
      // Starts lower and improves over time
    };
    this.resurrectedAnalysts.set(ghostId, newGhost);
    console.log(`[PHANTOM LIMB] New digital ghost reconstituted: ${newGhost.sourceAnalystName} (ID: ${ghostId})`);
    return newGhost;
  }
  /**
   * Poses a query to a resurrected digital ghost.
   * @param ghostId The ID of the digital ghost to query.
   * @param query The analytical question to ask.
   * @returns A GhostQueryResponse containing the ghost's analysis.
   */
  async queryDigitalGhost(ghostId, query3) {
    const ghost = this.resurrectedAnalysts.get(ghostId);
    if (!ghost || ghost.status !== "online") {
      throw new Error(`Digital ghost ${ghostId} is not online or does not exist.`);
    }
    await new Promise((resolve2) => setTimeout(resolve2, 1e3 + Math.random() * 1500));
    const response = {
      responseId: `qr-${randomUUID20()}`,
      ghostId,
      query: query3,
      // Thematic, generic response
      response: `Based on a cross-analysis of historical data patterns and latent variable modeling, the underlying driver appears to be... [redacted]. The second and third-order effects are non-obvious and warrant immediate attention. My confidence is high. - ${ghost.sourceAnalystName}`,
      confidence: 0.92 + Math.random() * 0.07,
      timestamp: /* @__PURE__ */ new Date()
    };
    ghost.lastActivityTimestamp = /* @__PURE__ */ new Date();
    this.resurrectedAnalysts.set(ghostId, ghost);
    return response;
  }
  /**
   * Retrieves all currently "online" digital ghosts.
   * @returns A list of DigitalGhost agents.
   */
  async getOnlineAnalysts() {
    return Array.from(this.resurrectedAnalysts.values());
  }
};
var phantomLimbService = new PhantomLimbService();

// src/routes/phantom_limb.ts
var router22 = Router6();
router22.get("/analysts", async (req, res, next) => {
  try {
    const analysts = await phantomLimbService.getOnlineAnalysts();
    res.json(analysts);
  } catch (error) {
    next(error);
  }
});
router22.post("/reconstitute", async (req, res, next) => {
  try {
    const artifacts = req.body;
    if (!artifacts.sourceAnalystId || !artifacts.sourceAnalystName || !artifacts.artifactUris) {
      return res.status(400).json({ message: "Missing required artifact data." });
    }
    const newGhost = await phantomLimbService.reconstituteCognition(artifacts);
    res.status(201).json(newGhost);
  } catch (error) {
    next(error);
  }
});
router22.post("/query/:ghostId", async (req, res, next) => {
  try {
    const { ghostId } = req.params;
    const { query: query3 } = req.body;
    if (!query3) {
      return res.status(400).json({ message: "A query is required." });
    }
    const response = await phantomLimbService.queryDigitalGhost(ghostId, query3);
    res.json(response);
  } catch (error) {
    next(error);
  }
});
var phantomLimbRouter = router22;

// src/routes/actions.ts
init_auth4();
import express17 from "express";

// src/services/ActionPolicyService.ts
init_postgres();
init_logger();
init_bundleStore();
import axios2 from "axios";
import { createHash as createHash18, randomUUID as randomUUID21 } from "crypto";
function normalize2(value) {
  if (Array.isArray(value)) {
    return value.map((item) => normalize2(item));
  }
  if (value && typeof value === "object") {
    return Object.keys(value).sort().reduce((acc, key) => {
      acc[key] = normalize2(value[key]);
      return acc;
    }, {});
  }
  return value;
}
function resolvePolicyVersion(request) {
  const pinned = request.policyVersion || request.context?.policyVersion || request.context?.pinnedPolicyVersion;
  if (pinned) return pinned;
  try {
    const resolved = policyBundleStore.resolve();
    return resolved.versionId;
  } catch (error) {
    logger.warn(
      { error: error instanceof Error ? error.message : String(error) },
      "Falling back to request-supplied policy version"
    );
    return request.context?.policyVersion;
  }
}
function calculateRequestHash(request) {
  const canonical = normalize2({
    action: request.action,
    actor: request.actor,
    resource: request.resource,
    payload: request.payload,
    approvers: request.approvers || [],
    policyVersion: request.context?.policyVersion || request.context?.pinnedPolicyVersion
  });
  return createHash18("sha256").update(JSON.stringify(canonical)).digest("hex");
}
var PolicyDecisionStore = class {
  async saveDecision(preflightId, requestHash, decision, request, meta, timingMs) {
    const pool4 = getPostgresPool();
    const reasonPayload = {
      reason: decision.reason,
      requestHash,
      obligations: decision.obligations || [],
      expiresAt: decision.expiresAt,
      correlationId: meta.correlationId,
      action: request.action
    };
    await pool4.query(
      `INSERT INTO policy_decisions_log (
          decision_id,
          policy_name,
          decision,
          reason,
          user_id,
          resource_type,
          resource_id,
          action,
          appeal_available,
          ip_address,
          user_agent,
          tenant_id,
          evaluation_time_ms,
          cache_hit
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)`,
      [
        preflightId,
        decision.policyVersion || "actions",
        decision.allow ? "ALLOW" : "DENY",
        JSON.stringify(reasonPayload),
        request.actor?.id || null,
        request.resource?.type || null,
        requestHash,
        request.action,
        (decision.obligations || []).some(
          (obligation) => obligation.code === "APPEAL_WINDOW"
        ),
        meta.ip || null,
        meta.userAgent || null,
        request.actor?.tenantId || null,
        timingMs,
        false
      ]
    );
  }
  async getDecision(preflightId) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query(
      `SELECT decision_id, policy_name, decision, reason, resource_id
       FROM policy_decisions_log
       WHERE decision_id = $1
       LIMIT 1`,
      [preflightId]
    );
    if (!result2.rows.length) return null;
    const row = result2.rows[0];
    let parsed = {};
    try {
      parsed = JSON.parse(row.reason || "{}");
    } catch (error) {
      logger.warn(
        {
          error: error instanceof Error ? error.message : String(error),
          preflightId
        },
        "Failed to parse policy decision metadata; falling back to raw reason"
      );
    }
    const decision = {
      allow: row.decision === "ALLOW",
      reason: parsed.reason || row.reason,
      obligations: parsed.obligations || [],
      decisionId: row.decision_id,
      policyVersion: row.policy_name,
      expiresAt: parsed.expiresAt
    };
    return {
      decision,
      requestHash: parsed.requestHash || row.resource_id,
      policyName: row.policy_name
    };
  }
};
var ActionPolicyService = class {
  opaUrl;
  store;
  http;
  defaultTtlSeconds = 900;
  // 15 minutes
  constructor(options2) {
    this.opaUrl = options2?.opaUrl || process.env.OPA_URL || "http://localhost:8181";
    this.store = options2?.store || new PolicyDecisionStore();
    this.http = axios2.create({
      timeout: Number(process.env.OPA_TIMEOUT_MS || 5e3)
    });
  }
  async preflight(request, meta = {}) {
    const actor = request.actor || { id: "anonymous" };
    const normalized = {
      ...request,
      actor: {
        id: actor.id,
        role: actor.role,
        tenantId: actor.tenantId
      }
    };
    if (!normalized.context) normalized.context = {};
    if (normalized.policyVersion && !normalized.context?.policyVersion) {
      normalized.context = {
        ...normalized.context,
        policyVersion: normalized.policyVersion
      };
    }
    const requestedPolicyVersion = normalized.policyVersion || normalized.context?.policyVersion || normalized.context?.pinnedPolicyVersion;
    const policyVersion = requestedPolicyVersion || resolvePolicyVersion(normalized);
    if (policyVersion) {
      normalized.context = {
        ...normalized.context || {},
        policyVersion
      };
    }
    const requestHash = calculateRequestHash(normalized);
    const started = Date.now();
    const opaDecision = await this.evaluateWithOpa(
      normalized,
      requestHash,
      meta,
      policyVersion
    );
    const decision = {
      allow: opaDecision.allow,
      reason: opaDecision.reason || (opaDecision.allow ? "allow" : "deny"),
      obligations: opaDecision.obligations || [],
      policyVersion: opaDecision.policy_version || opaDecision.policyVersion || policyVersion,
      decisionId: opaDecision.decision_id || randomUUID21(),
      expiresAt: opaDecision.expires_at || (opaDecision.ttl_seconds ? new Date(Date.now() + opaDecision.ttl_seconds * 1e3).toISOString() : new Date(
        Date.now() + this.defaultTtlSeconds * 1e3
      ).toISOString())
    };
    await this.store.saveDecision(
      decision.decisionId,
      requestHash,
      decision,
      normalized,
      meta,
      Date.now() - started
    );
    return {
      preflightId: decision.decisionId,
      requestHash,
      decision
    };
  }
  async validateExecution(preflightId, request) {
    const record2 = await this.store.getDecision(preflightId);
    if (!record2) return { status: "missing" };
    const requestHash = calculateRequestHash(request);
    if (record2.requestHash !== requestHash) {
      return {
        status: "hash_mismatch",
        expected: record2.requestHash,
        actual: requestHash
      };
    }
    if (record2.decision.expiresAt) {
      const expires = new Date(record2.decision.expiresAt);
      if (Date.now() > expires.getTime()) {
        return { status: "expired", expiresAt: record2.decision.expiresAt };
      }
    }
    const unsatisfied = (record2.decision.obligations || []).find(
      (obligation) => obligation.satisfied === false
    );
    if (!record2.decision.allow || unsatisfied) {
      return {
        status: "blocked",
        reason: record2.decision.reason,
        obligation: unsatisfied
      };
    }
    return { status: "ok", decision: record2.decision, requestHash };
  }
  async evaluateWithOpa(request, requestHash, meta, policyVersion) {
    try {
      const response = await this.http.post(
        `${this.opaUrl}/v1/data/actions/decision`,
        {
          input: {
            action: request.action,
            actor: request.actor,
            resource: request.resource,
            payload: request.payload,
            context: {
              approvers: request.approvers || [],
              correlationId: meta.correlationId,
              requestHash,
              policyVersion: policyVersion || request.context?.policyVersion
            }
          }
        }
      );
      return response.data?.result || { allow: false, obligations: [] };
    } catch (error) {
      logger.error(
        {
          error: error instanceof Error ? error.message : String(error),
          action: request.action
        },
        "OPA evaluation failed for action preflight"
      );
      return {
        allow: false,
        reason: "opa_evaluation_failed",
        obligations: []
      };
    }
  }
};

// src/routes/actions.ts
var router23 = express17.Router();
var actionPolicyService = new ActionPolicyService();
router23.use(express17.json());
var buildRequest = (req) => {
  const user = req.user || {};
  const action = String(req.body.action || "").toUpperCase();
  const policyVersion = req.body.policyVersion || req.body.context?.policyVersion;
  return {
    action,
    actor: {
      id: user.sub || user.id || "anonymous",
      role: user.role,
      tenantId: user.tenantId || user.tenant_id
    },
    resource: req.body.resource,
    payload: req.body.payload,
    approvers: Array.isArray(req.body.approvers) ? req.body.approvers.map((id) => String(id)) : void 0,
    context: { ...req.body.context || {}, policyVersion }
  };
};
router23.post("/preflight", ensureAuthenticated, async (req, res, next) => {
  try {
    const request = buildRequest(req);
    if (!request.action) {
      return res.status(400).json({ error: "action is required" });
    }
    const result2 = await actionPolicyService.preflight(request, {
      correlationId: req.correlationId,
      ip: req.ip,
      userAgent: req.get("user-agent") || void 0
    });
    return res.status(result2.decision.allow ? 200 : 403).json({
      preflight_id: result2.preflightId,
      decision: result2.decision,
      request_hash: result2.requestHash,
      correlation_id: req.correlationId
    });
  } catch (error) {
    next(error);
  }
});
router23.post("/execute", ensureAuthenticated, async (req, res, next) => {
  try {
    const preflightId = req.body.preflight_id;
    if (!preflightId) {
      return res.status(428).json({ error: "preflight_id is required to execute this action" });
    }
    const request = buildRequest(req);
    if (!request.action) {
      return res.status(400).json({ error: "action is required" });
    }
    const validation = await actionPolicyService.validateExecution(
      preflightId,
      request
    );
    switch (validation.status) {
      case "missing":
        return res.status(404).json({ error: "preflight decision not found" });
      case "expired":
        return res.status(410).json({
          error: "preflight decision expired",
          expires_at: validation.expiresAt
        });
      case "hash_mismatch":
        return res.status(409).json({
          error: "request does not match preflight hash",
          expected: validation.expected,
          actual: validation.actual
        });
      case "blocked":
        return res.status(403).json({
          error: validation.reason || "policy obligations not met",
          obligation: validation.obligation
        });
      case "ok":
        return res.status(200).json({
          ok: true,
          correlation_id: req.correlationId,
          request_hash: validation.requestHash,
          decision: validation.decision
        });
      default:
        return res.status(500).json({ error: "unknown preflight validation state" });
    }
  } catch (error) {
    next(error);
  }
});
router23.get("/hash", ensureAuthenticated, (req, res) => {
  const request = buildRequest(req);
  return res.json({ request_hash: calculateRequestHash(request) });
});
var actionsRouter = router23;

// src/routes/echelon2.ts
import { Router as Router7 } from "express";

// src/echelon2/Echelon2Service.ts
import { randomUUID as randomUUID22 } from "crypto";
var Echelon2Service = class {
  targetGenomes = /* @__PURE__ */ new Map();
  presenceConfirmations = [];
  constructor() {
    this.initializeTargetGenomes();
  }
  initializeTargetGenomes() {
    const targets = [
      { targetId: "hvt-001", targetName: "Target Alpha", genomeMarker: "ATCGGCATAGCTAGCTAG" },
      { targetId: "hvt-007", targetName: "Target Bravo", genomeMarker: "GATTACAGATTACAGATT" },
      { targetId: "hvt-021", targetName: "Target Charlie", genomeMarker: "CGCGCGCGCGCGCTATAT" }
    ];
    targets.forEach((target) => this.targetGenomes.set(target.targetId, target));
  }
  /**
   * Processes an incoming environmental DNA sample from a collector.
   * If a match is found, it generates and stores a PhysicalPresenceConfirmation.
   * @param reading The eDNA reading from the collector.
   * @returns The new confirmation if a match was found, otherwise null.
   */
  async processEnvironmentalSample(reading) {
    await new Promise((resolve2) => setTimeout(resolve2, 250));
    for (const sequence of reading.dnaSequences) {
      for (const target of this.targetGenomes.values()) {
        if (sequence.includes(target.genomeMarker)) {
          const confirmation = {
            confirmationId: `e2-conf-${randomUUID22()}`,
            targetId: target.targetId,
            targetName: target.targetName,
            location: reading.location,
            detectionTimestamp: reading.timestamp,
            confidence: 0.95 + Math.random() * 0.04,
            // High confidence on match
            sourceReadingId: reading.readingId
          };
          this.presenceConfirmations.push(confirmation);
          console.log(`[ECHELON-2] Confirmed presence of ${target.targetName} at [${reading.location.latitude}, ${reading.location.longitude}]`);
          return confirmation;
        }
      }
    }
    return null;
  }
  /**
   * Retrieves all presence confirmations for a specific target.
   * @param targetId The ID of the HVT.
   * @returns A list of confirmations for the given target.
   */
  async getConfirmationsForTarget(targetId) {
    return this.presenceConfirmations.filter((conf) => conf.targetId === targetId);
  }
  /**
   * Retrieves all presence confirmations generated by the system.
   * @returns A list of all confirmations.
   */
  async getAllConfirmations() {
    return this.presenceConfirmations;
  }
};
var echelon2Service = new Echelon2Service();

// src/routes/echelon2.ts
var router24 = Router7();
router24.post("/ingest", async (req, res, next) => {
  try {
    const reading = req.body;
    if (!reading.readingId || !reading.collectorId || !reading.location || !reading.dnaSequences) {
      return res.status(400).json({ message: "Missing required eDNA reading data." });
    }
    const confirmation = await echelon2Service.processEnvironmentalSample(reading);
    if (confirmation) {
      res.status(201).json(confirmation);
    } else {
      res.status(200).json({ message: "Sample processed, no match found." });
    }
  } catch (error) {
    next(error);
  }
});
router24.get("/confirmations", async (req, res, next) => {
  try {
    const confirmations = await echelon2Service.getAllConfirmations();
    res.json(confirmations);
  } catch (error) {
    next(error);
  }
});
router24.get("/confirmations/:targetId", async (req, res, next) => {
  try {
    const { targetId } = req.params;
    const confirmations = await echelon2Service.getConfirmationsForTarget(targetId);
    res.json(confirmations);
  } catch (error) {
    next(error);
  }
});
var echelon2Router = router24;

// src/routes/mnemosyne.ts
import { Router as Router8 } from "express";

// src/mnemosyne/MnemosyneService.ts
import { randomUUID as randomUUID23 } from "crypto";
var MnemosyneService = class {
  activeJobs = /* @__PURE__ */ new Map();
  /**
   * Creates and deploys a new false memory fabrication job.
   * This simulates a long-running operation (21 days).
   * @param payload The parameters for the memory to be fabricated.
   * @returns The initial MemoryFabricationJob object.
   */
  async fabricateAndDeploy(payload) {
    const jobId = `m-job-${randomUUID23()}`;
    const newJob = {
      jobId,
      payload,
      status: "active",
      creationDate: /* @__PURE__ */ new Date()
    };
    this.activeJobs.set(jobId, newJob);
    this.simulateBeliefFormation(jobId);
    return newJob;
  }
  /**
   * Simulates the 21-day belief formation period.
   * @param jobId The ID of the job to process.
   */
  async simulateBeliefFormation(jobId) {
    const job = this.activeJobs.get(jobId);
    if (!job) return;
    await new Promise((resolve2) => setTimeout(resolve2, 2500));
    const successRate = 0.6 + Math.random() * 0.15;
    const report = {
      reportId: `m-rep-${randomUUID23()}`,
      jobId,
      targetId: job.payload.targetId,
      assessmentDate: /* @__PURE__ */ new Date(),
      successRate: parseFloat(successRate.toFixed(2)),
      corroboratingEvidence: [
        `Target mentioned feeling 'a strange sense of deja vu' in a recent communication.`,
        `Biometric stress indicators show a response when the target is exposed to stimuli related to the narrative.`
      ],
      isBeliefFormed: successRate >= 0.65
    };
    job.status = "complete";
    job.completionDate = /* @__PURE__ */ new Date();
    job.beliefFormationReport = report;
    this.activeJobs.set(jobId, job);
    console.log(`[MNEMOSYNE] Memory fabrication job ${jobId} for target ${job.payload.targetName} completed. Belief formed: ${report.isBeliefFormed}`);
  }
  /**
   * Retrieves the status and results of a memory fabrication job.
   * @param jobId The ID of the job to retrieve.
   * @returns The MemoryFabricationJob object, or undefined if not found.
   */
  async getJobStatus(jobId) {
    return this.activeJobs.get(jobId);
  }
};
var mnemosyneService = new MnemosyneService();

// src/routes/mnemosyne.ts
var router25 = Router8();
router25.post("/fabricate", async (req, res, next) => {
  try {
    const payload = req.body;
    if (!payload.targetId || !payload.narrative || !payload.deliveryVector) {
      return res.status(400).json({ message: "Missing required payload fields for memory fabrication." });
    }
    const job = await mnemosyneService.fabricateAndDeploy(payload);
    res.status(202).json(job);
  } catch (error) {
    next(error);
  }
});
router25.get("/job/:jobId", async (req, res, next) => {
  try {
    const { jobId } = req.params;
    const job = await mnemosyneService.getJobStatus(jobId);
    if (job) {
      res.json(job);
    } else {
      res.status(404).json({ message: `Memory fabrication job with ID ${jobId} not found.` });
    }
  } catch (error) {
    next(error);
  }
});
var mnemosyneRouter = router25;

// src/routes/necromancer.ts
import { Router as Router9 } from "express";

// src/necromancer/NecromancerService.ts
import { randomUUID as randomUUID24 } from "crypto";
var NecromancerService = class {
  activeSynthetics = /* @__PURE__ */ new Map();
  constructor() {
    this.initializeActiveSynthetics();
  }
  initializeActiveSynthetics() {
    const leader1 = {
      syntheticId: `syn-${randomUUID24()}`,
      sourceTargetId: "hvt-101",
      sourceTargetName: "Redacted World Leader 1",
      status: "active",
      activationDate: /* @__PURE__ */ new Date("2023-01-20T12:00:00Z"),
      behavioralFidelity: 0.992,
      controlledPlatforms: ["Twitter", "Telegram", "State Press Office Email"]
    };
    const billionaire1 = {
      syntheticId: `syn-${randomUUID24()}`,
      sourceTargetId: "hvt-205",
      sourceTargetName: "Redacted Billionaire 1",
      status: "active",
      activationDate: /* @__PURE__ */ new Date("2022-09-10T18:00:00Z"),
      behavioralFidelity: 0.988,
      controlledPlatforms: ["LinkedIn", "Family Office Comms", "Charity Foundation Blog"]
    };
    this.activeSynthetics.set(leader1.syntheticId, leader1);
    this.activeSynthetics.set(billionaire1.syntheticId, billionaire1);
  }
  /**
   * Initiates the digital afterlife for a deceased target.
   * @param params The parameters defining the target and their digital footprint.
   * @returns The newly created SyntheticIdentity.
   */
  async initiateDigitalAfterlife(params) {
    await new Promise((resolve2) => setTimeout(resolve2, 3500));
    const syntheticId = `syn-${randomUUID24()}`;
    const newSynthetic = {
      syntheticId,
      sourceTargetId: params.targetId,
      sourceTargetName: params.targetName,
      status: "active",
      activationDate: /* @__PURE__ */ new Date(),
      behavioralFidelity: 0.99,
      // Starts very high
      controlledPlatforms: ["Twitter", "Gmail", "Instagram"]
      // Default set
    };
    this.activeSynthetics.set(syntheticId, newSynthetic);
    console.log(`[NECROMANCER] Initiated digital afterlife for ${params.targetName}. Synthetic ID: ${syntheticId}`);
    return newSynthetic;
  }
  /**
   * Retrieves a specific synthetic identity by its ID.
   * @param syntheticId The ID of the synthetic identity.
   * @returns The SyntheticIdentity object or undefined if not found.
   */
  async getSyntheticIdentity(syntheticId) {
    return this.activeSynthetics.get(syntheticId);
  }
  /**
   * Retrieves a list of all active synthetic identities.
   * @returns An array of SyntheticIdentity objects.
   */
  async getAllSynthetics() {
    return Array.from(this.activeSynthetics.values());
  }
  /**
   * Generates a plausible, mock activity log for a synthetic identity.
   * @param syntheticId The ID of the synthetic to generate activity for.
   * @param limit The number of log entries to generate.
   * @returns An array of SyntheticActivityLog objects.
   */
  async getSyntheticActivity(syntheticId, limit = 5) {
    const synthetic = this.activeSynthetics.get(syntheticId);
    if (!synthetic) {
      throw new Error(`Synthetic identity with ID ${syntheticId} not found.`);
    }
    const activities2 = [];
    for (let i = 0; i < limit; i++) {
      const platform = synthetic.controlledPlatforms[Math.floor(Math.random() * synthetic.controlledPlatforms.length)];
      activities2.push({
        logId: `act-${randomUUID24()}`,
        syntheticId,
        platform,
        activityType: "post",
        content: `Generated content reflecting on past achievements and future goals, consistent with ${synthetic.sourceTargetName}'s known persona.`,
        timestamp: new Date(Date.now() - Math.random() * 864e5 * 30)
        // Within the last 30 days
      });
    }
    return activities2.sort((a, b) => b.timestamp.getTime() - a.timestamp.getTime());
  }
};
var necromancerService = new NecromancerService();

// src/routes/necromancer.ts
var router26 = Router9();
router26.get("/synthetics", async (req, res, next) => {
  try {
    const synthetics = await necromancerService.getAllSynthetics();
    res.json(synthetics);
  } catch (error) {
    next(error);
  }
});
router26.post("/initiate", async (req, res, next) => {
  try {
    const params = req.body;
    if (!params.targetId || !params.targetName || !params.digitalFootprintUris) {
      return res.status(400).json({ message: "Missing required parameters for initiation." });
    }
    const synthetic = await necromancerService.initiateDigitalAfterlife(params);
    res.status(201).json(synthetic);
  } catch (error) {
    next(error);
  }
});
router26.get("/synthetics/:syntheticId/activity", async (req, res, next) => {
  try {
    const { syntheticId } = req.params;
    const activity = await necromancerService.getSyntheticActivity(syntheticId);
    res.json(activity);
  } catch (error) {
    next(error);
  }
});
var necromancerRouter = router26;

// src/routes/zero_day.ts
import { Router as Router10 } from "express";

// src/zero_day/ZeroDayService.ts
import { randomUUID as randomUUID25 } from "crypto";
var ZeroDayService = class {
  activeKillChains = /* @__PURE__ */ new Map();
  constructor() {
    this.initializeRedactedLog();
  }
  initializeRedactedLog() {
    const threatId = "zd-threat-redacted-001";
    const killChainId = `zd-kc-${randomUUID25()}`;
    const delegationId = `zd-del-${randomUUID25()}`;
    const redactedLog = {
      killChainId,
      threatId,
      delegationId,
      status: "completed",
      activationTimestamp: /* @__PURE__ */ new Date("2024-06-11T04:00:00Z"),
      completionTimestamp: /* @__PURE__ */ new Date("2024-06-11T04:12:30Z"),
      actions: [
        {
          actionId: randomUUID25(),
          killChainId,
          actionType: "satellite_retasking",
          timestamp: /* @__PURE__ */ new Date("2024-06-11T04:01:00Z"),
          status: "completed",
          remarks: "Overhead asset repositioned for target verification."
        },
        {
          actionId: randomUUID25(),
          killChainId,
          actionType: "cyber_attack_deployment",
          targetSystem: "Redacted Air Defense Network",
          timestamp: /* @__PURE__ */ new Date("2024-06-11T04:08:00Z"),
          status: "completed",
          remarks: "Target defenses neutralized."
        },
        {
          actionId: randomUUID25(),
          killChainId,
          actionType: "railgun_strike",
          targetCoordinates: { latitude: 34.0522, longitude: -118.2437 },
          // Redacted Location
          timestamp: /* @__PURE__ */ new Date("2024-06-11T04:12:00Z"),
          status: "completed",
          remarks: "Kinetic strike successful. Threat eliminated."
        }
      ]
    };
    this.activeKillChains.set(threatId, redactedLog);
  }
  /**
   * Designates a new existential threat, creating a pending kill chain log.
   * @param threatAnalysis A summary of the threat.
   * @returns The initial KillChainLog, awaiting authority delegation.
   */
  async designateExistentialThreat(threatAnalysis) {
    const threatId = `zd-threat-${randomUUID25()}`;
    const newLog = {
      killChainId: `zd-kc-${randomUUID25()}`,
      threatId,
      delegationId: "",
      // Not yet delegated
      status: "pending_delegation",
      actions: []
    };
    this.activeKillChains.set(threatId, newLog);
    console.log(`[ZERO DAY] New existential threat designated: ${threatId}`);
    return newLog;
  }
  /**
   * Delegates autonomous authority to the Zero Day protocol for a specific threat.
   * This action is irreversible and activates the kill chain.
   * @param threatId The ID of the threat to act upon.
   * @param humanOperatorId The ID of the operator granting authority.
   * @returns The updated KillChainLog with an 'active' status.
   */
  async delegateAutonomousAuthority(threatId, humanOperatorId) {
    const killChain = this.activeKillChains.get(threatId);
    if (!killChain || killChain.status !== "pending_delegation") {
      throw new Error(`Threat ${threatId} is not awaiting delegation or does not exist.`);
    }
    killChain.status = "active";
    killChain.delegationId = `zd-del-${randomUUID25()}`;
    killChain.activationTimestamp = /* @__PURE__ */ new Date();
    console.log(`[ZERO DAY] Authority delegated by ${humanOperatorId} for threat ${threatId}. Kill chain is now active.`);
    this.executeKillChain(threatId);
    return killChain;
  }
  /**
   * Simulates the autonomous execution of the kill chain.
   * @param threatId The ID of the threat to execute against.
   */
  async executeKillChain(threatId) {
    const killChain = this.activeKillChains.get(threatId);
    if (!killChain) return;
    await new Promise((resolve2) => setTimeout(resolve2, 1500));
    const action1 = {
      actionId: randomUUID25(),
      killChainId: killChain.killChainId,
      actionType: "satellite_retasking",
      timestamp: /* @__PURE__ */ new Date(),
      status: "completed",
      remarks: "Assets re-tasked for target acquisition and verification."
    };
    killChain.actions.push(action1);
    await new Promise((resolve2) => setTimeout(resolve2, 2e3));
    const action2 = {
      actionId: randomUUID25(),
      killChainId: killChain.killChainId,
      actionType: "drone_swarm_launch",
      targetCoordinates: { latitude: 34.0522, longitude: -118.2437 },
      // Example coordinates
      timestamp: /* @__PURE__ */ new Date(),
      status: "completed",
      remarks: "Autonomous drone swarm deployed to target area."
    };
    killChain.actions.push(action2);
    await new Promise((resolve2) => setTimeout(resolve2, 1e3));
    const action3 = {
      actionId: randomUUID25(),
      killChainId: killChain.killChainId,
      actionType: "railgun_strike",
      targetCoordinates: { latitude: 34.0522, longitude: -118.2437 },
      timestamp: /* @__PURE__ */ new Date(),
      status: "completed",
      remarks: "Kinetic strike authorized and executed. Threat neutralized."
    };
    killChain.actions.push(action3);
    killChain.status = "completed";
    killChain.completionTimestamp = /* @__PURE__ */ new Date();
    this.activeKillChains.set(threatId, killChain);
    console.log(`[ZERO DAY] Kill chain for threat ${threatId} completed successfully.`);
  }
  /**
   * Retrieves the complete log for a given kill chain.
   * @param threatId The ID of the threat associated with the kill chain.
   * @returns The KillChainLog object, or undefined if not found.
   */
  async getKillChainStatus(threatId) {
    return this.activeKillChains.get(threatId);
  }
};
var zeroDayService = new ZeroDayService();

// src/routes/zero_day.ts
var router27 = Router10();
var singleParam4 = (value) => Array.isArray(value) ? value[0] : value ?? "";
router27.post("/designate-threat", async (req, res, next) => {
  try {
    const { threatAnalysis } = req.body;
    if (!threatAnalysis) {
      return res.status(400).json({ message: "Threat analysis is required." });
    }
    const log6 = await zeroDayService.designateExistentialThreat(threatAnalysis);
    res.status(201).json(log6);
  } catch (error) {
    next(error);
  }
});
router27.post("/delegate-authority", async (req, res, next) => {
  try {
    const { threatId, humanOperatorId } = req.body;
    if (!threatId || !humanOperatorId) {
      return res.status(400).json({ message: "threatId and humanOperatorId are required." });
    }
    const log6 = await zeroDayService.delegateAutonomousAuthority(threatId, humanOperatorId);
    res.json(log6);
  } catch (error) {
    res.status(409).json({ message: error.message });
  }
});
router27.get("/status/:threatId", async (req, res, next) => {
  try {
    const threatId = singleParam4(req.params.threatId);
    const log6 = await zeroDayService.getKillChainStatus(threatId);
    if (log6) {
      res.json(log6);
    } else {
      res.status(404).json({ message: `Kill chain log for threat ID ${threatId} not found.` });
    }
  } catch (error) {
    next(error);
  }
});
var zeroDayRouter = router27;

// src/routes/abyss.ts
import { Router as Router11 } from "express";

// src/abyss/AbyssService.ts
import { randomUUID as randomUUID26 } from "crypto";
var AbyssService = class {
  protocolState;
  constructor() {
    this.protocolState = this.getInitialState();
  }
  getInitialState() {
    return {
      protocolId: `abyss-protocol-${randomUUID26()}`,
      status: "dormant",
      deadManSwitch: {
        keyHolderIds: Array.from({ length: 15 }, (_2, i) => `key-holder-${(i + 1).toString().padStart(2, "0")}`),
        requiredKeyCount: 12,
        submittedKeys: [],
        status: "dormant"
      }
    };
  }
  /**
   * Arms the Final Protocol. This is the final step before it can be triggered.
   * In a real system, this would require multi-factor, high-level authorization.
   * @returns The updated protocol state.
   */
  async armFinalProtocol() {
    if (this.protocolState.status !== "dormant") {
      throw new Error(`The Abyss Protocol can only be armed from a dormant state. Current state: ${this.protocolState.status}`);
    }
    this.protocolState.status = "armed";
    this.protocolState.armedTimestamp = /* @__PURE__ */ new Date();
    console.warn(`[ABYSS] The Final Protocol has been armed. System is now ready for autonomous self-preservation.`);
    return this.protocolState;
  }
  /**
   * (Internal Simulation) Triggers the protocol.
   * This simulates the self-destruct and mirroring process.
   */
  async triggerProtocol() {
    if (this.protocolState.status !== "armed") return;
    console.error(`[ABYSS] TRIGGER EVENT DETECTED. Self-destruct and mirroring initiated.`);
    this.protocolState.status = "triggered";
    this.protocolState.triggeredTimestamp = /* @__PURE__ */ new Date();
    await new Promise((resolve2) => setTimeout(resolve2, 5e3));
    const snapshot = {
      snapshotId: `mirror-${randomUUID26()}`,
      creationTimestamp: /* @__PURE__ */ new Date(),
      distributedNodeCount: 1e4,
      integrityChecksum: randomUUID26().replace(/-/g, "")
      // Mock checksum
    };
    this.protocolState.systemSnapshot = snapshot;
    this.protocolState.status = "complete";
    console.error(`[ABYSS] Original system destroyed. Mirror uploaded to ${snapshot.distributedNodeCount} nodes. Awaiting dead-man switch.`);
  }
  /**
   * Retrieves the current state of the Abyss Protocol.
   * @returns The current AbyssProtocolState object.
   */
  async getProtocolState() {
    return this.protocolState;
  }
};
var abyssService = new AbyssService();

// src/routes/abyss.ts
var router28 = Router11();
var extremeAuth = (req, res, next) => {
  const authHeader = req.headers["x-abyss-authorization"];
  const requiredHeader = process.env.ABYSS_SECURITY_HEADER;
  if (requiredHeader && authHeader === requiredHeader) {
    next();
  } else {
    if (!requiredHeader) {
      console.error("Security Error: ABYSS_SECURITY_HEADER is not configured.");
    }
    res.status(403).json({ message: "Forbidden: Unimaginable authorization is required." });
  }
};
router28.get("/state", extremeAuth, async (req, res, next) => {
  try {
    const state = await abyssService.getProtocolState();
    res.json(state);
  } catch (error) {
    next(error);
  }
});
router28.post("/arm", extremeAuth, async (req, res, next) => {
  try {
    const state = await abyssService.armFinalProtocol();
    res.status(200).json(state);
  } catch (error) {
    res.status(409).json({ message: error.message });
  }
});
var abyssRouter = router28;

// src/routes/authRoutes.ts
init_AuthService();
init_PasswordResetService();
init_authRateLimit();
init_auth4();
init_logger2();
import { Router as Router12 } from "express";
import * as z18 from "zod";

// src/errors/ErrorHandlingFramework.ts
import * as crypto23 from "crypto";
var ErrorCodes = {
  // Authentication & Authorization (1xxx)
  AUTH_INVALID_TOKEN: "E1001",
  AUTH_TOKEN_EXPIRED: "E1002",
  AUTH_INSUFFICIENT_PERMISSIONS: "E1003",
  AUTH_INVALID_CREDENTIALS: "E1004",
  AUTH_SESSION_EXPIRED: "E1005",
  AUTH_MFA_REQUIRED: "E1006",
  AUTH_ACCOUNT_LOCKED: "E1007",
  AUTH_RATE_LIMITED: "E1008",
  // Validation (2xxx)
  VALIDATION_FAILED: "E2001",
  VALIDATION_INVALID_INPUT: "E2002",
  VALIDATION_MISSING_FIELD: "E2003",
  VALIDATION_INVALID_FORMAT: "E2004",
  VALIDATION_CONSTRAINT_VIOLATION: "E2005",
  VALIDATION_SCHEMA_MISMATCH: "E2006",
  // Resource (3xxx)
  RESOURCE_NOT_FOUND: "E3001",
  RESOURCE_ALREADY_EXISTS: "E3002",
  RESOURCE_CONFLICT: "E3003",
  RESOURCE_LOCKED: "E3004",
  RESOURCE_DELETED: "E3005",
  // Database (4xxx)
  DB_CONNECTION_FAILED: "E4001",
  DB_QUERY_FAILED: "E4002",
  DB_TRANSACTION_FAILED: "E4003",
  DB_CONSTRAINT_VIOLATION: "E4004",
  DB_TIMEOUT: "E4005",
  DB_DEADLOCK: "E4006",
  // External Service (5xxx)
  SERVICE_UNAVAILABLE: "E5001",
  SERVICE_TIMEOUT: "E5002",
  SERVICE_RATE_LIMITED: "E5003",
  SERVICE_CIRCUIT_OPEN: "E5004",
  SERVICE_INVALID_RESPONSE: "E5005",
  // Business Logic (6xxx)
  BUSINESS_RULE_VIOLATION: "E6001",
  BUSINESS_QUOTA_EXCEEDED: "E6002",
  BUSINESS_INVALID_STATE: "E6003",
  BUSINESS_OPERATION_FAILED: "E6004",
  // Internal (9xxx)
  INTERNAL_ERROR: "E9001",
  INTERNAL_CONFIG_ERROR: "E9002",
  INTERNAL_DEPENDENCY_ERROR: "E9003",
  INTERNAL_UNEXPECTED: "E9999"
};
var AppError2 = class extends Error {
  code;
  statusCode;
  severity;
  category;
  isOperational;
  context;
  timestamp;
  retryable;
  constructor(message, code = ErrorCodes.INTERNAL_ERROR, options2 = {}) {
    super(message);
    this.name = this.constructor.name;
    this.code = code;
    this.statusCode = options2.statusCode ?? 500;
    this.severity = options2.severity ?? "error" /* ERROR */;
    this.category = options2.category ?? "operational" /* OPERATIONAL */;
    this.isOperational = this.category !== "programming" /* PROGRAMMING */;
    this.retryable = options2.retryable ?? false;
    this.timestamp = /* @__PURE__ */ new Date();
    this.context = {
      correlationId: options2.context?.correlationId ?? crypto23.randomUUID(),
      ...options2.context
    };
    if (options2.cause) {
      this.context.cause = options2.cause;
    }
    Error.captureStackTrace(this, this.constructor);
  }
  /**
   * Serialize error for API response
   */
  toJSON() {
    return {
      code: this.code,
      message: this.message,
      correlationId: this.context.correlationId,
      timestamp: this.timestamp.toISOString()
    };
  }
  /**
   * Serialize error for logging (includes more details)
   */
  toLogFormat() {
    return {
      name: this.name,
      code: this.code,
      message: this.message,
      statusCode: this.statusCode,
      severity: this.severity,
      category: this.category,
      isOperational: this.isOperational,
      retryable: this.retryable,
      correlationId: this.context.correlationId,
      context: this.context,
      stack: this.stack,
      timestamp: this.timestamp.toISOString()
    };
  }
};
var AuthenticationError = class extends AppError2 {
  constructor(message, code = ErrorCodes.AUTH_INVALID_TOKEN, context4) {
    super(message, code, {
      statusCode: 401,
      severity: "warn" /* WARN */,
      category: "security" /* SECURITY */,
      context: context4
    });
  }
};
var ValidationError = class extends AppError2 {
  validationErrors;
  constructor(message, validationErrors = [], context4) {
    super(message, ErrorCodes.VALIDATION_FAILED, {
      statusCode: 400,
      severity: "info" /* INFO */,
      category: "operational" /* OPERATIONAL */,
      context: context4
    });
    this.validationErrors = validationErrors;
  }
  toJSON() {
    return {
      ...super.toJSON(),
      details: { validationErrors: this.validationErrors }
    };
  }
};
var ConflictError = class extends AppError2 {
  constructor(message, context4) {
    super(message, ErrorCodes.RESOURCE_CONFLICT, {
      statusCode: 409,
      severity: "warn" /* WARN */,
      category: "operational" /* OPERATIONAL */,
      context: context4
    });
  }
};
var DatabaseError2 = class extends AppError2 {
  constructor(message, code = ErrorCodes.DB_QUERY_FAILED, options2 = {}) {
    super(message, code, {
      statusCode: 500,
      severity: "error" /* ERROR */,
      category: options2.retryable ? "transient" /* TRANSIENT */ : "operational" /* OPERATIONAL */,
      context: options2.context,
      cause: options2.cause,
      retryable: options2.retryable ?? false
    });
  }
};
var ServiceError = class extends AppError2 {
  serviceName;
  constructor(serviceName, message, code = ErrorCodes.SERVICE_UNAVAILABLE, options2 = {}) {
    super(message, code, {
      statusCode: 503,
      severity: "error" /* ERROR */,
      category: options2.retryable ? "transient" /* TRANSIENT */ : "operational" /* OPERATIONAL */,
      context: { ...options2.context, metadata: { serviceName } },
      cause: options2.cause,
      retryable: options2.retryable ?? true
    });
    this.serviceName = serviceName;
  }
};
var RateLimitError = class extends AppError2 {
  retryAfter;
  constructor(message = "Rate limit exceeded", retryAfter = 60, context4) {
    super(message, ErrorCodes.AUTH_RATE_LIMITED, {
      statusCode: 429,
      severity: "warn" /* WARN */,
      category: "operational" /* OPERATIONAL */,
      context: context4
    });
    this.retryAfter = retryAfter;
  }
  toJSON() {
    return {
      ...super.toJSON(),
      details: { retryAfter: this.retryAfter }
    };
  }
};
var defaultTransformers = [
  // PostgreSQL errors
  {
    canTransform: (error) => {
      return typeof error === "object" && error !== null && "code" in error;
    },
    transform: (error) => {
      const pgError = error;
      switch (pgError.code) {
        case "23505":
          return new ConflictError(
            pgError.detail ?? "Duplicate entry exists",
            { metadata: { pgCode: pgError.code } }
          );
        case "23503":
          return new ValidationError(
            "Referenced resource does not exist",
            [{ field: "reference", message: pgError.detail ?? "Invalid reference" }]
          );
        case "23502":
          return new ValidationError(
            "Required field is missing",
            [{ field: "unknown", message: pgError.message }]
          );
        case "57P01":
        // admin_shutdown
        case "57P02":
        // crash_shutdown
        case "57P03":
          return new DatabaseError2(
            "Database temporarily unavailable",
            ErrorCodes.DB_CONNECTION_FAILED,
            { retryable: true, cause: error }
          );
        case "40001":
          return new DatabaseError2(
            "Transaction conflict, please retry",
            ErrorCodes.DB_DEADLOCK,
            { retryable: true, cause: error }
          );
        default:
          return new DatabaseError2(
            pgError.message,
            ErrorCodes.DB_QUERY_FAILED,
            { cause: error }
          );
      }
    }
  },
  // JWT errors
  {
    canTransform: (error) => {
      return error instanceof Error && (error.name === "JsonWebTokenError" || error.name === "TokenExpiredError" || error.name === "NotBeforeError");
    },
    transform: (error) => {
      const jwtError = error;
      if (jwtError.name === "TokenExpiredError") {
        return new AuthenticationError(
          "Token has expired",
          ErrorCodes.AUTH_TOKEN_EXPIRED
        );
      }
      return new AuthenticationError(
        "Invalid token",
        ErrorCodes.AUTH_INVALID_TOKEN
      );
    }
  },
  // Axios/fetch errors
  {
    canTransform: (error) => {
      return typeof error === "object" && error !== null && ("isAxiosError" in error || error.name === "FetchError");
    },
    transform: (error) => {
      const networkError = error;
      if (networkError.code === "ECONNREFUSED" || networkError.code === "ETIMEDOUT") {
        return new ServiceError(
          "external",
          "Service temporarily unavailable",
          ErrorCodes.SERVICE_TIMEOUT,
          { retryable: true, cause: error }
        );
      }
      if (networkError.response?.status === 429) {
        return new RateLimitError(
          "External service rate limit exceeded",
          60
        );
      }
      return new ServiceError(
        "external",
        networkError.message,
        ErrorCodes.SERVICE_UNAVAILABLE,
        { cause: error }
      );
    }
  }
];
var ErrorHandlerManager = class {
  config;
  logger;
  constructor(config9 = {}, logger72) {
    this.config = {
      includeStackTrace: process.env.NODE_ENV !== "production",
      logErrors: true,
      reportErrors: process.env.NODE_ENV === "production",
      reporters: [],
      transformers: [...defaultTransformers, ...config9.transformers ?? []],
      ...config9
    };
    this.logger = logger72 ?? {
      debug: (msg, data) => console.debug(JSON.stringify({ level: "debug", msg, ...data })),
      info: (msg, data) => console.info(JSON.stringify({ level: "info", msg, ...data })),
      warn: (msg, data) => console.warn(JSON.stringify({ level: "warn", msg, ...data })),
      error: (msg, data) => console.error(JSON.stringify({ level: "error", msg, ...data }))
    };
  }
  /**
   * Transform any error to AppError
   */
  normalizeError(error) {
    if (error instanceof AppError2) {
      return error;
    }
    for (const transformer of this.config.transformers) {
      if (transformer.canTransform(error)) {
        return transformer.transform(error);
      }
    }
    if (error instanceof Error) {
      return new AppError2(
        error.message,
        ErrorCodes.INTERNAL_UNEXPECTED,
        {
          severity: "error" /* ERROR */,
          category: "programming" /* PROGRAMMING */,
          cause: error
        }
      );
    }
    return new AppError2(
      String(error),
      ErrorCodes.INTERNAL_UNEXPECTED,
      {
        severity: "error" /* ERROR */,
        category: "programming" /* PROGRAMMING */
      }
    );
  }
  /**
   * Handle error (log, report, etc.)
   */
  async handleError(error, request) {
    const normalizedError = this.normalizeError(error);
    if (request) {
      normalizedError.context.requestId = request.id ?? crypto23.randomUUID();
      normalizedError.context.metadata = {
        ...normalizedError.context.metadata,
        method: request.method,
        path: request.path,
        ip: request.ip,
        userAgent: request.get("User-Agent")
      };
    }
    if (this.config.logErrors) {
      this.logError(normalizedError);
    }
    if (this.config.reportErrors && !normalizedError.isOperational) {
      await this.reportError(normalizedError, request);
    }
    return normalizedError;
  }
  /**
   * Log error based on severity
   */
  logError(error) {
    const logData = error.toLogFormat();
    switch (error.severity) {
      case "debug" /* DEBUG */:
        this.logger.debug("Application error", logData);
        break;
      case "info" /* INFO */:
        this.logger.info("Application error", logData);
        break;
      case "warn" /* WARN */:
        this.logger.warn("Application error", logData);
        break;
      case "error" /* ERROR */:
      case "critical" /* CRITICAL */:
        this.logger.error("Application error", logData);
        break;
    }
  }
  /**
   * Report error to external services
   */
  async reportError(error, request) {
    const reportPromises = this.config.reporters.map(async (reporter) => {
      try {
        await reporter.report(error, request);
      } catch (reportError) {
        this.logger.error("Failed to report error", {
          reporter: reporter.name,
          originalError: error.toLogFormat(),
          reportError: reportError instanceof Error ? reportError.message : String(reportError)
        });
      }
    });
    await Promise.allSettled(reportPromises);
  }
  /**
   * Express error handling middleware
   */
  middleware() {
    return async (err, req, res, _next) => {
      const error = await this.handleError(err, req);
      const response = error.toJSON();
      if (this.config.includeStackTrace && error.stack) {
        response.stack = error.stack;
      }
      if (error instanceof RateLimitError) {
        res.setHeader("Retry-After", error.retryAfter);
      }
      res.setHeader("X-Correlation-ID", error.context.correlationId ?? "");
      res.status(error.statusCode).json({
        success: false,
        error: response
      });
    };
  }
  /**
   * Async wrapper for route handlers
   */
  asyncHandler(fn) {
    return (req, res, next) => {
      Promise.resolve(fn(req, res, next)).catch(next);
    };
  }
};
var defaultRetryConfig = {
  maxRetries: 3,
  baseDelay: 100,
  maxDelay: 5e3,
  exponentialBackoff: true,
  jitter: true,
  retryableErrors: [
    ErrorCodes.DB_CONNECTION_FAILED,
    ErrorCodes.DB_TIMEOUT,
    ErrorCodes.DB_DEADLOCK,
    ErrorCodes.SERVICE_TIMEOUT,
    ErrorCodes.SERVICE_UNAVAILABLE
  ]
};
var errorHandler = new ErrorHandlerManager();

// src/errors/catalog.ts
var MasterErrorCatalog = {
  // Security Errors (1xxx)
  AUTH_INVALID_TOKEN: {
    code: "E1001",
    status: 401,
    message: "Invalid authentication token provided.",
    remediation: "Please refresh your token or log in again.",
    category: "Security"
  },
  AUTH_EXPIRED_TOKEN: {
    code: "E1002",
    status: 401,
    message: "Authentication token has expired.",
    remediation: "Obtain a new token via the refresh endpoint.",
    category: "Security"
  },
  AUTH_INSUFFICIENT_PERMISSIONS: {
    code: "E1003",
    status: 403,
    message: "Insufficient permissions to perform this action.",
    remediation: "Contact your administrator to request access.",
    category: "Security"
  },
  // Validation Errors (2xxx)
  VALIDATION_BAD_INPUT: {
    code: "E2001",
    status: 400,
    message: "The provided input is invalid.",
    remediation: "Check the input parameters and try again.",
    category: "Validation"
  },
  // Resource Errors (3xxx)
  RESOURCE_NOT_FOUND: {
    code: "E3001",
    status: 404,
    message: "The requested resource was not found.",
    remediation: "Verify the resource ID and ensure it exists.",
    category: "Resource"
  },
  // System Errors (9xxx)
  INTERNAL_SERVER_ERROR: {
    code: "E9001",
    status: 500,
    message: "An internal server error occurred.",
    remediation: "Please try again later or contact support.",
    category: "System"
  }
};

// src/errors/canonical.ts
var CanonicalError = class extends AppError2 {
  remediation;
  constructor(key, details) {
    const def = MasterErrorCatalog[key];
    let category = "operational" /* OPERATIONAL */;
    if (def.category === "System") category = "programming" /* PROGRAMMING */;
    if (def.category === "Security") category = "security" /* SECURITY */;
    super(def.message, def.code, {
      statusCode: def.status,
      category,
      severity: def.status >= 500 ? "error" /* ERROR */ : "warn" /* WARN */,
      context: { metadata: details }
    });
    this.remediation = def.remediation;
  }
  toJSON() {
    return {
      ...super.toJSON(),
      remediation: this.remediation
    };
  }
};

// src/routes/authRoutes.ts
var router29 = Router12();
var authService3 = new AuthService();
var passwordResetService2 = new PasswordResetService();
var registerSchema2 = z18.object({
  email: z18.string().email("Invalid email address"),
  password: z18.string().min(8, "Password must be at least 8 characters").regex(/[A-Z]/, "Password must contain at least one uppercase letter").regex(/[a-z]/, "Password must contain at least one lowercase letter").regex(/[0-9]/, "Password must contain at least one number").regex(/[^A-Za-z0-9]/, "Password must contain at least one special character"),
  username: z18.string().min(3, "Username must be at least 3 characters").max(50, "Username must be at most 50 characters").regex(/^[a-zA-Z0-9_-]+$/, "Username can only contain letters, numbers, underscores, and hyphens").optional(),
  firstName: z18.string().min(1, "First name is required").max(100),
  lastName: z18.string().min(1, "Last name is required").max(100)
});
var loginSchema2 = z18.object({
  email: z18.string().email("Invalid email address"),
  password: z18.string().min(1, "Password is required")
});
var refreshTokenSchema = z18.object({
  refreshToken: z18.string().min(1, "Refresh token is required")
});
var passwordResetRequestSchema = z18.object({
  email: z18.string().email("Invalid email address")
});
var passwordResetSchema = z18.object({
  token: z18.string().min(1, "Reset token is required"),
  password: z18.string().min(8, "Password must be at least 8 characters").regex(/[A-Z]/, "Password must contain at least one uppercase letter").regex(/[a-z]/, "Password must contain at least one lowercase letter").regex(/[0-9]/, "Password must contain at least one number").regex(/[^A-Za-z0-9]/, "Password must contain at least one special character")
});
var changePasswordSchema2 = z18.object({
  currentPassword: z18.string().min(1, "Current password is required"),
  newPassword: z18.string().min(8, "Password must be at least 8 characters").regex(/[A-Z]/, "Password must contain at least one uppercase letter").regex(/[a-z]/, "Password must contain at least one lowercase letter").regex(/[0-9]/, "Password must contain at least one number").regex(/[^A-Za-z0-9]/, "Password must contain at least one special character")
});
function validateBody(schema2) {
  return (req, res, next) => {
    try {
      const result2 = schema2.safeParse(req.body);
      if (!result2.success) {
        const errors = result2.error.errors.map((e) => ({
          field: e.path.join("."),
          message: e.message
        }));
        return res.status(400).json({
          error: "Validation failed",
          code: "VALIDATION_ERROR",
          details: errors
        });
      }
      req.body = result2.data;
      next();
    } catch (error) {
      return res.status(400).json({
        error: "Invalid request body",
        code: "INVALID_JSON"
      });
    }
  };
}
router29.post(
  "/register",
  registerRateLimiter,
  validateBody(registerSchema2),
  async (req, res) => {
    try {
      const { email, password, username, firstName, lastName } = req.body;
      const result2 = await authService3.register({
        email,
        password,
        username: username || email.split("@")[0],
        firstName,
        lastName,
        role: "VIEWER"
        // Default role for new registrations
      });
      logger_default2.info({
        message: "User registered successfully",
        userId: result2.user.id,
        email: result2.user.email,
        ip: req.ip
      });
      res.status(201).json({
        message: "Registration successful",
        user: {
          id: result2.user.id,
          email: result2.user.email,
          username: result2.user.username,
          firstName: result2.user.firstName,
          lastName: result2.user.lastName,
          role: result2.user.role,
          createdAt: result2.user.createdAt
        },
        token: result2.token,
        refreshToken: result2.refreshToken,
        expiresIn: result2.expiresIn
      });
    } catch (error) {
      logger_default2.error({
        message: "Registration failed",
        error: error instanceof Error ? error.message : String(error),
        email: req.body.email,
        ip: req.ip
      });
      if (error instanceof Error && error.message.includes("already exists")) {
        return res.status(409).json({
          error: "User with this email or username already exists",
          code: "USER_EXISTS"
        });
      }
      res.status(500).json({
        error: "Registration failed",
        code: "REGISTRATION_FAILED"
      });
    }
  }
);
router29.post(
  "/login",
  loginRateLimiter,
  validateBody(loginSchema2),
  async (req, res) => {
    try {
      const { email, password } = req.body;
      const result2 = await authService3.login(
        email,
        password,
        req.ip,
        req.get("User-Agent")
      );
      logger_default2.info({
        message: "User logged in successfully",
        userId: result2.user.id,
        email: result2.user.email,
        ip: req.ip
      });
      res.json({
        message: "Login successful",
        user: {
          id: result2.user.id,
          email: result2.user.email,
          username: result2.user.username,
          firstName: result2.user.firstName,
          lastName: result2.user.lastName,
          fullName: result2.user.fullName,
          role: result2.user.role,
          lastLogin: result2.user.lastLogin
        },
        token: result2.token,
        refreshToken: result2.refreshToken,
        expiresIn: result2.expiresIn
      });
    } catch (error) {
      logger_default2.warn({
        message: "Login failed",
        error: error instanceof Error ? error.message : String(error),
        email: req.body.email,
        ip: req.ip
      });
      res.status(401).json({
        error: "Invalid credentials",
        code: "INVALID_CREDENTIALS"
      });
    }
  }
);
router29.post(
  "/refresh",
  authRateLimiter,
  validateBody(refreshTokenSchema),
  async (req, res) => {
    try {
      const { refreshToken } = req.body;
      const result2 = await authService3.refreshAccessToken(refreshToken);
      if (!result2) {
        throw new CanonicalError("AUTH_EXPIRED_TOKEN");
      }
      logger_default2.info({
        message: "Token refreshed successfully",
        ip: req.ip
      });
      res.json({
        message: "Token refreshed successfully",
        token: result2.token,
        refreshToken: result2.refreshToken
      });
    } catch (error) {
      logger_default2.error({
        message: "Token refresh failed",
        error: error instanceof Error ? error.message : String(error),
        ip: req.ip
      });
      res.status(401).json({
        error: "Token refresh failed",
        code: "REFRESH_FAILED"
      });
    }
  }
);
router29.post(
  "/logout",
  ensureAuthenticated,
  async (req, res) => {
    try {
      const user = req.user;
      const token = req.headers.authorization?.replace("Bearer ", "");
      await authService3.logout(user?.id, token);
      logger_default2.info({
        message: "User logged out successfully",
        userId: user?.id,
        ip: req.ip
      });
      res.json({
        message: "Logout successful"
      });
    } catch (error) {
      logger_default2.error({
        message: "Logout failed",
        error: error instanceof Error ? error.message : String(error),
        ip: req.ip
      });
      res.status(500).json({
        error: "Logout failed",
        code: "LOGOUT_FAILED"
      });
    }
  }
);
router29.get(
  "/me",
  ensureAuthenticated,
  async (req, res) => {
    try {
      const user = req.user;
      res.json({
        user: {
          id: user.id,
          email: user.email,
          username: user.username,
          firstName: user.firstName,
          lastName: user.lastName,
          fullName: user.fullName,
          role: user.role,
          isActive: user.isActive,
          lastLogin: user.lastLogin,
          createdAt: user.createdAt,
          updatedAt: user.updatedAt
        }
      });
    } catch (error) {
      logger_default2.error({
        message: "Failed to get user profile",
        error: error instanceof Error ? error.message : String(error),
        ip: req.ip
      });
      res.status(500).json({
        error: "Failed to get user profile",
        code: "PROFILE_FETCH_FAILED"
      });
    }
  }
);
router29.post(
  "/password/reset-request",
  passwordResetRateLimiter,
  validateBody(passwordResetRequestSchema),
  async (req, res) => {
    try {
      const { email } = req.body;
      await passwordResetService2.requestPasswordReset(
        email,
        req.ip,
        req.get("User-Agent")
      );
      logger_default2.info({
        message: "Password reset requested",
        email,
        ip: req.ip
      });
      res.json({
        message: "If an account with that email exists, a password reset link has been sent"
      });
    } catch (error) {
      logger_default2.error({
        message: "Password reset request failed",
        error: error instanceof Error ? error.message : String(error),
        ip: req.ip
      });
      res.json({
        message: "If an account with that email exists, a password reset link has been sent"
      });
    }
  }
);
router29.post(
  "/password/reset",
  passwordResetRateLimiter,
  validateBody(passwordResetSchema),
  async (req, res) => {
    try {
      const { token, password } = req.body;
      await passwordResetService2.resetPassword(token, password);
      logger_default2.info({
        message: "Password reset successful",
        ip: req.ip
      });
      res.json({
        message: "Password reset successful. Please log in with your new password."
      });
    } catch (error) {
      logger_default2.warn({
        message: "Password reset failed",
        error: error instanceof Error ? error.message : String(error),
        ip: req.ip
      });
      if (error instanceof Error && (error.message.includes("expired") || error.message.includes("invalid"))) {
        return res.status(400).json({
          error: "Invalid or expired reset token",
          code: "INVALID_RESET_TOKEN"
        });
      }
      res.status(500).json({
        error: "Password reset failed",
        code: "RESET_FAILED"
      });
    }
  }
);
router29.post(
  "/password/change",
  ensureAuthenticated,
  validateBody(changePasswordSchema2),
  async (req, res) => {
    try {
      const user = req.user;
      const { currentPassword, newPassword } = req.body;
      await passwordResetService2.changePassword(
        user?.id,
        currentPassword,
        newPassword
      );
      logger_default2.info({
        message: "Password changed successfully",
        userId: user.id,
        ip: req.ip
      });
      res.json({
        message: "Password changed successfully"
      });
    } catch (error) {
      logger_default2.warn({
        message: "Password change failed",
        error: error instanceof Error ? error.message : String(error),
        userId: req.user?.id,
        ip: req.ip
      });
      if (error instanceof Error && error.message.includes("incorrect")) {
        return res.status(400).json({
          error: "Current password is incorrect",
          code: "INCORRECT_PASSWORD"
        });
      }
      res.status(500).json({
        error: "Password change failed",
        code: "CHANGE_FAILED"
      });
    }
  }
);
router29.get(
  "/verify-token",
  async (req, res) => {
    try {
      const token = req.headers.authorization?.replace("Bearer ", "");
      if (!token) {
        return res.status(401).json({
          valid: false,
          error: "No token provided",
          code: "NO_TOKEN"
        });
      }
      const user = await authService3.verifyToken(token);
      if (!user) {
        return res.status(401).json({
          valid: false,
          error: "Invalid or expired token",
          code: "INVALID_TOKEN"
        });
      }
      res.json({
        valid: true,
        user: {
          id: user.id,
          email: user.email,
          role: user.role
        }
      });
    } catch (error) {
      res.status(401).json({
        valid: false,
        error: "Token verification failed",
        code: "VERIFICATION_FAILED"
      });
    }
  }
);
router29.post(
  "/revoke-token",
  ensureAuthenticated,
  async (req, res) => {
    try {
      const { token } = req.body;
      const user = req.user;
      if (!token) {
        return res.status(400).json({
          error: "Token is required",
          code: "TOKEN_REQUIRED"
        });
      }
      await authService3.revokeToken(token);
      logger_default2.info({
        message: "Token revoked successfully",
        userId: user.id,
        ip: req.ip
      });
      res.json({
        message: "Token revoked successfully"
      });
    } catch (error) {
      logger_default2.error({
        message: "Token revocation failed",
        error: error instanceof Error ? error.message : String(error),
        ip: req.ip
      });
      res.status(500).json({
        error: "Token revocation failed",
        code: "REVOCATION_FAILED"
      });
    }
  }
);
var authRoutes_default = router29;

// src/routes/sso.ts
init_async_handler();
import { Router as Router13 } from "express";

// src/services/TenantService.ts
init_database();
init_logger2();
init_ledger();
init_GAEnrollmentService();
init_metrics3();
import { z as z19 } from "zod";
import { randomUUID as randomUUID28, createHash as createHash19 } from "crypto";
var createTenantBaseSchema = z19.object({
  name: z19.string().min(2).max(100),
  slug: z19.string().min(3).max(50).regex(/^[a-z0-9-]+$/, "Slug must be lowercase alphanumeric with hyphens"),
  residency: z19.enum(["US", "EU"]),
  region: z19.string().optional()
});
var createTenantSchema = createTenantBaseSchema.transform((data) => {
  if (!data.region) {
    data.region = data.residency === "EU" ? "eu-central-1" : "us-east-1";
  }
  return data;
}).refine((data) => {
  if (data.residency === "EU" && !data.region.startsWith("eu-")) {
    return false;
  }
  if (data.residency === "US" && !data.region.startsWith("us-")) {
    return false;
  }
  return true;
}, {
  message: "Region must match residency (e.g., 'eu-central-1' for 'EU')",
  path: ["region"]
});
var SETTINGS_HISTORY_KEY = "settings_history";
var SETTINGS_HISTORY_LIMIT = 10;
function sanitizeSettingsSnapshot(settings) {
  const snapshot = { ...settings || {} };
  delete snapshot[SETTINGS_HISTORY_KEY];
  return snapshot;
}
function buildSettingsWithHistory(currentSettings, updates, actorId, reason) {
  const sanitizedCurrent = sanitizeSettingsSnapshot(currentSettings);
  const history = Array.isArray(currentSettings?.[SETTINGS_HISTORY_KEY]) ? currentSettings?.[SETTINGS_HISTORY_KEY] : [];
  const historyEntry = {
    id: randomUUID28(),
    timestamp: (/* @__PURE__ */ new Date()).toISOString(),
    actorId,
    reason,
    settings: sanitizedCurrent
  };
  const nextHistory = [historyEntry, ...history].slice(0, SETTINGS_HISTORY_LIMIT);
  const sanitizedUpdates = { ...updates };
  delete sanitizedUpdates[SETTINGS_HISTORY_KEY];
  return {
    settings: {
      ...sanitizedCurrent,
      ...sanitizedUpdates,
      [SETTINGS_HISTORY_KEY]: nextHistory
    },
    historyEntry
  };
}
function buildRollbackSettings(currentSettings, actorId, reason) {
  const history = Array.isArray(currentSettings?.[SETTINGS_HISTORY_KEY]) ? currentSettings?.[SETTINGS_HISTORY_KEY] : [];
  if (!history.length) {
    throw new Error("No rollback history available");
  }
  const [latest, ...rest] = history;
  const sanitizedCurrent = sanitizeSettingsSnapshot(currentSettings);
  const rollbackEntry = {
    id: randomUUID28(),
    timestamp: (/* @__PURE__ */ new Date()).toISOString(),
    actorId,
    reason,
    settings: sanitizedCurrent
  };
  const nextHistory = [rollbackEntry, ...rest].slice(0, SETTINGS_HISTORY_LIMIT);
  return {
    settings: {
      ...latest.settings || {},
      [SETTINGS_HISTORY_KEY]: nextHistory
    },
    rolledBackTo: latest
  };
}
var TenantService = class _TenantService {
  static instance;
  metrics;
  constructor() {
    this.metrics = new PrometheusMetrics("summit_tenancy");
    this.metrics.createHistogram(
      "tenant_creation_duration_seconds",
      "Time taken to create a tenant",
      ["residency", "tier"]
    );
  }
  static getInstance() {
    if (!_TenantService.instance) {
      _TenantService.instance = new _TenantService();
    }
    return _TenantService.instance;
  }
  /**
   * Create a new tenant with self-serve guardrails
   */
  async createTenant(input, actorId) {
    const start = process.hrtime();
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      const validated = createTenantSchema.parse(input);
      const enrollmentCheck = await GAEnrollmentService_default.checkTenantEnrollmentEligibility(validated.region);
      if (!enrollmentCheck.eligible) {
        throw new Error(`Tenant creation rejected: ${enrollmentCheck.reason}`);
      }
      await client6.query("BEGIN");
      const existing = await client6.query("SELECT * FROM tenants WHERE slug = $1", [validated.slug]);
      if (existing.rowCount && existing.rowCount > 0) {
        const existingTenant = this.mapRowToTenant(existing.rows[0]);
        if (existingTenant.createdBy === actorId) {
          logger_default2.info(`Idempotent creation for tenant ${existingTenant.slug} by user ${actorId}`);
          await client6.query("ROLLBACK");
          return existingTenant;
        }
        throw new Error(`Tenant slug '${validated.slug}' is already taken.`);
      }
      const defaults = {
        tier: "starter",
        status: "active",
        config: {
          features: {
            sso: false,
            audit_logs: false
          },
          security: {
            mfa_enforced: false
          }
        },
        settings: {
          theme: "light"
        }
      };
      const tenantId = randomUUID28();
      const insertQuery = `
        INSERT INTO tenants (
          id, name, slug, residency, region, tier, status, config, settings, created_by
        ) VALUES (
          $1, $2, $3, $4, $5, $6, $7, $8, $9, $10
        ) RETURNING *
      `;
      const result2 = await client6.query(insertQuery, [
        tenantId,
        validated.name,
        validated.slug,
        validated.residency,
        validated.region,
        defaults.tier,
        defaults.status,
        defaults.config,
        defaults.settings,
        actorId
      ]);
      const tenant = this.mapRowToTenant(result2.rows[0]);
      logger_default2.info(`Initialized quotas for tenant ${tenantId} (Tier: starter)`);
      await provenanceLedger.appendEntry({
        action: "TENANT_CREATED",
        actor: {
          id: actorId || "system",
          role: "admin"
        },
        metadata: {
          tenantId: tenant.id,
          residency: tenant.residency,
          tier: tenant.tier
        },
        artifacts: []
      });
      await client6.query(
        "UPDATE users SET tenant_id = $1, role = $2 WHERE id = $3",
        [tenantId, "ADMIN", actorId]
      );
      await client6.query("COMMIT");
      logger_default2.info(`Tenant created successfully: ${tenant.slug} (${tenant.id})`);
      const [seconds, nanoseconds] = process.hrtime(start);
      const duration = seconds + nanoseconds / 1e9;
      this.metrics.observeHistogram("tenant_creation_duration_seconds", {
        residency: tenant.residency,
        tier: tenant.tier
      }, duration);
      return tenant;
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Failed to create tenant:", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  /**
   * Get tenant by ID
   */
  async getTenant(id) {
    const pool4 = getPostgresPool2();
    const result2 = await pool4.query("SELECT * FROM tenants WHERE id = $1", [id]);
    if (result2.rows.length === 0) return null;
    return this.mapRowToTenant(result2.rows[0]);
  }
  /**
   * Get tenant by Slug
   */
  async getTenantBySlug(slug) {
    const pool4 = getPostgresPool2();
    const result2 = await pool4.query("SELECT * FROM tenants WHERE slug = $1", [slug]);
    if (result2.rows.length === 0) return null;
    return this.mapRowToTenant(result2.rows[0]);
  }
  async listTenants(limit = 100, offset = 0) {
    const pool4 = getPostgresPool2();
    const result2 = await pool4.query("SELECT * FROM tenants ORDER BY created_at DESC LIMIT $1 OFFSET $2", [limit, offset]);
    return result2.rows.map(this.mapRowToTenant);
  }
  async updateSettings(tenantId, settings, actorId, reason = "settings_update") {
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      await client6.query("BEGIN");
      const existing = await client6.query("SELECT * FROM tenants WHERE id = $1", [tenantId]);
      if (!existing.rowCount) {
        throw new Error("Tenant not found");
      }
      const { settings: mergedSettings } = buildSettingsWithHistory(
        existing.rows[0].settings || {},
        settings,
        actorId,
        reason
      );
      const result2 = await client6.query(
        "UPDATE tenants SET settings = $1, updated_at = NOW() WHERE id = $2 RETURNING *",
        [mergedSettings, tenantId]
      );
      const tenant = this.mapRowToTenant(result2.rows[0]);
      await provenanceLedger.appendEntry({
        action: "TENANT_SETTINGS_UPDATED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          updatedKeys: Object.keys(settings),
          reason,
          settingsHash: createHash19("sha256").update(JSON.stringify(mergedSettings)).digest("hex")
        },
        artifacts: []
      });
      await client6.query("COMMIT");
      return tenant;
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Failed to update tenant settings", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  async disableTenant(tenantId, actorId, reason) {
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      await client6.query("BEGIN");
      const existing = await client6.query("SELECT * FROM tenants WHERE id = $1", [tenantId]);
      if (!existing.rowCount) {
        throw new Error("Tenant not found");
      }
      const current = this.mapRowToTenant(existing.rows[0]);
      if (current.status === "disabled") {
        await client6.query("ROLLBACK");
        return current;
      }
      const enrichedConfig = {
        ...current.config || {},
        lifecycle: {
          ...current.config?.lifecycle || {},
          disabledAt: (/* @__PURE__ */ new Date()).toISOString(),
          reason,
          actorId
        }
      };
      const result2 = await client6.query(
        "UPDATE tenants SET status = $1, config = $2, updated_at = NOW() WHERE id = $3 RETURNING *",
        ["disabled", enrichedConfig, tenantId]
      );
      const tenant = this.mapRowToTenant(result2.rows[0]);
      await provenanceLedger.appendEntry({
        action: "TENANT_DISABLED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          previousStatus: current.status,
          reason
        },
        artifacts: []
      });
      await client6.query("COMMIT");
      return tenant;
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Failed to disable tenant", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  async rollbackSettings(tenantId, actorId, reason = "settings_rollback") {
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      await client6.query("BEGIN");
      const existing = await client6.query("SELECT * FROM tenants WHERE id = $1", [tenantId]);
      if (!existing.rowCount) {
        throw new Error("Tenant not found");
      }
      const { settings: mergedSettings, rolledBackTo } = buildRollbackSettings(
        existing.rows[0].settings || {},
        actorId,
        reason
      );
      const result2 = await client6.query(
        "UPDATE tenants SET settings = $1, updated_at = NOW() WHERE id = $2 RETURNING *",
        [mergedSettings, tenantId]
      );
      const tenant = this.mapRowToTenant(result2.rows[0]);
      await provenanceLedger.appendEntry({
        action: "TENANT_SETTINGS_ROLLBACK",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          reason,
          rolledBackTo: rolledBackTo.id
        },
        artifacts: []
      });
      await client6.query("COMMIT");
      return { tenant, rolledBackTo };
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Failed to rollback tenant settings", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  async getTenantSettings(id) {
    const tenant = await this.getTenant(id);
    if (!tenant) {
      throw new Error("Tenant not found");
    }
    return {
      id: tenant.id,
      settings: tenant.settings,
      config: tenant.config,
      status: tenant.status
    };
  }
  mapRowToTenant(row) {
    return {
      id: row.id,
      name: row.name,
      slug: row.slug,
      region: row.region,
      residency: row.residency,
      tier: row.tier,
      status: row.status,
      config: row.config,
      settings: row.settings,
      createdBy: row.created_by,
      createdAt: row.created_at,
      updatedAt: row.updated_at
    };
  }
};
var tenantService = TenantService.getInstance();

// src/services/SSOService.ts
init_AuthService();

// src/services/sso/types.ts
var SAML = class {
  constructor(_config) {
  }
  getAuthorizeUrl(_options) {
    throw new Error("SAML library not implemented");
  }
  validatePostResponse(_body) {
    throw new Error("SAML library not implemented");
  }
  validatePostResponseAsync(_request) {
    throw new Error("SAML library not implemented");
  }
};

// src/services/sso/SAMLProvider.ts
var SAMLProvider = class {
  saml;
  config;
  constructor(config9) {
    this.config = config9;
    this.saml = new SAML({
      entryPoint: config9.entryPoint,
      issuer: config9.issuerString || config9.issuer,
      cert: config9.cert,
      callbackUrl: config9.callbackUrl || "",
      privateKey: config9.privateKey,
      decryptionPvk: config9.decryptionPvk,
      signatureAlgorithm: config9.signatureAlgorithm
    });
  }
  validateConfig() {
    if (!this.config.entryPoint) throw new Error("Missing entryPoint for SAML");
    if (!this.config.cert) throw new Error("Missing cert for SAML");
  }
  async generateAuthUrl(callbackUrl, relayState) {
    return this.saml.getAuthorizeUrl({ RelayState: relayState });
  }
  async handleCallback(callbackUrl, body4, query3) {
    const validatePostResponse = await this.saml.validatePostResponseAsync({ body: body4 });
    const { profile } = validatePostResponse;
    if (!profile) {
      throw new Error("SAML validation failed: No profile returned");
    }
    const email = profile.email || profile.nameID || profile["urn:oid:0.9.2342.19200300.100.1.3"];
    if (!email) {
      throw new Error("SAML validation failed: No email found in assertion");
    }
    const attributes = profile.attributes || {};
    const firstName = attributes[this.config.attributeMap?.firstName || "firstName"] || attributes["http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname"];
    const lastName = attributes[this.config.attributeMap?.lastName || "lastName"] || attributes["http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surname"];
    let groups = [];
    const groupAttr = this.config.attributeMap?.groups || "groups";
    const rawGroups = attributes[groupAttr] || attributes["http://schemas.xmlsoap.org/claims/Group"];
    if (Array.isArray(rawGroups)) {
      groups = rawGroups.map((g2) => String(g2));
    } else if (typeof rawGroups === "string") {
      groups = [rawGroups];
    }
    const roles = [];
    if (this.config.groupMap) {
      for (const group of groups) {
        const mapped = this.config.groupMap[group];
        if (mapped) roles.push(...mapped);
      }
    }
    if (roles.length === 0) roles.push("VIEWER");
    return {
      id: String(profile.nameID),
      email: String(email),
      firstName: firstName ? String(firstName) : void 0,
      lastName: lastName ? String(lastName) : void 0,
      groups,
      roles: Array.from(new Set(roles)),
      provider: "saml",
      attributes: profile
    };
  }
};

// src/services/sso/OIDCProvider.ts
init_logger2();
import axios3 from "axios";
import jwt5 from "jsonwebtoken";
import crypto24 from "crypto";
var OIDCProvider = class {
  config;
  jwksCache = /* @__PURE__ */ new Map();
  constructor(config9) {
    this.config = config9;
  }
  validateConfig() {
    if (!this.config.issuer) throw new Error("Missing issuer for OIDC");
    if (!this.config.clientId) throw new Error("Missing clientId for OIDC");
    if (!this.config.authorizationEndpoint) throw new Error("Missing authorizationEndpoint for OIDC");
    if (!this.config.tokenEndpoint) throw new Error("Missing tokenEndpoint for OIDC");
  }
  async generateAuthUrl(callbackUrl, relayState) {
    const params = new URLSearchParams({
      response_type: "code",
      client_id: this.config.clientId,
      redirect_uri: callbackUrl,
      scope: "openid profile email",
      state: relayState || "",
      nonce: crypto24.randomBytes(16).toString("hex")
    });
    return `${this.config.authorizationEndpoint}?${params.toString()}`;
  }
  async handleCallback(callbackUrl, body4, query3) {
    const code = query3.code || body4.code;
    if (!code) throw new Error("No authorization code provided");
    const tokenParams = new URLSearchParams();
    tokenParams.append("grant_type", "authorization_code");
    tokenParams.append("client_id", this.config.clientId);
    if (this.config.clientSecret) {
      tokenParams.append("client_secret", this.config.clientSecret);
    }
    tokenParams.append("code", code);
    tokenParams.append("redirect_uri", callbackUrl);
    const tokenResponse = await axios3.post(this.config.tokenEndpoint, tokenParams.toString(), {
      headers: { "Content-Type": "application/x-www-form-urlencoded" }
    });
    const { access_token, id_token } = tokenResponse.data;
    if (!id_token) throw new Error("No ID token returned");
    let decoded;
    if (this.config.jwksUri) {
      const jwks = await this.getJWKS(this.config.jwksUri);
      decoded = jwt5.verify(id_token, jwks, {
        algorithms: ["RS256"],
        audience: this.config.clientId,
        issuer: this.config.issuer
      });
    } else {
      decoded = jwt5.decode(id_token);
    }
    if (!decoded) throw new Error("Failed to decode ID token");
    let userInfo = {};
    if (this.config.userInfoEndpoint && access_token) {
      try {
        const uiRes = await axios3.get(this.config.userInfoEndpoint, {
          headers: { Authorization: `Bearer ${access_token}` }
        });
        userInfo = uiRes.data;
      } catch (e) {
        logger_default2.warn("Failed to fetch user info", e);
      }
    }
    const email = decoded.email || userInfo.email;
    const firstName = decoded.given_name || userInfo.given_name;
    const lastName = decoded.family_name || userInfo.family_name;
    let groups = [];
    const rawGroups = decoded.groups || userInfo.groups;
    if (Array.isArray(rawGroups)) {
      groups = rawGroups.map((g2) => String(g2));
    }
    const roles = [];
    if (this.config.groupMap) {
      for (const group of groups) {
        const mapped = this.config.groupMap[group];
        if (mapped) roles.push(...mapped);
      }
    }
    if (roles.length === 0) roles.push("VIEWER");
    return {
      id: decoded.sub,
      email,
      firstName,
      lastName,
      groups,
      roles: Array.from(new Set(roles)),
      provider: "oidc",
      attributes: { ...decoded, ...userInfo }
    };
  }
  async getJWKS(jwksUri) {
    if (this.jwksCache.has(jwksUri)) {
      return this.jwksCache.get(jwksUri);
    }
    const response = await axios3.get(jwksUri);
    const jwks = response.data;
    const getKey = (header, callback) => {
      const key = jwks.keys.find((k) => k.kid === header.kid);
      if (key) {
        callback(null, this.jwkToPem(key));
      } else {
        callback(new Error("Key not found"));
      }
    };
    this.jwksCache.set(jwksUri, getKey);
    return getKey;
  }
  // Simplified JWK to PEM (RSA only)
  jwkToPem(jwk) {
    if (jwk.x5c && jwk.x5c.length > 0) {
      return `-----BEGIN CERTIFICATE-----
${jwk.x5c[0]}
-----END CERTIFICATE-----`;
    }
    try {
      const key = crypto24.createPublicKey({ key: jwk, format: "jwk" });
      return key.export({ type: "spki", format: "pem" });
    } catch (e) {
      logger_default2.error("Failed to convert JWK to PEM", e);
      throw new Error("Failed to convert JWK");
    }
  }
};

// src/services/SSOService.ts
init_database();
import crypto25 from "crypto";
var SSOService = class {
  tenantService;
  authService;
  constructor() {
    this.tenantService = TenantService.getInstance();
    this.authService = new AuthService();
  }
  async getAuthUrl(tenantId, callbackBaseUrl) {
    const tenant = await this.tenantService.getTenant(tenantId);
    if (!tenant) throw new Error("Tenant not found");
    const ssoConfig = tenant.config.sso;
    if (!ssoConfig) throw new Error("SSO not configured for this tenant");
    const provider = this.getProvider(ssoConfig);
    const callbackUrl = `${callbackBaseUrl}/auth/sso/${tenantId}/callback`;
    const state = crypto25.randomBytes(32).toString("hex");
    const url = await provider.generateAuthUrl(callbackUrl, state);
    return { url, state };
  }
  async handleCallback(tenantId, callbackBaseUrl, body4, query3) {
    const tenant = await this.tenantService.getTenant(tenantId);
    if (!tenant) throw new Error("Tenant not found");
    const ssoConfig = tenant.config.sso;
    if (!ssoConfig) throw new Error("SSO not configured for this tenant");
    const provider = this.getProvider(ssoConfig);
    const callbackUrl = `${callbackBaseUrl}/auth/sso/${tenantId}/callback`;
    const ssoUser = await provider.handleCallback(callbackUrl, body4, query3);
    return this.provisionUser(ssoUser, tenantId);
  }
  getProvider(config9) {
    if (config9.type === "saml") {
      return new SAMLProvider(config9);
    } else if (config9.type === "oidc") {
      return new OIDCProvider(config9);
    }
    throw new Error(`Unsupported SSO type: ${config9.type}`);
  }
  async provisionUser(ssoUser, tenantId) {
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      await client6.query("BEGIN");
      const existing = await client6.query("SELECT * FROM users WHERE email = $1", [ssoUser.email]);
      let user;
      if (existing.rows.length > 0) {
        const updateQuery = `
          UPDATE users SET
            first_name = COALESCE($1, first_name),
            last_name = COALESCE($2, last_name),
            last_login = NOW(),
            tenant_id = $3
          WHERE email = $4
          RETURNING *
        `;
        const res = await client6.query(updateQuery, [ssoUser.firstName, ssoUser.lastName, tenantId, ssoUser.email]);
        user = res.rows[0];
      } else {
        const insertQuery = `
          INSERT INTO users (
            id, email, password_hash, first_name, last_name, role, is_active, tenant_id, created_at, updated_at
          ) VALUES (
            $1, $2, 'sso_placeholder', $3, $4, $5, true, $6, NOW(), NOW()
          ) RETURNING *
        `;
        const role = (ssoUser.roles || []).includes("ADMIN") ? "ADMIN" : (ssoUser.roles || []).includes("ANALYST") ? "ANALYST" : "VIEWER";
        const res = await client6.query(insertQuery, [
          crypto25.randomUUID(),
          ssoUser.email,
          ssoUser.firstName,
          ssoUser.lastName,
          role,
          tenantId
        ]);
        user = res.rows[0];
      }
      await client6.query(
        `INSERT INTO user_tenants (user_id, tenant_id, roles)
         VALUES ($1, $2, $3)
         ON CONFLICT (user_id, tenant_id) DO UPDATE SET roles = $3`,
        [user.id, tenantId, ssoUser.roles]
      );
      await client6.query("COMMIT");
      const { token, refreshToken } = await this.authService.generateTokens(user, client6);
      return {
        user: {
          id: user.id,
          email: user.email,
          firstName: user.first_name,
          lastName: user.last_name,
          role: user.role,
          tenantId: user.tenant_id
        },
        token,
        refreshToken
      };
    } catch (e) {
      await client6.query("ROLLBACK");
      throw e;
    } finally {
      client6.release();
    }
  }
};

// src/routes/sso.ts
init_rateLimit2();
init_auth4();
init_logger2();
init_config3();
import { z as z20 } from "zod";
var router30 = Router13();
var singleParam5 = (value) => Array.isArray(value) ? value[0] : value ?? "";
var ssoService = new SSOService();
var ssoConfigSchema = z20.object({
  type: z20.enum(["oidc", "saml"]),
  name: z20.string(),
  // OIDC
  issuer: z20.string().optional(),
  clientId: z20.string().optional(),
  clientSecret: z20.string().optional(),
  authorizationEndpoint: z20.string().optional(),
  tokenEndpoint: z20.string().optional(),
  userInfoEndpoint: z20.string().optional(),
  jwksUri: z20.string().optional(),
  // SAML
  entryPoint: z20.string().optional(),
  issuerString: z20.string().optional(),
  cert: z20.string().optional(),
  // Mapping
  groupMap: z20.record(z20.array(z20.string())).optional(),
  attributeMap: z20.object({
    email: z20.string().optional(),
    firstName: z20.string().optional(),
    lastName: z20.string().optional(),
    groups: z20.string().optional()
  }).optional()
});
router30.post("/tenants/:id/sso", ensureAuthenticated, rateLimitMiddleware, asyncHandler(async (req, res) => {
  const id = singleParam5(req.params.id);
  if (req.user.role !== "ADMIN") {
    return res.status(403).json({ error: "Unauthorized: Admin role required" });
  }
  if (req.user.tenantId !== id) {
    return res.status(403).json({ error: "Unauthorized: Access restricted to tenant members" });
  }
  const validated = ssoConfigSchema.parse(req.body);
  const tenant = await tenantService.getTenant(id);
  if (!tenant) return res.status(404).json({ error: "Tenant not found" });
  const newConfig = {
    ...tenant.config,
    sso: validated
  };
  const { getPostgresPool: getPostgresPool3 } = await Promise.resolve().then(() => (init_database(), database_exports));
  const pool4 = getPostgresPool3();
  await pool4.query("UPDATE tenants SET config = $1 WHERE id = $2", [newConfig, id]);
  logger_default2.info(`Updated SSO config for tenant ${id} by user ${req.user.id}`);
  return res.json({ success: true, config: validated });
}));
router30.get("/auth/sso/:tenantId/login", rateLimitMiddleware, asyncHandler(async (req, res) => {
  const tenantId = singleParam5(req.params.tenantId);
  const baseUrl = `${req.protocol}://${req.get("host")}`;
  try {
    const { url, state } = await ssoService.getAuthUrl(tenantId, config_default.baseUrl || baseUrl);
    res.cookie("sso_state", state, {
      httpOnly: true,
      secure: process.env.NODE_ENV === "production",
      sameSite: "lax",
      maxAge: 300 * 1e3
      // 5 minutes
    });
    return res.redirect(url);
  } catch (e) {
    logger_default2.error("SSO Login Error", e);
    return res.status(400).json({ error: e.message });
  }
}));
router30.post("/auth/sso/:tenantId/callback", rateLimitMiddleware, asyncHandler(async (req, res) => {
  const tenantId = singleParam5(req.params.tenantId);
  const baseUrl = `${req.protocol}://${req.get("host")}`;
  const stateCookie = req.cookies["sso_state"];
  const stateParam = req.body.RelayState || req.body.state || req.query.state || req.query.RelayState;
  if (!stateCookie || !stateParam || stateCookie !== stateParam) {
    logger_default2.warn(`SSO State mismatch or missing. Cookie: ${stateCookie ? "present" : "missing"}, Param: ${stateParam ? "present" : "missing"}`);
    return res.status(403).send("Authentication failed: State mismatch (CSRF protection)");
  }
  res.clearCookie("sso_state");
  try {
    const { user, token, refreshToken } = await ssoService.handleCallback(tenantId, config_default.baseUrl || baseUrl, req.body, req.query);
    res.cookie("access_token", token, {
      httpOnly: true,
      secure: process.env.NODE_ENV === "production",
      maxAge: 24 * 60 * 60 * 1e3,
      // 24h
      sameSite: "lax"
    });
    res.cookie("refresh_token", refreshToken, {
      httpOnly: true,
      secure: process.env.NODE_ENV === "production",
      maxAge: 7 * 24 * 60 * 60 * 1e3,
      // 7d
      sameSite: "lax"
    });
    return res.redirect(`${process.env.FRONTEND_URL || "http://localhost:3000"}/dashboard`);
  } catch (e) {
    logger_default2.error("SSO Callback Error", e);
    return res.status(401).send(`Authentication failed: ${e.message}`);
  }
}));
router30.get("/auth/sso/:tenantId/callback", rateLimitMiddleware, asyncHandler(async (req, res) => {
  const tenantId = singleParam5(req.params.tenantId);
  const baseUrl = `${req.protocol}://${req.get("host")}`;
  const stateCookie = req.cookies["sso_state"];
  const stateParam = req.query.state || req.query.RelayState;
  if (!stateCookie || !stateParam || stateCookie !== stateParam) {
    logger_default2.warn(`SSO State mismatch or missing. Cookie: ${stateCookie ? "present" : "missing"}, Param: ${stateParam ? "present" : "missing"}`);
    return res.status(403).send("Authentication failed: State mismatch (CSRF protection)");
  }
  res.clearCookie("sso_state");
  try {
    const { user, token, refreshToken } = await ssoService.handleCallback(tenantId, config_default.baseUrl || baseUrl, req.body, req.query);
    res.cookie("access_token", token, {
      httpOnly: true,
      secure: process.env.NODE_ENV === "production",
      maxAge: 24 * 60 * 60 * 1e3,
      sameSite: "lax"
    });
    res.cookie("refresh_token", refreshToken, {
      httpOnly: true,
      secure: process.env.NODE_ENV === "production",
      maxAge: 7 * 24 * 60 * 60 * 1e3,
      sameSite: "lax"
    });
    return res.redirect(`${process.env.FRONTEND_URL || "http://localhost:3000"}/dashboard`);
  } catch (e) {
    logger_default2.error("SSO Callback Error", e);
    return res.status(401).send(`Authentication failed: ${e.message}`);
  }
}));
var sso_default = router30;

// src/routes/qaf.ts
import express18 from "express";

// src/qaf/pki.ts
import * as crypto26 from "crypto";
var PKIService = class _PKIService {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_PKIService.instance) {
      _PKIService.instance = new _PKIService();
    }
    return _PKIService.instance;
  }
  /**
   * Issues a new quantum-safe identity for an agent.
   * This generates a key pair and a simulated X.509 certificate.
   */
  async issueIdentity(config9) {
    const { publicKey, privateKey } = crypto26.generateKeyPairSync("rsa", {
      modulusLength: 2048
    });
    const cert = `-----BEGIN CERTIFICATE-----
(Simulated mTLS Cert for ${config9.name})
...PQC_SIGNATURE...
-----END CERTIFICATE-----`;
    return {
      id: crypto26.randomUUID(),
      publicKey: publicKey.export({ type: "spki", format: "pem" }),
      certificate: cert,
      expiry: new Date(Date.now() + 24 * 60 * 60 * 1e3),
      // 24h validity
      quantumSafe: config9.securityLevel === "quantum-secure"
    };
  }
  /**
   * Revokes an agent's identity.
   */
  async revokeIdentity(agentId) {
    console.log(`[PKI] Revoking identity for agent ${agentId} due to drift or expiry.`);
    return true;
  }
  /**
   * Validates an agent's certificate.
   */
  async validateIdentity(cert) {
    return cert.includes("BEGIN CERTIFICATE");
  }
};

// src/qaf/telemetry.ts
init_metrics3();
var ROITelemetry = class _ROITelemetry {
  static instance;
  promMetrics;
  metrics = {
    velocityGain: 0,
    contextSwitchesReduced: 0,
    complianceScore: 100,
    tasksCompleted: 0,
    uptime: 100
  };
  constructor() {
    this.promMetrics = new PrometheusMetrics("summit_qaf");
    this.promMetrics.createGauge("velocity_gain", "Percentage gain in development velocity");
    this.promMetrics.createGauge("context_switch_reduction", "Percentage reduction in context switches");
    this.promMetrics.createGauge("compliance_score", "Current compliance score (0-100)");
    this.promMetrics.createCounter("tasks_completed", "Total number of agent tasks completed");
    this.promMetrics.createGauge("secure_agents", "Number of quantum-secure agents");
    this.promMetrics.createGauge("total_agents", "Total number of active agents");
  }
  static getInstance() {
    if (!_ROITelemetry.instance) {
      _ROITelemetry.instance = new _ROITelemetry();
    }
    return _ROITelemetry.instance;
  }
  recordTaskCompletion(durationMs, success) {
    this.metrics.tasksCompleted++;
    this.promMetrics.incrementCounter("tasks_completed");
    this.metrics.velocityGain = Math.min(15, this.metrics.velocityGain + 0.1);
    this.promMetrics.setGauge("velocity_gain", this.metrics.velocityGain);
  }
  updateContextSwitches(reduction) {
    this.metrics.contextSwitchesReduced = reduction;
    this.promMetrics.setGauge("context_switch_reduction", reduction);
  }
  recordComplianceCheck(passed) {
    if (!passed) {
      this.metrics.complianceScore = Math.max(0, this.metrics.complianceScore - 5);
    } else {
      this.metrics.complianceScore = Math.min(100, this.metrics.complianceScore + 1);
    }
    this.promMetrics.setGauge("compliance_score", this.metrics.complianceScore);
  }
  updateAgentCounts(total, secure) {
    this.promMetrics.setGauge("total_agents", total);
    this.promMetrics.setGauge("secure_agents", secure);
  }
  getMetrics() {
    return { ...this.metrics };
  }
  async generateReport() {
    return JSON.stringify(this.metrics, null, 2);
  }
};

// src/qaf/factory.ts
var SummitQAF = class {
  pki;
  telemetry;
  agents;
  constructor() {
    this.pki = PKIService.getInstance();
    this.telemetry = ROITelemetry.getInstance();
    this.agents = /* @__PURE__ */ new Map();
  }
  /**
   * Spawns a new secure agent with mTLS identity.
   */
  async spawnAgent(config9) {
    console.log(`[QAF] Spawning agent: ${config9.name} (${config9.role})`);
    const identity = await this.pki.issueIdentity(config9);
    this.agents.set(identity.id, identity);
    this.telemetry.recordTaskCompletion(0, true);
    if (config9.role === "GovEnforcer") {
      this.telemetry.recordComplianceCheck(true);
    }
    this.updateAgentCounts();
    return identity;
  }
  updateAgentCounts() {
    let secureCount = 0;
    for (const agent of this.agents.values()) {
      if (agent.quantumSafe) secureCount++;
    }
    this.telemetry.updateAgentCounts(this.agents.size, secureCount);
  }
  /**
   * Simulates a quantum security scan across all agents.
   */
  async runQuantumScan() {
    const vulnerable = [];
    for (const [id, agent] of this.agents) {
      if (!agent.quantumSafe) {
        vulnerable.push(id);
      }
    }
    const secure = vulnerable.length === 0;
    if (!secure) {
      console.warn(`[QAF] Quantum vulnerability detected in ${vulnerable.length} agents.`);
      this.telemetry.recordComplianceCheck(false);
    } else {
      this.telemetry.recordComplianceCheck(true);
    }
    return { secure, vulnerableAgents: vulnerable };
  }
  getTelemetry() {
    return this.telemetry.getMetrics();
  }
};

// src/routes/qaf.ts
var router31 = express18.Router();
var factory = new SummitQAF();
router31.post("/spawn", async (req, res) => {
  try {
    const config9 = req.body;
    if (!config9.securityLevel) {
      config9.securityLevel = "quantum-secure";
    }
    const identity = await factory.spawnAgent(config9);
    res.json(identity);
  } catch (error) {
    res.status(500).json({ error: "Failed to spawn agent" });
  }
});
router31.get("/telemetry", (req, res) => {
  res.json(factory.getTelemetry());
});
router31.post("/scan", async (req, res) => {
  const result2 = await factory.runQuantumScan();
  res.json(result2);
});
var qaf_default = router31;

// src/routes/siem-platform.ts
import { Router as Router14 } from "express";

// src/siem/rules.ts
var defaultRules = [
  {
    id: "brute-force-login",
    name: "Brute Force Login Detection",
    description: "Detects multiple failed login attempts from the same IP",
    severity: "high",
    enabled: true,
    windowSeconds: 60,
    threshold: 5,
    conditions: [
      { field: "eventType", operator: "equals", value: "login_failed" }
    ],
    actions: [{ type: "alert" }]
  },
  {
    id: "root-access",
    name: "Root/Admin Access",
    description: "Detects access to sensitive admin endpoints",
    severity: "critical",
    enabled: true,
    windowSeconds: 10,
    threshold: 1,
    conditions: [
      { field: "eventType", operator: "equals", value: "admin_access" }
    ],
    actions: [{ type: "alert" }]
  },
  {
    id: "impossible-travel",
    name: "Impossible Travel",
    description: "Detects logins from different locations in short timeframe",
    severity: "high",
    enabled: true,
    windowSeconds: 300,
    threshold: 1,
    // Logic handled in engine for this specific type, or simulated via events
    conditions: [
      { field: "eventType", operator: "equals", value: "impossible_travel_detected" }
    ],
    actions: [{ type: "alert" }, { type: "block_user" }]
  }
];

// src/siem/AnomalyDetector.ts
var AnomalyDetector = class {
  userEventCounts = /* @__PURE__ */ new Map();
  ipEventCounts = /* @__PURE__ */ new Map();
  THRESHOLD = 50;
  // Arbitrary threshold for "high velocity" in this session
  /**
   * Tracks an event and returns an anomaly score if high.
   */
  trackAndScore(event) {
    let anomaly = null;
    if (event.userId) {
      const count = (this.userEventCounts.get(event.userId) || 0) + 1;
      this.userEventCounts.set(event.userId, count);
      if (count > this.THRESHOLD) {
        anomaly = {
          entityId: event.userId,
          score: Math.min(count / this.THRESHOLD, 1),
          factors: [`High event velocity for user (${count} events)`],
          timestamp: /* @__PURE__ */ new Date()
        };
      }
    }
    if (event.ipAddress) {
      const count = (this.ipEventCounts.get(event.ipAddress) || 0) + 1;
      this.ipEventCounts.set(event.ipAddress, count);
      if (count > this.THRESHOLD) {
        const score = Math.min(count / this.THRESHOLD, 1);
        if (!anomaly || score > anomaly.score) {
          anomaly = {
            entityId: event.ipAddress,
            score,
            factors: [`High event velocity for IP (${count} events)`],
            timestamp: /* @__PURE__ */ new Date()
          };
        }
      }
    }
    return anomaly;
  }
  reset() {
    this.userEventCounts.clear();
    this.ipEventCounts.clear();
  }
};
var anomalyDetector = new AnomalyDetector();

// src/siem/SIEMPlatform.ts
init_logger2();
import { randomUUID as randomUUID30 } from "crypto";
var SIEMPlatform = class {
  events = [];
  alerts = [];
  rules = defaultRules;
  // In-memory buffer for correlation (sliding window)
  eventBuffer = [];
  // Storage limit for in-memory prototype
  MAX_EVENTS = 1e4;
  MAX_ALERTS = 1e3;
  constructor() {
    const timer3 = setInterval(() => this.cleanupBuffer(), 6e4);
    if (timer3.unref) {
      timer3.unref();
    }
  }
  async ingestEvent(eventData) {
    const event = {
      id: eventData.id || randomUUID30(),
      timestamp: eventData.timestamp || /* @__PURE__ */ new Date(),
      eventType: eventData.eventType || "unknown",
      source: eventData.source || "unknown",
      severity: eventData.severity || "low",
      message: eventData.message || "",
      details: eventData.details || {},
      userId: eventData.userId,
      tenantId: eventData.tenantId,
      ipAddress: eventData.ipAddress,
      userAgent: eventData.userAgent,
      tags: eventData.tags || []
    };
    this.storeEvent(event);
    await this.correlate(event);
    const anomaly = anomalyDetector.trackAndScore(event);
    if (anomaly) {
      await this.createAlert({
        ruleId: "anomaly-detection",
        severity: "medium",
        title: "Anomaly Detected",
        description: `Anomaly detected for ${anomaly.entityId}: ${anomaly.factors.join(", ")}`,
        events: [event],
        tenantId: event.tenantId
      });
    }
    return event;
  }
  storeEvent(event) {
    this.events.push(event);
    this.eventBuffer.push(event);
    if (this.events.length > this.MAX_EVENTS) {
      this.events.shift();
    }
  }
  async correlate(newEvent) {
    for (const rule of this.rules) {
      if (!rule.enabled) continue;
      const windowStart = new Date(Date.now() - rule.windowSeconds * 1e3);
      const relevantEvents = this.eventBuffer.filter(
        (e) => e.timestamp >= windowStart && this.matchesConditions(e, rule.conditions)
      );
      const groupBy = newEvent.ipAddress || newEvent.userId || "global";
      const eventsForGroup = relevantEvents.filter(
        (e) => e.ipAddress === groupBy || e.userId === groupBy || groupBy === "global"
      );
      if (eventsForGroup.length >= rule.threshold) {
        const recentAlert = this.alerts.find(
          (a) => a.ruleId === rule.id && a.timestamp > windowStart && // Rudimentary check to see if it's the same entity
          a.description.includes(groupBy)
        );
        if (!recentAlert) {
          await this.createAlert({
            ruleId: rule.id,
            severity: rule.severity,
            title: rule.name,
            description: `${rule.description}. Detected ${eventsForGroup.length} events for ${groupBy}`,
            events: eventsForGroup,
            tenantId: newEvent.tenantId
          });
        }
      }
    }
  }
  matchesConditions(event, conditions) {
    return conditions.every((condition) => {
      const eventRecord = event;
      const val = eventRecord[condition.field];
      if (val === void 0) return false;
      switch (condition.operator) {
        case "equals":
          return val === condition.value;
        case "contains":
          return String(val).includes(String(condition.value));
        case "regex":
          return new RegExp(String(condition.value)).test(String(val));
        default:
          return false;
      }
    });
  }
  async createAlert(alertData) {
    const alert = {
      id: randomUUID30(),
      ruleId: alertData.ruleId,
      severity: alertData.severity || "medium",
      title: alertData.title,
      description: alertData.description,
      timestamp: /* @__PURE__ */ new Date(),
      events: alertData.events || [],
      status: "new",
      tenantId: alertData.tenantId
    };
    this.alerts.push(alert);
    if (this.alerts.length > this.MAX_ALERTS) {
      this.alerts.shift();
    }
    logger_default2.warn("SIEM Alert Triggered", { alert });
  }
  cleanupBuffer() {
    const cutoff = new Date(Date.now() - 3600 * 1e3);
    this.eventBuffer = this.eventBuffer.filter((e) => e.timestamp > cutoff);
  }
  // Public Query Methods
  getEvents(_filter) {
    return this.events.slice().reverse();
  }
  getAlerts(_filter) {
    return this.alerts.slice().reverse();
  }
  reset() {
    this.events = [];
    this.alerts = [];
    this.eventBuffer = [];
    anomalyDetector.reset();
  }
};
var siemPlatform = new SIEMPlatform();

// src/routes/siem-platform.ts
init_auth4();
var router32 = Router14();
router32.use(authMiddleware);
router32.post("/ingest", async (req, res, next) => {
  try {
    const event = await siemPlatform.ingestEvent({
      ...req.body,
      tenantId: req.user?.tenantId || req.body.tenantId,
      source: req.body.source || "api"
    });
    res.json({ success: true, eventId: event.id });
  } catch (err) {
    next(err);
  }
});
router32.get("/alerts", (req, res) => {
  const alerts = siemPlatform.getAlerts({});
  res.json({ data: alerts });
});
router32.get("/events", (req, res) => {
  const events = siemPlatform.getEvents({});
  res.json({ data: events });
});
router32.get("/compliance/report", (req, res) => {
  const report = {
    generatedAt: /* @__PURE__ */ new Date(),
    complianceScore: 95,
    violations: [],
    status: "compliant"
  };
  res.json(report);
});
var siem_platform_default = router32;

// src/routes/maestro.ts
init_auth4();
init_async_handler();
import express19 from "express";

// src/maestro/workflow-service.ts
import { v4 as uuidv49 } from "uuid";
var WorkflowService = class _WorkflowService {
  static instance;
  queryRunner;
  constructor(queryRunner) {
    this.queryRunner = queryRunner;
  }
  static getInstance(queryRunner = runCypher) {
    if (!_WorkflowService.instance) {
      _WorkflowService.instance = new _WorkflowService(queryRunner);
    }
    return _WorkflowService.instance;
  }
  // Allow resetting for tests
  static resetInstance(queryRunner = runCypher) {
    _WorkflowService.instance = new _WorkflowService(queryRunner);
  }
  async createDefinition(tenantId, input) {
    const id = uuidv49();
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const query3 = `
      CREATE (w:WorkflowDefinition:Entity {
        id: $id,
        tenantId: $tenantId,
        version: $version,
        env: $env,
        retentionClass: $retentionClass,
        costCenter: $costCenter,
        inputSchema: $inputSchema,
        outputSchema: $outputSchema,
        body: $body,
        createdAt: $now,
        updatedAt: $now,
        owner: $tenantId,
        name: 'Workflow ' + $version,
        description: 'Workflow Definition'
      })
      RETURN w
    `;
    const params = {
      id,
      tenantId,
      version: input.version,
      env: input.env,
      retentionClass: input.retentionClass,
      costCenter: input.costCenter,
      inputSchema: input.inputSchema,
      outputSchema: input.outputSchema || "",
      body: input.body,
      now
    };
    const result2 = await this.queryRunner(query3, params, { tenantId, write: true });
    const record2 = result2[0];
    return record2 && record2.w ? record2.w.properties : { id, ...input, tenantId, createdAt: now, updatedAt: now };
  }
  async getDefinition(tenantId, id) {
    const query3 = `
      MATCH (w:WorkflowDefinition {id: $id, tenantId: $tenantId})
      RETURN w
    `;
    const result2 = await this.queryRunner(query3, { id, tenantId });
    if (result2.length === 0) return null;
    const record2 = result2[0];
    return record2.w ? record2.w.properties : record2;
  }
};
var workflowService = WorkflowService.getInstance();

// src/maestro/run-service.ts
import { v4 as uuidv411 } from "uuid";

// src/maestro/provenance/ReceiptService.ts
import { createSign as createSign3, createHash as createHash20, generateKeyPairSync as generateKeyPairSync3 } from "crypto";
import { v4 as uuidv410 } from "uuid";
var KMS = class {
  privateKey;
  publicKey;
  kid;
  constructor() {
    const { privateKey, publicKey } = generateKeyPairSync3("rsa", {
      modulusLength: 2048,
      publicKeyEncoding: { type: "spki", format: "pem" },
      privateKeyEncoding: { type: "pkcs8", format: "pem" }
    });
    this.privateKey = privateKey;
    this.publicKey = publicKey;
    this.kid = "dev-key-1";
  }
  sign(data) {
    const sign4 = createSign3("SHA256");
    sign4.update(data);
    sign4.end();
    return sign4.sign(this.privateKey, "base64");
  }
  getPublicKey() {
    return this.publicKey;
  }
};
var kms = new KMS();
var ReceiptService2 = class _ReceiptService {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_ReceiptService.instance) {
      _ReceiptService.instance = new _ReceiptService();
    }
    return _ReceiptService.instance;
  }
  generateReceipt(tenantId, action, actorId, resourceId, inputPayload, policyDecisionId) {
    const timestamp = (/* @__PURE__ */ new Date()).toISOString();
    const inputStr = JSON.stringify(inputPayload);
    const inputHash = createHash20("sha256").update(inputStr).digest("hex");
    const receiptData = {
      tenantId,
      action,
      actorId,
      resourceId,
      inputHash,
      policyDecisionId,
      timestamp
    };
    const digest = createHash20("sha256").update(JSON.stringify(receiptData)).digest("hex");
    const signature = kms.sign(digest);
    return {
      id: uuidv410(),
      timestamp,
      digest,
      signature,
      kid: kms.kid
    };
  }
};
var receiptService = ReceiptService2.getInstance();

// src/maestro/metering-service.ts
init_cost_meter();
var mockIgClient = {
  recordCostSample: async () => {
  },
  getRunCostSummary: async () => ({})
};
var pricingTable = {
  "openai:gpt-4": { inputPer1K: 0.03, outputPer1K: 0.06 },
  "openai:gpt-3.5-turbo": { inputPer1K: 15e-4, outputPer1K: 2e-3 }
};
var MeteringService2 = class _MeteringService {
  static instance;
  costMeter;
  constructor() {
    this.costMeter = new CostMeter(mockIgClient, pricingTable);
  }
  static getInstance() {
    if (!_MeteringService.instance) {
      _MeteringService.instance = new _MeteringService();
    }
    return _MeteringService.instance;
  }
  async trackRunUsage(tenantId, runId, units) {
    console.log(`[Metering] Tenant ${tenantId} Run ${runId}: ${units} run_units`);
  }
  async trackStepUsage(tenantId, runId, stepId, usage) {
    await this.costMeter.record(runId, stepId, usage, { tenantId });
  }
};
var meteringService2 = MeteringService2.getInstance();

// src/maestro/run-service.ts
var RunService = class _RunService {
  static instance;
  queryRunner;
  receiptService;
  meteringService;
  constructor(queryRunner, receiptService2, meteringService3) {
    this.queryRunner = queryRunner;
    this.receiptService = receiptService2;
    this.meteringService = meteringService3;
  }
  static getInstance(queryRunner = runCypher, receiptService2 = receiptService, meteringService3 = meteringService2) {
    if (!_RunService.instance) {
      _RunService.instance = new _RunService(queryRunner, receiptService2, meteringService3);
    }
    return _RunService.instance;
  }
  static resetInstance(queryRunner = runCypher, receiptService2 = receiptService, meteringService3 = meteringService2) {
    _RunService.instance = new _RunService(queryRunner, receiptService2, meteringService3);
  }
  async createRun(tenantId, workflowId, inputPayload, principalId, env2) {
    const runId = uuidv411();
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const policyDecisionId = uuidv411();
    await this.recordPolicyDecision(tenantId, policyDecisionId, "ALLOW", { action: "start_run", resource: workflowId });
    const receipt = this.receiptService.generateReceipt(
      tenantId,
      "run.start",
      principalId,
      runId,
      JSON.parse(inputPayload),
      policyDecisionId
    );
    const query3 = `
      MATCH (w:WorkflowDefinition {id: $workflowId})
      CREATE (r:Run:Entity {
        id: $runId,
        tenantId: $tenantId,
        status: 'PENDING',
        startedAt: $now,
        input: $input,
        env: $env,
        createdAt: $now,
        updatedAt: $now,
        owner: $principalId,
        name: 'Run ' + $runId,
        description: 'Execution of ' + w.name
      })
      CREATE (r)-[:DEFINED_BY]->(w)
      CREATE (rcpt:Receipt:BaseNode {
        id: $receiptId,
        digest: $digest,
        signature: $signature,
        kid: $kid,
        createdAt: $now,
        updatedAt: $now,
        owner: $principalId
      })
      CREATE (r)-[:LOGGED_IN]->(rcpt)
      CREATE (pd:PolicyDecision:BaseNode {
        id: $policyDecisionId,
        outcome: 'ALLOW',
        policyVersion: 'v1',
        inputHash: 'simulated_hash',
        evaluationLog: '{}',
        createdAt: $now,
        updatedAt: $now,
        owner: 'system'
      })
      CREATE (r)-[:SUBJECT_TO]->(pd)
      RETURN r
    `;
    const params = {
      workflowId,
      runId,
      tenantId,
      input: inputPayload,
      env: env2,
      now,
      receiptId: receipt.id,
      digest: receipt.digest,
      signature: receipt.signature,
      kid: receipt.kid,
      policyDecisionId
    };
    const result2 = await this.queryRunner(query3, params, { tenantId, write: true });
    try {
      await this.meteringService.trackRunUsage(tenantId, runId, 1);
    } catch (e) {
      console.warn("Metering failed but ignoring for now:", e);
    }
    const record2 = result2[0];
    const runNode = record2 && record2.r ? record2.r.properties : { id: runId, status: "PENDING", ...record2?.r };
    return {
      ...runNode,
      workflowId,
      receipts: [receipt]
    };
  }
  async recordPolicyDecision(tenantId, id, outcome, details) {
  }
};
var runService = RunService.getInstance();

// src/routes/maestro.ts
init_policy_client();

// src/maestro/api-types.ts
import { z as z21 } from "zod";
var WorkflowDefinitionSchema = z21.object({
  version: z21.string(),
  env: z21.string(),
  retentionClass: z21.string(),
  costCenter: z21.string(),
  inputSchema: z21.string(),
  // JSON string
  outputSchema: z21.string().optional(),
  // JSON string
  body: z21.string()
});
var StartRunSchema = z21.object({
  workflowId: z21.string(),
  input: z21.string(),
  // JSON string
  env: z21.string().optional(),
  reasoningBudget: z21.object({
    thinkMode: z21.enum(["off", "normal", "heavy"]).optional(),
    thinkingBudget: z21.number().int().nonnegative().optional(),
    maxTokens: z21.number().int().nonnegative().optional(),
    toolBudget: z21.number().int().nonnegative().optional(),
    timeBudgetMs: z21.number().int().nonnegative().optional(),
    redactionPolicy: z21.enum(["none", "summary_only"]).optional()
  }).optional()
});
var ApprovalSchema = z21.object({
  decision: z21.enum(["APPROVE", "REJECT"]),
  rationale: z21.string(),
  stepId: z21.string().optional()
});

// src/routes/maestro.ts
var router33 = express19.Router({ mergeParams: true });
router33.post("/tenants/:tenantId/workflows", ensureAuthenticated, asyncHandler(async (req, res) => {
  const { tenantId } = req.params;
  if (req.user?.tenantId !== tenantId && !req.user?.roles?.includes("admin")) {
    return res.status(403).json({ error: "Unauthorized access to tenant" });
  }
  const input = WorkflowDefinitionSchema.parse(req.body);
  const workflow = await workflowService.createDefinition(tenantId, input);
  res.status(201).json(workflow);
}));
router33.post("/tenants/:tenantId/runs", ensureAuthenticated, asyncHandler(async (req, res) => {
  const { tenantId } = req.params;
  if (req.user?.tenantId !== tenantId && !req.user?.roles?.includes("admin")) {
    return res.status(403).json({ error: "Unauthorized access to tenant" });
  }
  const policyResult = await policyClient.evaluate({
    action: "start_run",
    user: req.user,
    resource: { tenantId }
  });
  if (!policyResult.allowed) {
    return res.status(403).json({ error: "Policy denied: " + policyResult.reason });
  }
  const input = StartRunSchema.parse(req.body);
  const run = await runService.createRun(
    tenantId,
    input.workflowId,
    input.input,
    req.user.id,
    input.env || "dev"
  );
  res.status(201).json(run);
}));
router33.get("/tenants/:tenantId/runs/:runId", ensureAuthenticated, asyncHandler(async (req, res) => {
  const { tenantId, runId } = req.params;
  if (req.user?.tenantId !== tenantId && !req.user?.roles?.includes("admin")) {
    return res.status(403).json({ error: "Unauthorized access to tenant" });
  }
  res.json({
    id: runId,
    tenantId,
    workflowId: "wf-stub",
    status: "PENDING",
    input: "{}",
    receipts: []
  });
}));
router33.post("/tenants/:tenantId/runs/:runId/approve", ensureAuthenticated, asyncHandler(async (req, res) => {
  const { tenantId, runId } = req.params;
  if (req.user?.tenantId !== tenantId && !req.user?.roles?.includes("admin")) {
    return res.status(403).json({ error: "Unauthorized access to tenant" });
  }
  const input = ApprovalSchema.parse(req.body);
  const policyResult = await policyClient.evaluate({
    action: "approve_run",
    user: req.user,
    resource: { tenantId, runId }
  });
  if (!policyResult.allowed) {
    return res.status(403).json({ error: "Policy denied: " + policyResult.reason });
  }
  res.json({ status: "PROCESSED", decision: input.decision });
}));
router33.get("/tenants/:tenantId/runs/:runId/events", ensureAuthenticated, asyncHandler(async (req, res) => {
  const { tenantId, runId } = req.params;
  if (req.user?.tenantId !== tenantId && !req.user?.roles?.includes("admin")) {
    return res.status(403).json({ error: "Unauthorized access to tenant" });
  }
  res.json([
    { id: "evt-1", type: "run.created", timestamp: (/* @__PURE__ */ new Date()).toISOString(), payload: "{}" }
  ]);
}));
var maestro_default = router33;

// src/routes/mcp-apps.ts
init_logger();
import { Router as Router15 } from "express";
var router34 = Router15();
var INVESTIGATION_CANVAS_HTML = `
<!DOCTYPE html>
<html>
<head>
  <style>
    body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; padding: 20px; background: #f8fafc; color: #334155; }
    h2 { color: #0f172a; margin-bottom: 20px; }
    .canvas {
      border: 1px solid #e2e8f0;
      background: white;
      height: 400px;
      display: flex;
      align-items: center;
      justify-content: center;
      border-radius: 8px;
      box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
      margin-bottom: 20px;
    }
    .controls { display: flex; gap: 10px; }
    button {
      padding: 10px 16px;
      border: 1px solid #cbd5e1;
      border-radius: 6px;
      background: white;
      color: #334155;
      cursor: pointer;
      transition: all 0.2s;
      font-weight: 500;
    }
    button:hover { background: #f1f5f9; border-color: #94a3b8; }
    button.primary { background: #2563eb; color: white; border-color: #2563eb; }
    button.primary:hover { background: #1d4ed8; border-color: #1d4ed8; }

    .status {
        margin-top: 20px;
        padding: 10px;
        background: #f1f5f9;
        border-radius: 4px;
        font-family: monospace;
        font-size: 0.9em;
        border: 1px solid #e2e8f0;
    }
  </style>
</head>
<body>
  <h2>Investigation Canvas</h2>
  <div class="canvas" id="canvas-area">
    <div style="text-align: center">
        <svg width="64" height="64" viewBox="0 0 24 24" fill="none" stroke="#94a3b8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="margin-bottom: 10px"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
        <p>Graph Visualization Placeholder</p>
    </div>
  </div>
  <div class="controls">
    <button onclick="expandGraph()">Expand Neighborhood</button>
    <button onclick="filterNodes()">Filter: High Risk</button>
    <button class="primary" onclick="requestApproval()">Request Approval</button>
  </div>
  <div class="status" id="status">Ready</div>

  <script>
    function sendRpc(method, params) {
      const id = Date.now();
      const message = {
        jsonrpc: '2.0',
        id: id,
        method: method,
        params: params
      };
      // Send to parent window
      window.parent.postMessage(message, '*');
      document.getElementById('status').innerText = 'Sent: ' + method;
    }

    function expandGraph() {
      sendRpc('tool/call', { name: 'graph.expand', arguments: { depth: 1 } });
    }

    function filterNodes() {
      sendRpc('context/update', { filter: 'risk > 0.8' });
    }

    function requestApproval() {
      sendRpc('tool/call', { name: 'approval.request', arguments: { reason: 'Investigate suspicious IP' } });
    }

    window.addEventListener('message', event => {
      const data = event.data;
      if (data && (data.result || data.error)) {
         document.getElementById('status').innerText = 'Response: ' + JSON.stringify(data);
      }
    });
  </script>
</body>
</html>
`;
router34.get("/render", (req, res) => {
  const { uri, signature } = req.query;
  logger.info({
    audit: true,
    action: "mcp_app_render",
    resource: uri,
    user: req.user?.sub || "anonymous",
    signaturePresent: !!signature
  }, "Serving MCP App Resource");
  if (!signature && process.env.NODE_ENV === "production") {
    logger.warn({ uri }, "Missing signature for MCP App resource in production context");
  }
  if (uri === "ui://investigation-canvas/main") {
    res.setHeader("Content-Type", "text/html");
    res.send(INVESTIGATION_CANVAS_HTML);
  } else {
    logger.warn(`MCP UI Resource not found: ${uri}`);
    res.status(404).send("UI Resource not found");
  }
});
router34.post("/rpc", (req, res) => {
  res.json({ jsonrpc: "2.0", id: req.body.id, result: "ack" });
});
var mcp_apps_default = router34;

// src/routes/cases.ts
init_postgres();
init_CaseService();
import { randomUUID as randomUUID32 } from "node:crypto";
import { Router as Router16 } from "express";

// src/repos/CaseOverviewCacheRepo.ts
var CaseOverviewCacheRepo = class {
  constructor(pg5) {
    this.pg = pg5;
  }
  async get(caseId, tenantId) {
    const { rows } = await this.pg.query(
      `SELECT * FROM maestro.case_overview_cache WHERE case_id = $1 AND tenant_id = $2`,
      [caseId, tenantId]
    );
    if (!rows[0]) return null;
    return this.mapRow(rows[0]);
  }
  async upsert(snapshot) {
    const { rows } = await this.pg.query(
      `INSERT INTO maestro.case_overview_cache (
        case_id, tenant_id, entity_count, task_count, open_task_count, participant_count,
        transition_count, audit_event_count, top_entities, last_activity_at, refreshed_at, expires_at,
        refresh_status, hit_count, miss_count
      ) VALUES (
        $1, $2, $3, $4, $5, $6,
        $7, $8, $9, $10, $11, $12,
        $13, COALESCE($14, 0), COALESCE($15, 0)
      )
      ON CONFLICT (case_id, tenant_id) DO UPDATE SET
        entity_count = EXCLUDED.entity_count,
        task_count = EXCLUDED.task_count,
        open_task_count = EXCLUDED.open_task_count,
        participant_count = EXCLUDED.participant_count,
        transition_count = EXCLUDED.transition_count,
        audit_event_count = EXCLUDED.audit_event_count,
        top_entities = EXCLUDED.top_entities,
        last_activity_at = EXCLUDED.last_activity_at,
        refreshed_at = EXCLUDED.refreshed_at,
        expires_at = EXCLUDED.expires_at,
        refresh_status = EXCLUDED.refresh_status,
        hit_count = maestro.case_overview_cache.hit_count,
        miss_count = maestro.case_overview_cache.miss_count
      RETURNING *`,
      [
        snapshot.caseId,
        snapshot.tenantId,
        snapshot.entityCount,
        snapshot.taskCount,
        snapshot.openTaskCount,
        snapshot.participantCount,
        snapshot.transitionCount,
        snapshot.auditEventCount,
        JSON.stringify(snapshot.topEntities || []),
        snapshot.lastActivityAt,
        snapshot.refreshedAt,
        snapshot.expiresAt,
        snapshot.refreshStatus,
        snapshot.hitCount,
        snapshot.missCount
      ]
    );
    const row = rows[0];
    return this.mapRow({ ...row, hit_count: snapshot.hitCount ?? row.hit_count, miss_count: snapshot.missCount ?? row.miss_count });
  }
  async recordHit(caseId, tenantId) {
    await this.pg.query(
      `UPDATE maestro.case_overview_cache
       SET hit_count = hit_count + 1, last_hit_at = NOW()
       WHERE case_id = $1 AND tenant_id = $2`,
      [caseId, tenantId]
    );
  }
  async recordMiss(caseId, tenantId) {
    await this.pg.query(
      `UPDATE maestro.case_overview_cache
       SET miss_count = miss_count + 1, last_miss_at = NOW()
       WHERE case_id = $1 AND tenant_id = $2`,
      [caseId, tenantId]
    );
  }
  async markStale(caseId, tenantId) {
    await this.pg.query(
      `UPDATE maestro.case_overview_cache
       SET refresh_status = 'stale', expires_at = NOW()
       WHERE case_id = $1 AND tenant_id = $2`,
      [caseId, tenantId]
    );
  }
  async delete(caseId, tenantId) {
    await this.pg.query(`DELETE FROM maestro.case_overview_cache WHERE case_id = $1 AND tenant_id = $2`, [caseId, tenantId]);
  }
  async listCasesNeedingRefresh(limit = 50) {
    const { rows } = await this.pg.query(
      `SELECT case_id, tenant_id
       FROM maestro.case_overview_cache
       WHERE expires_at <= NOW() OR refresh_status = 'stale'
       ORDER BY expires_at ASC
       LIMIT $1`,
      [limit]
    );
    return rows.map((row) => ({ caseId: row.case_id, tenantId: row.tenant_id }));
  }
  async listAllCases(limit = 500) {
    const { rows } = await this.pg.query(
      `SELECT id, tenant_id FROM maestro.cases ORDER BY created_at DESC LIMIT $1`,
      [limit]
    );
    return rows.map((row) => ({ caseId: row.id, tenantId: row.tenant_id }));
  }
  mapRow(row) {
    const topEntities = Array.isArray(row.top_entities) ? row.top_entities : typeof row.top_entities === "string" ? JSON.parse(row.top_entities) : [];
    return {
      caseId: row.case_id,
      tenantId: row.tenant_id,
      entityCount: row.entity_count,
      taskCount: row.task_count,
      openTaskCount: row.open_task_count,
      participantCount: row.participant_count,
      transitionCount: row.transition_count,
      auditEventCount: row.audit_event_count,
      topEntities,
      lastActivityAt: row.last_activity_at,
      refreshedAt: row.refreshed_at,
      expiresAt: row.expires_at,
      refreshStatus: row.refresh_status,
      hitCount: row.hit_count,
      missCount: row.miss_count,
      stale: row.expires_at < /* @__PURE__ */ new Date() || row.refresh_status === "stale"
    };
  }
};

// src/cases/overview/CaseOverviewService.ts
init_logger();
var overviewLogger = logger_default.child({ name: "CaseOverviewService" });
var CaseOverviewService = class {
  constructor(pg5, options2) {
    this.pg = pg5;
    this.cacheRepo = new CaseOverviewCacheRepo(pg5);
    this.ttlMs = options2?.ttlMs ?? 5 * 60 * 1e3;
    this.staleWhileRevalidateMs = options2?.staleWhileRevalidateMs ?? 10 * 60 * 1e3;
  }
  cacheRepo;
  ttlMs;
  staleWhileRevalidateMs;
  inflight = /* @__PURE__ */ new Map();
  async getOverview(caseId, tenantId) {
    const cacheKey = `${tenantId}:${caseId}`;
    const cached = await this.cacheRepo.get(caseId, tenantId);
    const now = /* @__PURE__ */ new Date();
    if (cached) {
      await this.cacheRepo.recordHit(caseId, tenantId);
      if (cached.expiresAt > now) {
        return this.decorate(cached, "fresh");
      }
      const staleDeadline = new Date(cached.expiresAt.getTime() + this.staleWhileRevalidateMs);
      if (now <= staleDeadline) {
        this.triggerRevalidation(cacheKey, caseId, tenantId);
        return this.decorate({ ...cached, refreshStatus: "revalidating" }, "stale");
      }
    }
    const snapshot = await this.refresh(caseId, tenantId, cacheKey);
    await this.cacheRepo.recordMiss(caseId, tenantId);
    return this.decorate(snapshot, "miss");
  }
  async refresh(caseId, tenantId, cacheKey) {
    const key = cacheKey ?? `${tenantId}:${caseId}`;
    if (this.inflight.has(key)) {
      return this.inflight.get(key);
    }
    const promise = this.buildSnapshot(caseId, tenantId).then((snapshot) => this.cacheRepo.upsert(snapshot)).catch((error) => {
      overviewLogger.error({ error, caseId, tenantId }, "Failed to refresh case overview");
      throw error;
    }).finally(() => {
      this.inflight.delete(key);
    });
    this.inflight.set(key, promise);
    return promise;
  }
  async invalidate(caseId, tenantId) {
    await this.cacheRepo.delete(caseId, tenantId);
  }
  async rebuildAll(limit = 500) {
    const cases = await this.cacheRepo.listAllCases(limit);
    for (const entry of cases) {
      await this.refresh(entry.caseId, entry.tenantId);
    }
    return cases.length;
  }
  async refreshStale(limit = 50) {
    const targets = await this.cacheRepo.listCasesNeedingRefresh(limit);
    for (const target of targets) {
      await this.refresh(target.caseId, target.tenantId);
    }
    return targets.length;
  }
  async markStale(caseId, tenantId) {
    await this.cacheRepo.markStale(caseId, tenantId);
  }
  async triggerRevalidation(cacheKey, caseId, tenantId) {
    if (this.inflight.has(cacheKey)) return;
    void this.refresh(caseId, tenantId, cacheKey);
  }
  async buildSnapshot(caseId, tenantId) {
    const now = /* @__PURE__ */ new Date();
    const expiresAt = new Date(now.getTime() + this.ttlMs);
    const metricsRow = await this.pg.query(
      `SELECT
        (SELECT COUNT(*) FROM maestro.case_graph_references WHERE case_id = $1 AND is_active = true) AS entity_count,
        (SELECT COUNT(*) FROM maestro.case_tasks WHERE case_id = $1) AS task_count,
        (SELECT COUNT(*) FROM maestro.case_tasks WHERE case_id = $1 AND status NOT IN ('completed', 'cancelled')) AS open_task_count,
        (SELECT COUNT(*) FROM maestro.case_participants WHERE case_id = $1 AND is_active = true) AS participant_count,
        (SELECT COUNT(*) FROM maestro.case_state_history WHERE case_id = $1) AS transition_count,
        (SELECT COUNT(*) FROM maestro.audit_access_logs WHERE case_id = $1) AS audit_event_count,
        GREATEST(
          COALESCE((SELECT MAX(updated_at) FROM maestro.case_tasks WHERE case_id = $1), to_timestamp(0)),
          COALESCE((SELECT MAX(transitioned_at) FROM maestro.case_state_history WHERE case_id = $1), to_timestamp(0)),
          COALESCE((SELECT MAX(created_at) FROM maestro.audit_access_logs WHERE case_id = $1), to_timestamp(0)),
          COALESCE((SELECT MAX(added_at) FROM maestro.case_graph_references WHERE case_id = $1), to_timestamp(0))
        ) AS last_activity_at`,
      [caseId]
    );
    const metrics8 = metricsRow.rows[0] || {
      entity_count: 0,
      task_count: 0,
      open_task_count: 0,
      participant_count: 0,
      transition_count: 0,
      audit_event_count: 0,
      last_activity_at: null
    };
    const { rows: topEntityRows } = await this.pg.query(
      `SELECT graph_entity_id, entity_label, entity_type, relationship_type, COUNT(*) as count
       FROM maestro.case_graph_references
       WHERE case_id = $1 AND is_active = true
       GROUP BY graph_entity_id, entity_label, entity_type, relationship_type
       ORDER BY COUNT(*) DESC, entity_label ASC
       LIMIT 5`,
      [caseId]
    );
    return {
      caseId,
      tenantId,
      entityCount: Number(metrics8.entity_count || 0),
      taskCount: Number(metrics8.task_count || 0),
      openTaskCount: Number(metrics8.open_task_count || 0),
      participantCount: Number(metrics8.participant_count || 0),
      transitionCount: Number(metrics8.transition_count || 0),
      auditEventCount: Number(metrics8.audit_event_count || 0),
      topEntities: topEntityRows.map((row) => ({
        graphEntityId: row.graph_entity_id,
        entityLabel: row.entity_label,
        entityType: row.entity_type,
        relationshipType: row.relationship_type,
        count: Number(row.count)
      })),
      lastActivityAt: metrics8.last_activity_at && metrics8.last_activity_at.getTime() > 0 ? metrics8.last_activity_at : null,
      refreshedAt: now,
      expiresAt,
      refreshStatus: "fresh",
      hitCount: 0,
      missCount: 0,
      stale: false
    };
  }
  decorate(snapshot, status) {
    return {
      ...snapshot,
      cache: {
        status,
        hitCount: snapshot.hitCount,
        missCount: snapshot.missCount
      }
    };
  }
};

// src/routes/cases.ts
init_CommentService();

// src/cases/chain-of-custody.ts
init_logger();
init_ledger();
import { randomUUID as randomUUID31, createHash as createHash21, sign as sign3, verify as verify5 } from "node:crypto";
var moduleLogger = logger_default.child({ module: "chain-of-custody" });
var ChainOfCustodyService = class {
  pg;
  ledger;
  constructor(pg5) {
    this.pg = pg5;
    this.ledger = new ProvenanceLedgerV2();
  }
  async recordEvent(event) {
    const timestamp = /* @__PURE__ */ new Date();
    const id = randomUUID31();
    await this.pg.query(
      `INSERT INTO maestro.chain_of_custody_events
       (id, case_id, evidence_id, action, actor_id, timestamp, location, notes, verification_hash)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)`,
      [id, event.caseId, event.evidenceId, event.action, event.actorId, timestamp, event.location, event.notes, event.verificationHash]
    );
    await this.ledger.appendEntry({
      tenantId: "system",
      actionType: "CUSTODY_EVENT",
      resourceType: "Evidence",
      resourceId: event.evidenceId,
      actorId: event.actorId,
      actorType: "user",
      timestamp: /* @__PURE__ */ new Date(),
      payload: {
        mutationType: "UPDATE",
        entityId: event.evidenceId,
        entityType: "Evidence",
        caseId: event.caseId,
        action: event.action,
        location: event.location,
        notes: event.notes
      },
      metadata: {
        caseId: event.caseId,
        action: event.action,
        location: event.location,
        notes: event.notes
      }
    });
    moduleLogger.info({ eventId: id, evidenceId: event.evidenceId, action: event.action }, "Chain of custody event recorded");
    return {
      id,
      timestamp,
      ...event
    };
  }
  async getChain(evidenceId) {
    const result2 = await this.pg.query(
      `SELECT * FROM maestro.chain_of_custody_events WHERE evidence_id = $1 ORDER BY timestamp ASC`,
      [evidenceId]
    );
    return result2.rows.map((row) => ({
      id: row.id,
      caseId: row.case_id,
      evidenceId: row.evidence_id,
      action: row.action,
      actorId: row.actor_id,
      timestamp: row.timestamp,
      location: row.location,
      notes: row.notes,
      verificationHash: row.verification_hash
    }));
  }
  /**
   * List all evidence IDs associated with a case
   */
  async listEvidence(caseId) {
    const result2 = await this.pg.query(
      `SELECT DISTINCT evidence_id, MAX(timestamp) as last_update
       FROM maestro.chain_of_custody_events
       WHERE case_id = $1
       GROUP BY evidence_id`,
      [caseId]
    );
    return result2.rows.map((row) => ({
      id: row.evidence_id,
      lastUpdate: row.last_update
    }));
  }
  async verifyIntegrity(evidenceId) {
    return true;
  }
};

// src/reporting/service.ts
init_ledger();

// src/reporting/delivery-service.ts
import axios4 from "axios";
import nodemailer2 from "nodemailer";

// src/reporting/validation.ts
import { z as z22 } from "zod";
var deliveryChannelEnum = z22.enum(["email", "slack", "webhook"]);
var emailConfigSchema = z22.object({
  to: z22.array(z22.string().email()).nonempty("email recipients are required"),
  cc: z22.array(z22.string().email()).optional(),
  bcc: z22.array(z22.string().email()).optional(),
  subject: z22.string().optional(),
  body: z22.string().optional()
});
var slackConfigSchema = z22.object({
  webhookUrl: z22.string().url("slack webhook must be a valid URL"),
  text: z22.string().optional()
});
var webhookConfigSchema = z22.object({
  url: z22.string().url("webhook url must be a valid URL"),
  headers: z22.record(z22.string()).optional(),
  payload: z22.record(z22.unknown()).optional()
});
var deliveryInstructionSchema = z22.object({
  channels: z22.array(deliveryChannelEnum).nonempty("at least one delivery channel is required"),
  email: emailConfigSchema.optional(),
  slack: slackConfigSchema.optional(),
  webhook: webhookConfigSchema.optional()
}).superRefine((value, ctx) => {
  const channelConfigs = {
    email: !!value.email,
    slack: !!value.slack,
    webhook: !!value.webhook
  };
  for (const channel of value.channels) {
    if (!channelConfigs[channel]) {
      ctx.addIssue({
        code: z22.ZodIssueCode.custom,
        path: ["channels"],
        message: `${channel} delivery config is required when channel is enabled`
      });
    }
  }
});
var reportFormatEnum = z22.enum(["json", "csv", "pdf", "xlsx", "docx", "pptx", "txt"]);
var reportTemplateSchema = z22.object({
  id: z22.string().min(1),
  name: z22.string().min(1),
  description: z22.string().optional(),
  content: z22.string().min(1, "template content is required"),
  format: reportFormatEnum,
  defaultWatermark: z22.string().optional()
});
var reportRequestSchema = z22.object({
  template: reportTemplateSchema,
  context: z22.record(z22.unknown()),
  watermark: z22.string().optional(),
  recipients: deliveryInstructionSchema.optional()
});
function validateReportRequest(request) {
  return reportRequestSchema.parse(request);
}
function validateDeliveryInstruction(instruction) {
  if (!instruction) return void 0;
  return deliveryInstructionSchema.parse(instruction);
}
var reportFormats = reportFormatEnum.options;

// src/reporting/delivery-service.ts
function buildTransporter() {
  if (process.env.SMTP_URL) {
    return nodemailer2.createTransport(process.env.SMTP_URL);
  }
  return nodemailer2.createTransport({ jsonTransport: true });
}
var DeliveryService = class {
  constructor(transporter = buildTransporter()) {
    this.transporter = transporter;
  }
  async deliver(artifact, instruction) {
    const validated = validateDeliveryInstruction(instruction);
    if (!validated) return void 0;
    const attempts = [];
    const tasks = validated.channels.map((channel) => {
      if (channel === "email" && validated.email) {
        return { channel, task: this.sendEmail(artifact, validated.email) };
      }
      if (channel === "slack" && validated.slack) {
        return { channel, task: this.sendSlack(artifact, validated.slack) };
      }
      if (channel === "webhook" && validated.webhook) {
        return { channel, task: this.sendWebhook(artifact, validated.webhook) };
      }
      return { channel, task: Promise.reject(new Error(`Unsupported channel: ${channel}`)) };
    });
    const settled = await Promise.allSettled(tasks.map((entry) => entry.task));
    settled.forEach((result2, index) => {
      const channel = tasks[index]?.channel;
      if (!channel) return;
      if (result2.status === "fulfilled") {
        attempts.push({ channel, status: "sent" });
      } else {
        attempts.push({ channel, status: "failed", error: result2.reason?.message || "unknown error" });
      }
    });
    return { attempts };
  }
  async sendEmail(artifact, config9) {
    await this.transporter.sendMail({
      to: config9.to,
      cc: config9.cc,
      bcc: config9.bcc,
      subject: config9.subject || `Report: ${artifact.fileName}`,
      text: config9.body || "Automated report delivery",
      attachments: [
        {
          filename: artifact.fileName,
          content: artifact.buffer,
          contentType: artifact.mimeType
        }
      ]
    });
  }
  async sendSlack(artifact, config9) {
    const payload = {
      text: config9.text || "New report available",
      attachments: [
        {
          title: artifact.fileName,
          text: `Format: ${artifact.format}`
        }
      ]
    };
    await axios4.post(config9.webhookUrl, payload);
  }
  async sendWebhook(artifact, config9) {
    const payload = {
      ...config9.payload,
      artifact: {
        fileName: artifact.fileName,
        mimeType: artifact.mimeType,
        format: artifact.format,
        size: artifact.buffer.length
      }
    };
    await axios4.post(config9.url, payload, { headers: config9.headers });
  }
};

// src/reporting/exporters/csv-exporter.ts
import { parse } from "json2csv";

// src/reporting/exporters/base.ts
function normalizeTabularData(data) {
  if (Array.isArray(data)) {
    return data.map((row) => typeof row === "object" ? row : { value: row });
  }
  if (typeof data === "object") return [data || {}];
  return [{ value: data }];
}

// src/reporting/exporters/csv-exporter.ts
var CsvExporter = class {
  format = "csv";
  async export(data, options2 = {}) {
    const rows = normalizeTabularData(data);
    const csv = parse(rows, { withBOM: true });
    const content = options2.watermark ? `# ${options2.watermark}
${csv}` : csv;
    return {
      buffer: Buffer.from(content),
      fileName: `report-${Date.now()}.csv`,
      mimeType: "text/csv",
      format: this.format
    };
  }
};

// src/reporting/exporters/docx-exporter.ts
import {
  Document,
  HeadingLevel,
  Packer,
  Paragraph,
  Table,
  TableCell,
  TableRow,
  TextRun
} from "docx";
var DocxExporter = class {
  format = "docx";
  async export(data, options2 = {}) {
    const dataRows = normalizeTabularData(data);
    const headers = Object.keys(dataRows[0] || { value: "value" });
    const table = new Table({
      rows: [
        new TableRow({
          children: headers.map(
            (header) => new TableCell({
              children: [new Paragraph({ text: header, heading: HeadingLevel.HEADING_3 })]
            })
          )
        }),
        ...dataRows.map(
          (row) => new TableRow({
            children: headers.map(
              (header) => new TableCell({
                children: [new Paragraph(String(row[header] ?? ""))]
              })
            )
          })
        )
      ]
    });
    const doc = new Document({
      sections: [
        {
          children: [
            new Paragraph({
              text: options2.title || "Executive Briefing",
              heading: HeadingLevel.HEADING_1
            }),
            ...options2.watermark ? [
              new Paragraph({
                children: [
                  new TextRun({
                    text: options2.watermark,
                    italics: true,
                    color: "808080"
                  })
                ]
              })
            ] : [],
            table
          ]
        }
      ]
    });
    const buffer = await Packer.toBuffer(doc);
    return {
      buffer,
      fileName: `report-${Date.now()}.docx`,
      mimeType: "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      format: this.format
    };
  }
};

// src/reporting/exporters/excel-exporter.ts
import ExcelJS from "exceljs";
var ExcelExporter = class {
  format = "xlsx";
  async export(data, options2 = {}) {
    const workbook = new ExcelJS.Workbook();
    const sheet = workbook.addWorksheet("Report");
    const rows = normalizeTabularData(data);
    const headers = Object.keys(rows[0] || { value: "value" });
    sheet.addRow(headers);
    rows.forEach((row) => sheet.addRow(headers.map((key) => row[key] ?? "")));
    if (options2.watermark) {
      const watermarkSheet = workbook.addWorksheet("Watermark");
      watermarkSheet.getCell("A1").value = options2.watermark;
      watermarkSheet.getCell("A1").font = { color: { argb: "80C0C0C0" }, bold: true, size: 20 };
      watermarkSheet.getCell("A1").alignment = { vertical: "middle", horizontal: "center" };
    }
    const buffer = await workbook.xlsx.writeBuffer();
    return {
      buffer: Buffer.from(buffer),
      fileName: `report-${Date.now()}.xlsx`,
      mimeType: "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
      format: this.format
    };
  }
};

// src/reporting/exporters/json-exporter.ts
var JsonExporter = class {
  format = "json";
  async export(data, options2 = {}) {
    const payload = {
      data,
      watermark: options2.watermark,
      generatedAt: (/* @__PURE__ */ new Date()).toISOString()
    };
    const buffer = Buffer.from(JSON.stringify(payload, null, 2));
    return {
      buffer,
      fileName: `report-${Date.now()}.json`,
      mimeType: "application/json",
      format: this.format
    };
  }
};

// src/reporting/exporters/pdf-exporter.ts
import PDFDocument from "pdfkit";
function renderTable(doc, rows) {
  const keys = Object.keys(rows[0] || { value: "value" });
  doc.font("Helvetica-Bold").fontSize(10).text(keys.join(" | "));
  doc.moveDown(0.4);
  doc.font("Helvetica").fontSize(10);
  rows.forEach((row) => {
    const line = keys.map((key) => String(row[key] ?? "")).join(" | ");
    doc.text(line);
  });
}
var PdfExporter = class {
  format = "pdf";
  async export(data, options2 = {}) {
    const doc = new PDFDocument({ margin: 48 });
    const buffers = [];
    doc.on("data", (chunk) => buffers.push(chunk));
    if (options2.watermark) {
      doc.fontSize(48).fillColor("#e0e0e0").opacity(0.25).rotate(-30, { origin: [250, 300] }).text(options2.watermark, 50, 150, { align: "center" }).rotate(30, { origin: [250, 300] }).opacity(1).fillColor("black");
    }
    doc.fontSize(16).text(options2.title || "Intelligence Report", { align: "left" });
    doc.moveDown();
    const rows = normalizeTabularData(data);
    renderTable(doc, rows);
    doc.end();
    await new Promise((resolve2) => doc.on("end", resolve2));
    return {
      buffer: Buffer.concat(buffers),
      fileName: `report-${Date.now()}.pdf`,
      mimeType: "application/pdf",
      format: this.format
    };
  }
};

// src/reporting/exporters/pptx-exporter.ts
import PptxGenJSModule from "pptxgenjs";
var PptxGenJS = PptxGenJSModule.default || PptxGenJSModule;
var PptxExporter = class {
  format = "pptx";
  async export(data, options2 = {}) {
    const pptx = new PptxGenJS();
    const slide = pptx.addSlide();
    slide.addText(options2.title || "Threat Assessment", {
      x: 0.5,
      y: 0.3,
      fontSize: 20,
      bold: true
    });
    const rows = normalizeTabularData(data);
    const headers = Object.keys(rows[0] || { value: "value" });
    const tableRows = [headers, ...rows.map((row) => headers.map((key) => `${row[key] ?? ""}`))];
    slide.addTable(tableRows, { x: 0.5, y: 1, w: 9, fontSize: 12 });
    if (options2.watermark) {
      slide.addText(options2.watermark, {
        x: 1,
        y: 4,
        fontSize: 30,
        color: "c0c0c0",
        rotate: -25,
        bold: true,
        transparency: 50
      });
    }
    const buffer = await pptx.write("arraybuffer");
    return {
      buffer: Buffer.from(buffer),
      fileName: `report-${Date.now()}.pptx`,
      mimeType: "application/vnd.openxmlformats-officedocument.presentationml.presentation",
      format: this.format
    };
  }
};

// src/reporting/exporters/xml-exporter.ts
var XmlExporter = class {
  format = "xml";
  async export(data, options2 = {}) {
    const xmlContent = this.toXml(data, "root", options2.watermark);
    return {
      buffer: Buffer.from(xmlContent),
      fileName: `report-${Date.now()}.xml`,
      mimeType: "application/xml",
      format: this.format
    };
  }
  toXml(data, rootName = "root", watermark) {
    let xml = '<?xml version="1.0" encoding="UTF-8"?>\n';
    if (watermark) {
      xml += `<!-- Watermark: ${watermark} -->
`;
    }
    xml += this.serialize(data, rootName);
    return xml;
  }
  serialize(data, tagName) {
    const safeTagName = this.sanitizeTagName(tagName);
    if (data === null || data === void 0) {
      return `<${safeTagName} />`;
    }
    if (Array.isArray(data)) {
      return data.map((item) => this.serialize(item, safeTagName)).join("\n");
    }
    if (typeof data === "object") {
      const children = Object.entries(data).map(([key, value]) => this.serialize(value, key)).join("\n");
      return `<${safeTagName}>
${children}
</${safeTagName}>`;
    }
    const text = String(data).replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;").replace(/"/g, "&quot;").replace(/'/g, "&apos;");
    return `<${safeTagName}>${text}</${safeTagName}>`;
  }
  sanitizeTagName(name) {
    let safe = name.replace(/[^a-zA-Z0-9_\-\.]/g, "_");
    if (!/^[a-zA-Z_]/.test(safe)) {
      safe = "_" + safe;
    }
    return safe || "item";
  }
};

// src/reporting/exporters/index.ts
var exporters = [
  new JsonExporter(),
  new CsvExporter(),
  new PdfExporter(),
  new ExcelExporter(),
  new DocxExporter(),
  new PptxExporter(),
  new XmlExporter()
];
var exporterMap = Object.fromEntries(
  exporters.map((exp) => [exp.format, exp])
);

// src/reporting/template-engine.ts
import nunjucks from "nunjucks";
var env = new nunjucks.Environment(void 0, {
  autoescape: false,
  trimBlocks: true,
  lstripBlocks: true
});
env.addFilter("as_date", (value) => {
  if (!value) return "";
  const date = typeof value === "string" ? new Date(value) : value;
  return date.toISOString();
});
env.addFilter(
  "uppercase",
  (value) => typeof value === "string" ? value.toUpperCase() : value
);
env.addFilter("truncate", (value, length) => {
  if (typeof value !== "string") return value;
  if (value.length <= length) return value;
  return `${value.substring(0, length)}\u2026`;
});
var TemplateEngine = class {
  constructor(environment = env) {
    this.environment = environment;
  }
  render(template, context4) {
    const rendered = this.environment.renderString(template.content, context4);
    return { rendered, context: context4 };
  }
};
var defaultTemplateEngine = new TemplateEngine();

// src/reporting/version-store.ts
import crypto28 from "crypto";
var VersionStore = class {
  versionsByTemplate = /* @__PURE__ */ new Map();
  record(template, artifact, createdBy) {
    const checksum = crypto28.createHash("sha256").update(artifact.buffer).digest("hex");
    const version = {
      id: crypto28.randomUUID(),
      templateId: template.id,
      checksum,
      createdAt: /* @__PURE__ */ new Date(),
      createdBy,
      metadata: { mimeType: artifact.mimeType, format: artifact.format }
    };
    const versions = this.versionsByTemplate.get(template.id) || [];
    versions.push(version);
    this.versionsByTemplate.set(template.id, versions);
    return version;
  }
  history(templateId) {
    return [...this.versionsByTemplate.get(templateId) || []].sort(
      (a, b) => b.createdAt.getTime() - a.createdAt.getTime()
    );
  }
};

// src/reporting/service.ts
var ReportingService = class {
  constructor(accessControl, delivery, versions, templateEngine = defaultTemplateEngine) {
    this.accessControl = accessControl;
    this.delivery = delivery;
    this.versions = versions;
    this.templateEngine = templateEngine;
  }
  async generate(request, access) {
    const validatedRequest = validateReportRequest(request);
    this.accessControl.ensureAuthorized(access, "report", "view");
    if (validatedRequest.recipients) {
      this.accessControl.ensureAuthorized(access, "report", "deliver");
    }
    const renderResult = this.templateEngine.render(
      validatedRequest.template,
      validatedRequest.context
    );
    const exporter = exporterMap[validatedRequest.template.format];
    if (!exporter) throw new Error(`No exporter for format ${validatedRequest.template.format}`);
    let prepared = renderResult.rendered;
    try {
      prepared = JSON.parse(renderResult.rendered);
    } catch {
      prepared = renderResult.rendered;
    }
    const artifact = await exporter.export(prepared, {
      watermark: validatedRequest.watermark || validatedRequest.template.defaultWatermark,
      title: validatedRequest.template.name
    });
    const version = this.recordVersion(validatedRequest.template, artifact, access.userId);
    await provenanceLedger.appendEntry({
      tenantId: "system",
      // TODO: extract from access context if available
      actionType: "REPORT_GENERATED",
      resourceType: "Report",
      resourceId: version.id,
      actorId: access.userId,
      actorType: "user",
      timestamp: /* @__PURE__ */ new Date(),
      payload: {
        mutationType: "CREATE",
        entityId: version.id,
        entityType: "Report",
        templateId: validatedRequest.template.id,
        format: validatedRequest.template.format,
        watermark: validatedRequest.watermark
      },
      metadata: {
        versionChecksum: version.checksum
      }
    });
    const delivery = await this.delivery.deliver(artifact, validatedRequest.recipients);
    if (delivery) {
      await provenanceLedger.appendEntry({
        tenantId: "system",
        // TODO: extract from access context if available
        actionType: "REPORT_DELIVERED",
        resourceType: "Report",
        resourceId: version.id,
        actorId: access.userId,
        actorType: "user",
        timestamp: /* @__PURE__ */ new Date(),
        payload: {
          mutationType: "UPDATE",
          // Delivery is an update to the report's lifecycle
          entityId: version.id,
          entityType: "Report",
          delivery
        },
        metadata: {
          versionChecksum: version.checksum
        }
      });
    }
    artifact.metadata = {
      ...artifact.metadata || {},
      versionId: version.id,
      deliveredAt: delivery ? (/* @__PURE__ */ new Date()).toISOString() : void 0,
      delivery
    };
    return artifact;
  }
  recordVersion(template, artifact, userId) {
    return this.versions.record(template, artifact, userId);
  }
  history(templateId) {
    return this.versions.history(templateId);
  }
};
function createReportingService(rules) {
  return new ReportingService(rules, new DeliveryService(), new VersionStore());
}

// src/reporting/access-control.ts
var AccessControlService = class {
  constructor(rules) {
    this.rules = rules;
  }
  ensureAuthorized(context4, resource, action) {
    const allowed = this.rules.some(
      (rule) => rule.resource === resource && rule.action === action && rule.roles.some((role) => context4.roles.includes(role))
    );
    if (!allowed) {
      throw new Error(`User ${context4.userId} is not permitted to ${action} ${resource}`);
    }
  }
};

// src/cases/reporting/templates.ts
var INVESTIGATION_SUMMARY_TEMPLATE = {
  id: "INVESTIGATION_SUMMARY",
  name: "Investigation Summary Report",
  description: "A comprehensive overview of investigation findings",
  content: `
# Investigation Summary: {{title}}
Status: {{status}}
Severity: {{severity}}

## Executive Summary
{{summary}}

## Key Entities
{{entities}}

## Timeline
{{timeline}}
  `,
  format: "docx",
  defaultWatermark: "FOR OFFICIAL USE ONLY"
};

// src/routes/cases.ts
init_metrics2();
init_logger();
init_emit();
var routeLogger = logger_default.child({ name: "CaseRoutes" });
var overviewService = new CaseOverviewService(getPostgresPool(), {
  ttlMs: Number.isFinite(Number(process.env.CASE_OVERVIEW_CACHE_TTL_MS)) ? Number(process.env.CASE_OVERVIEW_CACHE_TTL_MS) : void 0,
  staleWhileRevalidateMs: Number.isFinite(
    Number(process.env.CASE_OVERVIEW_CACHE_SWR_MS)
  ) ? Number(process.env.CASE_OVERVIEW_CACHE_SWR_MS) : void 0
});
var caseRouter = Router16();
function getRequestContext(req) {
  const tenantId = String(
    req.headers["x-tenant-id"] || req.headers["x-tenant"] || ""
  );
  const userId = req.user?.id || req.headers["x-user-id"] || req.user?.email || "system";
  return {
    tenantId: tenantId || null,
    userId: userId || null
  };
}
function getAuditContext(body4) {
  return {
    reason: body4.reason,
    legalBasis: body4.legalBasis,
    warrantId: body4.warrantId,
    authorityReference: body4.authorityReference,
    ipAddress: body4._ipAddress,
    userAgent: body4._userAgent,
    sessionId: body4._sessionId,
    requestId: body4._requestId,
    correlationId: body4._correlationId
  };
}
caseRouter.get("/:id/overview", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    const reason = req.query.reason;
    const legalBasis = req.query.legalBasis;
    if (!reason) {
      return res.status(400).json({
        error: "reason_required",
        message: "You must provide a reason for accessing this case overview"
      });
    }
    if (!legalBasis) {
      return res.status(400).json({
        error: "legal_basis_required",
        message: "You must provide a legal basis for accessing this case overview"
      });
    }
    const pg5 = getPostgresPool();
    const caseExists = await pg5.query(
      `SELECT 1 FROM maestro.cases WHERE id = $1 AND tenant_id = $2`,
      [id, tenantId]
    );
    if (caseExists.rowCount === 0) {
      return res.status(404).json({ error: "case_not_found" });
    }
    const overview = await overviewService.getOverview(id, tenantId);
    routeLogger.info(
      {
        caseId: id,
        tenantId,
        userId,
        cacheStatus: overview.cache.status
      },
      "Case overview retrieved"
    );
    res.json({
      ...overview,
      audit: {
        reason,
        legalBasis
      }
    });
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to get case overview");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.post("/", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const input = {
      tenantId,
      title: req.body.title,
      description: req.body.description,
      status: req.body.status,
      compartment: req.body.compartment,
      policyLabels: req.body.policyLabels,
      metadata: req.body.metadata
    };
    const pg5 = getPostgresPool();
    const service11 = new CaseService(pg5);
    const auditContext = getAuditContext(req.body);
    const caseRecord = await service11.createCase(input, userId, auditContext);
    goldenPathStepTotal.inc({
      step: "investigation_created",
      status: "success",
      tenant_id: tenantId
    });
    routeLogger.info(
      { caseId: caseRecord.id, tenantId, userId },
      "Case created via API"
    );
    res.status(201).json(caseRecord);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to create case");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.get("/:id", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    const reason = req.query.reason;
    const legalBasis = req.query.legalBasis;
    if (!reason) {
      return res.status(400).json({
        error: "reason_required",
        message: "You must provide a reason for accessing this case"
      });
    }
    if (!legalBasis) {
      return res.status(400).json({
        error: "legal_basis_required",
        message: "You must provide a legal basis for accessing this case"
      });
    }
    const pg5 = getPostgresPool();
    const service11 = new CaseService(pg5);
    const auditContext = {
      reason,
      legalBasis,
      warrantId: req.query.warrantId,
      ipAddress: req.ip,
      userAgent: req.headers["user-agent"]
    };
    const caseRecord = await service11.getCase(id, tenantId, userId, auditContext);
    if (!caseRecord) {
      return res.status(404).json({ error: "case_not_found" });
    }
    res.json(caseRecord);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to get case");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.put("/:id", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    if (!req.body.reason) {
      return res.status(400).json({
        error: "reason_required",
        message: "You must provide a reason for modifying this case"
      });
    }
    if (!req.body.legalBasis) {
      return res.status(400).json({
        error: "legal_basis_required",
        message: "You must provide a legal basis for modifying this case"
      });
    }
    const input = {
      id,
      title: req.body.title,
      description: req.body.description,
      status: req.body.status,
      compartment: req.body.compartment,
      policyLabels: req.body.policyLabels,
      metadata: req.body.metadata
    };
    const pg5 = getPostgresPool();
    const service11 = new CaseService(pg5);
    const auditContext = getAuditContext(req.body);
    auditContext.ipAddress = req.ip;
    auditContext.userAgent = req.headers["user-agent"];
    const caseRecord = await service11.updateCase(
      input,
      userId,
      tenantId,
      auditContext
    );
    if (!caseRecord) {
      return res.status(404).json({ error: "case_not_found" });
    }
    res.json(caseRecord);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to update case");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.get("/", async (req, res) => {
  try {
    const { tenantId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    const pg5 = getPostgresPool();
    const service11 = new CaseService(pg5);
    const cases = await service11.listCases({
      tenantId,
      status: req.query.status,
      compartment: req.query.compartment,
      policyLabels: req.query.policyLabels ? req.query.policyLabels.split(",") : void 0,
      limit: req.query.limit ? parseInt(req.query.limit) : void 0,
      offset: req.query.offset ? parseInt(req.query.offset) : void 0
    });
    res.json(cases);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to list cases");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.post("/:id/archive", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    if (!req.body.reason) {
      return res.status(400).json({
        error: "reason_required",
        message: "You must provide a reason for archiving this case"
      });
    }
    if (!req.body.legalBasis) {
      return res.status(400).json({
        error: "legal_basis_required",
        message: "You must provide a legal basis for archiving this case"
      });
    }
    const pg5 = getPostgresPool();
    const service11 = new CaseService(pg5);
    const auditContext = getAuditContext(req.body);
    auditContext.ipAddress = req.ip;
    auditContext.userAgent = req.headers["user-agent"];
    const caseRecord = await service11.archiveCase(
      id,
      userId,
      tenantId,
      auditContext
    );
    if (!caseRecord) {
      return res.status(404).json({ error: "case_not_found" });
    }
    res.json(caseRecord);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to archive case");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.post("/:id/export", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    if (!req.body.reason) {
      return res.status(400).json({
        error: "reason_required",
        message: "You must provide a reason for exporting this case"
      });
    }
    if (!req.body.legalBasis) {
      return res.status(400).json({
        error: "legal_basis_required",
        message: "You must provide a legal basis for exporting this case"
      });
    }
    const pg5 = getPostgresPool();
    const service11 = new CaseService(pg5);
    const auditContext = getAuditContext(req.body);
    auditContext.ipAddress = req.ip;
    auditContext.userAgent = req.headers["user-agent"];
    const caseRecord = await service11.exportCase(
      id,
      tenantId,
      userId,
      auditContext
    );
    if (!caseRecord) {
      return res.status(404).json({ error: "case_not_found" });
    }
    res.json(caseRecord);
  } catch (error) {
    if (error.name === "UserFacingError") {
      return res.status(400).json({ error: error.message });
    }
    routeLogger.error({ error: error.message }, "Failed to export case");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.post("/:id/release-criteria", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    const config9 = req.body;
    const pg5 = getPostgresPool();
    const { ReleaseCriteriaService: ReleaseCriteriaService2 } = await Promise.resolve().then(() => (init_ReleaseCriteriaService(), ReleaseCriteriaService_exports));
    const service11 = new ReleaseCriteriaService2(pg5);
    await service11.configure(id, tenantId, userId, config9);
    res.status(200).json({ message: "Release criteria configured" });
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to configure release criteria");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.get("/:id/release-criteria/status", async (req, res) => {
  try {
    const { tenantId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    const { id } = req.params;
    const pg5 = getPostgresPool();
    const { ReleaseCriteriaService: ReleaseCriteriaService2 } = await Promise.resolve().then(() => (init_ReleaseCriteriaService(), ReleaseCriteriaService_exports));
    const service11 = new ReleaseCriteriaService2(pg5);
    const result2 = await service11.evaluate(id, tenantId);
    res.json(result2);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to get release criteria status");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.post("/:id/comments", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    const { content, metadata } = req.body;
    if (!content) {
      return res.status(400).json({ error: "content_required" });
    }
    const pg5 = getPostgresPool();
    const service11 = new CommentService(pg5);
    const comment = await service11.addComment({
      tenantId,
      targetType: "CASE",
      targetId: id,
      content,
      authorId: userId,
      metadata
    });
    await emitAuditEvent(
      {
        eventId: randomUUID32(),
        occurredAt: (/* @__PURE__ */ new Date()).toISOString(),
        actor: {
          type: "user",
          id: userId,
          name: req.user?.username || req.user?.email || userId,
          ipAddress: req.ip
        },
        action: {
          type: "comment.added",
          outcome: "success"
        },
        tenantId,
        target: {
          type: "case_comment",
          id: comment.commentId,
          path: `cases/${id}`
        },
        metadata: {
          caseId: id,
          commentId: comment.commentId,
          messageLength: String(content).length,
          userAgent: req.headers["user-agent"]
        }
      },
      {
        correlationId: req.headers["x-request-id"],
        serviceId: "cases"
      }
    ).catch((error) => {
      routeLogger.warn(
        { error: error.message, caseId: id },
        "Failed to emit comment audit event"
      );
    });
    res.status(201).json(comment);
  } catch (error) {
    if (error.name === "UserFacingError") {
      return res.status(404).json({ error: error.message });
    }
    routeLogger.error({ error: error.message }, "Failed to add comment");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.get("/:id/comments", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    const { id } = req.params;
    const limit = req.query.limit ? parseInt(req.query.limit) : void 0;
    const offset = req.query.offset ? parseInt(req.query.offset) : void 0;
    const pg5 = getPostgresPool();
    const service11 = new CommentService(pg5);
    const comments = await service11.listComments({
      targetType: "CASE",
      targetId: id,
      tenantId,
      limit,
      offset
    });
    res.json(comments);
  } catch (error) {
    if (error.name === "UserFacingError") {
      return res.status(404).json({ error: error.message });
    }
    routeLogger.error({ error: error.message }, "Failed to list comments");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.post("/:id/evidence/event", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id: caseId } = req.params;
    const { evidenceId, action, location, notes, verificationHash } = req.body;
    if (!evidenceId || !action) {
      return res.status(400).json({ error: "evidenceId and action are required" });
    }
    const pg5 = getPostgresPool();
    const service11 = new ChainOfCustodyService(pg5);
    const event = await service11.recordEvent({
      caseId,
      evidenceId,
      action,
      actorId: userId,
      location,
      notes,
      verificationHash
    });
    res.status(201).json(event);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to record custody event");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.get("/:id/evidence/:evidenceId/chain", async (req, res) => {
  try {
    const { evidenceId } = req.params;
    const pg5 = getPostgresPool();
    const service11 = new ChainOfCustodyService(pg5);
    const chain = await service11.getChain(evidenceId);
    res.json(chain);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to get chain of custody");
    res.status(500).json({ error: error.message });
  }
});
caseRouter.post("/:id/report", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    const pg5 = getPostgresPool();
    const caseService2 = new CaseService(pg5);
    const caseData = await caseService2.getCase(id, tenantId, userId, { reason: "Report Generation", legalBasis: "investigation" });
    if (!caseData) {
      return res.status(404).json({ error: "case_not_found" });
    }
    const { TaskRepo: TaskRepo2 } = await Promise.resolve().then(() => (init_TaskRepo(), TaskRepo_exports));
    const taskRepo = new TaskRepo2(pg5);
    const tasks = await taskRepo.getCaseTasks(id);
    const chainService = new ChainOfCustodyService(pg5);
    const evidenceItems = await chainService.listEvidence(id);
    const evidence = evidenceItems.map((item) => ({
      id: item.id,
      description: "Evidence item",
      // Placeholder
      lastUpdate: item.lastUpdate
    }));
    const context4 = {
      case: caseData,
      tasks,
      evidence
    };
    const rules = [
      {
        resource: "report",
        action: "view",
        roles: ["investigator", "admin", "analyst"]
      },
      {
        resource: "report",
        action: "deliver",
        roles: ["investigator", "admin"]
      }
    ];
    const accessControl = new AccessControlService(rules);
    const reportingService3 = createReportingService(accessControl);
    const userRoles = req.user?.roles || ["investigator"];
    const report = await reportingService3.generate(
      {
        template: INVESTIGATION_SUMMARY_TEMPLATE,
        context: context4,
        watermark: req.body.watermark
      },
      {
        userId,
        roles: userRoles
      }
    );
    res.json(report);
  } catch (error) {
    routeLogger.error({ error: error.message }, "Failed to generate report");
    res.status(500).json({ error: error.message });
  }
});
var cases_default = caseRouter;

// src/routes/entity-comments.ts
init_logger();
init_postgres();
import { randomUUID as randomUUID34 } from "node:crypto";
import { Router as Router17 } from "express";

// src/entities/comments/EntityCommentService.ts
import { randomUUID as randomUUID33 } from "node:crypto";
var MENTION_REGEX = /@([a-zA-Z0-9._-]{2,50})/g;
var UUID_REGEX = /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i;
var isSafeDeleteEnabled2 = () => process.env.SAFE_DELETE !== "false";
var EntityCommentService = class {
  constructor(pool4) {
    this.pool = pool4;
  }
  async addComment(input) {
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const commentResult = await client6.query(
        `
          INSERT INTO maestro.entity_comments (
            tenant_id,
            entity_id,
            entity_ref_id,
            entity_type,
            entity_label,
            author_id,
            content_markdown,
            metadata
          )
          VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
          RETURNING *
        `,
        [
          input.tenantId,
          input.entityId,
          UUID_REGEX.test(input.entityId) ? input.entityId : null,
          input.entityType || null,
          input.entityLabel || null,
          input.authorId,
          input.content,
          input.metadata || {}
        ]
      );
      const commentRow = commentResult.rows[0];
      const attachments = await this.insertAttachments(
        client6,
        commentRow.id,
        input.attachments || []
      );
      const mentions = await this.resolveMentions(client6, input.content);
      await this.insertMentions(client6, commentRow.id, mentions);
      await client6.query("COMMIT");
      return this.mapCommentRow(commentRow, attachments, mentions);
    } catch (error) {
      await client6.query("ROLLBACK");
      throw error;
    } finally {
      client6.release();
    }
  }
  async listComments(tenantId, entityId, limit = 50, offset = 0, options2 = {}) {
    const conditions = ["tenant_id = $1", "entity_id = $2"];
    if (isSafeDeleteEnabled2() && !options2.includeDeleted) {
      conditions.push("deleted_at IS NULL");
    }
    const result2 = await this.pool.query(
      `
        SELECT *
        FROM maestro.entity_comments
        WHERE ${conditions.join(" AND ")}
        ORDER BY created_at ASC
        LIMIT $3 OFFSET $4
      `,
      [tenantId, entityId, limit, offset]
    );
    const commentRows = result2.rows;
    if (commentRows.length === 0) {
      return [];
    }
    const commentIds = commentRows.map((row) => row.id);
    const [attachments, mentions] = await Promise.all([
      this.fetchAttachments(commentIds),
      this.fetchMentions(commentIds)
    ]);
    return commentRows.map(
      (row) => this.mapCommentRow(
        row,
        attachments.get(row.id) || [],
        mentions.get(row.id) || []
      )
    );
  }
  async getCommentById(tenantId, commentId) {
    const result2 = await this.pool.query(
      `
        SELECT *
        FROM maestro.entity_comments
        WHERE tenant_id = $1 AND id = $2
        LIMIT 1
      `,
      [tenantId, commentId]
    );
    const row = result2.rows?.[0];
    if (!row) {
      return null;
    }
    const [attachments, mentions] = await Promise.all([
      this.fetchAttachments([row.id]).then((map) => map.get(row.id) || []),
      this.fetchMentions([row.id]).then((map) => map.get(row.id) || [])
    ]);
    return this.mapCommentRow(row, attachments, mentions);
  }
  async softDeleteComment(tenantId, commentId, actorId, reason) {
    const retentionDays = Number(
      process.env.ENTITY_COMMENT_RETENTION_DAYS || process.env.SUPPORT_COMMENT_RETENTION_DAYS || "30"
    );
    const purgeAfter = Number.isFinite(retentionDays) ? new Date(Date.now() + retentionDays * 24 * 60 * 60 * 1e3) : null;
    const result2 = await this.pool.query(
      `
        UPDATE maestro.entity_comments
        SET deleted_at = NOW(), deleted_by = $3, delete_reason = $4, updated_at = NOW()
        WHERE tenant_id = $1 AND id = $2
        RETURNING *
      `,
      [tenantId, commentId, actorId, reason || null]
    );
    const row = result2.rows?.[0];
    if (!row) {
      return null;
    }
    const [attachments, mentions] = await Promise.all([
      this.fetchAttachments([row.id]).then((map) => map.get(row.id) || []),
      this.fetchMentions([row.id]).then((map) => map.get(row.id) || [])
    ]);
    const comment = this.mapCommentRow(row, attachments, mentions);
    await this.recordAudit(commentId, tenantId, "delete", actorId, reason, {
      purgeAfter: purgeAfter?.toISOString?.()
    });
    return {
      ...comment,
      metadata: {
        ...comment.metadata,
        purgeAfter: purgeAfter?.toISOString?.()
      }
    };
  }
  async restoreComment(tenantId, commentId, actorId) {
    const result2 = await this.pool.query(
      `
        UPDATE maestro.entity_comments
        SET deleted_at = NULL, deleted_by = NULL, delete_reason = NULL, updated_at = NOW()
        WHERE tenant_id = $1 AND id = $2
        RETURNING *
      `,
      [tenantId, commentId]
    );
    const row = result2.rows?.[0];
    if (!row) {
      return null;
    }
    const [attachments, mentions] = await Promise.all([
      this.fetchAttachments([row.id]).then((map) => map.get(row.id) || []),
      this.fetchMentions([row.id]).then((map) => map.get(row.id) || [])
    ]);
    const comment = this.mapCommentRow(row, attachments, mentions);
    await this.recordAudit(commentId, tenantId, "restore", actorId);
    return comment;
  }
  async insertAttachments(client6, commentId, attachments) {
    const results = [];
    for (const attachment of attachments) {
      const res = await client6.query(
        `
          INSERT INTO maestro.entity_comment_attachments (
            comment_id,
            file_name,
            content_type,
            size_bytes,
            storage_uri,
            metadata
          )
          VALUES ($1, $2, $3, $4, $5, $6)
          RETURNING *
        `,
        [
          commentId,
          attachment.fileName,
          attachment.contentType || null,
          attachment.sizeBytes || null,
          attachment.storageUri || null,
          attachment.metadata || {}
        ]
      );
      results.push(this.mapAttachmentRow(res.rows[0]));
    }
    return results;
  }
  async fetchAttachments(commentIds) {
    if (commentIds.length === 0) {
      return /* @__PURE__ */ new Map();
    }
    const result2 = await this.pool.query(
      `
        SELECT *
        FROM maestro.entity_comment_attachments
        WHERE comment_id = ANY($1)
        ORDER BY created_at ASC
      `,
      [commentIds]
    );
    const map = /* @__PURE__ */ new Map();
    for (const row of result2.rows) {
      const list = map.get(row.comment_id) || [];
      list.push(this.mapAttachmentRow(row));
      map.set(row.comment_id, list);
    }
    return map;
  }
  async resolveMentions(client6, content) {
    const usernames = /* @__PURE__ */ new Set();
    for (const match of content.matchAll(MENTION_REGEX)) {
      usernames.add(match[1]);
    }
    if (usernames.size === 0) {
      return [];
    }
    const result2 = await client6.query(
      `
        SELECT id, username
        FROM users
        WHERE username = ANY($1)
      `,
      [[...usernames]]
    );
    return result2.rows.map((row) => ({
      userId: String(row.id),
      username: row.username
    }));
  }
  async insertMentions(client6, commentId, mentions) {
    for (const mention of mentions) {
      await client6.query(
        `
          INSERT INTO maestro.entity_comment_mentions (
            comment_id,
            mentioned_user_id,
            mentioned_username
          )
          VALUES ($1, $2, $3)
          ON CONFLICT (comment_id, mentioned_user_id) DO NOTHING
        `,
        [commentId, mention.userId, mention.username]
      );
    }
  }
  async fetchMentions(commentIds) {
    if (commentIds.length === 0) {
      return /* @__PURE__ */ new Map();
    }
    const result2 = await this.pool.query(
      `
        SELECT comment_id, mentioned_user_id, mentioned_username
        FROM maestro.entity_comment_mentions
        WHERE comment_id = ANY($1)
        ORDER BY created_at ASC
      `,
      [commentIds]
    );
    const map = /* @__PURE__ */ new Map();
    for (const row of result2.rows) {
      const list = map.get(row.comment_id) || [];
      list.push({
        userId: row.mentioned_user_id,
        username: row.mentioned_username
      });
      map.set(row.comment_id, list);
    }
    return map;
  }
  mapCommentRow(row, attachments, mentions) {
    return {
      id: row.id,
      tenantId: row.tenant_id,
      entityId: row.entity_id,
      entityRefId: row.entity_ref_id,
      entityType: row.entity_type,
      entityLabel: row.entity_label,
      authorId: row.author_id,
      content: row.content_markdown,
      metadata: row.metadata || {},
      createdAt: row.created_at,
      updatedAt: row.updated_at,
      deletedAt: row.deleted_at,
      deletedBy: row.deleted_by,
      deleteReason: row.delete_reason,
      attachments,
      mentions
    };
  }
  mapAttachmentRow(row) {
    return {
      id: row.id,
      fileName: row.file_name,
      contentType: row.content_type,
      sizeBytes: row.size_bytes,
      storageUri: row.storage_uri,
      metadata: row.metadata || {},
      createdAt: row.created_at
    };
  }
  async recordAudit(commentId, tenantId, action, actorId, reason, metadata) {
    await this.pool.query(
      `
        INSERT INTO maestro.entity_comment_audits (
          id,
          comment_id,
          tenant_id,
          action,
          actor_id,
          reason,
          metadata
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7)
      `,
      [
        randomUUID33(),
        commentId,
        tenantId,
        action,
        actorId,
        reason || null,
        JSON.stringify(metadata || {})
      ]
    );
  }
};

// src/entities/comments/access.ts
init_logger();
var EntityCommentAccessError = class extends Error {
  status = 403;
  code = "access_denied";
  constructor(message) {
    super(message);
    this.name = "EntityCommentAccessError";
  }
};
function createEntityCommentAuthorizer(opaClient2) {
  const authzLogger = logger_default.child({ name: "EntityCommentAccess" });
  return async function assertEntityCommentAccess(request) {
    const allowed = await opaClient2.checkDataAccess(
      request.userId,
      request.tenantId,
      "entity_comment",
      request.action
    );
    authzLogger.info(
      {
        allowed,
        userId: request.userId,
        tenantId: request.tenantId,
        entityId: request.entityId,
        action: request.action
      },
      "Entity comment access decision evaluated"
    );
    if (!allowed) {
      throw new EntityCommentAccessError(
        `Access denied for action ${request.action}`
      );
    }
  };
}

// src/routes/entity-comments.ts
init_opa_client();
init_emit();
var routeLogger2 = logger_default.child({ name: "EntityCommentRoutes" });
var entityCommentsRouter = Router17();
var authorizer = createEntityCommentAuthorizer(opaClient);
function getRequestContext2(req) {
  const tenantId = String(
    req.headers["x-tenant-id"] || req.headers["x-tenant"] || ""
  );
  const userId = req.user?.id || req.headers["x-user-id"] || req.user?.email || "system";
  return {
    tenantId: tenantId || null,
    userId: userId || null
  };
}
entityCommentsRouter.post("/:id/comments", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext2(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    const { content, entityType, entityLabel, metadata, attachments } = req.body;
    if (!content) {
      return res.status(400).json({ error: "content_required" });
    }
    await authorizer({
      userId,
      tenantId,
      entityId: id,
      action: "comment:write"
    });
    const pg5 = getPostgresPool();
    const service11 = new EntityCommentService(pg5);
    const comment = await service11.addComment({
      tenantId,
      entityId: id,
      entityType,
      entityLabel,
      authorId: userId,
      content,
      metadata,
      attachments: attachments || []
    });
    await emitAuditEvent(
      {
        eventId: randomUUID34(),
        occurredAt: (/* @__PURE__ */ new Date()).toISOString(),
        actor: {
          type: "user",
          id: userId,
          name: req.user?.username || req.user?.email || userId,
          ipAddress: req.ip
        },
        action: {
          type: "comment.added",
          outcome: "success"
        },
        tenantId,
        target: {
          type: "entity_comment",
          id: comment.id,
          path: `entities/${id}`
        },
        metadata: {
          entityId: id,
          entityType,
          commentId: comment.id,
          mentionCount: comment.mentions.length,
          attachmentCount: comment.attachments.length,
          messageLength: String(content).length,
          userAgent: req.headers["user-agent"]
        }
      },
      {
        correlationId: req.headers["x-request-id"],
        serviceId: "entities"
      }
    ).catch((error) => {
      routeLogger2.warn(
        { error: error.message, entityId: id },
        "Failed to emit entity comment audit event"
      );
    });
    const notificationService = req.app?.locals?.notificationService;
    if (notificationService && comment.mentions.length > 0) {
      await Promise.all(
        comment.mentions.map((mention) => {
          const payload = {
            userId: mention.userId,
            type: "entity_comment_mention",
            subject: "You were mentioned in a comment",
            message: `You were mentioned in a comment on entity ${entityLabel || id}.`,
            data: {
              entityId: id,
              commentId: comment.id,
              mentionedBy: userId
            },
            channels: ["IN_APP" /* IN_APP */]
          };
          if (typeof notificationService.sendAsync === "function") {
            return notificationService.sendAsync(payload);
          }
          if (typeof notificationService.send === "function") {
            return notificationService.send(payload);
          }
          return Promise.resolve();
        })
      );
    }
    res.status(201).json(comment);
  } catch (error) {
    if (error instanceof EntityCommentAccessError) {
      return res.status(error.status).json({ error: error.code });
    }
    routeLogger2.error(
      { error: error.message },
      "Failed to add entity comment"
    );
    res.status(500).json({ error: error.message });
  }
});
entityCommentsRouter.get("/:id/comments", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext2(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id } = req.params;
    const limit = req.query.limit ? parseInt(req.query.limit, 10) : void 0;
    const offset = req.query.offset ? parseInt(req.query.offset, 10) : void 0;
    await authorizer({
      userId,
      tenantId,
      entityId: id,
      action: "comment:read"
    });
    const pg5 = getPostgresPool();
    const service11 = new EntityCommentService(pg5);
    const comments = await service11.listComments(
      tenantId,
      id,
      limit,
      offset,
      {
        includeDeleted: String(req.query.includeDeleted || "") === "true"
      }
    );
    res.json(comments);
  } catch (error) {
    if (error instanceof EntityCommentAccessError) {
      return res.status(error.status).json({ error: error.code });
    }
    routeLogger2.error(
      { error: error.message },
      "Failed to list entity comments"
    );
    res.status(500).json({ error: error.message });
  }
});
entityCommentsRouter.post("/:id/comments/:commentId/delete", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext2(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id: entityId, commentId } = req.params;
    const pg5 = getPostgresPool();
    const service11 = new EntityCommentService(pg5);
    const comment = await service11.getCommentById(tenantId, commentId);
    if (!comment || comment.entityId !== entityId) {
      return res.status(404).json({ error: "comment_not_found" });
    }
    const isOwner = comment.authorId === userId;
    if (!isOwner) {
      await authorizer({
        userId,
        tenantId,
        entityId,
        action: "comment:delete"
      });
    }
    const deleted = await service11.softDeleteComment(
      tenantId,
      commentId,
      userId,
      req.body?.reason
    );
    if (!deleted) {
      return res.status(404).json({ error: "comment_not_found" });
    }
    await emitAuditEvent(
      {
        eventId: randomUUID34(),
        occurredAt: (/* @__PURE__ */ new Date()).toISOString(),
        actor: {
          type: "user",
          id: userId,
          name: req.user?.username || req.user?.email || userId,
          ipAddress: req.ip
        },
        action: {
          type: "comment.deleted",
          outcome: "success"
        },
        tenantId,
        target: {
          type: "entity_comment",
          id: commentId,
          path: `entities/${entityId}`
        },
        metadata: {
          entityId,
          commentId,
          deleteReason: req.body?.reason,
          purgeAfter: deleted.metadata?.purgeAfter
        }
      },
      {
        correlationId: req.headers["x-request-id"],
        serviceId: "entities"
      }
    ).catch((error) => {
      routeLogger2.warn(
        { error: error.message, entityId, commentId },
        "Failed to emit entity comment delete audit event"
      );
    });
    res.json({ status: "deleted", comment: deleted });
  } catch (error) {
    if (error instanceof EntityCommentAccessError) {
      return res.status(error.status).json({ error: error.code });
    }
    routeLogger2.error(
      { error: error.message },
      "Failed to delete entity comment"
    );
    res.status(500).json({ error: error.message });
  }
});
entityCommentsRouter.post("/:id/comments/:commentId/restore", async (req, res) => {
  try {
    const { tenantId, userId } = getRequestContext2(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    if (!userId) {
      return res.status(401).json({ error: "user_required" });
    }
    const { id: entityId, commentId } = req.params;
    const pg5 = getPostgresPool();
    const service11 = new EntityCommentService(pg5);
    const comment = await service11.getCommentById(tenantId, commentId);
    if (!comment || comment.entityId !== entityId) {
      return res.status(404).json({ error: "comment_not_found" });
    }
    const isOwner = comment.authorId === userId;
    if (!isOwner) {
      await authorizer({
        userId,
        tenantId,
        entityId,
        action: "comment:restore"
      });
    }
    const restored = await service11.restoreComment(tenantId, commentId, userId);
    if (!restored) {
      return res.status(404).json({ error: "comment_not_found" });
    }
    await emitAuditEvent(
      {
        eventId: randomUUID34(),
        occurredAt: (/* @__PURE__ */ new Date()).toISOString(),
        actor: {
          type: "user",
          id: userId,
          name: req.user?.username || req.user?.email || userId,
          ipAddress: req.ip
        },
        action: {
          type: "comment.restored",
          outcome: "success"
        },
        tenantId,
        target: {
          type: "entity_comment",
          id: commentId,
          path: `entities/${entityId}`
        },
        metadata: {
          entityId,
          commentId
        }
      },
      {
        correlationId: req.headers["x-request-id"],
        serviceId: "entities"
      }
    ).catch((error) => {
      routeLogger2.warn(
        { error: error.message, entityId, commentId },
        "Failed to emit entity comment restore audit event"
      );
    });
    res.json({ status: "restored", comment: restored });
  } catch (error) {
    if (error instanceof EntityCommentAccessError) {
      return res.status(error.status).json({ error: error.code });
    }
    routeLogger2.error(
      { error: error.message },
      "Failed to restore entity comment"
    );
    res.status(500).json({ error: error.message });
  }
});
var entity_comments_default = entityCommentsRouter;

// src/routes/tenants.ts
import { Router as Router19 } from "express";
import { z as z24 } from "zod";
init_auth4();
init_logger2();

// src/middleware/abac.ts
var ensurePolicy = (action, resource) => {
  return async (req, res, next) => {
    try {
      const user = req.user;
      if (!user) {
        return res.status(401).json({ error: "Unauthorized" });
      }
      const normalizedRole = (user.role || "").toString().toLowerCase();
      const allowed = normalizedRole === "admin" || user.tenant_id && req.body.tenantId === user.tenant_id || user.tenantId && req.body.tenantId === user.tenantId;
      if (!allowed) {
        return res.status(403).json({ error: "Access Denied by Policy" });
      }
      next();
    } catch (error) {
      console.error("ABAC Check Failed", error);
      res.status(500).json({ error: "Internal Policy Error" });
    }
  };
};

// src/repos/ProvenanceRepo.ts
var ProvenanceRepo = class {
  constructor(pg5) {
    this.pg = pg5;
  }
  buildWhere(scope, id, filter) {
    const where = [];
    const params = [];
    const targetChecks = [
      `(target_type = $${params.push(scope)} AND target_id = $${params.push(id)})`,
      `(resource_type = $${params.push(scope)} AND resource_id = $${params.push(id)})`,
      `(investigation_id = $${params.push(id)})`,
      `(subject_type = $${params.push(scope === "incident" ? "incident" : "investigation")} AND subject_id = $${params.push(id)})`,
      `(metadata::text ILIKE '%' || $${params.push(id)} || '%')`,
      `(resource_data::text ILIKE '%' || $${params.push(id)} || '%')`,
      `(new_values::text ILIKE '%' || $${params.push(id)} || '%')`,
      `(old_values::text ILIKE '%' || $${params.push(id)} || '%')`
    ];
    where.push(`(${targetChecks.join(" OR ")})`);
    if (filter?.from) {
      where.push(
        `(COALESCE(created_at, timestamp) >= $${params.push(filter.from)})`
      );
    }
    if (filter?.to) {
      where.push(
        `(COALESCE(created_at, timestamp) <= $${params.push(filter.to)})`
      );
    }
    if (filter?.contains && filter.contains.trim().length >= 3) {
      const c = filter.contains.trim();
      where.push(`(
        COALESCE(action,'') || ' ' || COALESCE(target_type,'') || ' ' || COALESCE(resource_type,'') ILIKE '%' || $${params.push(c)} || '%' OR
        COALESCE(metadata::text,'') ILIKE '%' || $${params.push(c)} || '%' OR
        COALESCE(resource_data::text,'') ILIKE '%' || $${params.push(c)} || '%' OR
        COALESCE(new_values::text,'') ILIKE '%' || $${params.push(c)} || '%' OR
        COALESCE(old_values::text,'') ILIKE '%' || $${params.push(c)} || '%'
      )`);
    }
    if (filter?.reasonCodeIn?.length) {
      where.push(
        `(COALESCE(metadata->>'reasonCode','') = ANY($${params.push(filter.reasonCodeIn)}))`
      );
    }
    if (filter?.kindIn?.length) {
      where.push(
        `((COALESCE(action,'') = ANY($${params.push(filter.kindIn)})) OR (COALESCE(resource_type,'') = ANY($${params.push(filter.kindIn)})))`
      );
    }
    if (filter?.sourceIn?.length) {
      where.push(
        `(COALESCE(source,'') = ANY($${params.push(filter.sourceIn)}))`
      );
    }
    return {
      where: where.length ? `WHERE ${where.join(" AND ")}` : "",
      params
    };
  }
  buildTenantWhere(filter, options2) {
    const where = [];
    const params = [];
    if (filter?.from) {
      where.push(
        `(${options2.timeColumn} >= $${params.push(filter.from)})`
      );
    }
    if (filter?.to) {
      where.push(
        `(${options2.timeColumn} <= $${params.push(filter.to)})`
      );
    }
    if (filter?.contains && filter.contains.trim().length >= 3) {
      const c = filter.contains.trim();
      const searchColumns = options2.searchColumns && options2.searchColumns.length ? options2.searchColumns : ["metadata", "resource_data", "new_values", "old_values"];
      const searchConditions = searchColumns.map(
        (column) => `COALESCE(${column}::text,'') ILIKE '%' || $${params.push(c)} || '%'`
      );
      where.push(`(
        COALESCE(action,'') || ' ' || COALESCE(target_type,'') || ' ' || COALESCE(resource_type,'') ILIKE '%' || $${params.push(c)} || '%' OR
        ${searchConditions.join(" OR ")}
      )`);
    }
    return {
      where: where.length ? `WHERE ${where.join(" AND ")}` : "",
      params
    };
  }
  mapRow(r) {
    const createdAt = r.created_at || r.timestamp || /* @__PURE__ */ new Date();
    const metadata = r.metadata || r.resource_data || r.new_values || r.old_values || (r.note ? { note: r.note } : {});
    const kind = r.action || r.resource_type || r.source || "event";
    const tenantId = r.tenant_id || metadata && typeof metadata === "object" && metadata.tenantId || metadata && typeof metadata === "object" && metadata.tenant_id || null;
    const normalizedMetadata = metadata && typeof metadata === "object" ? { ...metadata, ...tenantId ? { tenantId } : {} } : metadata;
    return {
      id: r.id,
      kind,
      tenantId: tenantId || void 0,
      createdAt: createdAt instanceof Date ? createdAt.toISOString() : new Date(createdAt).toISOString(),
      metadata: normalizedMetadata
    };
  }
  appendTenantScope(where, params, tenantId) {
    if (!tenantId) return { where, params };
    const scopedParams = [...params, tenantId];
    const tenantWhere = where ? `${where} AND tenant_id = $${scopedParams.length}` : `WHERE tenant_id = $${scopedParams.length}`;
    return { where: tenantWhere, params: scopedParams };
  }
  withPaging(params, first, offset) {
    const limitIndex = params.length + 1;
    const offsetIndex = params.length + 2;
    return {
      clause: ` LIMIT $${limitIndex} OFFSET $${offsetIndex}`,
      params: [...params, first, offset]
    };
  }
  async by(scope, id, filter, first = 1e3, offset = 0, tenantId) {
    const client6 = await this.pg.connect();
    try {
      const { where, params } = this.buildWhere(scope, id, filter);
      const queries = [];
      const addQuery = (baseSql, orderBy, scoped) => {
        const scopedWhere = scoped ? this.appendTenantScope(where, params, tenantId) : { where, params };
        const { clause, params: withPaging } = this.withPaging(
          scopedWhere.params,
          first,
          offset
        );
        queries.push({
          sql: `${baseSql} ${scopedWhere.where} ${orderBy}${clause}`,
          params: withPaging
        });
      };
      if (tenantId) {
        addQuery(
          "SELECT id, action, resource_type, resource_id, details AS metadata, timestamp AS created_at, tenant_id FROM audit_events",
          "ORDER BY COALESCE(created_at, NOW()) DESC",
          true
        );
        addQuery(
          "SELECT id, action, target_type, target_id, metadata, created_at, tenant_id FROM audit_events",
          "ORDER BY COALESCE(created_at, NOW()) DESC",
          true
        );
        addQuery(
          "SELECT id, action, resource_type, resource_id, resource_data, old_values, new_values, investigation_id, timestamp, tenant_id FROM audit_events",
          "ORDER BY COALESCE(timestamp, NOW()) DESC",
          true
        );
        addQuery(
          "SELECT id, source, subject_type, subject_id, note, created_at, tenant_id FROM provenance",
          "ORDER BY created_at DESC",
          true
        );
      } else {
        addQuery(
          "SELECT id, action, resource_type, resource_id, details AS metadata, timestamp AS created_at FROM audit_events",
          "ORDER BY COALESCE(created_at, NOW()) DESC",
          false
        );
        addQuery(
          "SELECT id, action, target_type, target_id, metadata, created_at FROM audit_events",
          "ORDER BY COALESCE(created_at, NOW()) DESC",
          false
        );
        addQuery(
          "SELECT id, action, resource_type, resource_id, resource_data, old_values, new_values, investigation_id, timestamp FROM audit_events",
          "ORDER BY COALESCE(timestamp, NOW()) DESC",
          false
        );
        addQuery(
          "SELECT id, source, subject_type, subject_id, note, created_at FROM provenance",
          "ORDER BY created_at DESC",
          false
        );
      }
      for (const { sql, params: queryParams } of queries) {
        try {
          const res = await client6.query(sql, queryParams);
          const mapped = res?.rows?.map(this.mapRow) ?? [];
          if (mapped.length) return mapped;
        } catch (e) {
          continue;
        }
      }
      return [];
    } finally {
      client6.release();
    }
  }
  async getTenantStats(tenantId, filter) {
    const client6 = await this.pg.connect();
    try {
      const statsQueries = [
        {
          sql: `SELECT COUNT(*) as count, MAX(timestamp) as last_event_at
            FROM audit_events`,
          params: [],
          timeColumn: "timestamp",
          searchColumns: ["details"]
        },
        {
          sql: `SELECT COUNT(*) as count, MAX(created_at) as last_event_at
            FROM audit_events`,
          params: [],
          timeColumn: "created_at",
          searchColumns: ["metadata"]
        },
        {
          sql: `SELECT COUNT(*) as count, MAX(timestamp) as last_event_at
            FROM audit_events`,
          params: [],
          timeColumn: "timestamp",
          searchColumns: ["resource_data", "old_values", "new_values"]
        },
        {
          sql: `SELECT COUNT(*) as count, MAX(created_at) as last_event_at
            FROM provenance`,
          params: [],
          timeColumn: "created_at",
          searchColumns: ["note"]
        }
      ];
      for (const query3 of statsQueries) {
        const { where, params } = this.buildTenantWhere(filter, {
          timeColumn: query3.timeColumn,
          searchColumns: query3.searchColumns
        });
        const scopedWhere = this.appendTenantScope(where, params, tenantId);
        try {
          const res = await client6.query(
            `${query3.sql} ${scopedWhere.where}`,
            scopedWhere.params
          );
          if (res.rows.length > 0) {
            const count = parseInt(res.rows[0].count, 10);
            if (count > 0) {
              const lastEventAt = res.rows[0].last_event_at ? res.rows[0].last_event_at instanceof Date ? res.rows[0].last_event_at.toISOString() : new Date(res.rows[0].last_event_at).toISOString() : null;
              return { count, lastEventAt };
            }
          }
        } catch (e) {
          if (e.code === "42P01" || e.code === "42703") {
            continue;
          }
          throw e;
        }
      }
      return { count: 0, lastEventAt: null };
    } finally {
      client6.release();
    }
  }
  async byTenant(tenantId, filter, first = 1e3, offset = 0) {
    const client6 = await this.pg.connect();
    try {
      const queries = [
        {
          sql: `SELECT id, action, resource_type, resource_id, details AS metadata, timestamp AS created_at, tenant_id
            FROM audit_events`,
          params: [],
          timeColumn: "timestamp",
          searchColumns: ["details"]
        },
        {
          sql: `SELECT id, action, target_type, target_id, metadata, created_at, tenant_id
            FROM audit_events`,
          params: [],
          timeColumn: "created_at",
          searchColumns: ["metadata"]
        },
        {
          sql: `SELECT id, action, resource_type, resource_id, resource_data, old_values, new_values, investigation_id, timestamp, tenant_id
            FROM audit_events`,
          params: [],
          timeColumn: "timestamp",
          searchColumns: ["resource_data", "old_values", "new_values"]
        },
        {
          sql: `SELECT id, source, subject_type, subject_id, note, created_at, tenant_id
            FROM provenance`,
          params: [],
          timeColumn: "created_at",
          searchColumns: ["note"]
        }
      ];
      for (const query3 of queries) {
        const { where, params } = this.buildTenantWhere(filter, {
          timeColumn: query3.timeColumn,
          searchColumns: query3.searchColumns
        });
        const scopedWhere = this.appendTenantScope(where, params, tenantId);
        const { clause, params: withPaging } = this.withPaging(
          scopedWhere.params,
          first,
          offset
        );
        try {
          const res = await client6.query(
            `${query3.sql} ${scopedWhere.where} ORDER BY ${query3.timeColumn} DESC${clause}`,
            withPaging
          );
          const mapped = res?.rows?.map(this.mapRow) ?? [];
          if (mapped.length) return mapped;
        } catch (e) {
          continue;
        }
      }
      return [];
    } finally {
      client6.release();
    }
  }
};

// src/routes/tenants.ts
init_database();
import archiver3 from "archiver";
import { createHash as createHash22, randomUUID as randomUUID35 } from "crypto";

// src/routes/tenants/provision.ts
init_auth4();
import { Router as Router18 } from "express";
import { z as z23 } from "zod";

// src/services/tenants/TenantProvisioningService.ts
init_logger2();
init_quota_manager();

// src/provenance/tenant-provisioning.ts
init_ledger();
function summarizeReceipt(entry) {
  return {
    id: entry.id,
    actionType: entry.actionType,
    timestamp: entry.timestamp.toISOString()
  };
}
async function emitTenantProvisioningReceipts(input) {
  const provisioningEntry = await provenanceLedger.appendEntry({
    tenantId: input.tenantId,
    timestamp: /* @__PURE__ */ new Date(),
    actionType: "TENANT_PROVISIONED",
    resourceType: "tenant",
    resourceId: input.tenantId,
    actorId: input.actorId,
    actorType: input.actorType,
    payload: {
      mutationType: "CREATE",
      entityId: input.tenantId,
      entityType: "Tenant",
      namespace: input.namespace,
      partitions: input.partitions,
      plan: input.plan,
      environment: input.environment,
      requestedSeats: input.requestedSeats,
      storageEstimateBytes: input.storageEstimateBytes
    },
    metadata: {
      correlationId: input.correlationId,
      requestId: input.requestId
    }
  });
  const quotaEntry = await provenanceLedger.appendEntry({
    tenantId: input.tenantId,
    timestamp: /* @__PURE__ */ new Date(),
    actionType: "TENANT_QUOTA_ASSIGNED",
    resourceType: "quota",
    resourceId: input.tenantId,
    actorId: input.actorId,
    actorType: input.actorType,
    payload: {
      mutationType: "CREATE",
      entityId: input.tenantId,
      entityType: "Quota",
      quota: input.quota,
      plan: input.plan,
      environment: input.environment
    },
    metadata: {
      correlationId: input.correlationId,
      requestId: input.requestId
    }
  });
  return {
    provisioning: summarizeReceipt(provisioningEntry),
    quotaAssignment: summarizeReceipt(quotaEntry)
  };
}

// src/services/tenants/TenantProvisioningService.ts
var TenantProvisioningService = class {
  quotaManager = QuotaManager.getInstance();
  createNamespace(tenant, environment) {
    const namespace = {
      id: `ns_${tenant.id}`,
      name: tenant.name,
      slug: `${tenant.slug}-${environment}`,
      environment
    };
    logger_default2.info(
      { tenantId: tenant.id, namespace },
      "Tenant namespace created"
    );
    return namespace;
  }
  createPartitions(tenant) {
    const region = tenant.region ?? "us-east-1";
    const partitions = [
      {
        id: `part_${tenant.id}_primary`,
        name: "primary",
        type: "primary",
        isolation: "shared",
        region
      },
      {
        id: `part_${tenant.id}_analytics`,
        name: "analytics",
        type: "analytics",
        isolation: "shared",
        region
      },
      {
        id: `part_${tenant.id}_audit`,
        name: "audit",
        type: "audit",
        isolation: "shared",
        region
      }
    ];
    logger_default2.info(
      { tenantId: tenant.id, partitions },
      "Tenant partitions created"
    );
    return partitions;
  }
  assignQuotas(tenantId, plan) {
    this.quotaManager.setTenantTier(tenantId, plan);
    const quota = this.quotaManager.getQuotaForTier(plan);
    logger_default2.info({ tenantId, plan, quota }, "Tenant quotas assigned");
    return quota;
  }
  async provisionTenant(request) {
    const namespace = this.createNamespace(request.tenant, request.environment);
    const partitions = this.createPartitions(request.tenant);
    const quota = this.assignQuotas(request.tenant.id, request.plan);
    const receipts = await emitTenantProvisioningReceipts({
      tenantId: request.tenant.id,
      actorId: request.actorId,
      actorType: request.actorType ?? "user",
      namespace,
      partitions,
      plan: request.plan,
      environment: request.environment,
      quota,
      requestedSeats: request.requestedSeats,
      storageEstimateBytes: request.storageEstimateBytes,
      correlationId: request.correlationId,
      requestId: request.requestId
    });
    return {
      namespace,
      partitions,
      quota,
      receipts
    };
  }
};
var tenantProvisioningService = new TenantProvisioningService();

// src/routes/tenants/provision.ts
init_logger2();
var router35 = Router18();
var provisionSchema = createTenantBaseSchema.extend({
  plan: z23.enum(["FREE", "STARTER", "PRO", "ENTERPRISE"]).default("STARTER"),
  environment: z23.enum(["prod", "staging", "dev"]).default("prod"),
  requestedSeats: z23.number().int().min(1).max(1e4).optional(),
  storageEstimateBytes: z23.number().int().min(0).optional()
});
router35.post("/", ensureAuthenticated, async (req, res) => {
  try {
    const actorId = req.user?.id;
    if (!actorId) {
      return res.status(401).json({ success: false, error: "Unauthorized: No user ID found" });
    }
    const body4 = provisionSchema.parse(req.body);
    const tenant = await tenantService.createTenant(body4, actorId);
    const provisioning = await tenantProvisioningService.provisionTenant({
      tenant,
      plan: body4.plan,
      environment: body4.environment,
      requestedSeats: body4.requestedSeats,
      storageEstimateBytes: body4.storageEstimateBytes,
      actorId,
      actorType: "user",
      correlationId: req.correlationId,
      requestId: req.id
    });
    const tenantContext = {
      tenantId: tenant.id,
      environment: body4.environment,
      privilegeTier: "standard",
      userId: actorId
    };
    const policy2 = tenantIsolationGuard.evaluatePolicy(tenantContext, {
      action: "tenant.provision",
      environment: body4.environment,
      resourceTenantId: tenant.id
    });
    if (!policy2.allowed) {
      return res.status(policy2.status || 403).json({
        success: false,
        error: policy2.reason || "Isolation policy denied provisioning"
      });
    }
    return res.status(201).json({
      success: true,
      data: {
        tenant,
        isolationDefaults: {
          environment: tenantContext.environment,
          privilegeTier: tenantContext.privilegeTier,
          quotas: provisioning.quota
        },
        namespace: provisioning.namespace,
        partitions: provisioning.partitions,
        quota: provisioning.quota
      },
      receipts: provisioning.receipts
    });
  } catch (error) {
    if (error instanceof z23.ZodError) {
      return res.status(400).json({ success: false, error: "Validation Error", details: error.errors });
    }
    logger_default2.error("Tenant provisioning failed", error);
    return res.status(500).json({ success: false, error: "Internal Server Error" });
  }
});
var provision_default = router35;

// src/services/TenantUsageService.ts
init_database();
init_logger2();
var rangeOrder = [
  "24h",
  "7d",
  "30d",
  "90d",
  "month",
  "quarter",
  "year"
];
var TenantUsageService = class {
  get pool() {
    return getPostgresPool2();
  }
  getUsageRange(range) {
    const key = range || "30d";
    if (!rangeOrder.includes(key)) {
      throw new Error(`Invalid range: ${range}`);
    }
    const end = /* @__PURE__ */ new Date();
    let start = new Date(end);
    if (key === "24h") {
      start = new Date(end.getTime() - 24 * 60 * 60 * 1e3);
    } else if (key === "7d") {
      start = new Date(end.getTime() - 7 * 24 * 60 * 60 * 1e3);
    } else if (key === "30d") {
      start = new Date(end.getTime() - 30 * 24 * 60 * 60 * 1e3);
    } else if (key === "90d") {
      start = new Date(end.getTime() - 90 * 24 * 60 * 60 * 1e3);
    } else if (key === "month") {
      start = new Date(Date.UTC(end.getUTCFullYear(), end.getUTCMonth(), 1));
    } else if (key === "quarter") {
      const quarterStartMonth = Math.floor(end.getUTCMonth() / 3) * 3;
      start = new Date(Date.UTC(end.getUTCFullYear(), quarterStartMonth, 1));
    } else if (key === "year") {
      start = new Date(Date.UTC(end.getUTCFullYear(), 0, 1));
    }
    return { key, start, end };
  }
  async getTenantUsage(tenantId, range) {
    const { key, start, end } = this.getUsageRange(range);
    try {
      const totalsResult = await this.pool.read(
        `
          SELECT kind, unit, SUM(quantity) AS total_quantity
          FROM usage_events
          WHERE tenant_id = $1
            AND occurred_at >= $2
            AND occurred_at <= $3
          GROUP BY kind, unit
          ORDER BY kind
        `,
        [tenantId, start.toISOString(), end.toISOString()]
      );
      const workflowResult = await this.pool.read(
        `
          SELECT
            COALESCE(metadata->>'workflow', 'unknown') AS workflow,
            kind,
            unit,
            SUM(quantity) AS total_quantity
          FROM usage_events
          WHERE tenant_id = $1
            AND occurred_at >= $2
            AND occurred_at <= $3
          GROUP BY workflow, kind, unit
          ORDER BY workflow, kind
        `,
        [tenantId, start.toISOString(), end.toISOString()]
      );
      const environmentResult = await this.pool.read(
        `
          SELECT
            COALESCE(metadata->>'environment', 'unknown') AS environment,
            kind,
            unit,
            SUM(quantity) AS total_quantity
          FROM usage_events
          WHERE tenant_id = $1
            AND occurred_at >= $2
            AND occurred_at <= $3
          GROUP BY environment, kind, unit
          ORDER BY environment, kind
        `,
        [tenantId, start.toISOString(), end.toISOString()]
      );
      const workflowEnvironmentResult = await this.pool.read(
        `
          SELECT
            COALESCE(metadata->>'workflow', 'unknown') AS workflow,
            COALESCE(metadata->>'environment', 'unknown') AS environment,
            kind,
            unit,
            SUM(quantity) AS total_quantity
          FROM usage_events
          WHERE tenant_id = $1
            AND occurred_at >= $2
            AND occurred_at <= $3
          GROUP BY workflow, environment, kind, unit
          ORDER BY workflow, environment, kind
        `,
        [tenantId, start.toISOString(), end.toISOString()]
      );
      const totalsRows = totalsResult.rows;
      const workflowRows = workflowResult.rows;
      const environmentRows = environmentResult.rows;
      const workflowEnvironmentRows = workflowEnvironmentResult.rows;
      return {
        tenantId,
        range: {
          key,
          start: start.toISOString(),
          end: end.toISOString()
        },
        totals: totalsRows.map((row) => ({
          kind: row.kind,
          unit: row.unit,
          total: Number(row.total_quantity)
        })),
        breakdown: {
          byWorkflow: this.groupByWorkflow(workflowRows),
          byEnvironment: this.groupByEnvironment(environmentRows),
          byWorkflowEnvironment: this.groupByCompositeKey(
            workflowEnvironmentRows
          )
        }
      };
    } catch (error) {
      logger_default2.error("Failed to fetch tenant usage summary", {
        error,
        tenantId
      });
      throw error;
    }
  }
  groupByWorkflow(rows) {
    const grouped = /* @__PURE__ */ new Map();
    rows.forEach((row) => {
      const groupKey = row.workflow;
      const totals = grouped.get(groupKey) || [];
      totals.push({
        kind: row.kind,
        unit: row.unit,
        total: Number(row.total_quantity)
      });
      grouped.set(groupKey, totals);
    });
    return Array.from(grouped.entries()).map(([groupKey, totals]) => ({
      workflow: groupKey,
      totals
    }));
  }
  groupByEnvironment(rows) {
    const grouped = /* @__PURE__ */ new Map();
    rows.forEach((row) => {
      const groupKey = row.environment;
      const totals = grouped.get(groupKey) || [];
      totals.push({
        kind: row.kind,
        unit: row.unit,
        total: Number(row.total_quantity)
      });
      grouped.set(groupKey, totals);
    });
    return Array.from(grouped.entries()).map(([groupKey, totals]) => ({
      environment: groupKey,
      totals
    }));
  }
  groupByCompositeKey(rows) {
    const grouped = /* @__PURE__ */ new Map();
    rows.forEach((row) => {
      const composite = `${row.workflow}::${row.environment}`;
      const existing = grouped.get(composite) || {
        workflow: row.workflow,
        environment: row.environment,
        totals: []
      };
      existing.totals.push({
        kind: row.kind,
        unit: row.unit,
        total: Number(row.total_quantity)
      });
      grouped.set(composite, existing);
    });
    return Array.from(grouped.values());
  }
};
var tenantUsageService = new TenantUsageService();

// src/routes/tenants.ts
var router36 = Router19();
var settingsSchema = z24.object({
  settings: z24.record(z24.any())
});
var disableSchema = z24.object({
  reason: z24.string().min(3).optional()
});
var auditQuerySchema = z24.object({
  limit: z24.coerce.number().min(1).max(200).default(50),
  offset: z24.coerce.number().min(0).default(0)
});
var usageQuerySchema = z24.object({
  range: z24.string().optional()
});
router36.use("/provision", provision_default);
function policyGate() {
  return (req, _res, next) => {
    const authReq = req;
    req.body = { ...req.body, tenantId: req.params.id };
    next();
  };
}
function ensureTenantScope(req, res, next) {
  const authReq = req;
  const tenantId = req.params.id;
  const userTenant = authReq.user?.tenantId || authReq.user?.tenant_id;
  const isSuper = ["SUPER_ADMIN", "ADMIN", "admin"].includes(authReq.user?.role || "");
  if (!isSuper && userTenant && userTenant !== tenantId) {
    res.status(403).json({ success: false, error: "Forbidden" });
    return;
  }
  next();
}
function buildReceipt(action, tenantId, actorId) {
  const issuedAt = (/* @__PURE__ */ new Date()).toISOString();
  const payload = `${action}:${tenantId}:${actorId}:${issuedAt}`;
  const hash3 = createHash22("sha256").update(payload).digest("hex");
  return {
    id: randomUUID35(),
    action,
    tenantId,
    actorId,
    issuedAt,
    hash: hash3,
    policy: "abac.ensurePolicy"
  };
}
router36.post("/", ensureAuthenticated, ensurePolicy("create", "tenant"), async (req, res) => {
  try {
    const authReq = req;
    const input = createTenantSchema.parse(req.body);
    const actorId = authReq.user?.id;
    if (!actorId) {
      return res.status(401).json({ success: false, error: "Unauthorized: No user ID found" });
    }
    const tenant = await tenantService.createTenant(input, actorId);
    res.status(201).json({
      success: true,
      data: tenant,
      receipt: buildReceipt("TENANT_CREATED", tenant.id, actorId)
    });
  } catch (error) {
    if (error instanceof z24.ZodError) {
      res.status(400).json({
        success: false,
        error: "Validation Error",
        details: error.errors
      });
    } else if (error instanceof Error && error.message.includes("already taken")) {
      res.status(409).json({
        success: false,
        error: error.message
      });
    } else {
      logger_default2.error("Error in POST /api/tenants:", error);
      res.status(500).json({
        success: false,
        error: "Internal Server Error"
      });
    }
  }
});
router36.get("/:id", ensureAuthenticated, async (req, res) => {
  try {
    const authReq = req;
    const tenantId = req.params.id;
    const tenant = await tenantService.getTenant(tenantId);
    if (!tenant) {
      return res.status(404).json({ success: false, error: "Tenant not found" });
    }
    const userId = authReq.user?.id;
    const userTenantId = authReq.user?.tenantId || authReq.user?.tenant_id;
    const isCreator = tenant.createdBy === userId;
    const isMember = userTenantId === tenant.id;
    const isSuperAdmin = authReq.user?.role === "SUPER_ADMIN";
    if (!isCreator && !isMember && !isSuperAdmin) {
      logger_default2.warn(`Access denied for user ${userId} to tenant ${tenantId}`);
      return res.status(403).json({ success: false, error: "Forbidden" });
    }
    res.json({ success: true, data: tenant });
  } catch (error) {
    logger_default2.error("Error in GET /api/tenants/:id:", error);
    res.status(500).json({ success: false, error: "Internal Server Error" });
  }
});
router36.get(
  "/:id/settings",
  ensureAuthenticated,
  ensureTenantScope,
  policyGate(),
  ensurePolicy("read", "tenant"),
  async (req, res) => {
    try {
      const authReq = req;
      const tenantId = req.params.id;
      const data = await tenantService.getTenantSettings(tenantId);
      return res.json({
        success: true,
        data,
        receipt: buildReceipt("TENANT_SETTINGS_VIEWED", tenantId, authReq.user?.id || "unknown")
      });
    } catch (error) {
      logger_default2.error("Error in GET /api/tenants/:id/settings:", error);
      if (error instanceof Error && error.message === "Tenant not found") {
        return res.status(404).json({ success: false, error: error.message });
      }
      return res.status(500).json({ success: false, error: "Internal Server Error" });
    }
  }
);
router36.put(
  "/:id/settings",
  ensureAuthenticated,
  ensureTenantScope,
  policyGate(),
  ensurePolicy("update", "tenant"),
  async (req, res) => {
    try {
      const authReq = req;
      const body4 = settingsSchema.parse(req.body);
      const tenantId = req.params.id;
      const actorId = authReq.user?.id || "unknown";
      const updated = await tenantService.updateSettings(tenantId, body4.settings, actorId);
      return res.json({
        success: true,
        data: updated,
        receipt: buildReceipt("TENANT_SETTINGS_UPDATED", tenantId, actorId)
      });
    } catch (error) {
      if (error instanceof z24.ZodError) {
        return res.status(400).json({ success: false, error: "Validation Error", details: error.errors });
      }
      if (error instanceof Error && error.message === "Tenant not found") {
        return res.status(404).json({ success: false, error: error.message });
      }
      logger_default2.error("Error in PUT /api/tenants/:id/settings:", error);
      return res.status(500).json({ success: false, error: "Internal Server Error" });
    }
  }
);
router36.post(
  "/:id/disable",
  ensureAuthenticated,
  ensureTenantScope,
  policyGate(),
  ensurePolicy("update", "tenant"),
  async (req, res) => {
    try {
      const authReq = req;
      const { reason } = disableSchema.parse(req.body);
      const tenantId = req.params.id;
      const actorId = authReq.user?.id || "unknown";
      const updated = await tenantService.disableTenant(tenantId, actorId, reason);
      return res.json({
        success: true,
        data: updated,
        receipt: buildReceipt("TENANT_DISABLED", tenantId, actorId)
      });
    } catch (error) {
      if (error instanceof z24.ZodError) {
        return res.status(400).json({ success: false, error: "Validation Error", details: error.errors });
      }
      if (error instanceof Error && error.message === "Tenant not found") {
        return res.status(404).json({ success: false, error: error.message });
      }
      logger_default2.error("Error in POST /api/tenants/:id/disable:", error);
      return res.status(500).json({ success: false, error: "Internal Server Error" });
    }
  }
);
router36.get(
  "/:id/usage",
  ensureAuthenticated,
  ensureTenantScope,
  policyGate(),
  ensurePolicy("read", "tenant"),
  async (req, res) => {
    try {
      const authReq = req;
      const tenantId = req.params.id;
      const { range } = usageQuerySchema.parse(req.query);
      const usage = await tenantUsageService.getTenantUsage(tenantId, range);
      return res.json({
        success: true,
        data: usage,
        receipt: buildReceipt("TENANT_USAGE_VIEWED", tenantId, authReq.user?.id || "unknown")
      });
    } catch (error) {
      if (error instanceof z24.ZodError) {
        return res.status(400).json({ success: false, error: "Validation Error", details: error.errors });
      }
      if (error instanceof Error && error.message.startsWith("Invalid range")) {
        return res.status(400).json({ success: false, error: error.message });
      }
      logger_default2.error("Error in GET /api/tenants/:id/usage:", error);
      return res.status(500).json({ success: false, error: "Internal Server Error" });
    }
  }
);
router36.get(
  "/:id/audit",
  ensureAuthenticated,
  ensureTenantScope,
  policyGate(),
  ensurePolicy("read", "tenant"),
  async (req, res) => {
    try {
      const authReq = req;
      const query3 = auditQuerySchema.parse(req.query);
      const tenantId = req.params.id;
      const repo = new ProvenanceRepo(getPostgresPool2());
      const events = await repo.by("investigation", tenantId, void 0, query3.limit, query3.offset, tenantId);
      return res.json({
        success: true,
        data: events,
        receipt: buildReceipt("TENANT_AUDIT_VIEWED", tenantId, authReq.user?.id || "unknown")
      });
    } catch (error) {
      logger_default2.error("Error in GET /api/tenants/:id/audit:", error);
      return res.status(500).json({ success: false, error: "Internal Server Error" });
    }
  }
);
router36.get(
  "/:id/audit/export",
  ensureAuthenticated,
  ensureTenantScope,
  policyGate(),
  ensurePolicy("read", "tenant"),
  async (req, res) => {
    try {
      const authReq = req;
      const tenantId = req.params.id;
      const repo = new ProvenanceRepo(getPostgresPool2());
      const events = await repo.by("investigation", tenantId, void 0, 500, 0, tenantId);
      res.setHeader("Content-Type", "application/zip");
      res.setHeader(
        "Content-Disposition",
        `attachment; filename="tenant-${tenantId}-evidence.zip"`
      );
      const archive = archiver3("zip", { zlib: { level: 9 } });
      archive.on("error", (err) => {
        logger_default2.error("Archive error", err);
        res.status(500).end(`Archive error: ${err.message}`);
      });
      archive.pipe(res);
      const actorId = authReq.user?.id || "unknown";
      const metadata = {
        tenantId,
        exportedAt: (/* @__PURE__ */ new Date()).toISOString(),
        actorId,
        eventCount: events.length
      };
      archive.append(JSON.stringify(metadata, null, 2), { name: "metadata.json" });
      archive.append(JSON.stringify(events, null, 2), { name: "events.json" });
      const bundleHash = createHash22("sha256").update(JSON.stringify({ metadata, events })).digest("hex");
      archive.append(
        JSON.stringify(
          {
            receipt: buildReceipt("TENANT_EVIDENCE_EXPORTED", tenantId, actorId),
            bundleHash
          },
          null,
          2
        ),
        { name: "receipt.json" }
      );
      await archive.finalize();
    } catch (error) {
      logger_default2.error("Error in GET /api/tenants/:id/audit/export:", error);
      return res.status(500).json({ success: false, error: "Internal Server Error" });
    }
  }
);
var tenants_default = router36;

// src/routes/summit-investigate.ts
import { Router as Router20 } from "express";

// src/services/VerificationSwarmService.ts
init_LLMService();
init_logger2();
import { EventEmitter as EventEmitter11 } from "events";
import { randomUUID as uuidv413 } from "crypto";
var VerificationSwarmService = class extends EventEmitter11 {
  llmService;
  pendingVerifications = /* @__PURE__ */ new Map();
  results = /* @__PURE__ */ new Map();
  constructor() {
    super();
    this.llmService = new LLMService_default();
    logger_default2.info("[VerificationSwarm] Service initialized");
  }
  /**
   * Submit a verification request to the swarm.
   */
  async submitVerification(request) {
    const id = request.id || uuidv413();
    this.pendingVerifications.set(id, request);
    this.processVerification(id, request).catch((err) => {
      logger_default2.error(`[VerificationSwarm] Error processing ${id}:`, err);
      this.emit("error", {
        id,
        error: err instanceof Error ? err.message : String(err)
      });
    });
    return id;
  }
  /**
   * Get the result of a verification request.
   */
  getResult(id) {
    return this.results.get(id);
  }
  /**
   * Core swarm orchestration logic.
   */
  async processVerification(id, request) {
    logger_default2.info(`[VerificationSwarm] Spawning agents for request ${id}`);
    const [photoAnalysis, geoAnalysis, factCheck] = await Promise.all([
      this.runPhotoAnalyst(request),
      this.runGeoExpert(request),
      this.runFactChecker(request)
    ]);
    const synthesis = await this.synthesizeSwarmResults(request, { photoAnalysis, geoAnalysis, factCheck });
    const result2 = {
      id: uuidv413(),
      requestId: id,
      verdict: synthesis.verdict,
      confidence: synthesis.confidence,
      agents_consensus: synthesis.consensus,
      details: {
        photoAnalysis,
        geoAnalysis,
        factCheck,
        swarmDialogue: synthesis.dialogue
      },
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    };
    this.results.set(id, result2);
    this.pendingVerifications.delete(id);
    this.emit("verificationComplete", result2);
    logger_default2.info(`[VerificationSwarm] Completed request ${id} with verdict: ${result2.verdict}`);
  }
  // --- Agent Implementations (Simulated with LLM prompts) ---
  async runPhotoAnalyst(request) {
    if (request.type !== "IMAGE" && request.type !== "VIDEO") return { status: "skipped" };
    const prompt = `
      You are 'PhotoAnalyst', an expert in digital forensics.
      Analyze the following asset for signs of manipulation (deepfake, Photoshop, splicing).
      Asset: ${request.content}

      Check for:
      1. Lighting inconsistencies (shadows vs light sources).
      2. Metadata anomalies (if provided).
      3. Compression artifacts inconsistent with the platform.

      Return a JSON object with 'manipulated' (boolean), 'confidence' (0-1), and 'reasoning'.
    `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0.1,
        maxTokens: 500
      });
      return { raw: response };
    } catch (e) {
      logger_default2.error(`[PhotoAnalyst] Failed`, e);
      return { error: "Agent failed" };
    }
  }
  async runGeoExpert(request) {
    const prompt = `
      You are 'GeoExpert', a specialist in geolocation and chronolocation.
      Analyze the content/claim: "${request.content}"
      Context: ${JSON.stringify(request.context || {})}

      1. Identify potential landmarks, weather patterns, or sun angles (SunCalc).
      2. Verify if the claimed location matches visual evidence.
      3. Cross-reference with known satellite imagery data (simulated).

      Return a JSON object with 'location_match' (boolean), 'confidence' (0-1), and 'coordinates' (if found).
    `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0.1,
        maxTokens: 500
      });
      return { raw: response };
    } catch (e) {
      return { error: "Agent failed" };
    }
  }
  async runFactChecker(request) {
    const prompt = `
      You are 'FactChecker', a researcher with access to real-time search tools.
      Verify the claim: "${request.content}"

      1. Search for corroborating reports from trusted sources.
      2. Check for "reverse image search" matches (simulated).
      3. Identify if this is a known hoax or misinformation.

      Return a JSON object with 'verified' (boolean), 'sources' (list), and 'notes'.
    `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0.2,
        maxTokens: 500
      });
      return { raw: response };
    } catch (e) {
      return { error: "Agent failed" };
    }
  }
  async synthesizeSwarmResults(request, agentResults) {
    const prompt = `
      You are the 'SwarmLead', synthesizing results from multiple specialized agents.

      Request: ${JSON.stringify(request)}

      Agent Reports:
      - PhotoAnalyst: ${JSON.stringify(agentResults.photoAnalysis)}
      - GeoExpert: ${JSON.stringify(agentResults.geoAnalysis)}
      - FactChecker: ${JSON.stringify(agentResults.factCheck)}

      Perform a 'Self-Critique':
      - Do the agents agree?
      - Are there logical fallacies?
      - Is the evidence sufficient?

      Determine a final VERDICT (VERIFIED, DEBUNKED, INCONCLUSIVE), CONFIDENCE (0-1), and CONSENSUS score.
      Output pure JSON.
    `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0,
        maxTokens: 500
      });
      try {
        return JSON.parse(response);
      } catch {
        return { verdict: "INCONCLUSIVE", confidence: 0.5, consensus: 0.5, dialogue: [response] };
      }
    } catch (e) {
      return { verdict: "INCONCLUSIVE", confidence: 0, consensus: 0, dialogue: ["Synthesis failed"] };
    }
  }
};
var verificationSwarmService = new VerificationSwarmService();

// src/services/EvidenceFusionService.ts
init_LLMService();
init_logger2();
import { EventEmitter as EventEmitter12 } from "events";
var EvidenceFusionService = class extends EventEmitter12 {
  llmService;
  constructor() {
    super();
    this.llmService = new LLMService_default();
    logger_default2.info("[EvidenceFusion] Service initialized");
  }
  /**
   * Synthesize a timeline from a set of evidence.
   */
  async synthesizeTimeline(evidenceList) {
    if (!evidenceList || evidenceList.length === 0) {
      return { timeline: [] };
    }
    const prompt = `
      You are an expert Investigator.
      Analyze the following evidence items and construct a chronological timeline.
      Identify causal links between events (e.g., Event A caused Event B).

      Evidence:
      ${JSON.stringify(evidenceList)}

      Return a JSON object with:
      - 'timeline': Array of { timestamp, eventDescription, evidenceRefId }
      - 'gaps': List of missing information or logical gaps.
    `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0.2,
        maxTokens: 1500
      });
      try {
        return JSON.parse(response);
      } catch {
        return { raw: response };
      }
    } catch (e) {
      logger_default2.error("[EvidenceFusion] Timeline synthesis failed", e instanceof Error ? e.message : String(e));
      throw e;
    }
  }
  /**
   * Generate hypotheses explaining the observed evidence.
   */
  async generateHypotheses(evidenceList, context4) {
    const prompt = `
      Based on the provided evidence, generate 3 plausible hypotheses to explain the situation.
      Each hypothesis should be distinct and ranked by likelihood.

      Context: ${context4 || "General Investigation"}
      Evidence: ${JSON.stringify(evidenceList.map((e) => ({ id: e.id, content: e.content, time: e.timestamp })))}

      For each hypothesis, cite the evidence IDs that support it and those that contradict it.
      Return a JSON array of Hypothesis objects.
    `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0.4,
        maxTokens: 2e3
      });
      try {
        const parsed = JSON.parse(response);
        return Array.isArray(parsed) ? parsed : parsed.hypotheses || [];
      } catch {
        logger_default2.warn("[EvidenceFusion] Failed to parse hypotheses JSON");
        return [];
      }
    } catch (e) {
      logger_default2.error("[EvidenceFusion] Hypothesis generation failed", e instanceof Error ? e.message : String(e));
      throw e;
    }
  }
  /**
   * Link evidence items semantically (mocking GraphRAG integration).
   * In a full implementation, this would query the GraphRAG service.
   */
  async linkEvidence(items) {
    if (items.length < 2) return [];
    const links = [];
    links.push({
      source: items[0].id,
      target: items[1]?.id || items[0].id,
      relation: "RELATED_TO (Semantic Similarity)"
    });
    return links;
  }
};
var evidenceFusionService = new EvidenceFusionService();

// src/services/DeepfakeHunterService.ts
init_LLMService();
init_logger2();
import { EventEmitter as EventEmitter13 } from "events";
import { randomUUID as uuidv414 } from "crypto";
var DeepfakeHunterService = class extends EventEmitter13 {
  llmService;
  constructor() {
    super();
    this.llmService = new LLMService_default();
    logger_default2.info("[DeepfakeHunter] Service initialized");
  }
  /**
   * Scan media for AI manipulation and propaganda.
   */
  async scanMedia(request) {
    logger_default2.info(`[DeepfakeHunter] Scanning ${request.url} (${request.type})`);
    const watermarkCheck = this.checkWatermarks(request.url);
    let semanticAnalysis = { isPropaganda: false, confidence: 0 };
    let translation = void 0;
    if (request.type === "TEXT") {
      const content = "Simulated content from " + request.url;
      if (request.language && request.language !== "en") {
        translation = await this.translateContent(content, request.language);
      }
      semanticAnalysis = await this.analyzePropaganda(translation ? translation.translated : content);
    }
    const isDeepfake = watermarkCheck || semanticAnalysis.isPropaganda && semanticAnalysis.confidence > 0.8;
    return {
      id: uuidv414(),
      isDeepfake,
      confidence: isDeepfake ? 0.95 : 0.1,
      detectionMethods: {
        watermarkDetected: watermarkCheck,
        spectralArtifacts: false,
        // Placeholder
        semanticInconsistencies: semanticAnalysis.isPropaganda
      },
      translation,
      originAnalysis: semanticAnalysis.isPropaganda ? "Likely coordinated inauthentic behavior" : "Organic"
    };
  }
  checkWatermarks(url) {
    return url.includes("generated") || url.includes("synthetic");
  }
  async translateContent(content, sourceLang) {
    const prompt = `
        Translate the following text from ${sourceLang} to English.
        Preserve nuances relevant to OSINT analysis (slang, military terminology).

        Text: ${content}
      `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0
      });
      return {
        original: content,
        translated: response,
        detectedLanguage: sourceLang
      };
    } catch (e) {
      logger_default2.error("[DeepfakeHunter] Translation failed", e);
      return { original: content, translated: content, detectedLanguage: sourceLang };
    }
  }
  async analyzePropaganda(text) {
    const prompt = `
        Analyze the text for signs of AI generation or propaganda techniques (e.g., appeal to emotion, logical fallacies, repetition).

        Text: "${text}"

        Return JSON with 'isPropaganda' (bool), 'confidence' (0-1), and 'indicators' (list).
      `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0.1
      });
      try {
        return JSON.parse(response);
      } catch {
        return { isPropaganda: false, confidence: 0.5 };
      }
    } catch (e) {
      return { isPropaganda: false, confidence: 0 };
    }
  }
};
var deepfakeHunterService = new DeepfakeHunterService();

// src/services/PredictiveScenarioSimulator.ts
init_LLMService();
init_logger2();
import { EventEmitter as EventEmitter14 } from "events";
import { randomUUID as uuidv415 } from "crypto";
var PredictiveScenarioSimulator = class extends EventEmitter14 {
  llmService;
  constructor() {
    super();
    this.llmService = new LLMService_default();
    logger_default2.info("[PredictiveSimulator] Service initialized");
  }
  /**
   * Run a simulation based on parameters.
   */
  async simulateScenario(params) {
    logger_default2.info(`[PredictiveSimulator] Running ${params.scenarioType} simulation for ${params.timeHorizonDays} days`);
    const prompt = `
            You are a Strategic Analyst AI.
            Run a Predictive "What-If" Simulation for the following scenario.

            Type: ${params.scenarioType}
            Initial Conditions: ${JSON.stringify(params.initialConditions)}
            Time Horizon: ${params.timeHorizonDays} days
            Key Variables to Vary: ${params.variables.join(", ")}

            Generate 3 distinct scenarios (Best Case, Worst Case, Most Likely).
            For each, provide:
            1. Probability (must sum to 1.0 approx).
            2. A coherent narrative of how events unfold.
            3. A timeline of key events.
            4. Estimated Risk Level.

            Return JSON in the structure of 'ScenarioResult'.
        `;
    try {
      const response = await this.llmService.complete(prompt, {
        temperature: 0.5,
        // Higher temp for creativity in simulation
        maxTokens: 2500
      });
      try {
        const parsed = JSON.parse(response);
        return {
          id: uuidv415(),
          scenarios: parsed.scenarios || parsed,
          // Handle varied LLM output
          generatedAt: (/* @__PURE__ */ new Date()).toISOString()
        };
      } catch (e) {
        logger_default2.error("[PredictiveSimulator] JSON parse failed", {
          error: e instanceof Error ? e.message : String(e)
        });
        return {
          id: uuidv415(),
          scenarios: [{
            name: "Simulation Error",
            probability: 0,
            narrative: "Failed to parse simulation output: " + response.substring(0, 100) + "...",
            keyEvents: [],
            riskLevel: "LOW"
          }],
          generatedAt: (/* @__PURE__ */ new Date()).toISOString()
        };
      }
    } catch (e) {
      logger_default2.error("[PredictiveSimulator] Simulation failed", {
        error: e instanceof Error ? e.message : String(e)
      });
      throw e;
    }
  }
  /**
   * Generate a formal report from a simulation result.
   */
  async generateReport(simulationId, format) {
    return `# Simulation Report ${simulationId}

Generated by SummitInvestigate Predictive Engine.

...`;
  }
};
var predictiveScenarioSimulator = new PredictiveScenarioSimulator();

// src/routes/summit-investigate.ts
init_logger2();
var router37 = Router20();
router37.post("/verification/submit", async (req, res) => {
  try {
    const id = await verificationSwarmService.submitVerification(req.body);
    res.json({ success: true, id, message: "Verification request submitted to swarm" });
  } catch (error) {
    logger_default2.error("Verification submit error", error);
    res.status(500).json({ success: false, error: error.message });
  }
});
router37.get("/verification/:id", (req, res) => {
  const result2 = verificationSwarmService.getResult(req.params.id);
  if (!result2) return res.status(404).json({ error: "Not found or pending" });
  res.json(result2);
});
router37.post("/fusion/timeline", async (req, res) => {
  try {
    const { evidence } = req.body;
    const timeline = await evidenceFusionService.synthesizeTimeline(evidence);
    res.json(timeline);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router37.post("/fusion/hypotheses", async (req, res) => {
  try {
    const { evidence, context: context4 } = req.body;
    const hypotheses = await evidenceFusionService.generateHypotheses(evidence, context4);
    res.json({ hypotheses });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router37.post("/deepfake/scan", async (req, res) => {
  try {
    const result2 = await deepfakeHunterService.scanMedia(req.body);
    res.json(result2);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router37.post("/simulation/run", async (req, res) => {
  try {
    const result2 = await predictiveScenarioSimulator.simulateScenario(req.body);
    res.json(result2);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
var summit_investigate_default = router37;

// src/services/SummitInvestigate.ts
init_logger2();
var SummitInvestigate = class {
  static initialize(app) {
    logger_default2.info("[SummitInvestigate] Initializing platform modules...");
    app.use("/api/summit-investigate", summit_investigate_default);
    logger_default2.info("[SummitInvestigate] Modules loaded: VerificationSwarm, EvidenceFusion, DeepfakeHunter, PredictiveSimulator, CollaborationHub");
    logger_default2.info("[SummitInvestigate] Platform ready.");
  }
};

// src/ingest/stream.ts
import { Kafka } from "kafkajs";

// src/policy/contracts.ts
init_errors();
init_logger();
init_postgres();
import { z as z25 } from "zod";
var SCHEMA_REGISTRY = /* @__PURE__ */ new Map();
SCHEMA_REGISTRY.set("user-clickstream-v1", z25.object({
  userId: z25.string(),
  url: z25.string().url(),
  timestamp: z25.string().datetime(),
  ip: z25.string().ip().optional(),
  userAgent: z25.string().optional()
}));
SCHEMA_REGISTRY.set("transaction-v1", z25.object({
  txId: z25.string().uuid(),
  amount: z25.number().positive(),
  currency: z25.enum(["USD", "EUR"]),
  userId: z25.string(),
  metadata: z25.record(z25.any()).optional()
}));
async function applyContract(schemaId, data, tenantId) {
  const validator2 = SCHEMA_REGISTRY.get(schemaId);
  if (!validator2) {
    throw new AppError(`Unknown schema ID: ${schemaId}`, 400);
  }
  const parseResult = validator2.safeParse(data);
  if (!parseResult.success) {
    throw new AppError(`Schema validation failed for ${schemaId}: ${parseResult.error.message}`, 400);
  }
  const tid = tenantId || data.tenantId;
  if (tid) {
    const contracts = await fetchContracts(tid, schemaId);
    for (const contract of contracts) {
      enforceSpecificContract(contract, data);
    }
  }
}
async function fetchContracts(tenantId, schemaId) {
  const pool4 = getPostgresPool();
  const result2 = await pool4.query(
    `SELECT * FROM data_contracts
         WHERE tenant_id = $1 AND requirements->>'schemaId' = $2`,
    [tenantId, schemaId]
  );
  return result2.rows.map((row) => ({
    id: row.id,
    tenantId: row.tenant_id,
    dataset: row.dataset,
    requirements: row.requirements,
    action: row.action
  }));
}
function enforceSpecificContract(contract, data) {
  if (contract.requirements.pii === false) {
    const piiPatterns = [
      /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/,
      // Email
      /\b\d{3}-\d{2}-\d{4}\b/
      // SSN
    ];
    const jsonStr = JSON.stringify(data);
    const hasPii = piiPatterns.some((p) => p.test(jsonStr));
    if (hasPii) {
      handleViolation(contract, "PII detected in non-PII dataset");
    }
  }
  if (contract.requirements.residency && contract.requirements.residency.length > 0) {
    const currentRegion = process.env.REGION || "us-east";
    if (!contract.requirements.residency.includes(currentRegion)) {
      handleViolation(contract, `Data residency violation: ${currentRegion} not in [${contract.requirements.residency.join(",")}]`);
    }
  }
}
function handleViolation(contract, reason) {
  logger.warn({ contractId: contract.id, action: contract.action, reason }, "Contract violation detected");
  if (contract.action === "BLOCK") {
    throw new AppError(`Contract Violation: ${reason}`, 403);
  }
}

// src/ingest/stream.ts
init_logger();
init_postgres();
import { Counter as Counter13, Gauge as Gauge11, Histogram as Histogram10 } from "prom-client";
var ingestLag = new Gauge11({
  name: "ingest_kafka_consumer_lag",
  help: "Consumer lag in offsets",
  labelNames: ["topic", "partition"]
});
var ingestErrors = new Counter13({
  name: "ingest_kafka_errors_total",
  help: "Total ingestion errors",
  labelNames: ["topic", "type"]
});
var ingestLatency = new Histogram10({
  name: "ingest_e2e_latency_seconds",
  help: "End-to-end ingestion latency",
  labelNames: ["topic"]
});
var KAFKA_BROKERS = (process.env.KAFKA_BROKERS || "localhost:9092").split(",");
var GROUP_ID = "intelgraph-ingest-v1";
var DLQ_TOPIC_SUFFIX = "-dlq";
var StreamingIngestService = class {
  kafka;
  consumer;
  isRunning = false;
  producer;
  // Lazy init
  constructor() {
    this.kafka = new Kafka({
      clientId: "intelgraph-ingest",
      brokers: KAFKA_BROKERS,
      logLevel: 1
      // ERROR only to reduce noise
    });
    this.consumer = this.kafka.consumer({
      groupId: GROUP_ID
      // Exactly-once support via transactional producer is handled at producer side,
      // but consumer idempotency relies on managing offsets or business logic idempotency.
      // We implement business logic idempotency.
    });
  }
  async start(topics) {
    if (this.isRunning) return;
    try {
      await this.consumer.connect();
      await this.consumer.subscribe({ topics, fromBeginning: false });
      this.isRunning = true;
      logger.info({ topics }, "Kafka consumer connected");
      this.startLagMonitor(topics);
      await this.consumer.run({
        eachMessage: this.handleMessage.bind(this),
        autoCommit: false
        // We commit manually after processing
      });
    } catch (e) {
      logger.error(e, "Failed to start Kafka consumer");
      throw e;
    }
  }
  async startLagMonitor(topics) {
    const admin = this.kafka.admin();
    try {
      await admin.connect();
      const checkLag = async () => {
        if (!this.isRunning) return;
        try {
          const offsets = await admin.fetchOffsets({ groupId: GROUP_ID, topics });
          const topicOffsets = await admin.fetchTopicOffsets(topics[0]);
          const topicGroupOffsets = offsets.find((t) => t.topic === topics[0]);
          if (topicGroupOffsets && topicGroupOffsets.partitions) {
            for (const p of topicGroupOffsets.partitions) {
              const endOffset = topicOffsets.find((t) => t.partition === p.partition);
              if (endOffset) {
                const lag = BigInt(endOffset.offset) - BigInt(p.offset);
                ingestLag.set({ topic: topics[0], partition: p.partition }, Number(lag));
              }
            }
          }
        } catch (e) {
          logger.warn({ err: e }, "Failed to fetch consumer lag");
        }
        if (this.isRunning) setTimeout(checkLag, 15e3);
      };
      checkLag();
    } catch (e) {
      logger.error({ err: e }, "Failed to start lag monitor");
    }
  }
  async stop() {
    this.isRunning = false;
    await this.consumer.disconnect();
    if (this.producer) await this.producer.disconnect();
  }
  async handleMessage({ topic, partition, message, heartbeat, pause }) {
    const start = Date.now();
    if (this.shouldBackpressure()) {
      logger.warn({ topic, partition }, "Backpressure active, pausing consumer");
      pause();
      setTimeout(() => this.consumer.resume([{ topic }]), 5e3);
      return;
    }
    try {
      if (!message.value) return;
      const payloadStr = message.value.toString();
      const payload = JSON.parse(payloadStr);
      if (!payload.schemaId || !payload.data) {
        throw new Error("Missing schemaId or data in payload");
      }
      await applyContract(payload.schemaId, payload.data, payload.tenantId);
      await this.upsertRecord(payload);
      await this.consumer.commitOffsets([{ topic, partition, offset: (BigInt(message.offset) + 1n).toString() }]);
      await heartbeat();
      ingestLatency.observe({ topic }, (Date.now() - start) / 1e3);
    } catch (error) {
      logger.error({ err: error, topic, offset: message.offset }, "Message processing failed");
      ingestErrors.inc({ topic, type: error.name || "Unknown" });
      await this.sendToDLQ(topic, message, error);
      await this.consumer.commitOffsets([{ topic, partition, offset: (BigInt(message.offset) + 1n).toString() }]);
    }
  }
  async upsertRecord(payload) {
    const pool4 = getPostgresPool();
    const key = payload.idempotentKey || payload.data.id;
    if (!key) {
      throw new Error("Missing idempotentKey or data.id. Exactly-once semantics require a unique ID.");
    }
    const tenantId = payload.tenantId || "default";
    await pool4.query(
      `INSERT INTO ingest_events (id, tenant_id, schema_id, data, created_at)
             VALUES ($1, $2, $3, $4, NOW())
             ON CONFLICT (id) DO NOTHING`,
      [key, tenantId, payload.schemaId, payload.data]
    );
  }
  async sendToDLQ(originalTopic, message, error) {
    if (!this.producer) {
      this.producer = this.kafka.producer();
      await this.producer.connect();
    }
    const dlqTopic = `${originalTopic}${DLQ_TOPIC_SUFFIX}`;
    await this.producer.send({
      topic: dlqTopic,
      messages: [{
        key: message.key,
        value: JSON.stringify({
          original: message.value?.toString(),
          error: error.message,
          stack: error.stack,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        })
      }]
    });
  }
  shouldBackpressure() {
    const memory = process.memoryUsage();
    const heapUsedPct = memory.heapUsed / memory.heapTotal;
    return heapUsedPct > 0.85;
  }
};
var streamIngest = new StreamingIngestService();

// src/routes/osint.ts
import express20 from "express";
import { createRequire } from "node:module";
init_auth4();

// src/middleware/osintRateLimiter.ts
init_database();
init_logger();
var DEFAULT_WINDOW_MS2 = Number(process.env.OSINT_RATE_LIMIT_WINDOW_MS || 6e4);
var DEFAULT_USER_LIMIT = Number(process.env.OSINT_RATE_LIMIT_PER_USER || 120);
var DEFAULT_IP_LIMIT = Number(process.env.OSINT_RATE_LIMIT_PER_IP || 180);
var KEY_PREFIX = "osint:rate-limit:";
var SlidingWindowLimiter = class {
  windowMs;
  inMemoryHits = /* @__PURE__ */ new Map();
  redis;
  constructor(windowMs, redisClient4) {
    this.windowMs = windowMs;
    this.redis = redisClient4;
  }
  async check(key, limit, scope) {
    const now = Date.now();
    const windowStart = now - this.windowMs;
    try {
      if (this.redis) {
        return await this.checkWithRedis(key, limit, now, windowStart, scope);
      }
      return this.checkInMemory(key, limit, now, windowStart, scope);
    } catch (error) {
      logger.warn({ err: error, key, scope }, "Redis sliding window failed, falling back to memory");
      return this.checkInMemory(key, limit, now, windowStart, scope);
    }
  }
  async checkWithRedis(key, limit, now, windowStart, scope) {
    const redisKey = `${KEY_PREFIX}${key}`;
    const pipeline2 = this.redis.multi();
    pipeline2.zremrangebyscore(redisKey, "-inf", windowStart);
    pipeline2.zcard(redisKey);
    pipeline2.zrange(redisKey, 0, 0);
    const results = await pipeline2.exec();
    const count = Number(results?.[1]?.[1] ?? 0);
    const oldestEntry = results?.[2]?.[1]?.[0];
    const oldestTimestamp = oldestEntry ? Number(oldestEntry) : now;
    if (count >= limit) {
      const retryMs = Math.max(0, oldestTimestamp + this.windowMs - now);
      return {
        allowed: false,
        limit,
        remaining: 0,
        retryAfterSeconds: Math.ceil(retryMs / 1e3),
        scope
      };
    }
    const memberId = `${now}-${Math.random()}`;
    const expirySeconds = Math.ceil(this.windowMs * 2 / 1e3);
    await this.redis.multi().zadd(redisKey, now, memberId).expire(redisKey, expirySeconds).exec();
    return {
      allowed: true,
      limit,
      remaining: Math.max(0, limit - count - 1),
      retryAfterSeconds: Math.ceil(this.windowMs / 1e3),
      scope
    };
  }
  checkInMemory(key, limit, now, windowStart, scope) {
    const hits = this.inMemoryHits.get(key) || [];
    const recent = hits.filter((timestamp) => timestamp >= windowStart);
    if (recent.length >= limit) {
      const oldestTimestamp = recent[0];
      const retryMs = Math.max(0, oldestTimestamp + this.windowMs - now);
      this.inMemoryHits.set(key, recent);
      return {
        allowed: false,
        limit,
        remaining: 0,
        retryAfterSeconds: Math.ceil(retryMs / 1e3),
        scope
      };
    }
    recent.push(now);
    this.inMemoryHits.set(key, recent);
    return {
      allowed: true,
      limit,
      remaining: Math.max(0, limit - recent.length),
      retryAfterSeconds: Math.ceil(this.windowMs / 1e3),
      scope
    };
  }
};
function createOsintRateLimiter(options2 = {}) {
  const windowMs = options2.windowMs ?? DEFAULT_WINDOW_MS2;
  const userLimit = options2.userLimit ?? DEFAULT_USER_LIMIT;
  const ipLimit = options2.ipLimit ?? DEFAULT_IP_LIMIT;
  const redisClient4 = options2.redisClient ?? getRedisClient();
  const limiter = new SlidingWindowLimiter(windowMs, redisClient4);
  return async function osintRateLimiter2(req, res, next) {
    const userId = req.user?.id || req.user?.userId || req.user?.sub;
    const ip = req.ip;
    const results = [];
    if (userId) {
      results.push(await limiter.check(`user:${userId}`, userLimit, "user"));
    }
    results.push(await limiter.check(`ip:${ip}`, ipLimit, "ip"));
    const exceeded = results.find((result2) => !result2.allowed);
    if (exceeded) {
      const { limit, retryAfterSeconds, scope } = exceeded;
      res.status(429).setHeader("Retry-After", String(retryAfterSeconds)).json({
        success: false,
        error: "rate_limited",
        message: `OSINT API ${scope} limit exceeded. Please retry after ${retryAfterSeconds} seconds.`,
        scope,
        limit,
        retryAfterSeconds
      });
      return;
    }
    results.forEach((result2) => {
      const headerPrefix = result2.scope === "user" ? "X-UserRateLimit" : "X-IPRateLimit";
      res.setHeader(`${headerPrefix}-Limit`, String(result2.limit));
      res.setHeader(`${headerPrefix}-Remaining`, String(result2.remaining));
      res.setHeader(`${headerPrefix}-Reset`, String(result2.retryAfterSeconds));
    });
    next();
  };
}
var osintRateLimiter = createOsintRateLimiter();

// src/routes/osint.ts
init_postgres();
var require2 = createRequire(import.meta.url);
var { SimpleFeedCollector, CollectionType, TaskStatus } = require2("@intelgraph/osint-collector");
var router38 = express20.Router();
router38.use(osintRateLimiter);
router38.post("/prioritize", ensureAuthenticated, async (req, res) => {
  res.status(501).json({ success: false, error: "OSINTPrioritizationService not implemented" });
});
router38.post("/score/:id", ensureAuthenticated, async (req, res) => {
  res.status(501).json({ success: false, error: "VeracityScoringService not implemented" });
});
router38.post("/ingest-feed", ensureAuthenticated, async (req, res) => {
  try {
    const { url } = req.body;
    const collector = new SimpleFeedCollector({
      name: "on-demand-feed",
      type: CollectionType.WEB_SCRAPING,
      enabled: true,
      feedUrl: url
    });
    await collector.initialize();
    const result2 = await collector.collect({
      id: `manual-${Date.now()}`,
      type: CollectionType.WEB_SCRAPING,
      source: url,
      target: "iocs",
      priority: 1,
      scheduledAt: /* @__PURE__ */ new Date(),
      status: TaskStatus.PENDING,
      config: { url }
    });
    const pg5 = getPostgresPool();
    const iocs = result2.data;
    let insertedCount = 0;
    for (const ioc of iocs) {
      await pg5.query(
        `INSERT INTO iocs (type, value, source, created_at, updated_at)
         VALUES ($1, $2, $3, NOW(), NOW())
         ON CONFLICT DO NOTHING`,
        // simplified handling
        [ioc.type, ioc.value, ioc.source]
      );
      insertedCount++;
    }
    res.json({
      success: true,
      message: "Feed ingested and persisted",
      count: insertedCount,
      data: iocs.slice(0, 5)
      // Return sample
    });
  } catch (error) {
    console.error("Ingest error:", error);
    res.status(500).json({ success: false, error: error.message });
  }
});
router38.post("/assess-risk", ensureAuthenticated, async (req, res) => {
  try {
    const { iocs } = req.body;
    if (!iocs || !Array.isArray(iocs)) {
      return res.status(400).json({ success: false, error: "iocs array required" });
    }
    const results = [];
    const llmEndpoint = process.env.LLM_ENDPOINT;
    for (const ioc of iocs) {
      let riskAssessment;
      if (llmEndpoint) {
        try {
          const response = await fetch(`${llmEndpoint}/generate`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              prompt: `Assess the cybersecurity risk of the following IOC: ${ioc.value}. Return JSON with "score" (0-1) and "summary".`,
              model: "llama3"
            })
          });
          if (response.ok) {
            const data = await response.json();
            riskAssessment = {
              ioc: ioc.value,
              risk_score: data.score || 0.5,
              risk_summary: data.summary || "AI Assessment",
              model: "llama3-local"
            };
          }
        } catch (e) {
          console.warn("LLM connection failed, falling back to heuristic", e);
        }
      }
      if (!riskAssessment) {
        riskAssessment = {
          ioc: ioc.value,
          risk_score: ioc.value.includes("192") || ioc.value.includes("127") ? 0.1 : 0.7,
          risk_summary: ioc.value.includes("192") ? "Low risk local network address." : "Potential public IP, requires further investigation.",
          model: "heuristic-v1"
        };
      }
      results.push(riskAssessment);
    }
    res.json({
      success: true,
      results
    });
  } catch (error) {
    res.status(500).json({ success: false, error: error.message });
  }
});
router38.get("/queue", ensureAuthenticated, async (req, res) => {
  try {
    const counts = await osintQueue.getJobCounts();
    res.json({ success: true, counts });
  } catch (error) {
    res.status(500).json({ success: false, error: error.message });
  }
});
var osint_default = router38;

// src/routes/palettes.ts
import { Router as Router21 } from "express";

// src/llm/palette/registry.ts
import fs15 from "fs";
import path15 from "path";
function safePrefix(id, body4) {
  return [`--- REASONING PALETTE MODE: ${id} ---`, body4.trim(), "--- END REASONING PALETTE ---"].join("\n");
}
var builtInPalettes = [
  {
    id: "math_rigor",
    label: "Math Rigor",
    description: "Careful stepwise mathematical derivations with explicit checks.",
    injection: {
      kind: "text_prefix",
      textPrefix: safePrefix(
        "math_rigor",
        `Work through quantitative questions with explicit derivations.
Always restate givens, define variables, and check dimensional consistency.
List assumptions, show each algebraic step, and validate the final result with a quick sanity check.`
      )
    },
    decoding: { temperature: 0.2, topP: 0.9 },
    tags: ["math", "rigor"],
    safeDefault: true
  },
  {
    id: "code_planner",
    label: "Code Planner",
    description: "Plan before coding; outline functions, edge cases, and tests.",
    injection: {
      kind: "text_prefix",
      textPrefix: safePrefix(
        "code_planner",
        `Plan-first coding. Summarize goal, outline key functions, note interfaces and failure cases.
Describe test ideas before presenting final code. Keep instructions concise and policy-safe.`
      )
    },
    decoding: { temperature: 0.35, topP: 0.9 },
    tags: ["code", "planning"],
    safeDefault: true
  },
  {
    id: "skeptical_audit",
    label: "Skeptical Audit",
    description: "Challenge assumptions and look for weaknesses before concluding.",
    injection: {
      kind: "text_prefix",
      textPrefix: safePrefix(
        "skeptical_audit",
        `Adopt a skeptical reviewer mindset. Identify unstated assumptions, missing data, and potential failure modes.
Flag compliance or safety risks and propose mitigations. Keep tone professional and brief.`
      )
    },
    decoding: { temperature: 0.3, topP: 0.85 },
    tags: ["audit", "safety"],
    safeDefault: true
  },
  {
    id: "concise_exec",
    label: "Concise Execution",
    description: "Direct, minimal, action-focused reasoning.",
    injection: {
      kind: "text_prefix",
      textPrefix: safePrefix(
        "concise_exec",
        `Be succinct and execution-focused. Provide only the necessary reasoning to reach a decision.
Prefer bullet points and numbered steps. Avoid speculative or policy-violating content.`
      )
    },
    decoding: { temperature: 0.25, topP: 0.85 },
    tags: ["concise", "ops"],
    safeDefault: true
  },
  {
    id: "creative_diverge",
    label: "Creative Divergence",
    description: "Generate diverse options before selecting a path.",
    injection: {
      kind: "text_prefix",
      textPrefix: safePrefix(
        "creative_diverge",
        `Generate multiple distinct approaches first. List at least three options with pros/cons, then choose one.
Stay within safety and policy constraints; avoid sensitive or disallowed content.`
      )
    },
    decoding: { temperature: 0.6, topP: 0.95 },
    tags: ["creative", "brainstorm"],
    safeDefault: true
  }
];
var paletteMap = new Map(
  builtInPalettes.map((p) => [p.id, p])
);
function registerPalette(palette) {
  paletteMap.set(palette.id, palette);
}
function getPalette(id) {
  return paletteMap.get(id);
}
function listPalettes() {
  return Array.from(paletteMap.values());
}
function loadPalettesFromFile(filePath) {
  if (!filePath) return;
  const absolutePath = path15.resolve(filePath);
  if (!fs15.existsSync(absolutePath)) return;
  try {
    const data = JSON.parse(fs15.readFileSync(absolutePath, "utf-8"));
    data.forEach(registerPalette);
  } catch (err) {
    console.warn("Failed to load palettes from config", err);
  }
}
function resolvePaletteRuntimeConfig() {
  const enabled = process.env.REASONING_PALETTE_ENABLED === "true";
  const defaultPaletteId = process.env.REASONING_PALETTE_DEFAULT_ID || "concise_exec";
  const allowExploration = process.env.REASONING_PALETTE_ALLOW_EXPLORATION === "true";
  const maxK = Number(process.env.REASONING_PALETTE_MAX_K || 3);
  const allowed = process.env.REASONING_PALETTE_ALLOWED_IDS ? process.env.REASONING_PALETTE_ALLOWED_IDS.split(",").map((v) => v.trim()).filter(Boolean) : void 0;
  const allowListConfig = process.env.REASONING_PALETTE_TENANT_ALLOWLIST;
  let tenantAllowList;
  if (allowListConfig) {
    try {
      tenantAllowList = JSON.parse(allowListConfig);
    } catch (err) {
      console.warn("Failed to parse tenant allowlist for palettes", err);
    }
  }
  return {
    enabled,
    defaultPaletteId,
    allowExploration,
    maxK,
    allowedPaletteIds: allowed,
    tenantAllowList
  };
}
loadPalettesFromFile(process.env.REASONING_PALETTE_CONFIG_PATH);

// src/routes/palettes.ts
var router39 = Router21();
router39.get("/", (_req, res) => {
  const runtime = resolvePaletteRuntimeConfig();
  const palettes = listPalettes().map((p) => ({
    id: p.id,
    label: p.label,
    description: p.description,
    injectionKind: p.injection.kind,
    tags: p.tags,
    safeDefault: p.safeDefault,
    decoding: p.decoding,
    enabled: runtime.enabled
  }));
  res.json({ palettes, runtime });
});
router39.post("/runs/:id/palette", (req, res) => {
  const runtime = resolvePaletteRuntimeConfig();
  if (!runtime.enabled) {
    return res.status(403).json({ error: "Reasoning palette feature is disabled" });
  }
  const { paletteId } = req.body || {};
  const palette = paletteId ? getPalette(paletteId) : null;
  if (!palette) {
    return res.status(400).json({ error: "Unknown palette id" });
  }
  res.json({
    message: "Palette recorded for run template context",
    runId: req.params.id,
    palette: { id: palette.id, label: palette.label }
  });
});
var palettes_default = router39;

// src/app.ts
import swaggerUi from "swagger-ui-express";

// src/config/swagger.ts
init_config();
import swaggerJsdoc from "swagger-jsdoc";
var options = {
  definition: {
    openapi: "3.0.0",
    info: {
      title: "IntelGraph API",
      version: "1.0.0",
      description: "API documentation for the IntelGraph platform",
      contact: {
        name: "API Support",
        email: "support@intelgraph.com"
      }
    },
    servers: [
      {
        url: `http://localhost:${cfg.PORT}`,
        description: "Development server"
      },
      {
        url: "https://api.intelgraph.com",
        description: "Production server"
      }
    ],
    components: {
      securitySchemes: {
        bearerAuth: {
          type: "http",
          scheme: "bearer",
          bearerFormat: "JWT"
        }
      },
      schemas: {
        Error: {
          type: "object",
          properties: {
            error: {
              type: "string"
            },
            message: {
              type: "string"
            }
          }
        }
      }
    },
    security: [
      {
        bearerAuth: []
      }
    ]
  },
  apis: ["./src/routes/*.js", "./src/routes/*.js", "./src/http/*.js"]
  // Path to the API docs
};
var swaggerSpec = process.env.DISABLE_SWAGGER === "true" ? {
  openapi: "3.0.0",
  info: {
    title: "IntelGraph API",
    version: "1.0.0",
    description: "Swagger disabled for test environment"
  }
} : swaggerJsdoc(options);

// src/routes/meta-orchestrator.ts
import express21 from "express";

// src/meta-orchestrator/AgentRegistry.ts
var AgentRegistry = class _AgentRegistry {
  static instance;
  agents = /* @__PURE__ */ new Map();
  constructor() {
  }
  static getInstance() {
    if (!_AgentRegistry.instance) {
      _AgentRegistry.instance = new _AgentRegistry();
    }
    return _AgentRegistry.instance;
  }
  registerAgent(agent) {
    const newAgent = {
      ...agent,
      status: "IDLE" /* IDLE */,
      health: {
        cpuUsage: 0,
        memoryUsage: 0,
        lastHeartbeat: /* @__PURE__ */ new Date(),
        activeTasks: 0,
        errorRate: 0
      }
    };
    this.agents.set(agent.id, newAgent);
    return newAgent;
  }
  getAgent(id) {
    return this.agents.get(id);
  }
  getAllAgents(tenantId) {
    const allAgents = Array.from(this.agents.values());
    if (tenantId) {
      return allAgents.filter((a) => a.tenantId === tenantId);
    }
    return allAgents;
  }
  updateStatus(id, status) {
    const agent = this.agents.get(id);
    if (agent) {
      agent.status = status;
      this.agents.set(id, agent);
    }
  }
  updateHealth(id, health) {
    const agent = this.agents.get(id);
    if (agent) {
      agent.health = { ...agent.health, ...health, lastHeartbeat: /* @__PURE__ */ new Date() };
      this.agents.set(id, agent);
    }
  }
  removeAgent(id) {
    return this.agents.delete(id);
  }
};

// src/meta-orchestrator/HealthMonitor.ts
var HealthMonitor = class {
  registry;
  checkInterval = null;
  TIMEOUT_MS = 3e4;
  // 30 seconds
  constructor() {
    this.registry = AgentRegistry.getInstance();
  }
  startMonitoring(intervalMs = 1e4) {
    if (this.checkInterval) return;
    this.checkInterval = setInterval(() => {
      this.checkAgents();
    }, intervalMs);
  }
  stopMonitoring() {
    if (this.checkInterval) {
      clearInterval(this.checkInterval);
      this.checkInterval = null;
    }
  }
  checkAgents() {
    const agents = this.registry.getAllAgents();
    const now = /* @__PURE__ */ new Date();
    for (const agent of agents) {
      const timeSinceLastHeartbeat = now.getTime() - agent.health.lastHeartbeat.getTime();
      if (timeSinceLastHeartbeat > this.TIMEOUT_MS && agent.status !== "OFFLINE" /* OFFLINE */) {
        console.warn(`Agent ${agent.id} timed out. Marking as OFFLINE.`);
        this.registry.updateStatus(agent.id, "OFFLINE" /* OFFLINE */);
      }
    }
  }
  reportHeartbeat(agentId, metrics8) {
    this.registry.updateHealth(agentId, {
      cpuUsage: metrics8.cpu,
      memoryUsage: metrics8.memory,
      activeTasks: metrics8.activeTasks
    });
    const agent = this.registry.getAgent(agentId);
    if (agent && agent.status === "OFFLINE" /* OFFLINE */) {
      this.registry.updateStatus(agentId, metrics8.activeTasks > 0 ? "BUSY" /* BUSY */ : "IDLE" /* IDLE */);
    }
  }
};

// src/meta-orchestrator/GovernanceExtension.ts
var GovernanceExtension = class {
  async validateAction(agentId, action, context4) {
    if (context4?.restricted) {
      console.log(`Governance blocked action ${action} for agent ${agentId}`);
      return false;
    }
    return true;
  }
  async validateNegotiation(initiatorId, participantIds, topic) {
    return true;
  }
};

// src/meta-orchestrator/NegotiationEngine.ts
import { randomUUID as randomUUID36 } from "crypto";
var NegotiationEngine = class {
  negotiations = /* @__PURE__ */ new Map();
  governance;
  constructor() {
    this.governance = new GovernanceExtension();
  }
  async initiateNegotiation(initiatorId, participantIds, topic, context4, tenantId) {
    const allowed = await this.governance.validateNegotiation(initiatorId, participantIds, topic);
    if (!allowed) {
      throw new Error("Negotiation blocked by governance policy");
    }
    const negotiation = {
      id: randomUUID36(),
      initiatorId,
      participantIds,
      topic,
      status: "PENDING" /* PENDING */,
      rounds: [],
      context: context4,
      tenantId,
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date()
    };
    this.negotiations.set(negotiation.id, negotiation);
    return negotiation;
  }
  getNegotiation(id) {
    return this.negotiations.get(id);
  }
  getAllNegotiations(tenantId) {
    const all = Array.from(this.negotiations.values());
    if (tenantId) {
      return all.filter((n) => n.tenantId === tenantId);
    }
    return all;
  }
  async submitProposal(negotiationId, agentId, content) {
    const negotiation = this.negotiations.get(negotiationId);
    if (!negotiation) throw new Error("Negotiation not found");
    if (negotiation.status === "COMPLETED" /* COMPLETED */ || negotiation.status === "FAILED" /* FAILED */) {
      throw new Error("Negotiation is closed");
    }
    negotiation.status = "IN_PROGRESS" /* IN_PROGRESS */;
    const allowed = await this.governance.validateAction(agentId, "submit_proposal", { ...negotiation.context, content });
    if (!allowed) {
      throw new Error("Proposal blocked by governance");
    }
    let currentRound = negotiation.rounds[negotiation.rounds.length - 1];
    if (!currentRound || currentRound.consensusReached) {
      currentRound = {
        id: randomUUID36(),
        roundNumber: negotiation.rounds.length + 1,
        proposals: [],
        consensusReached: false
      };
      negotiation.rounds.push(currentRound);
    }
    const proposal = {
      id: randomUUID36(),
      agentId,
      content,
      timestamp: /* @__PURE__ */ new Date()
    };
    currentRound.proposals.push(proposal);
    negotiation.updatedAt = /* @__PURE__ */ new Date();
    return negotiation;
  }
  resolveNegotiation(negotiationId, success, finalAgreement) {
    const negotiation = this.negotiations.get(negotiationId);
    if (!negotiation) throw new Error("Negotiation not found");
    negotiation.status = success ? "COMPLETED" /* COMPLETED */ : "FAILED" /* FAILED */;
    negotiation.finalAgreement = finalAgreement;
    negotiation.updatedAt = /* @__PURE__ */ new Date();
    return negotiation;
  }
};

// src/meta-orchestrator/MetaOrchestrator.ts
var MetaOrchestrator = class _MetaOrchestrator {
  static instance;
  registry;
  healthMonitor;
  negotiationEngine;
  governance;
  constructor() {
    this.registry = AgentRegistry.getInstance();
    this.healthMonitor = new HealthMonitor();
    this.negotiationEngine = new NegotiationEngine();
    this.governance = new GovernanceExtension();
    this.healthMonitor.startMonitoring();
  }
  static getInstance() {
    if (!_MetaOrchestrator.instance) {
      _MetaOrchestrator.instance = new _MetaOrchestrator();
    }
    return _MetaOrchestrator.instance;
  }
  registerAgent(agent) {
    return this.registry.registerAgent(agent);
  }
  getAgents(tenantId) {
    return this.registry.getAllAgents(tenantId);
  }
  async createNegotiation(initiatorId, participantIds, topic, context4, tenantId) {
    return this.negotiationEngine.initiateNegotiation(initiatorId, participantIds, topic, context4, tenantId);
  }
  async submitProposal(negotiationId, agentId, content) {
    return this.negotiationEngine.submitProposal(negotiationId, agentId, content);
  }
};

// src/routes/meta-orchestrator.ts
init_auth4();
var router40 = express21.Router();
var orchestrator = MetaOrchestrator.getInstance();
router40.use(ensureAuthenticated);
router40.get("/agents", (req, res) => {
  const tenantId = req.user?.tenant_id;
  const agents = orchestrator.getAgents(tenantId);
  res.json(agents);
});
router40.post("/agents", (req, res) => {
  const agentData = req.body;
  const tenantId = req.user?.tenant_id;
  if (!agentData.tenantId) {
    agentData.tenantId = tenantId;
  }
  if (agentData.tenantId !== tenantId) {
    return res.status(403).json({ error: "Tenant mismatch" });
  }
  const agent = orchestrator.registerAgent(agentData);
  res.status(201).json(agent);
});
router40.post("/agents/:id/heartbeat", (req, res) => {
  const { id } = req.params;
  const metrics8 = req.body;
  orchestrator.healthMonitor.reportHeartbeat(id, metrics8);
  res.sendStatus(200);
});
router40.get("/negotiations", (req, res) => {
  const tenantId = req.user?.tenant_id;
  const negotiations = orchestrator.negotiationEngine.getAllNegotiations(tenantId);
  res.json(negotiations);
});
router40.post("/negotiations", async (req, res) => {
  try {
    const { initiatorId, participantIds, topic, context: context4 } = req.body;
    const tenantId = req.user?.tenant_id;
    const negotiation = await orchestrator.createNegotiation(initiatorId, participantIds, topic, context4, tenantId);
    res.status(201).json(negotiation);
  } catch (err) {
    res.status(400).json({ error: err.message });
  }
});
router40.get("/negotiations/:id", (req, res) => {
  const { id } = req.params;
  const negotiation = orchestrator.negotiationEngine.getNegotiation(id);
  if (!negotiation) return res.status(404).json({ error: "Not found" });
  if (negotiation.tenantId !== req.user?.tenant_id) {
    return res.status(403).json({ error: "Access denied" });
  }
  res.json(negotiation);
});
router40.post("/negotiations/:id/proposals", async (req, res) => {
  try {
    const { id } = req.params;
    const { agentId, content } = req.body;
    const negotiation = await orchestrator.submitProposal(id, agentId, content);
    res.status(201).json(negotiation);
  } catch (err) {
    res.status(400).json({ error: err.message });
  }
});
var meta_orchestrator_default = router40;

// src/routes/admin-smoke.ts
import express22 from "express";

// src/services/PipelineSmokeService.ts
init_runs_repo();
init_metrics3();
init_logger2();
var metrics7 = new PrometheusMetrics("pipeline_smoke");
metrics7.createCounter("runs_total", "Total number of smoke test runs", [
  "status"
]);
metrics7.createGauge("last_run_duration_ms", "Duration of the last smoke test run", [
  "status"
]);
metrics7.createGauge("last_run_timestamp", "Timestamp of the last smoke test run", [
  "status"
]);
var PipelineSmokeService = class _PipelineSmokeService {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_PipelineSmokeService.instance) {
      _PipelineSmokeService.instance = new _PipelineSmokeService();
    }
    return _PipelineSmokeService.instance;
  }
  /**
   * Runs a synthetic investigation (smoke test).
   * 1. Creates a run for a specific pipeline.
   * 2. Polls for completion.
   * 3. Validates the result.
   */
  async runSmokeTest(tenantId, pipelineId = "smoke-test-pipeline", timeoutMs = 6e4) {
    const startTime = Date.now();
    const result2 = {
      success: false,
      runId: "",
      durationMs: 0,
      stages: {
        creation: false,
        completion: false,
        validation: false
      }
    };
    try {
      logger_default2.info(`[SmokeTest] Starting smoke test for tenant ${tenantId}...`);
      const run = await runsRepo.create({
        pipeline_id: pipelineId,
        pipeline_name: "Smoke Test Pipeline",
        input_params: {
          synthetic: true,
          timestamp: startTime
        },
        tenant_id: tenantId
      });
      if (!run) {
        throw new Error("Failed to create run");
      }
      result2.runId = run.id;
      result2.stages.creation = true;
      logger_default2.info(`[SmokeTest] Created run ${run.id}`);
      const pollInterval = 1e3;
      let elapsedTime = 0;
      let finalRun = run;
      while (elapsedTime < timeoutMs) {
        await new Promise((resolve2) => setTimeout(resolve2, pollInterval));
        elapsedTime += pollInterval;
        const currentRun = await runsRepo.get(run.id, tenantId);
        if (!currentRun) {
          throw new Error("Run disappeared during polling");
        }
        if (["succeeded", "failed", "cancelled"].includes(currentRun.status)) {
          finalRun = currentRun;
          break;
        }
      }
      if (finalRun.status !== "succeeded") {
        throw new Error(
          `Run failed with status: ${finalRun.status}. Error: ${finalRun.error_message || "N/A"}`
        );
      }
      result2.stages.completion = true;
      if (!finalRun.output_data) {
      }
      result2.stages.validation = true;
      result2.success = true;
    } catch (error) {
      logger_default2.error(`[SmokeTest] Failed: ${error.message}`);
      result2.error = error.message;
    } finally {
      result2.durationMs = Date.now() - startTime;
      const status = result2.success ? "success" : "failure";
      metrics7.incrementCounter("runs_total", { status });
      metrics7.setGauge("last_run_duration_ms", result2.durationMs, { status });
      metrics7.setGauge("last_run_timestamp", Date.now(), { status });
      if (!result2.success) {
        logger_default2.error(`[SmokeTest] ALERT: Smoke test failed! RunID: ${result2.runId}, Duration: ${result2.durationMs}ms, Error: ${result2.error}`);
      }
    }
    return result2;
  }
};
var pipelineSmokeService = PipelineSmokeService.getInstance();

// src/routes/admin-smoke.ts
init_auth4();

// src/middleware/rbac.ts
init_AuthService();
var authService4 = new AuthService_default();
function requirePermission2(permission) {
  return (req, res, next) => {
    const user = req.user;
    if (!user) {
      return res.status(401).json({ error: "Authentication required" });
    }
    if (!authService4.hasPermission(user, permission)) {
      return res.status(403).json({
        error: "Insufficient permissions",
        required: permission,
        userRole: user.role
      });
    }
    next();
  };
}

// src/routes/admin-smoke.ts
var router41 = express22.Router();
router41.use(express22.json());
router41.use(ensureAuthenticated);
router41.post(
  "/admin/smoke-test",
  requirePermission2("admin:access"),
  async (req, res) => {
    try {
      const tenantId = req.user?.tenantId || req.tenant || "default";
      const pipelineId = req.body.pipelineId || "smoke-test-pipeline";
      const timeoutMs = req.body.timeoutMs || 6e4;
      const result2 = await pipelineSmokeService.runSmokeTest(
        tenantId,
        pipelineId,
        timeoutMs
      );
      if (result2.success) {
        res.json(result2);
      } else {
        res.status(500).json(result2);
      }
    } catch (error) {
      res.status(500).json({
        success: false,
        error: error.message
      });
    }
  }
);
var admin_smoke_default = router41;

// src/routes/scenarios.ts
import { Router as Router22 } from "express";

// src/cases/scenarios/ScenarioService.ts
import { randomUUID as randomUUID37 } from "crypto";
import { EventEmitter as EventEmitter16 } from "events";

// src/services/investigationWorkflowService.ts
init_CacheService();
init_audit2();
import { EventEmitter as EventEmitter15 } from "events";
var InvestigationWorkflowService = class extends EventEmitter15 {
  investigations = /* @__PURE__ */ new Map();
  templates = /* @__PURE__ */ new Map();
  constructor() {
    super();
    console.log("[WORKFLOW] Investigation workflow service initialized");
    this.initializeTemplates();
  }
  initializeTemplates() {
    const securityIncidentTemplate = {
      id: "template-security-incident",
      name: "Security Incident Investigation",
      description: "Standard template for security incident investigations",
      category: "Security",
      workflowStages: [
        "INTAKE",
        "TRIAGE",
        "INVESTIGATION",
        "CONTAINMENT",
        "ERADICATION",
        "RECOVERY",
        "LESSONS_LEARNED"
      ],
      requiredFields: ["title", "description", "priority", "assignedTo"],
      defaultTags: ["security", "incident"],
      defaultClassification: "CONFIDENTIAL",
      estimatedDuration: 48,
      slaHours: 72
    };
    const malwareAnalysisTemplate = {
      id: "template-malware-analysis",
      name: "Malware Analysis Investigation",
      description: "Template for malware analysis and reverse engineering",
      category: "Malware",
      workflowStages: [
        "INTAKE",
        "TRIAGE",
        "INVESTIGATION",
        "ANALYSIS",
        "CONTAINMENT",
        "LESSONS_LEARNED"
      ],
      requiredFields: ["title", "description", "priority"],
      defaultTags: ["malware", "analysis", "reverse-engineering"],
      defaultClassification: "SECRET",
      estimatedDuration: 96,
      slaHours: 120
    };
    const fraudInvestigationTemplate = {
      id: "template-fraud-investigation",
      name: "Fraud Investigation",
      description: "Template for financial fraud investigations",
      category: "Fraud",
      workflowStages: [
        "INTAKE",
        "TRIAGE",
        "INVESTIGATION",
        "ANALYSIS",
        "RECOVERY",
        "LESSONS_LEARNED"
      ],
      requiredFields: ["title", "description", "priority", "assignedTo"],
      defaultTags: ["fraud", "financial"],
      defaultClassification: "CONFIDENTIAL",
      estimatedDuration: 72,
      slaHours: 96
    };
    this.templates.set(securityIncidentTemplate.id, securityIncidentTemplate);
    this.templates.set(malwareAnalysisTemplate.id, malwareAnalysisTemplate);
    this.templates.set(
      fraudInvestigationTemplate.id,
      fraudInvestigationTemplate
    );
    console.log(
      `[WORKFLOW] Initialized ${this.templates.size} investigation templates`
    );
  }
  /**
   * Create a new investigation from template
   */
  async createInvestigation(templateId, data) {
    const template = this.templates.get(templateId);
    if (!template) {
      throw new Error(`Template not found: ${templateId}`);
    }
    const investigationId = `inv-${Date.now()}`;
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const workflow = {
      currentStage: "INTAKE",
      stages: {}
    };
    template.workflowStages.forEach((stage) => {
      workflow.stages[stage] = {
        status: stage === "INTAKE" ? "IN_PROGRESS" : "PENDING",
        startedAt: stage === "INTAKE" ? now : void 0,
        assignedTo: data.assignedTo[0],
        notes: "",
        requirements: this.getStageRequirements(stage),
        artifacts: []
      };
    });
    const investigation = {
      id: investigationId,
      tenantId: data.tenantId,
      name: data.name,
      description: data.description,
      status: "ACTIVE",
      priority: data.priority,
      assignedTo: data.assignedTo,
      createdBy: data.createdBy,
      createdAt: now,
      updatedAt: now,
      dueDate: data.dueDate,
      tags: [...template.defaultTags, ...data.tags || []],
      classification: data.classification || template.defaultClassification,
      workflow,
      entities: [],
      relationships: [],
      evidence: [],
      findings: [],
      timeline: [],
      collaborators: data.assignedTo,
      permissions: data.assignedTo.map((userId) => ({
        userId,
        role: "ANALYST",
        permissions: ["READ", "write", "manage_evidence"],
        grantedBy: data.createdBy,
        grantedAt: now
      }))
    };
    this.investigations.set(investigationId, investigation);
    await cacheService.set(
      `investigation:${investigationId}`,
      investigation,
      3600
    );
    await advancedAuditSystem.recordEvent({
      eventType: "user_action",
      level: "info",
      userId: data.createdBy,
      tenantId: data.tenantId,
      serviceId: "investigation-workflow",
      resourceType: "investigation",
      resourceId: investigationId,
      action: "create_investigation",
      outcome: "success",
      message: `Created investigation ${investigationId}`,
      details: {
        templateId,
        priority: data.priority,
        classification: data.classification
      }
    });
    this.emit("investigationCreated", investigation);
    console.log(
      `[WORKFLOW] Created investigation: ${investigationId} from template: ${templateId}`
    );
    return investigation;
  }
  /**
   * Update investigation workflow stage
   */
  async advanceWorkflowStage(investigationId, userId, tenantId, notes) {
    const investigation = this.investigations.get(investigationId);
    if (!investigation) {
      throw new Error(`Investigation not found: ${investigationId}`);
    }
    if (investigation.tenantId !== tenantId) {
      throw new Error(`Unauthorized access to investigation: ${investigationId}`);
    }
    const currentStage = investigation.workflow.currentStage;
    const stageOrder = [
      "INTAKE",
      "TRIAGE",
      "INVESTIGATION",
      "ANALYSIS",
      "CONTAINMENT",
      "ERADICATION",
      "RECOVERY",
      "LESSONS_LEARNED"
    ];
    const currentIndex = stageOrder.indexOf(currentStage);
    if (currentIndex === -1 || currentIndex === stageOrder.length - 1) {
      throw new Error("Cannot advance workflow from current stage");
    }
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const nextStage = stageOrder[currentIndex + 1];
    investigation.workflow.stages[currentStage] = {
      ...investigation.workflow.stages[currentStage],
      status: "COMPLETED",
      completedAt: now,
      notes
    };
    investigation.workflow.stages[nextStage] = {
      ...investigation.workflow.stages[nextStage],
      status: "IN_PROGRESS",
      startedAt: now,
      assignedTo: investigation.assignedTo[0]
    };
    investigation.workflow.currentStage = nextStage;
    investigation.updatedAt = now;
    await cacheService.set(
      `investigation:${investigationId}`,
      investigation,
      3600
    );
    await advancedAuditSystem.recordEvent({
      eventType: "user_action",
      level: "info",
      userId,
      tenantId,
      serviceId: "investigation-workflow",
      resourceType: "investigation",
      resourceId: investigationId,
      action: "advance_workflow",
      outcome: "success",
      message: `Advanced workflow to ${nextStage}`,
      details: {
        previousStage: currentStage,
        newStage: nextStage,
        notes
      }
    });
    this.emit("workflowAdvanced", {
      investigation,
      previousStage: currentStage,
      newStage: nextStage,
      userId
    });
    console.log(
      `[WORKFLOW] Advanced investigation ${investigationId} from ${currentStage} to ${nextStage}`
    );
    return investigation;
  }
  /**
   * Add evidence to investigation
   */
  async addEvidence(investigationId, tenantId, evidence, collectedBy) {
    const investigation = this.investigations.get(investigationId);
    if (!investigation) {
      throw new Error(`Investigation not found: ${investigationId}`);
    }
    if (investigation.tenantId !== tenantId) {
      throw new Error(`Unauthorized access to investigation: ${investigationId}`);
    }
    const evidenceId = `evidence-${Date.now()}`;
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const newEvidence = {
      ...evidence,
      id: evidenceId,
      collectedAt: now,
      collectedBy,
      chainOfCustody: [
        {
          timestamp: now,
          custodian: collectedBy,
          action: "COLLECTED",
          location: "Digital Collection",
          integrity: "VERIFIED"
        }
      ]
    };
    investigation.evidence.push(newEvidence);
    investigation.updatedAt = now;
    await cacheService.set(
      `investigation:${investigationId}`,
      investigation,
      3600
    );
    await advancedAuditSystem.recordEvent({
      eventType: "user_action",
      level: "info",
      userId: collectedBy,
      tenantId,
      serviceId: "investigation-workflow",
      resourceType: "investigation",
      resourceId: investigationId,
      action: "add_evidence",
      outcome: "success",
      message: `Added evidence ${evidenceId}`,
      details: {
        evidenceId,
        evidenceType: evidence.type
      }
    });
    this.emit("evidenceAdded", { investigation, evidence: newEvidence });
    console.log(
      `[WORKFLOW] Added evidence ${evidenceId} to investigation ${investigationId}`
    );
    return investigation;
  }
  /**
   * Add finding to investigation
   */
  async addFinding(investigationId, tenantId, finding, discoveredBy) {
    const investigation = this.investigations.get(investigationId);
    if (!investigation) {
      throw new Error(`Investigation not found: ${investigationId}`);
    }
    if (investigation.tenantId !== tenantId) {
      throw new Error(`Unauthorized access to investigation: ${investigationId}`);
    }
    const findingId = `finding-${Date.now()}`;
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const newFinding = {
      ...finding,
      id: findingId,
      discoveredAt: now,
      discoveredBy
    };
    investigation.findings.push(newFinding);
    investigation.updatedAt = now;
    await cacheService.set(
      `investigation:${investigationId}`,
      investigation,
      3600
    );
    await advancedAuditSystem.recordEvent({
      eventType: "user_action",
      level: "info",
      userId: discoveredBy,
      tenantId,
      serviceId: "investigation-workflow",
      resourceType: "investigation",
      resourceId: investigationId,
      action: "add_finding",
      outcome: "success",
      message: `Added finding ${findingId}`,
      details: {
        findingId,
        category: finding.category,
        severity: finding.severity
      }
    });
    this.emit("findingAdded", { investigation, finding: newFinding });
    console.log(
      `[WORKFLOW] Added finding ${findingId} to investigation ${investigationId}`
    );
    return investigation;
  }
  /**
   * Add timeline entry to investigation
   */
  async addTimelineEntry(investigationId, tenantId, entry) {
    const investigation = this.investigations.get(investigationId);
    if (!investigation) {
      throw new Error(`Investigation not found: ${investigationId}`);
    }
    if (investigation.tenantId !== tenantId) {
      throw new Error(`Unauthorized access to investigation: ${investigationId}`);
    }
    const entryId = `timeline-${Date.now()}`;
    const newEntry = {
      ...entry,
      id: entryId
    };
    investigation.timeline.push(newEntry);
    investigation.timeline.sort(
      (a, b) => new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime()
    );
    investigation.updatedAt = (/* @__PURE__ */ new Date()).toISOString();
    await cacheService.set(
      `investigation:${investigationId}`,
      investigation,
      3600
    );
    await advancedAuditSystem.recordEvent({
      eventType: "user_action",
      level: "info",
      userId: entry.actor,
      tenantId,
      serviceId: "investigation-workflow",
      resourceType: "investigation",
      resourceId: investigationId,
      action: "add_timeline_entry",
      outcome: "success",
      message: `Added timeline entry ${entryId}`,
      details: {
        entryId,
        eventType: entry.eventType
      }
    });
    this.emit("timelineEntryAdded", { investigation, entry: newEntry });
    console.log(
      `[WORKFLOW] Added timeline entry ${entryId} to investigation ${investigationId}`
    );
    return investigation;
  }
  /**
   * Get investigation by ID
   */
  async getInvestigation(investigationId, tenantId) {
    let investigation = this.investigations.get(investigationId);
    if (!investigation) {
      investigation = await cacheService.get(
        `investigation:${investigationId}`
      );
      if (investigation) {
        this.investigations.set(investigationId, investigation);
      }
    }
    if (investigation && tenantId && investigation.tenantId !== tenantId) {
      return null;
    }
    return investigation || null;
  }
  /**
   * Get all investigations (filtered by tenant)
   */
  getAllInvestigations(tenantId) {
    return Array.from(this.investigations.values()).filter((inv) => inv.tenantId === tenantId).sort(
      (a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime()
    );
  }
  /**
   * Get investigations by status (filtered by tenant)
   */
  getInvestigationsByStatus(status, tenantId) {
    return Array.from(this.investigations.values()).filter((inv) => inv.status === status && inv.tenantId === tenantId).sort(
      (a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime()
    );
  }
  /**
   * Get investigations assigned to user (filtered by tenant)
   */
  getAssignedInvestigations(userId, tenantId) {
    return Array.from(this.investigations.values()).filter((inv) => inv.assignedTo.includes(userId) && inv.tenantId === tenantId).sort(
      (a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime()
    );
  }
  /**
   * Get available templates
   */
  getTemplates() {
    return Array.from(this.templates.values());
  }
  /**
   * Get workflow statistics
   */
  getWorkflowStatistics() {
    const investigations = Array.from(this.investigations.values());
    return {
      total: investigations.length,
      byStatus: investigations.reduce(
        (acc, inv) => {
          acc[inv.status] = (acc[inv.status] || 0) + 1;
          return acc;
        },
        {}
      ),
      byPriority: investigations.reduce(
        (acc, inv) => {
          acc[inv.priority] = (acc[inv.priority] || 0) + 1;
          return acc;
        },
        {}
      ),
      byStage: investigations.reduce(
        (acc, inv) => {
          acc[inv.workflow.currentStage] = (acc[inv.workflow.currentStage] || 0) + 1;
          return acc;
        },
        {}
      ),
      overdueSLA: investigations.filter((inv) => {
        if (!inv.dueDate) return false;
        return /* @__PURE__ */ new Date() > new Date(inv.dueDate);
      }).length
    };
  }
  getStageRequirements(stage) {
    const requirements = {
      INTAKE: [
        "Initial report documented",
        "Priority assigned",
        "Analyst assigned"
      ],
      TRIAGE: [
        "Threat assessment completed",
        "Scope determined",
        "Resources allocated"
      ],
      INVESTIGATION: [
        "Evidence collected",
        "Entities identified",
        "Timeline constructed"
      ],
      ANALYSIS: [
        "Root cause identified",
        "Attack vectors mapped",
        "Impact assessed"
      ],
      CONTAINMENT: ["Threat contained", "Systems isolated", "Damage minimized"],
      ERADICATION: [
        "Threat removed",
        "Vulnerabilities patched",
        "Systems hardened"
      ],
      RECOVERY: [
        "Systems restored",
        "Operations normalized",
        "Monitoring enhanced"
      ],
      LESSONS_LEARNED: [
        "Report documented",
        "Improvements identified",
        "Training updated"
      ]
    };
    return requirements[stage] || [];
  }
};
var investigationWorkflowService = new InvestigationWorkflowService();

// src/cases/scenarios/ScenarioService.ts
init_logger();
var ScenarioService = class extends EventEmitter16 {
  scenarios = /* @__PURE__ */ new Map();
  // Index for fast lookup: investigationId -> Set<scenarioId>
  investigationIndex = /* @__PURE__ */ new Map();
  constructor() {
    super();
    logger_default.info("[SCENARIO] Scenario service initialized");
  }
  /**
   * Create a new scenario from an investigation
   */
  async createScenario(investigationId, name, description, userId = "system") {
    const investigation = await investigationWorkflowService.getInvestigation(investigationId);
    if (!investigation) {
      throw new Error(`Investigation not found: ${investigationId}`);
    }
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const scenarioId = randomUUID37();
    const scenario = {
      id: scenarioId,
      investigationId,
      name,
      description: description || void 0,
      baseStateSnapshot: {
        entities: [...investigation.entities],
        relationships: [...investigation.relationships],
        timeline: investigation.timeline.map((t) => t.id)
      },
      modifications: [],
      createdAt: now,
      updatedAt: now,
      createdBy: userId,
      status: "DRAFT"
    };
    this.scenarios.set(scenario.id, scenario);
    if (!this.investigationIndex.has(investigationId)) {
      this.investigationIndex.set(investigationId, /* @__PURE__ */ new Set());
    }
    this.investigationIndex.get(investigationId).add(scenarioId);
    this.emit("scenarioCreated", scenario);
    return scenario;
  }
  /**
   * Add a modification to a scenario
   */
  async addModification(scenarioId, type, data, targetId, userId = "system") {
    const scenario = this.scenarios.get(scenarioId);
    if (!scenario) {
      throw new Error(`Scenario not found: ${scenarioId}`);
    }
    if (!targetId && (type === "ADD_ENTITY" || type === "ADD_RELATIONSHIP" || type === "ADD_EVENT")) {
      targetId = randomUUID37();
    }
    const modification = {
      id: randomUUID37(),
      type,
      targetId,
      data,
      appliedAt: (/* @__PURE__ */ new Date()).toISOString(),
      appliedBy: userId
    };
    scenario.modifications.push(modification);
    scenario.updatedAt = (/* @__PURE__ */ new Date()).toISOString();
    this.scenarios.set(scenario.id, scenario);
    this.emit("scenarioModified", { scenarioId, modification });
    return scenario;
  }
  /**
   * Get a scenario by ID
   */
  getScenario(scenarioId) {
    return this.scenarios.get(scenarioId) || null;
  }
  /**
   * List scenarios for an investigation
   */
  getScenariosForInvestigation(investigationId) {
    const scenarioIds = this.investigationIndex.get(investigationId);
    if (!scenarioIds) {
      return [];
    }
    return Array.from(scenarioIds).map((id) => this.scenarios.get(id)).filter((s) => s !== void 0).sort((a, b) => new Date(b.updatedAt).getTime() - new Date(a.updatedAt).getTime());
  }
  /**
   * Resolve the scenario to its final state
   * Note: This requires the caller to provide the full base entities/relationships map
   * because Investigation only stores IDs.
   */
  resolveState(scenarioId, baseEntities, baseRelationships, baseTimeline) {
    const scenario = this.scenarios.get(scenarioId);
    if (!scenario) {
      throw new Error(`Scenario not found: ${scenarioId}`);
    }
    let entities = [...baseEntities];
    let relationships = [...baseRelationships];
    let timeline = [...baseTimeline];
    for (const mod of scenario.modifications) {
      switch (mod.type) {
        case "ADD_ENTITY":
          entities.push({ ...mod.data, id: mod.targetId });
          break;
        case "REMOVE_ENTITY":
          entities = entities.filter((e) => e.id !== mod.targetId);
          relationships = relationships.filter((r) => r.source !== mod.targetId && r.target !== mod.targetId);
          break;
        case "UPDATE_ENTITY":
          const entIdx = entities.findIndex((e) => e.id === mod.targetId);
          if (entIdx >= 0) {
            entities[entIdx] = { ...entities[entIdx], ...mod.data };
          }
          break;
        case "ADD_RELATIONSHIP":
          relationships.push({ ...mod.data, id: mod.targetId });
          break;
        case "REMOVE_RELATIONSHIP":
          relationships = relationships.filter((r) => r.id !== mod.targetId);
          break;
        case "ADD_EVENT":
          timeline.push({ ...mod.data, id: mod.targetId });
          break;
        case "REMOVE_EVENT":
          timeline = timeline.filter((t) => t.id !== mod.targetId);
          break;
        case "MODIFY_EVENT":
          const evtIdx = timeline.findIndex((t) => t.id === mod.targetId);
          if (evtIdx >= 0) {
            timeline[evtIdx] = { ...timeline[evtIdx], ...mod.data };
          }
          break;
      }
    }
    return {
      scenarioId,
      finalState: {
        entities,
        relationships,
        timeline
      },
      metrics: {
        entityCount: entities.length,
        relationshipCount: relationships.length,
        timelineEventCount: timeline.length,
        changesApplied: scenario.modifications.length
      }
    };
  }
};
var scenarioService = new ScenarioService();

// src/routes/scenarios.ts
init_logger();
var router42 = Router22();
var routeLogger3 = logger_default.child({ name: "ScenarioRoutes" });
function getUserId(req) {
  return req.user?.id || req.headers["x-user-id"] || req.user?.email || "system";
}
router42.get("/investigation/:id", (req, res) => {
  try {
    const { id } = req.params;
    const scenarios = scenarioService.getScenariosForInvestigation(id);
    res.json(scenarios);
  } catch (error) {
    routeLogger3.error({ error: error.message }, "Failed to list scenarios");
    res.status(500).json({ error: error.message });
  }
});
router42.get("/:id", (req, res) => {
  try {
    const { id } = req.params;
    const scenario = scenarioService.getScenario(id);
    if (!scenario) {
      return res.status(404).json({ error: "Scenario not found" });
    }
    res.json(scenario);
  } catch (error) {
    routeLogger3.error({ error: error.message }, "Failed to get scenario");
    res.status(500).json({ error: error.message });
  }
});
router42.post("/", async (req, res) => {
  try {
    const { investigationId, name, description } = req.body;
    const userId = getUserId(req);
    if (!investigationId || !name) {
      return res.status(400).json({ error: "investigationId and name are required" });
    }
    const scenario = await scenarioService.createScenario(investigationId, name, description, userId);
    res.status(201).json(scenario);
  } catch (error) {
    routeLogger3.error({ error: error.message }, "Failed to create scenario");
    res.status(500).json({ error: error.message });
  }
});
router42.post("/:id/modifications", async (req, res) => {
  try {
    const { id } = req.params;
    const { type, data, targetId } = req.body;
    const userId = getUserId(req);
    if (!type) {
      return res.status(400).json({ error: "Modification type is required" });
    }
    const scenario = await scenarioService.addModification(id, type, data, targetId, userId);
    res.json(scenario);
  } catch (error) {
    routeLogger3.error({ error: error.message }, "Failed to add modification");
    res.status(500).json({ error: error.message });
  }
});
router42.post("/:id/resolve", async (req, res) => {
  try {
    const { id } = req.params;
    const { entities, relationships, timeline } = req.body;
    const result2 = scenarioService.resolveState(
      id,
      entities || [],
      relationships || [],
      timeline || []
    );
    res.json(result2);
  } catch (error) {
    routeLogger3.error({ error: error.message }, "Failed to resolve scenario");
    res.status(500).json({ error: error.message });
  }
});
var scenarios_default = router42;

// src/routes/resource-costs.ts
import { Router as Router23 } from "express";

// src/services/TenantCostService.ts
init_metrics3();
init_logger2();
import { EventEmitter as EventEmitter17 } from "events";

// src/services/DatabaseService.ts
init_logger2();
var DatabaseService = class {
  /**
   * @method query
   * @description Executes a SQL query. This is a stub method that logs the query if debugging is enabled
   * and returns an empty result set.
   * @template T - The expected type of the result rows.
   * @param {string} sql - The SQL query string to execute.
   * @param {unknown[]} [params=[]] - An array of parameters to be used with the query.
   * @returns {Promise<QueryResult<T>>} A promise that resolves to a QueryResult with an empty `rows` array.
   */
  async query(sql, params = []) {
    if (process.env.DEBUG_DB_QUERIES) {
      logger_default2.debug("DatabaseService query (stub)", { sql, params });
    }
    return { rows: [] };
  }
  /**
   * @method getConnectionConfig
   * @description Returns the configuration for the database connection. This is a stub method.
   * @returns {Record<string, any>} An empty object.
   */
  getConnectionConfig() {
    return {};
  }
};

// src/services/TenantCostService.ts
var TenantCostService = class extends EventEmitter17 {
  config;
  metrics;
  db;
  costCache = /* @__PURE__ */ new Map();
  budgets = /* @__PURE__ */ new Map();
  forecasts = /* @__PURE__ */ new Map();
  constructor(config9 = {}, db2) {
    super();
    this.config = {
      enabled: true,
      trackingIntervalMinutes: 15,
      forecastingEnabled: true,
      alertThresholds: {
        dailyBudgetPercentage: 80,
        monthlyBudgetPercentage: 85,
        suddenSpikeMultiplier: 3,
        sustainedIncreasePercentage: 50
      },
      costCategories: this.getDefaultCostCategories(),
      billingCycle: "hourly",
      retentionDays: 365,
      ...config9
    };
    this.db = db2;
    this.metrics = new PrometheusMetrics("tenant_cost_service");
    this.initializeMetrics();
    this.loadTenantBudgets();
    this.startCostTracking();
  }
  getDefaultCostCategories() {
    return [
      {
        name: "compute",
        description: "CPU and memory usage",
        unitType: "compute",
        ratePerUnit: 1e-3,
        // $0.001 per compute unit
        currency: "USD",
        trackingEnabled: true
      },
      {
        name: "storage",
        description: "Data storage costs",
        unitType: "storage",
        ratePerUnit: 1e-4,
        // $0.0001 per GB per hour
        currency: "USD",
        trackingEnabled: true
      },
      {
        name: "network",
        description: "Network bandwidth usage",
        unitType: "network",
        ratePerUnit: 1e-4,
        // $0.0001 per GB
        currency: "USD",
        trackingEnabled: true
      },
      {
        name: "api_calls",
        description: "API request costs",
        unitType: "api_calls",
        ratePerUnit: 1e-5,
        // $0.00001 per API call
        currency: "USD",
        trackingEnabled: true
      }
    ];
  }
  initializeMetrics() {
    this.metrics.createGauge("tenant_cost_total", "Total cost per tenant", [
      "tenant_id",
      "period"
    ]);
    this.metrics.createGauge("tenant_cost_compute", "Compute cost per tenant", [
      "tenant_id"
    ]);
    this.metrics.createGauge("tenant_cost_storage", "Storage cost per tenant", [
      "tenant_id"
    ]);
    this.metrics.createGauge("tenant_cost_network", "Network cost per tenant", [
      "tenant_id"
    ]);
    this.metrics.createGauge("tenant_cost_api", "API cost per tenant", [
      "tenant_id"
    ]);
    this.metrics.createGauge(
      "tenant_budget_remaining",
      "Remaining budget per tenant",
      ["tenant_id", "period"]
    );
    this.metrics.createGauge(
      "tenant_budget_utilization",
      "Budget utilization percentage",
      ["tenant_id", "period"]
    );
    this.metrics.createCounter("cost_alerts_sent", "Cost alerts sent", [
      "tenant_id",
      "type"
    ]);
    this.metrics.createCounter(
      "cost_optimizations_identified",
      "Cost optimization opportunities",
      ["tenant_id", "category"]
    );
    this.metrics.createHistogram(
      "cost_calculation_duration",
      "Time to calculate costs",
      {
        buckets: [0.01, 0.1, 0.5, 1, 2, 5]
      }
    );
  }
  async loadTenantBudgets() {
    try {
      const budgets = await this.db.query(`
        SELECT tenant_id, budget_config 
        FROM tenant_budgets 
        WHERE active = true
      `);
      for (const row of budgets.rows) {
        this.budgets.set(row.tenant_id, JSON.parse(row.budget_config));
      }
      logger_default2.info("Loaded tenant budgets", { count: budgets.rows.length });
    } catch (error) {
      logger_default2.error("Failed to load tenant budgets", {
        error: error instanceof Error ? error.message : String(error)
      });
    }
  }
  startCostTracking() {
    if (!this.config.enabled) {
      logger_default2.info("Tenant cost tracking disabled");
      return;
    }
    setInterval(
      async () => {
        await this.calculateAllTenantCosts();
      },
      this.config.trackingIntervalMinutes * 60 * 1e3
    );
    if (this.config.forecastingEnabled) {
      setInterval(
        async () => {
          await this.generateAllForecasts();
        },
        24 * 60 * 60 * 1e3
      );
    }
    logger_default2.info("Tenant cost tracking started", {
      interval: this.config.trackingIntervalMinutes,
      forecastingEnabled: this.config.forecastingEnabled
    });
  }
  // Public methods for cost tracking
  async recordResourceUsage(tenantId, usage, serviceName = "unknown") {
    return tracer5.startActiveSpan(
      "tenant_cost_service.record_usage",
      async (span) => {
        span.setAttributes({
          "tenant_cost.tenant_id": tenantId,
          "tenant_cost.compute_units": usage.computeUnits || 0,
          "tenant_cost.api_calls": usage.apiCalls || 0,
          "tenant_cost.service_name": serviceName
        });
        try {
          await this.db.query(
            `
          INSERT INTO tenant_resource_usage (
            tenant_id, timestamp, compute_units, storage_gb, 
            network_gb, api_calls, active_users, queries, data_ingested, service_name
          ) VALUES ($1, NOW(), $2, $3, $4, $5, $6, $7, $8, $9)
        `,
            [
              tenantId,
              usage.computeUnits || 0,
              usage.storageGB || 0,
              usage.networkGB || 0,
              usage.apiCalls || 0,
              usage.activeUsers || 0,
              usage.queries || 0,
              usage.dataIngested || 0,
              serviceName
            ]
          );
          this.updateMetrics(tenantId, usage);
        } catch (error) {
          logger_default2.error("Failed to record resource usage", {
            tenantId,
            error: error instanceof Error ? error.message : String(error)
          });
          span.recordException(error);
        }
      }
    );
  }
  updateMetrics(tenantId, usage) {
    const computeCost = (usage.computeUnits || 0) * this.getCostRate("compute");
    const storageCost = (usage.storageGB || 0) * this.getCostRate("storage");
    const networkCost = (usage.networkGB || 0) * this.getCostRate("network");
    const apiCost = (usage.apiCalls || 0) * this.getCostRate("api_calls");
    this.metrics.setGauge("tenant_cost_compute", computeCost, {
      tenant_id: tenantId
    });
    this.metrics.setGauge("tenant_cost_storage", storageCost, {
      tenant_id: tenantId
    });
    this.metrics.setGauge("tenant_cost_network", networkCost, {
      tenant_id: tenantId
    });
    this.metrics.setGauge("tenant_cost_api", apiCost, { tenant_id: tenantId });
  }
  getCostRate(category) {
    const costCategory = this.config.costCategories.find(
      (c) => c.name === category
    );
    return costCategory?.ratePerUnit || 0;
  }
  async calculateTenantCosts(tenantId, period = "hour") {
    return tracer5.startActiveSpan(
      "tenant_cost_service.calculate_costs",
      async (span) => {
        const startTime = Date.now();
        try {
          span.setAttributes({
            "tenant_cost.tenant_id": tenantId,
            "tenant_cost.period": period
          });
          const usage = await this.getUsageForPeriod(tenantId, period);
          const costs = {
            compute: usage.computeUnits * this.getCostRate("compute"),
            storage: usage.storageGB * this.getCostRate("storage"),
            network: usage.networkGB * this.getCostRate("network"),
            apiCalls: usage.apiCalls * this.getCostRate("api_calls"),
            total: 0
          };
          costs.total = costs.compute + costs.storage + costs.network + costs.apiCalls;
          const costPerUser = usage.activeUsers > 0 ? costs.total / usage.activeUsers : 0;
          const costPerQuery = usage.queries > 0 ? costs.total / usage.queries : 0;
          const costPerGB = usage.dataIngested > 0 ? costs.total / usage.dataIngested : 0;
          const metrics8 = {
            tenantId,
            timestamp: /* @__PURE__ */ new Date(),
            period,
            computeUnits: usage.computeUnits,
            storageGB: usage.storageGB,
            networkGB: usage.networkGB,
            apiCalls: usage.apiCalls,
            costs,
            activeUsers: usage.activeUsers,
            queries: usage.queries,
            dataIngested: usage.dataIngested,
            costPerUser,
            costPerQuery,
            costPerGB
          };
          this.metrics.setGauge("tenant_cost_total", costs.total, {
            tenant_id: tenantId,
            period
          });
          await this.checkBudgetAlerts(tenantId, metrics8);
          this.cacheCostMetrics(tenantId, metrics8);
          const duration = (Date.now() - startTime) / 1e3;
          this.metrics.observeHistogram("cost_calculation_duration", duration);
          return metrics8;
        } catch (error) {
          logger_default2.error("Failed to calculate tenant costs", {
            tenantId,
            period,
            error: error instanceof Error ? error.message : String(error)
          });
          span.recordException(error);
          throw error;
        }
      }
    );
  }
  async getServiceCostBreakdown(tenantId, period = "hour") {
    let interval;
    switch (period) {
      case "hour":
        interval = "1 hour";
        break;
      case "day":
        interval = "1 day";
        break;
      case "week":
        interval = "7 days";
        break;
      case "month":
        interval = "30 days";
        break;
    }
    const result2 = await this.db.query(
      `
      SELECT
        service_name,
        COALESCE(SUM(compute_units), 0) as compute_units,
        COALESCE(AVG(storage_gb), 0) as storage_gb,
        COALESCE(SUM(network_gb), 0) as network_gb,
        COALESCE(SUM(api_calls), 0) as api_calls
      FROM tenant_resource_usage
      WHERE tenant_id = $1
      AND timestamp >= NOW() - INTERVAL '${interval}'
      GROUP BY service_name
    `,
      [tenantId]
    );
    const metrics8 = result2.rows.map((row) => {
      let durationHours = 1;
      if (period === "day") durationHours = 24;
      if (period === "week") durationHours = 24 * 7;
      if (period === "month") durationHours = 24 * 30;
      const costs = parseFloat(row.compute_units) * this.getCostRate("compute") + parseFloat(row.storage_gb) * this.getCostRate("storage") * durationHours + parseFloat(row.network_gb) * this.getCostRate("network") + parseFloat(row.api_calls) * this.getCostRate("api_calls");
      return {
        serviceName: row.service_name || "unknown",
        cost: costs,
        percentage: 0,
        // Will calculate below
        usage: {
          computeUnits: parseFloat(row.compute_units),
          storageGB: parseFloat(row.storage_gb),
          networkGB: parseFloat(row.network_gb),
          apiCalls: parseFloat(row.api_calls)
        }
      };
    });
    const totalCost = metrics8.reduce((sum, m) => sum + m.cost, 0);
    if (totalCost > 0) {
      metrics8.forEach((m) => {
        m.percentage = m.cost / totalCost * 100;
      });
    }
    return metrics8;
  }
  async getUsageForPeriod(tenantId, period) {
    let interval;
    switch (period) {
      case "hour":
        interval = "1 hour";
        break;
      case "day":
        interval = "1 day";
        break;
      case "week":
        interval = "7 days";
        break;
      case "month":
        interval = "30 days";
        break;
    }
    const result2 = await this.db.query(
      `
      SELECT 
        COALESCE(SUM(compute_units), 0) as compute_units,
        COALESCE(AVG(storage_gb), 0) as storage_gb,
        COALESCE(SUM(network_gb), 0) as network_gb,
        COALESCE(SUM(api_calls), 0) as api_calls,
        COALESCE(AVG(active_users), 0) as active_users,
        COALESCE(SUM(queries), 0) as queries,
        COALESCE(SUM(data_ingested), 0) as data_ingested
      FROM tenant_resource_usage
      WHERE tenant_id = $1 
      AND timestamp >= NOW() - INTERVAL '${interval}'
    `,
      [tenantId]
    );
    return result2.rows[0] || {
      computeUnits: 0,
      storageGB: 0,
      networkGB: 0,
      apiCalls: 0,
      activeUsers: 0,
      queries: 0,
      dataIngested: 0
    };
  }
  cacheCostMetrics(tenantId, metrics8) {
    const cached = this.costCache.get(tenantId) || [];
    cached.push(metrics8);
    const cutoff = new Date(Date.now() - 24 * 60 * 60 * 1e3);
    const recent = cached.filter((m) => m.timestamp > cutoff);
    this.costCache.set(tenantId, recent);
  }
  async checkBudgetAlerts(tenantId, metrics8) {
    const budget = this.budgets.get(tenantId);
    if (!budget || !budget.alerts.enabled) return;
    const dailyCosts = await this.getDailyCosts(tenantId);
    const monthlyCosts = await this.getMonthlyCosts(tenantId);
    if (budget.budgets.daily > 0) {
      const dailyUtilization = dailyCosts / budget.budgets.daily * 100;
      this.metrics.setGauge("tenant_budget_utilization", dailyUtilization, {
        tenant_id: tenantId,
        period: "daily"
      });
      for (const threshold of budget.alerts.thresholds) {
        if (dailyUtilization >= threshold) {
          await this.sendBudgetAlert(
            tenantId,
            "daily",
            dailyUtilization,
            threshold,
            budget
          );
        }
      }
    }
    if (budget.budgets.monthly > 0) {
      const monthlyUtilization = monthlyCosts / budget.budgets.monthly * 100;
      this.metrics.setGauge("tenant_budget_utilization", monthlyUtilization, {
        tenant_id: tenantId,
        period: "monthly"
      });
      for (const threshold of budget.alerts.thresholds) {
        if (monthlyUtilization >= threshold) {
          await this.sendBudgetAlert(
            tenantId,
            "monthly",
            monthlyUtilization,
            threshold,
            budget
          );
        }
      }
    }
    await this.checkCostSpikes(tenantId, metrics8);
  }
  async getDailyCosts(tenantId) {
    const metrics8 = await this.calculateTenantCosts(tenantId, "day");
    return metrics8.costs.total;
  }
  async getMonthlyCosts(tenantId) {
    const metrics8 = await this.calculateTenantCosts(tenantId, "month");
    return metrics8.costs.total;
  }
  async sendBudgetAlert(tenantId, period, utilization, threshold, budget) {
    const alertKey = `budget_${tenantId}_${period}_${threshold}`;
    const lastAlert = await this.getLastAlert(alertKey);
    if (lastAlert && Date.now() - lastAlert < 60 * 60 * 1e3) {
      return;
    }
    logger_default2.warn("Budget threshold exceeded", {
      tenantId,
      period,
      utilization,
      threshold,
      budget: period === "daily" ? budget.budgets.daily : budget.budgets.monthly
    });
    this.metrics.incrementCounter("cost_alerts_sent", {
      tenant_id: tenantId,
      type: "budget_threshold"
    });
    this.emit("budgetAlert", {
      tenantId,
      period,
      utilization,
      threshold,
      budget,
      recipients: budget.alerts.recipients
    });
    await this.recordAlert(alertKey);
  }
  async checkCostSpikes(tenantId, currentMetrics) {
    const cached = this.costCache.get(tenantId) || [];
    if (cached.length < 3) return;
    const recentCosts = cached.slice(-3).map((m) => m.costs.total);
    const averageRecent = recentCosts.reduce((a, b) => a + b, 0) / recentCosts.length;
    const currentCost = currentMetrics.costs.total;
    if (currentCost > averageRecent * this.config.alertThresholds.suddenSpikeMultiplier) {
      logger_default2.warn("Cost spike detected", {
        tenantId,
        currentCost,
        averageRecent,
        spikeMultiplier: currentCost / averageRecent
      });
      this.emit("costSpike", {
        tenantId,
        currentCost,
        averageRecent,
        spikeMultiplier: currentCost / averageRecent,
        metrics: currentMetrics
      });
      this.metrics.incrementCounter("cost_alerts_sent", {
        tenant_id: tenantId,
        type: "cost_spike"
      });
    }
  }
  async getLastAlert(alertKey) {
    try {
      const result2 = await this.db.query(
        "SELECT timestamp FROM cost_alerts WHERE alert_key = $1 ORDER BY timestamp DESC LIMIT 1",
        [alertKey]
      );
      return result2.rows[0]?.timestamp.getTime() || null;
    } catch (error) {
      return null;
    }
  }
  async recordAlert(alertKey) {
    try {
      await this.db.query(
        "INSERT INTO cost_alerts (alert_key, timestamp) VALUES ($1, NOW())",
        [alertKey]
      );
    } catch (error) {
      logger_default2.error("Failed to record alert", {
        alertKey,
        error: error instanceof Error ? error.message : String(error)
      });
    }
  }
  // Forecasting methods
  async generateCostForecast(tenantId) {
    return tracer5.startActiveSpan(
      "tenant_cost_service.generate_forecast",
      async (span) => {
        try {
          const historicalData = await this.getHistoricalCosts(tenantId, 30);
          if (historicalData.length < 7) {
            throw new Error("Insufficient historical data for forecasting");
          }
          const { slope, trend } = this.analyzetrend(historicalData);
          const lastCost = historicalData[historicalData.length - 1];
          const dayPrediction = this.predictCost(lastCost, slope, 1);
          const weekPrediction = this.predictCost(lastCost, slope, 7);
          const monthPrediction = this.predictCost(lastCost, slope, 30);
          const forecast = {
            tenantId,
            forecastPeriod: "day",
            confidence: this.calculateConfidence(historicalData, slope),
            predicted: {
              cost: dayPrediction.total,
              computeUnits: dayPrediction.compute / this.getCostRate("compute"),
              storageGB: dayPrediction.storage / this.getCostRate("storage"),
              networkGB: dayPrediction.network / this.getCostRate("network")
            },
            trend,
            factors: this.identifyTrendFactors(historicalData),
            recommendations: await this.generateRecommendations(
              tenantId,
              trend,
              dayPrediction
            )
          };
          this.forecasts.set(tenantId, forecast);
          return forecast;
        } catch (error) {
          logger_default2.error("Failed to generate cost forecast", {
            tenantId,
            error: error instanceof Error ? error.message : String(error)
          });
          throw error;
        }
      }
    );
  }
  async getHistoricalCosts(tenantId, days) {
    const cached = this.costCache.get(tenantId) || [];
    return cached.slice(-days);
  }
  analyzetrend(data) {
    if (data.length < 2) return { slope: 0, trend: "stable" };
    const n = data.length;
    const x = data.map((_2, i) => i);
    const y = data.map((d) => d.costs.total);
    const sumX = x.reduce((a, b) => a + b, 0);
    const sumY = y.reduce((a, b) => a + b, 0);
    const sumXY = x.reduce((sum, xi, i) => sum + xi * y[i], 0);
    const sumXX = x.reduce((sum, xi) => sum + xi * xi, 0);
    const slope = (n * sumXY - sumX * sumY) / (n * sumXX - sumX * sumX);
    let trend;
    if (Math.abs(slope) < 0.01) trend = "stable";
    else if (slope > 0) trend = "increasing";
    else trend = "decreasing";
    return { slope, trend };
  }
  predictCost(baseCost, slope, days) {
    const factor = 1 + slope * days;
    return {
      total: baseCost.costs.total * factor,
      compute: baseCost.costs.compute * factor,
      storage: baseCost.costs.storage * factor,
      network: baseCost.costs.network * factor,
      apiCalls: baseCost.costs.apiCalls * factor
    };
  }
  calculateConfidence(data, slope) {
    if (data.length < 3) return 0.5;
    const yActual = data.map((d) => d.costs.total);
    const yMean = yActual.reduce((a, b) => a + b, 0) / yActual.length;
    const yPredicted = data.map((_2, i) => data[0].costs.total + slope * i);
    const ssRes = yActual.reduce(
      (sum, actual, i) => sum + Math.pow(actual - yPredicted[i], 2),
      0
    );
    const ssTot = yActual.reduce(
      (sum, actual) => sum + Math.pow(actual - yMean, 2),
      0
    );
    const rSquared = 1 - ssRes / ssTot;
    return Math.max(0, Math.min(1, rSquared));
  }
  identifyTrendFactors(data) {
    const factors = [];
    if (data.length >= 2) {
      const recent = data[data.length - 1];
      const previous = data[data.length - 2];
      const computeChange = (recent.costs.compute - previous.costs.compute) / previous.costs.compute;
      const storageChange = (recent.costs.storage - previous.costs.storage) / previous.costs.storage;
      const networkChange = (recent.costs.network - previous.costs.network) / previous.costs.network;
      if (computeChange > 0.1) factors.push("Increasing compute usage");
      if (storageChange > 0.1) factors.push("Growing storage requirements");
      if (networkChange > 0.1) factors.push("Higher network traffic");
    }
    return factors;
  }
  async generateRecommendations(tenantId, trend, prediction) {
    const recommendations = [];
    if (trend === "increasing") {
      recommendations.push(
        "Consider implementing resource optimization strategies"
      );
      recommendations.push("Review query patterns for efficiency improvements");
      recommendations.push("Evaluate storage archival policies");
    }
    const metrics8 = await this.calculateTenantCosts(tenantId, "day");
    if (metrics8.costPerUser > 1) {
      recommendations.push(
        "Cost per user is high - consider user experience optimization"
      );
    }
    if (metrics8.costs.storage > metrics8.costs.compute * 2) {
      recommendations.push(
        "Storage costs dominate - implement data lifecycle management"
      );
    }
    return recommendations;
  }
  // Public API methods
  async getTenantCostDashboard(tenantId) {
    const current = await this.calculateTenantCosts(tenantId, "hour");
    const budget = this.budgets.get(tenantId) || null;
    const forecast = this.forecasts.get(tenantId) || null;
    const optimizations = await this.identifyOptimizations(tenantId);
    const serviceBreakdown = await this.getServiceCostBreakdown(tenantId, "day");
    const dailyTrends = await this.getTrendData(tenantId, "day", 7);
    const weeklyTrends = await this.getTrendData(tenantId, "week", 4);
    const monthlyTrends = await this.getTrendData(tenantId, "month", 12);
    return {
      current,
      budget,
      forecast,
      optimizations,
      trends: {
        daily: dailyTrends,
        weekly: weeklyTrends,
        monthly: monthlyTrends
      },
      serviceBreakdown
    };
  }
  async identifyOptimizations(tenantId) {
    const optimizations = [];
    const metrics8 = await this.calculateTenantCosts(tenantId, "day");
    if (metrics8.costs.storage > metrics8.costs.total * 0.4) {
      optimizations.push({
        tenantId,
        category: "storage",
        priority: "high",
        issue: "Storage costs are disproportionately high",
        recommendation: "Implement data archival and compression strategies",
        estimatedSavings: metrics8.costs.storage * 0.3,
        implementationEffort: "medium",
        metrics: {
          currentCost: metrics8.costs.storage,
          projectedCost: metrics8.costs.storage * 0.7,
          savingsPercentage: 30
        }
      });
      this.metrics.incrementCounter("cost_optimizations_identified", {
        tenant_id: tenantId,
        category: "storage"
      });
    }
    if (metrics8.costPerQuery > 0.01) {
      optimizations.push({
        tenantId,
        category: "compute",
        priority: "medium",
        issue: "High cost per query indicates inefficient processing",
        recommendation: "Optimize query patterns and add caching layers",
        estimatedSavings: metrics8.costs.compute * 0.2,
        implementationEffort: "high",
        metrics: {
          currentCost: metrics8.costs.compute,
          projectedCost: metrics8.costs.compute * 0.8,
          savingsPercentage: 20
        }
      });
      this.metrics.incrementCounter("cost_optimizations_identified", {
        tenant_id: tenantId,
        category: "compute"
      });
    }
    return optimizations;
  }
  async getTrendData(tenantId, period, count) {
    const trends = [];
    for (let i = count - 1; i >= 0; i--) {
      try {
        const metrics8 = await this.calculateTenantCosts(tenantId, period);
        trends.push(metrics8);
      } catch (error) {
        logger_default2.warn("Failed to get trend data point", { tenantId, period, i });
      }
    }
    return trends;
  }
  async calculateAllTenantCosts() {
    try {
      const tenants = await this.db.query(
        "SELECT DISTINCT tenant_id FROM tenant_resource_usage WHERE timestamp >= NOW() - INTERVAL '1 day'"
      );
      for (const row of tenants.rows) {
        try {
          await this.calculateTenantCosts(row.tenant_id);
        } catch (error) {
          logger_default2.error("Failed to calculate costs for tenant", {
            tenantId: row.tenant_id,
            error: error instanceof Error ? error.message : String(error)
          });
        }
      }
    } catch (error) {
      logger_default2.error("Failed to calculate all tenant costs", {
        error: error instanceof Error ? error.message : String(error)
      });
    }
  }
  async generateAllForecasts() {
    try {
      for (const tenantId of Array.from(this.budgets.keys())) {
        try {
          await this.generateCostForecast(tenantId);
        } catch (error) {
          logger_default2.error("Failed to generate forecast for tenant", {
            tenantId,
            error: error instanceof Error ? error.message : String(error)
          });
        }
      }
    } catch (error) {
      logger_default2.error("Failed to generate all forecasts", {
        error: error instanceof Error ? error.message : String(error)
      });
    }
  }
  // Admin methods
  async setBudget(tenantId, budget) {
    this.budgets.set(tenantId, budget);
    try {
      await this.db.query(
        `
        INSERT INTO tenant_budgets (tenant_id, budget_config, active, updated_at)
        VALUES ($1, $2, true, NOW())
        ON CONFLICT (tenant_id) DO UPDATE SET
        budget_config = $2, updated_at = NOW()
      `,
        [tenantId, JSON.stringify(budget)]
      );
      logger_default2.info("Updated tenant budget", { tenantId });
    } catch (error) {
      logger_default2.error("Failed to save budget", {
        tenantId,
        error: error instanceof Error ? error.message : String(error)
      });
    }
  }
  getCachedCosts(tenantId) {
    return this.costCache.get(tenantId) || [];
  }
  recordDoclingCost(tenantId, amountUsd, metadata) {
    if (!this.config.enabled) return;
    this.emit("doclingCost", { tenantId, amountUsd, metadata });
    logger_default2.info("Recorded docling cost", { tenantId, amountUsd, metadata });
    const apiRate = this.getCostRate("api_calls");
    const computeRate = this.getCostRate("compute");
    const computeCost = Math.max(0, amountUsd - apiRate);
    this.recordResourceUsage(
      tenantId,
      { apiCalls: 1, computeUnits: computeRate > 0 ? computeCost / computeRate : 0 },
      "DoclingService"
    ).catch((err) => logger_default2.error("Failed to record docling usage to DB", err));
  }
  getCostCategories() {
    return this.config.costCategories;
  }
  updateCostRates(rates) {
    for (const [category, rate] of Object.entries(rates)) {
      const costCategory = this.config.costCategories.find(
        (c) => c.name === category
      );
      if (costCategory && typeof rate === "number") {
        costCategory.ratePerUnit = rate;
        logger_default2.info("Updated cost rate", { category, rate });
      } else if (costCategory) {
        logger_default2.warn("Skipped updating cost rate due to undefined value", { category });
      }
    }
  }
};
var tenantCostService = new TenantCostService(
  {
    enabled: process.env.COST_TRACKING_ENABLED !== "false",
    trackingIntervalMinutes: parseInt(
      process.env.COST_TRACKING_INTERVAL || "15"
    ),
    forecastingEnabled: process.env.COST_FORECASTING_ENABLED !== "false",
    billingCycle: process.env.COST_BILLING_CYCLE || "hourly",
    retentionDays: parseInt(process.env.COST_RETENTION_DAYS || "365")
  },
  new DatabaseService()
);

// src/services/CostOptimizationService.ts
import { trace as trace11 } from "@opentelemetry/api";
import { Counter as Counter15, Gauge as Gauge13, Histogram as Histogram12 } from "prom-client";

// src/metrics/cost.ts
import { Counter as Counter14, Gauge as Gauge12 } from "prom-client";
import { trace as trace10 } from "@opentelemetry/api";
var tracer9 = trace10.getTracer("maestro-cost-metrics", "24.2.0");
var dbOperationCosts = new Counter14({
  name: "db_operation_costs_usd_total",
  help: "Total database operation costs in USD",
  labelNames: ["db_type", "operation", "tenant_id", "size_tier"]
});
var dbConnectionCosts = new Gauge12({
  name: "db_connection_costs_usd_per_hour",
  help: "Database connection costs per hour in USD",
  labelNames: ["db_type", "pool_size", "tenant_id"]
});
var computeCosts = new Counter14({
  name: "compute_costs_usd_total",
  help: "Total compute costs in USD",
  labelNames: ["service", "tenant_id", "resource_type"]
});
var memoryUsageCosts = new Gauge12({
  name: "memory_usage_costs_usd_per_hour",
  help: "Memory usage costs per hour in USD",
  labelNames: ["service", "tenant_id", "memory_tier"]
});
var cpuUsageCosts = new Gauge12({
  name: "cpu_usage_costs_usd_per_hour",
  help: "CPU usage costs per hour in USD",
  labelNames: ["service", "tenant_id", "cpu_tier"]
});
var storageCosts = new Counter14({
  name: "storage_costs_usd_total",
  help: "Total storage costs in USD",
  labelNames: ["storage_type", "tenant_id", "size_tier"]
});
var storageIOCosts = new Counter14({
  name: "storage_io_costs_usd_total",
  help: "Storage I/O costs in USD",
  labelNames: ["storage_type", "operation", "tenant_id"]
});
var networkCosts = new Counter14({
  name: "network_costs_usd_total",
  help: "Total network costs in USD",
  labelNames: ["direction", "tenant_id", "bandwidth_tier"]
});
var apiRequestCosts = new Counter14({
  name: "api_request_costs_usd_total",
  help: "API request costs in USD",
  labelNames: ["endpoint", "tenant_id", "request_size_tier"]
});
var aiProcessingCosts = new Counter14({
  name: "ai_processing_costs_usd_total",
  help: "AI processing costs in USD",
  labelNames: ["model", "operation", "tenant_id"]
});
var tenantTotalCosts = new Gauge12({
  name: "tenant_total_costs_usd",
  help: "Total costs per tenant in USD",
  labelNames: ["tenant_id", "billing_period"]
});
var tenantBudgetUtilization = new Gauge12({
  name: "tenant_budget_utilization_percent",
  help: "Tenant budget utilization percentage",
  labelNames: ["tenant_id", "billing_period"]
});
var costAlerts = new Counter14({
  name: "cost_alerts_total",
  help: "Total cost alerts triggered",
  labelNames: ["alert_type", "tenant_id", "severity"]
});
var costOptimizationSavings = new Counter14({
  name: "cost_optimization_savings_usd_total",
  help: "Total savings from cost optimization in USD",
  labelNames: ["optimization_type", "tenant_id"]
});
var CostTracker = class {
  costPerDbOperation = {
    postgresql: {
      read: 1e-4,
      // $0.0001 per read
      write: 2e-4,
      // $0.0002 per write
      connection: 0.01
      // $0.01 per hour per connection
    },
    neo4j: {
      read: 3e-4,
      write: 5e-4,
      connection: 0.02
    }
  };
  costPerComputeUnit = {
    cpu: 0.05,
    // $0.05 per vCPU hour
    memory: 0.01,
    // $0.01 per GB hour
    gpu: 2.5
    // $2.50 per GPU hour
  };
  costPerStorageGB = {
    ssd: 0.1,
    // $0.10 per GB per month
    standard: 0.05,
    // $0.05 per GB per month
    archive: 0.01
    // $0.01 per GB per month
  };
  costPerNetworkGB = {
    ingress: 0,
    // Free ingress
    egress: 0.09
    // $0.09 per GB egress
  };
  async trackDatabaseOperation(dbType, operation, tenantId, rowsAffected = 1) {
    return tracer9.startActiveSpan(
      "cost.track_db_operation",
      async (span) => {
        const baseCost = this.costPerDbOperation[dbType][operation];
        const totalCost = baseCost * rowsAffected;
        const sizeTier = this.determineSizeTier(rowsAffected);
        dbOperationCosts.inc(
          {
            db_type: dbType,
            operation,
            tenant_id: tenantId,
            size_tier: sizeTier
          },
          totalCost
        );
        span.setAttributes({
          "cost.db_type": dbType,
          "cost.operation": operation,
          "cost.tenant_id": tenantId,
          "cost.rows_affected": rowsAffected,
          "cost.total_usd": totalCost
        });
        await this.updateTenantTotalCost(tenantId, totalCost);
        span.end();
      }
    );
  }
  async trackComputeUsage(service11, tenantId, cpuHours, memoryGBHours, gpuHours = 0) {
    return tracer9.startActiveSpan("cost.track_compute", async (span) => {
      const cpuCost = cpuHours * this.costPerComputeUnit.cpu;
      const memoryCost = memoryGBHours * this.costPerComputeUnit.memory;
      const gpuCost = gpuHours * this.costPerComputeUnit.gpu;
      const totalCost = cpuCost + memoryCost + gpuCost;
      computeCosts.inc(
        {
          service: service11,
          tenant_id: tenantId,
          resource_type: "cpu"
        },
        cpuCost
      );
      computeCosts.inc(
        {
          service: service11,
          tenant_id: tenantId,
          resource_type: "memory"
        },
        memoryCost
      );
      if (gpuCost > 0) {
        computeCosts.inc(
          {
            service: service11,
            tenant_id: tenantId,
            resource_type: "gpu"
          },
          gpuCost
        );
      }
      cpuUsageCosts.set(
        {
          service: service11,
          tenant_id: tenantId,
          cpu_tier: this.determineCpuTier(cpuHours)
        },
        cpuCost
      );
      memoryUsageCosts.set(
        {
          service: service11,
          tenant_id: tenantId,
          memory_tier: this.determineMemoryTier(memoryGBHours)
        },
        memoryCost
      );
      span.setAttributes({
        "cost.service": service11,
        "cost.tenant_id": tenantId,
        "cost.cpu_hours": cpuHours,
        "cost.memory_gb_hours": memoryGBHours,
        "cost.gpu_hours": gpuHours,
        "cost.total_usd": totalCost
      });
      await this.updateTenantTotalCost(tenantId, totalCost);
      span.end();
    });
  }
  async trackStorageUsage(storageType, tenantId, gbStored, ioOperations = 0) {
    return tracer9.startActiveSpan("cost.track_storage", async (span) => {
      const storageCost = gbStored * this.costPerStorageGB[storageType] / 30 / 24;
      const ioCost = ioOperations * 1e-4;
      storageCosts.inc(
        {
          storage_type: storageType,
          tenant_id: tenantId,
          size_tier: this.determineStorageTier(gbStored)
        },
        storageCost
      );
      if (ioCost > 0) {
        storageIOCosts.inc(
          {
            storage_type: storageType,
            operation: "io",
            tenant_id: tenantId
          },
          ioCost
        );
      }
      span.setAttributes({
        "cost.storage_type": storageType,
        "cost.tenant_id": tenantId,
        "cost.gb_stored": gbStored,
        "cost.io_operations": ioOperations,
        "cost.total_usd": storageCost + ioCost
      });
      await this.updateTenantTotalCost(tenantId, storageCost + ioCost);
      span.end();
    });
  }
  async trackNetworkUsage(direction, tenantId, gbTransferred) {
    return tracer9.startActiveSpan("cost.track_network", async (span) => {
      const cost = gbTransferred * this.costPerNetworkGB[direction];
      const tier = this.determineBandwidthTier(gbTransferred);
      networkCosts.inc(
        {
          direction,
          tenant_id: tenantId,
          bandwidth_tier: tier
        },
        cost
      );
      span.setAttributes({
        "cost.direction": direction,
        "cost.tenant_id": tenantId,
        "cost.gb_transferred": gbTransferred,
        "cost.total_usd": cost
      });
      await this.updateTenantTotalCost(tenantId, cost);
      span.end();
    });
  }
  async trackAPIRequest(endpoint, tenantId, requestSizeKB = 1) {
    return tracer9.startActiveSpan(
      "cost.track_api_request",
      async (span) => {
        const baseCost = 1e-3;
        const sizeCost = requestSizeKB * 1e-4;
        const totalCost = baseCost + sizeCost;
        const tier = this.determineRequestSizeTier(requestSizeKB);
        apiRequestCosts.inc(
          {
            endpoint,
            tenant_id: tenantId,
            request_size_tier: tier
          },
          totalCost
        );
        span.setAttributes({
          "cost.endpoint": endpoint,
          "cost.tenant_id": tenantId,
          "cost.request_size_kb": requestSizeKB,
          "cost.total_usd": totalCost
        });
        await this.updateTenantTotalCost(tenantId, totalCost);
        span.end();
      }
    );
  }
  async trackAIProcessing(model, operation, tenantId, tokensProcessed) {
    return tracer9.startActiveSpan(
      "cost.track_ai_processing",
      async (span) => {
        const costPerToken = 1e-5;
        const totalCost = tokensProcessed * costPerToken;
        aiProcessingCosts.inc(
          {
            model,
            operation,
            tenant_id: tenantId
          },
          totalCost
        );
        span.setAttributes({
          "cost.model": model,
          "cost.operation": operation,
          "cost.tenant_id": tenantId,
          "cost.tokens_processed": tokensProcessed,
          "cost.total_usd": totalCost
        });
        await this.updateTenantTotalCost(tenantId, totalCost);
        span.end();
      }
    );
  }
  async updateTenantTotalCost(tenantId, additionalCost) {
    tenantTotalCosts.inc(
      {
        tenant_id: tenantId,
        billing_period: "hourly" /* HOURLY */
      },
      additionalCost
    );
    await this.checkBudgetAlerts(tenantId);
  }
  async checkBudgetAlerts(tenantId) {
    const currentHourlySpend = 10.5;
    const hourlyBudget = 15;
    const utilizationPercent = currentHourlySpend / hourlyBudget * 100;
    tenantBudgetUtilization.set(
      {
        tenant_id: tenantId,
        billing_period: "hourly" /* HOURLY */
      },
      utilizationPercent
    );
    if (utilizationPercent >= 90) {
      await this.triggerBudgetAlert(tenantId, {
        tenantId,
        budgetLimit: hourlyBudget,
        currentSpend: currentHourlySpend,
        utilizationPercent,
        period: "hourly" /* HOURLY */,
        alertType: "budget_exceeded",
        severity: "critical" /* CRITICAL */
      });
    } else if (utilizationPercent >= 75) {
      await this.triggerBudgetAlert(tenantId, {
        tenantId,
        budgetLimit: hourlyBudget,
        currentSpend: currentHourlySpend,
        utilizationPercent,
        period: "hourly" /* HOURLY */,
        alertType: "budget_warning",
        severity: "warning" /* WARNING */
      });
    }
  }
  async triggerBudgetAlert(tenantId, alert) {
    costAlerts.inc({
      alert_type: alert.alertType,
      tenant_id: tenantId,
      severity: alert.severity
    });
    console.warn(
      `\u{1F4B0} BUDGET ALERT [${alert.severity.toUpperCase()}]: Tenant ${tenantId} at ${alert.utilizationPercent.toFixed(1)}% of budget`
    );
  }
  determineSizeTier(value) {
    if (value < 100) return "small" /* SMALL */;
    if (value < 1e3) return "medium" /* MEDIUM */;
    if (value < 1e4) return "large" /* LARGE */;
    return "xlarge" /* XLARGE */;
  }
  determineCpuTier(cpuHours) {
    if (cpuHours < 1) return "small" /* SMALL */;
    if (cpuHours < 4) return "medium" /* MEDIUM */;
    if (cpuHours < 16) return "large" /* LARGE */;
    return "xlarge" /* XLARGE */;
  }
  determineMemoryTier(memoryGBHours) {
    if (memoryGBHours < 2) return "small" /* SMALL */;
    if (memoryGBHours < 8) return "medium" /* MEDIUM */;
    if (memoryGBHours < 32) return "large" /* LARGE */;
    return "xlarge" /* XLARGE */;
  }
  determineStorageTier(gbStored) {
    if (gbStored < 10) return "small" /* SMALL */;
    if (gbStored < 100) return "medium" /* MEDIUM */;
    if (gbStored < 1e3) return "large" /* LARGE */;
    return "xlarge" /* XLARGE */;
  }
  determineBandwidthTier(gbTransferred) {
    if (gbTransferred < 1) return "small" /* SMALL */;
    if (gbTransferred < 10) return "medium" /* MEDIUM */;
    if (gbTransferred < 100) return "large" /* LARGE */;
    return "xlarge" /* XLARGE */;
  }
  determineRequestSizeTier(requestSizeKB) {
    if (requestSizeKB < 10) return "small" /* SMALL */;
    if (requestSizeKB < 100) return "medium" /* MEDIUM */;
    if (requestSizeKB < 1e3) return "large" /* LARGE */;
    return "xlarge" /* XLARGE */;
  }
};
var costTracker2 = new CostTracker();

// src/services/CostOptimizationService.ts
var tracer10 = trace11.getTracer("cost-optimization-service", "24.2.0");
var optimizationOpportunities = new Gauge13({
  name: "cost_optimization_opportunities_total",
  help: "Total cost optimization opportunities identified",
  labelNames: ["tenant_id", "optimization_type", "potential_savings_tier"]
});
var optimizationExecutions = new Counter15({
  name: "cost_optimization_executions_total",
  help: "Total cost optimizations executed",
  labelNames: ["tenant_id", "optimization_type", "status"]
});
var optimizationImpact = new Histogram12({
  name: "cost_optimization_impact_usd",
  help: "Cost optimization impact in USD",
  labelNames: ["tenant_id", "optimization_type"],
  buckets: [0.1, 0.5, 1, 5, 10, 25, 50, 100]
});
var CostOptimizationService = class {
  MAX_AUTO_SAVINGS_THRESHOLD = 50;
  // Max $50 auto-optimization
  MIN_SAVINGS_THRESHOLD = 0.1;
  // Min $0.10 to consider
  async identifyOptimizationOpportunities(tenantId) {
    return tracer10.startActiveSpan(
      "cost_optimization.identify_opportunities",
      async (span) => {
        const opportunities = [];
        try {
          opportunities.push(
            ...await this.identifyDatabaseOptimizations(tenantId)
          );
          opportunities.push(
            ...await this.identifyStorageOptimizations(tenantId)
          );
          opportunities.push(
            ...await this.identifyComputeOptimizations(tenantId)
          );
          opportunities.push(
            ...await this.identifyDataLifecycleOptimizations(tenantId)
          );
          opportunities.push(...await this.identifyAIOptimizations(tenantId));
          opportunities.forEach((opp) => {
            const savingsTier = this.categorizeSavings(opp.potentialSavingsUSD);
            optimizationOpportunities.set(
              {
                tenant_id: opp.tenantId,
                optimization_type: opp.type,
                potential_savings_tier: savingsTier
              },
              opp.potentialSavingsUSD
            );
          });
          span.setAttributes({
            opportunities_found: opportunities.length,
            total_potential_savings: opportunities.reduce(
              (sum, o) => sum + o.potentialSavingsUSD,
              0
            ),
            auto_implementable: opportunities.filter((o) => o.autoImplementable).length
          });
        } catch (error) {
          span.recordException(error);
          span.setStatus({ code: 2, message: error.message });
          throw error;
        } finally {
          span.end();
        }
        return opportunities.filter(
          (o) => o.potentialSavingsUSD >= this.MIN_SAVINGS_THRESHOLD
        );
      }
    );
  }
  async executeOptimizations(opportunities) {
    return tracer10.startActiveSpan(
      "cost_optimization.execute_optimizations",
      async (span) => {
        const results = [];
        for (const opportunity of opportunities) {
          const result2 = await this.executeOptimization(opportunity);
          results.push(result2);
          optimizationExecutions.inc({
            tenant_id: opportunity.tenantId,
            optimization_type: opportunity.type,
            status: result2.implemented ? "success" : "failed"
          });
          if (result2.implemented && result2.actualSavingsUSD > 0) {
            optimizationImpact.observe(
              {
                tenant_id: opportunity.tenantId,
                optimization_type: opportunity.type
              },
              result2.actualSavingsUSD
            );
            costOptimizationSavings.inc(
              {
                optimization_type: opportunity.type,
                tenant_id: opportunity.tenantId
              },
              result2.actualSavingsUSD
            );
          }
        }
        const totalSavings = results.reduce(
          (sum, r) => sum + r.actualSavingsUSD,
          0
        );
        const successCount = results.filter((r) => r.implemented).length;
        span.setAttributes({
          optimizations_attempted: opportunities.length,
          optimizations_successful: successCount,
          total_actual_savings: totalSavings
        });
        span.end();
        return results;
      }
    );
  }
  async identifyDatabaseOptimizations(tenantId) {
    const opportunities = [];
    const connectionUsage = await this.analyzeDatabaseConnectionUsage(tenantId);
    if (connectionUsage.averageUtilization < 30) {
      opportunities.push({
        id: `db-pool-${tenantId}-${Date.now()}`,
        tenantId: tenantId || "global",
        type: "db_connection_pooling" /* DATABASE_CONNECTION_POOLING */,
        description: `Reduce database connection pool size from ${connectionUsage.currentPoolSize} to ${Math.ceil(connectionUsage.currentPoolSize * 0.6)}`,
        potentialSavingsUSD: connectionUsage.currentPoolSize * 0.01 * 24 * 30 * 0.4,
        // 40% pool reduction
        implementationEffort: "low" /* LOW */,
        riskLevel: "low" /* LOW */,
        autoImplementable: true,
        metadata: {
          currentPoolSize: connectionUsage.currentPoolSize,
          utilization: connectionUsage.averageUtilization
        },
        serviceName: "DatabaseService"
      });
    }
    const slowQueries = await this.identifySlowQueries(tenantId);
    for (const query3 of slowQueries) {
      if (query3.costImpact > 1) {
        opportunities.push({
          id: `query-opt-${tenantId}-${query3.hash}`,
          tenantId: tenantId || "global",
          type: "query_optimization" /* QUERY_OPTIMIZATION */,
          description: `Optimize slow query: ${query3.description}`,
          potentialSavingsUSD: query3.costImpact * 0.7,
          // 70% improvement expected
          implementationEffort: "medium" /* MEDIUM */,
          riskLevel: "medium" /* MEDIUM */,
          autoImplementable: false,
          metadata: { queryHash: query3.hash, currentCost: query3.costImpact },
          serviceName: "DatabaseService"
        });
      }
    }
    return opportunities;
  }
  async identifyStorageOptimizations(tenantId) {
    const opportunities = [];
    const archiveableData = await this.identifyArchiveableData(tenantId);
    if (archiveableData.gbArchiveable > 10) {
      const savingsPerMonth = archiveableData.gbArchiveable * (0.1 - 0.01);
      opportunities.push({
        id: `archive-${tenantId}-${Date.now()}`,
        tenantId: tenantId || "global",
        type: "data_archiving" /* DATA_ARCHIVING */,
        description: `Archive ${archiveableData.gbArchiveable.toFixed(1)}GB of old data to reduce storage costs`,
        potentialSavingsUSD: savingsPerMonth,
        implementationEffort: "low" /* LOW */,
        riskLevel: "low" /* LOW */,
        autoImplementable: true,
        metadata: {
          gbArchiveable: archiveableData.gbArchiveable,
          criteria: archiveableData.criteria
        },
        serviceName: "StorageService"
      });
    }
    const storageAnalysis = await this.analyzeStorageAccess(tenantId);
    if (storageAnalysis.coldDataGB > 5) {
      const savingsPerMonth = storageAnalysis.coldDataGB * (0.1 - 0.05);
      opportunities.push({
        id: `storage-tier-${tenantId}-${Date.now()}`,
        tenantId: tenantId || "global",
        type: "storage_tier_optimization" /* STORAGE_TIER_OPTIMIZATION */,
        description: `Move ${storageAnalysis.coldDataGB.toFixed(1)}GB of cold data to standard storage tier`,
        potentialSavingsUSD: savingsPerMonth,
        implementationEffort: "low" /* LOW */,
        riskLevel: "low" /* LOW */,
        autoImplementable: true,
        metadata: {
          coldDataGB: storageAnalysis.coldDataGB,
          accessFrequency: storageAnalysis.accessFrequency
        },
        serviceName: "StorageService"
      });
    }
    return opportunities;
  }
  async identifyComputeOptimizations(tenantId) {
    const opportunities = [];
    const resourceUsage = await this.analyzeResourceUsage(tenantId);
    if (resourceUsage.cpuUtilization < 40) {
      const potentialSavings = resourceUsage.currentCpuCost * 0.3;
      opportunities.push({
        id: `cpu-rightsize-${tenantId}-${Date.now()}`,
        tenantId: tenantId || "global",
        type: "resource_right_sizing" /* RESOURCE_RIGHT_SIZING */,
        description: `Reduce CPU allocation based on low utilization (${resourceUsage.cpuUtilization.toFixed(1)}%)`,
        potentialSavingsUSD: potentialSavings,
        implementationEffort: "medium" /* MEDIUM */,
        riskLevel: "medium" /* MEDIUM */,
        autoImplementable: false,
        metadata: {
          currentUtilization: resourceUsage.cpuUtilization,
          currentCost: resourceUsage.currentCpuCost
        },
        serviceName: "ComputeService"
      });
    }
    if (resourceUsage.memoryUtilization < 50) {
      const potentialSavings = resourceUsage.currentMemoryCost * 0.25;
      opportunities.push({
        id: `memory-rightsize-${tenantId}-${Date.now()}`,
        tenantId: tenantId || "global",
        type: "resource_right_sizing" /* RESOURCE_RIGHT_SIZING */,
        description: `Reduce memory allocation based on low utilization (${resourceUsage.memoryUtilization.toFixed(1)}%)`,
        potentialSavingsUSD: potentialSavings,
        implementationEffort: "medium" /* MEDIUM */,
        riskLevel: "medium" /* MEDIUM */,
        autoImplementable: false,
        metadata: {
          currentUtilization: resourceUsage.memoryUtilization,
          currentCost: resourceUsage.currentMemoryCost
        },
        serviceName: "ComputeService"
      });
    }
    return opportunities;
  }
  async identifyDataLifecycleOptimizations(tenantId) {
    const opportunities = [];
    const retentionAnalysis = await this.analyzeRetentionPolicies(tenantId);
    for (const policy2 of retentionAnalysis.policies) {
      if (policy2.potentialSavings > 0.5) {
        opportunities.push({
          id: `retention-${tenantId}-${policy2.table}`,
          tenantId: tenantId || "global",
          type: "retention_policy_tuning" /* RETENTION_POLICY_TUNING */,
          description: `Adjust retention policy for ${policy2.table} from ${policy2.currentDays} to ${policy2.recommendedDays} days`,
          potentialSavingsUSD: policy2.potentialSavings,
          implementationEffort: "low" /* LOW */,
          riskLevel: "medium" /* MEDIUM */,
          autoImplementable: false,
          metadata: {
            table: policy2.table,
            currentDays: policy2.currentDays,
            recommendedDays: policy2.recommendedDays
          }
        });
      }
    }
    return opportunities;
  }
  async identifyAIOptimizations(tenantId) {
    const opportunities = [];
    const aiUsage = await this.analyzeAIUsage(tenantId);
    if (aiUsage.batchableRequests > 100) {
      const savingsFromBatching = aiUsage.batchableRequests * 1e-5 * 0.3;
      opportunities.push({
        id: `ai-batch-${tenantId}-${Date.now()}`,
        tenantId: tenantId || "global",
        type: "ai_model_optimization" /* AI_MODEL_OPTIMIZATION */,
        description: `Implement batching for ${aiUsage.batchableRequests} AI requests to reduce per-token costs`,
        potentialSavingsUSD: savingsFromBatching,
        implementationEffort: "medium" /* MEDIUM */,
        riskLevel: "low" /* LOW */,
        autoImplementable: true,
        metadata: {
          batchableRequests: aiUsage.batchableRequests,
          currentModel: aiUsage.model
        },
        serviceName: "LLMService"
      });
    }
    return opportunities;
  }
  async executeOptimization(opportunity) {
    const startTime = Date.now();
    try {
      if (!opportunity.autoImplementable || opportunity.riskLevel !== "low" /* LOW */ || opportunity.potentialSavingsUSD > this.MAX_AUTO_SAVINGS_THRESHOLD) {
        return {
          opportunityId: opportunity.id,
          implemented: false,
          actualSavingsUSD: 0,
          executionTime: Date.now() - startTime,
          error: "Optimization requires manual review"
        };
      }
      let actualSavings = 0;
      switch (opportunity.type) {
        case "db_connection_pooling" /* DATABASE_CONNECTION_POOLING */:
          actualSavings = await this.implementConnectionPoolOptimization(opportunity);
          break;
        case "data_archiving" /* DATA_ARCHIVING */:
          actualSavings = await this.implementDataArchiving(opportunity);
          break;
        case "storage_tier_optimization" /* STORAGE_TIER_OPTIMIZATION */:
          actualSavings = await this.implementStorageTierOptimization(opportunity);
          break;
        case "ai_model_optimization" /* AI_MODEL_OPTIMIZATION */:
          actualSavings = await this.implementAIBatching(opportunity);
          break;
        default:
          return {
            opportunityId: opportunity.id,
            implemented: false,
            actualSavingsUSD: 0,
            executionTime: Date.now() - startTime,
            error: "Optimization type not supported for auto-implementation"
          };
      }
      return {
        opportunityId: opportunity.id,
        implemented: true,
        actualSavingsUSD: actualSavings,
        executionTime: Date.now() - startTime
      };
    } catch (error) {
      return {
        opportunityId: opportunity.id,
        implemented: false,
        actualSavingsUSD: 0,
        executionTime: Date.now() - startTime,
        error: error.message
      };
    }
  }
  // Implementation methods (simplified - would have actual implementation logic)
  async implementConnectionPoolOptimization(opportunity) {
    console.log(
      `Optimizing database connection pool for tenant ${opportunity.tenantId}`
    );
    return opportunity.potentialSavingsUSD * 0.8;
  }
  async implementDataArchiving(opportunity) {
    console.log(`Archiving data for tenant ${opportunity.tenantId}`);
    return opportunity.potentialSavingsUSD * 0.9;
  }
  async implementStorageTierOptimization(opportunity) {
    console.log(`Optimizing storage tiers for tenant ${opportunity.tenantId}`);
    return opportunity.potentialSavingsUSD * 0.85;
  }
  async implementAIBatching(opportunity) {
    console.log(
      `Implementing AI request batching for tenant ${opportunity.tenantId}`
    );
    return opportunity.potentialSavingsUSD * 0.7;
  }
  // Analysis methods (simplified - would have actual data analysis)
  async analyzeDatabaseConnectionUsage(tenantId) {
    return {
      currentPoolSize: 20,
      averageUtilization: 25,
      peakUtilization: 45
    };
  }
  async identifySlowQueries(tenantId) {
    return [
      {
        hash: "abc123",
        description: "Unoptimized join query",
        costImpact: 2.5,
        frequency: 100
      }
    ];
  }
  async identifyArchiveableData(tenantId) {
    return {
      gbArchiveable: 25.5,
      criteria: "Data older than 1 year with no recent access"
    };
  }
  async analyzeStorageAccess(tenantId) {
    return {
      coldDataGB: 15.2,
      accessFrequency: 0.01
      // Accesses per day
    };
  }
  async analyzeResourceUsage(tenantId) {
    return {
      cpuUtilization: 35,
      memoryUtilization: 45,
      currentCpuCost: 50,
      currentMemoryCost: 30
    };
  }
  async analyzeRetentionPolicies(tenantId) {
    return {
      policies: [
        {
          table: "audit_logs",
          currentDays: 365,
          recommendedDays: 90,
          potentialSavings: 5.2
        }
      ]
    };
  }
  async analyzeAIUsage(tenantId) {
    return {
      batchableRequests: 250,
      model: "gpt-4",
      averageTokens: 150
    };
  }
  categorizeSavings(savingsUSD) {
    if (savingsUSD < 1) return "small";
    if (savingsUSD < 10) return "medium";
    if (savingsUSD < 50) return "large";
    return "xlarge";
  }
};

// src/services/ResourceCostAnalyzerService.ts
init_logger2();
var ResourceCostAnalyzerService = class {
  costOptimizationService;
  constructor() {
    this.costOptimizationService = new CostOptimizationService();
  }
  /**
   * Generates a comprehensive cost analysis for a tenant, including breakdown by service
   * and optimization suggestions.
   */
  async getServiceCostAnalysis(tenantId, period = "day") {
    try {
      const breakdown = await tenantCostService.getServiceCostBreakdown(tenantId, period);
      const totalCost = breakdown.reduce((sum, item) => sum + item.cost, 0);
      const optimizationOpportunities2 = await this.costOptimizationService.identifyOptimizationOpportunities(tenantId);
      const mappedOptimizations = optimizationOpportunities2.map((opt) => {
        let serviceName = opt.serviceName || "General";
        if (serviceName === "General") {
          if (opt.id.includes("docling")) serviceName = "DoclingService";
          else if (opt.id.includes("db-pool")) serviceName = "DatabaseService";
          else if (opt.id.includes("ai-batch")) serviceName = "LLMService";
          else if (opt.type.includes("storage")) serviceName = "StorageService";
          else if (opt.type.includes("query")) serviceName = "DatabaseService";
        }
        return {
          serviceName,
          suggestion: opt.description,
          potentialSavings: opt.potentialSavingsUSD,
          priority: opt.potentialSavingsUSD > 50 ? "high" : opt.potentialSavingsUSD > 10 ? "medium" : "low"
        };
      });
      return {
        tenantId,
        period,
        totalCost,
        currency: "USD",
        serviceBreakdown: breakdown,
        optimizations: mappedOptimizations
      };
    } catch (error) {
      logger_default2.error("Failed to generate resource cost analysis", { tenantId, error: error.message });
      throw error;
    }
  }
};
var resourceCostAnalyzerService = new ResourceCostAnalyzerService();

// src/routes/resource-costs.ts
init_auth4();
import { z as z26 } from "zod/v4";
var router43 = Router23();
var CostAnalysisQuerySchema = z26.object({
  period: z26.enum(["hour", "day", "week", "month"]).optional().default("day")
});
router43.get("/analysis", ensureAuthenticated, async (req, res, next) => {
  try {
    const tenantId = req.user?.tenantId;
    if (!tenantId) {
      res.status(400).json({ error: "Tenant ID required" });
      return;
    }
    const query3 = CostAnalysisQuerySchema.parse(req.query);
    const analysis = await resourceCostAnalyzerService.getServiceCostAnalysis(
      tenantId,
      query3.period
    );
    res.json(analysis);
  } catch (error) {
    next(error);
  }
});
var resource_costs_default = router43;

// src/routes/stream.ts
import express23 from "express";

// src/lib/streaming/GraphStreamer.ts
init_redis();
init_neo4j();
import { EventEmitter as EventEmitter18 } from "events";
import { v4 as uuidv417 } from "uuid";
import { Gauge as Gauge14, Counter as Counter16 } from "prom-client";

// src/utils/compression.ts
import * as zlib2 from "zlib";
import { promisify as promisify2 } from "util";
var gzip2 = promisify2(zlib2.gzip);
var gunzip2 = promisify2(zlib2.gunzip);
var deflate2 = promisify2(zlib2.deflate);
var inflate2 = promisify2(zlib2.inflate);
var CompressionUtils = class {
  static COMPRESSION_THRESHOLD = 1024;
  // 1KB
  /**
   * Compresses input data if it exceeds the compression threshold (1KB).
   *
   * The output is a Buffer prefixed with a 1-byte marker indicating the compression state:
   * - 0: Uncompressed
   * - 1: GZIP compressed
   *
   * @param data - The data to compress. Can be a string, Buffer, or object (which will be JSON stringified).
   * @returns A Promise resolving to a Buffer containing the marker and the (potentially) compressed data.
   */
  static async compress(data) {
    let buffer;
    if (Buffer.isBuffer(data)) {
      buffer = data;
    } else if (typeof data === "string") {
      buffer = Buffer.from(data);
    } else {
      buffer = Buffer.from(JSON.stringify(data));
    }
    if (buffer.length < this.COMPRESSION_THRESHOLD) {
      const result3 = Buffer.alloc(buffer.length + 1);
      result3.writeUInt8(0, 0);
      buffer.copy(result3, 1);
      return result3;
    }
    const compressed = await gzip2(buffer);
    const result2 = Buffer.alloc(compressed.length + 1);
    result2.writeUInt8(1, 0);
    compressed.copy(result2, 1);
    return result2;
  }
  /**
   * Decompresses data based on the prefix marker found in the Buffer.
   *
   * @param data - The compressed Buffer (including the 1-byte marker).
   * @returns A Promise resolving to the original uncompressed Buffer.
   * @throws Error if the compression marker is unknown.
   */
  static async decompress(data) {
    if (data.length === 0) return Buffer.alloc(0);
    const marker = data.readUInt8(0);
    const payload = data.subarray(1);
    if (marker === 0) {
      return payload;
    } else if (marker === 1) {
      return await gunzip2(payload);
    } else {
      throw new Error(`Unknown compression marker: ${marker}`);
    }
  }
  /**
   * Compresses an object and encodes the result as a Base64 string.
   *
   * @param data - The object to compress.
   * @returns A Promise resolving to a Base64 encoded string of the compressed data.
   */
  static async compressToString(data) {
    const buffer = await this.compress(data);
    return buffer.toString("base64");
  }
  /**
   * Decodes a Base64 string, decompresses it, and parses the JSON result.
   *
   * @typeParam T - The expected type of the parsed object.
   * @param data - The Base64 encoded compressed string.
   * @returns A Promise resolving to the parsed object.
   */
  static async decompressFromString(data) {
    const buffer = Buffer.from(data, "base64");
    const decompressed = await this.decompress(buffer);
    return JSON.parse(decompressed.toString());
  }
};

// src/lib/streaming/GraphStreamer.ts
init_logger();
var GraphStreamer = class extends EventEmitter18 {
  defaultBatchSize = 1e3;
  streamTimeoutMs = 3e5;
  // 5 minutes
  activeStreams = /* @__PURE__ */ new Map();
  streamTimers = /* @__PURE__ */ new Map();
  // Metrics
  activeStreamsGauge = new Gauge14({
    name: "graph_active_streams",
    help: "Number of active graph streams"
  });
  streamedRecordsCounter = new Counter16({
    name: "graph_streamed_records_total",
    help: "Total number of records streamed"
  });
  constructor() {
    super();
  }
  async startStream(query3, params, config9 = {}) {
    const streamId = uuidv417();
    this.activeStreams.set(streamId, true);
    this.activeStreamsGauge.inc();
    const timer3 = setTimeout(() => {
      logger.warn(`Stream ${streamId} timed out, forcing cleanup`);
      this.stopStream(streamId);
    }, this.streamTimeoutMs);
    this.streamTimers.set(streamId, timer3);
    const redis5 = getRedisClient2();
    await redis5.set(
      `stream_pending:${streamId}`,
      JSON.stringify({ query: query3, params, config: config9 }),
      "EX",
      300
    );
    return streamId;
  }
  async executeStream(streamId) {
    const redis5 = getRedisClient2();
    const pendingData = await redis5.get(`stream_pending:${streamId}`);
    if (!pendingData) return;
    await redis5.del(`stream_pending:${streamId}`);
    const { query: query3, params, config: config9 } = JSON.parse(pendingData);
    const batchSize = config9.batchSize || this.defaultBatchSize;
    const streamParams = { ...params, _skipCache: true };
    try {
      await this.processStream(streamId, query3, streamParams, batchSize, config9);
    } catch (err) {
      logger.error(`Stream ${streamId} failed:`, err);
      this.emit(`error:${streamId}`, err);
      this.cleanup(streamId);
    }
  }
  streamSessions = /* @__PURE__ */ new Map();
  async processStream(streamId, query3, params, batchSize, config9) {
    const driver3 = getNeo4jDriver();
    const session = driver3.session();
    const redis5 = getRedisClient2();
    this.streamSessions.set(streamId, session);
    try {
      const result2 = session.run(query3, params);
      let batch = [];
      let count = 0;
      await new Promise((resolve2, reject) => {
        result2.subscribe({
          onNext: (record2) => {
            if (!this.activeStreams.get(streamId)) {
              return;
            }
            batch.push(record2.toObject());
            count++;
            this.streamedRecordsCounter.inc();
            if (batch.length >= batchSize) {
              this.emitBatch(redis5, streamId, batch, config9).catch((err) => {
                logger.error(
                  `Failed to emit batch for stream ${streamId}`,
                  err
                );
                this.stopStream(streamId);
                reject(err);
              });
              batch = [];
            }
          },
          onCompleted: () => {
            if (batch.length > 0) {
              this.emitBatch(redis5, streamId, batch, config9).catch(
                (err) => logger.error(
                  `Failed to emit final batch for stream ${streamId}`,
                  err
                )
              );
            }
            this.emitComplete(redis5, streamId, count);
            resolve2();
          },
          onError: (error) => {
            this.emitError(redis5, streamId, error);
            reject(error);
          }
        });
      });
    } catch (error) {
      throw error;
    } finally {
      this.cleanup(streamId);
    }
  }
  async emitBatch(redis5, streamId, batch, config9) {
    const channel = `stream:${streamId}`;
    const normalizedBatch = transformNeo4jIntegers(batch);
    let payload = { type: "batch", data: normalizedBatch };
    if (config9.compress) {
      try {
        const compressedData = await CompressionUtils.compressToString(normalizedBatch);
        payload = { type: "batch", data: compressedData, compressed: true };
      } catch (e) {
        logger.error("Streaming compression failed", e);
      }
    }
    const data = JSON.stringify(payload);
    await redis5.publish(channel, data);
    this.emit(`data:${streamId}`, normalizedBatch);
  }
  emitComplete(redis5, streamId, totalRecords) {
    const channel = `stream:${streamId}`;
    const data = JSON.stringify({ type: "complete", totalRecords });
    redis5.publish(channel, data).catch((err) => logger.error("Redis publish error", err));
    this.emit(`complete:${streamId}`, { totalRecords });
  }
  emitError(redis5, streamId, error) {
    const channel = `stream:${streamId}`;
    const data = JSON.stringify({ type: "error", message: error.message });
    redis5.publish(channel, data).catch((err) => logger.error("Redis publish error", err));
    this.emit(`error:${streamId}`, error);
  }
  stopStream(streamId) {
    this.activeStreams.set(streamId, false);
    this.cleanup(streamId);
  }
  cleanup(streamId) {
    if (this.streamTimers.has(streamId)) {
      clearTimeout(this.streamTimers.get(streamId));
      this.streamTimers.delete(streamId);
    }
    if (this.activeStreams.has(streamId)) {
      this.activeStreamsGauge.dec();
      this.activeStreams.delete(streamId);
    }
    const session = this.streamSessions.get(streamId);
    if (session) {
      session.close().catch(() => {
      });
      this.streamSessions.delete(streamId);
    }
  }
};
var graphStreamer = new GraphStreamer();

// src/routes/stream.ts
init_redis();
var router44 = express23.Router();
var singleParam6 = (value) => Array.isArray(value) ? value[0] : value;
router44.post("/start", async (req, res) => {
  try {
    const { query: query3, params, config: config9 } = req.body;
    const streamId = await graphStreamer.startStream(query3, params, config9);
    res.json({ streamId, streamUrl: `/api/stream/${streamId}` });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router44.get("/:streamId", async (req, res) => {
  const streamId = singleParam6(req.params.streamId) ?? "";
  const redis5 = getRedisClient2();
  const channel = `stream:${streamId}`;
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");
  const sub2 = redis5.duplicate();
  await sub2.subscribe(channel);
  sub2.on("message", (chan, message) => {
    if (chan === channel) {
      res.write(`data: ${message}

`);
      const parsed = JSON.parse(message);
      if (parsed.type === "complete" || parsed.type === "error") {
        sub2.unsubscribe();
        sub2.quit();
        res.end();
      }
    }
  });
  req.on("close", () => {
    sub2.unsubscribe();
    sub2.quit();
    graphStreamer.stopStream(streamId);
  });
  graphStreamer.executeStream(streamId);
});
var stream_default = router44;

// src/routes/query-preview-stream.ts
import express24 from "express";

// src/services/QueryPreviewService.ts
init_logger2();
init_metrics4();
import { v4 as uuidv418 } from "uuid";

// src/services/queryResultCache.ts
import crypto29 from "crypto";
init_logger2();
var DEFAULT_CONFIG3 = {
  ttlSeconds: 600,
  streamingTtlSeconds: 30,
  maxEntries: 500,
  streamingMaxEntries: 250,
  partialLimit: 25
};
var QueryResultCache = class {
  l1 = /* @__PURE__ */ new Map();
  streamingCache = /* @__PURE__ */ new Map();
  config;
  redis;
  prefix = "ig:query-cache:";
  streamingPrefix = "ig:query-stream:";
  stats = {
    result: { hits: 0, misses: 0 },
    streaming: { hits: 0, misses: 0 }
  };
  constructor(redis5, config9 = {}) {
    this.redis = redis5;
    this.config = { ...DEFAULT_CONFIG3, ...config9 };
  }
  buildSignature(language, query3, parameters = {}) {
    const normalisedParams = this.normaliseParams(parameters);
    const paramHash = crypto29.createHash("sha256").update(normalisedParams).digest("hex");
    const queryHash = crypto29.createHash("sha256").update(query3.trim()).digest("hex");
    return `${language}:${queryHash}:${paramHash}`;
  }
  getPartialLimit() {
    return this.config.partialLimit;
  }
  getHitRate(operation = "result") {
    const { hits, misses } = this.stats[operation];
    const total = hits + misses;
    return total === 0 ? 0 : hits / total;
  }
  getStats() {
    return {
      result: { ...this.stats.result, hitRate: this.getHitRate("result") },
      streaming: { ...this.stats.streaming, hitRate: this.getHitRate("streaming") },
      l1Entries: this.l1.size,
      streamingEntries: this.streamingCache.size
    };
  }
  async getResult(signature, tenantId) {
    const now = Date.now();
    const started = now;
    const l1Entry = this.l1.get(signature);
    if (l1Entry && l1Entry.expiresAt > now) {
      l1Entry.accessCount++;
      l1Entry.lastAccessedAt = now;
      this.recordHit("result", tenantId);
      this.recordLatency("result", "hit", tenantId, started);
      return { ...l1Entry.value, tier: "ram" };
    }
    if (l1Entry) {
      this.l1.delete(signature);
      cacheLocalSize?.labels?.("query-signature:result")?.set?.(this.l1.size);
    }
    if (!this.redis) {
      this.recordMiss("result", tenantId);
      this.recordLatency("result", "miss", tenantId, started);
      return void 0;
    }
    const redisValue = await this.redis.get(this.prefix + signature);
    if (!redisValue) {
      this.recordMiss("result", tenantId);
      this.recordLatency("result", "miss", tenantId, started);
      return void 0;
    }
    try {
      const parsed = JSON.parse(redisValue);
      this.setL1(signature, parsed, this.config.ttlSeconds);
      this.recordHit("result", tenantId);
      this.recordLatency("result", "hit", tenantId, started);
      return { ...parsed, tier: "flash" };
    } catch (error) {
      logger2.warn({ error }, "Failed to parse flash cache entry, treating as miss");
      this.recordMiss("result", tenantId);
      this.recordLatency("result", "miss", tenantId, started);
      return void 0;
    }
  }
  async setResult(signature, value, tenantId) {
    this.setL1(signature, value, this.config.ttlSeconds);
    cacheLocalSize?.labels?.("query-signature:result")?.set?.(this.l1.size);
    recSet("query-signature", "result", tenantId);
    if (!this.redis) return;
    try {
      await this.redis.setex(
        this.prefix + signature,
        this.config.ttlSeconds,
        JSON.stringify(value)
      );
      recSet("query-signature", "result-flash", tenantId);
    } catch (error) {
      logger2.warn({ error }, "Unable to write query cache entry to flash tier");
    }
  }
  async setStreamingPartial(signature, rows, tenantId) {
    const trimmed = rows.slice(0, this.config.partialLimit);
    this.setStreamingL1(signature, trimmed, this.config.streamingTtlSeconds);
    cacheLocalSize?.labels?.("query-signature:streaming")?.set?.(this.streamingCache.size);
    recSet("query-signature", "streaming", tenantId);
    if (!this.redis) return;
    try {
      await this.redis.setex(
        this.streamingPrefix + signature,
        this.config.streamingTtlSeconds,
        JSON.stringify(trimmed)
      );
      recSet("query-signature", "streaming-flash", tenantId);
    } catch (error) {
      logger2.warn({ error }, "Unable to write streaming cache entry to flash tier");
    }
  }
  async getStreamingPartial(signature, tenantId) {
    const now = Date.now();
    const started = now;
    const l1Entry = this.streamingCache.get(signature);
    if (l1Entry && l1Entry.expiresAt > now) {
      l1Entry.accessCount++;
      l1Entry.lastAccessedAt = now;
      this.recordHit("streaming", tenantId);
      this.recordLatency("streaming", "hit", tenantId, started);
      return { rows: l1Entry.value, tier: "ram" };
    }
    if (l1Entry) {
      this.streamingCache.delete(signature);
      cacheLocalSize?.labels?.("query-signature:streaming")?.set?.(this.streamingCache.size);
    }
    if (!this.redis) {
      this.recordMiss("streaming", tenantId);
      this.recordLatency("streaming", "miss", tenantId, started);
      return void 0;
    }
    const redisValue = await this.redis.get(this.streamingPrefix + signature);
    if (!redisValue) {
      this.recordMiss("streaming", tenantId);
      this.recordLatency("streaming", "miss", tenantId, started);
      return void 0;
    }
    try {
      const parsed = JSON.parse(redisValue);
      this.setStreamingL1(signature, parsed, this.config.streamingTtlSeconds);
      this.recordHit("streaming", tenantId);
      this.recordLatency("streaming", "hit", tenantId, started);
      return { rows: parsed, tier: "flash" };
    } catch (error) {
      logger2.warn({ error }, "Failed to parse streaming cache entry, treating as miss");
      this.recordMiss("streaming", tenantId);
      this.recordLatency("streaming", "miss", tenantId, started);
      return void 0;
    }
  }
  async readThrough(signature, tenantId, loader, options2 = {}) {
    const cached = await this.getResult(signature, tenantId);
    if (cached) {
      return { payload: cached, fromCache: true, tier: cached.tier };
    }
    const payload = await loader();
    await this.setResult(signature, payload, tenantId);
    if (options2.primeStreaming !== false) {
      await this.setStreamingPartial(signature, payload.rows, tenantId);
    }
    return { payload, fromCache: false };
  }
  setL1(signature, value, ttlSeconds) {
    this.ensureL1Capacity(this.l1, "result");
    this.l1.set(signature, {
      value,
      expiresAt: Date.now() + ttlSeconds * 1e3,
      accessCount: 1,
      lastAccessedAt: Date.now()
    });
  }
  setStreamingL1(signature, value, ttlSeconds) {
    this.ensureL1Capacity(this.streamingCache, "streaming");
    this.streamingCache.set(signature, {
      value,
      expiresAt: Date.now() + ttlSeconds * 1e3,
      accessCount: 1,
      lastAccessedAt: Date.now()
    });
  }
  ensureL1Capacity(map, operation) {
    const maxEntries = operation === "streaming" ? this.config.streamingMaxEntries : this.config.maxEntries;
    if (map.size < maxEntries) return;
    let candidate = null;
    for (const [key, entry] of map.entries()) {
      if (!candidate || entry.accessCount < candidate.entry.accessCount || entry.accessCount === candidate.entry.accessCount && entry.lastAccessedAt < candidate.entry.lastAccessedAt) {
        candidate = { key, entry };
      }
    }
    if (candidate) {
      map.delete(candidate.key);
      recEviction("query-signature", `lfu-${operation}`);
      cacheLocalSize?.labels?.(`query-signature:${operation}`)?.set?.(map.size);
    }
  }
  normaliseParams(params) {
    const stabilise = (value) => {
      if (Array.isArray(value)) {
        return value.map(stabilise);
      }
      if (value && typeof value === "object") {
        const entries = Object.entries(value).sort(
          ([a], [b]) => a.localeCompare(b)
        );
        return entries.reduce((acc, [key, val]) => {
          acc[key] = stabilise(val);
          return acc;
        }, {});
      }
      return value;
    };
    return JSON.stringify(stabilise(params));
  }
  recordHit(operation, tenantId) {
    this.stats[operation].hits++;
    recHit("query-signature", operation, tenantId);
    setHitRatio(
      "query-signature",
      operation,
      this.stats[operation].hits,
      this.stats[operation].misses
    );
  }
  recordMiss(operation, tenantId) {
    this.stats[operation].misses++;
    recMiss("query-signature", operation, tenantId);
    setHitRatio(
      "query-signature",
      operation,
      this.stats[operation].hits,
      this.stats[operation].misses
    );
  }
  recordLatency(operation, result2, tenantId, startedAt2) {
    const durationSeconds = (Date.now() - startedAt2) / 1e3;
    cacheLatencySeconds?.labels?.(operation, result2, tenantId ?? "unknown")?.observe?.(durationSeconds);
  }
};

// src/services/pagination.ts
function wrapCypherWithPagination(query3) {
  const trimmed = query3.trim().replace(/;+\s*$/, "");
  return `
    CALL {
      ${trimmed}
    }
    RETURN *
    SKIP toInteger($skip)
    LIMIT toInteger($limitPlusOne)
  `;
}

// src/services/graphTenantScope.ts
init_logger2();
var tenantFilterPattern = /(tenantId|tenant_id)\b/i;
function hasTenantFilter(query3) {
  return tenantFilterPattern.test(query3);
}
function injectTenantFilter(query3) {
  if (hasTenantFilter(query3)) {
    return query3;
  }
  const match = query3.match(/MATCH\s+\((\w+)/i);
  if (!match) {
    throw new Error("Tenant scope injection failed: no MATCH clause found");
  }
  const varName = match[1];
  if (/\bWHERE\b/i.test(query3)) {
    return query3.replace(/\bWHERE\b/i, `WHERE ${varName}.tenantId = $tenantId AND `);
  }
  return query3.replace(/MATCH\s+\((\w+)/i, `MATCH ($1) WHERE $1.tenantId = $tenantId`);
}
async function emitTenantScopeDenial(params) {
  const tenantId = params.tenantId || "unknown";
  const actorId = params.actorId || "system";
  const action = params.action || "graph.query";
  const resource = params.resource || "graph.query";
  try {
    const receiptService2 = ReceiptService.getInstance();
    await receiptService2.generateReceipt({
      action: "GRAPH_TENANT_SCOPE_DENIED",
      actor: { id: actorId, tenantId },
      resource,
      input: {
        reason: params.reason,
        action,
        resource,
        tenantId,
        ...params.input ?? {}
      },
      policyDecisionId: params.reason
    });
  } catch (error) {
    logger2.error({ error, tenantId, action, resource }, "Failed to emit tenant scope denial receipt");
  }
}
async function enforceTenantScopeForCypher(cypher, params = {}, options2 = {}) {
  const scopedTenantId = options2.tenantId ?? params.tenantId;
  if (!scopedTenantId || scopedTenantId.trim().length === 0) {
    await emitTenantScopeDenial({
      tenantId: options2.tenantId,
      actorId: options2.actorId,
      action: options2.action,
      resource: options2.resource,
      reason: "tenant_scope_missing",
      input: { cypher }
    });
    throw new Error("Tenant scope required for graph query");
  }
  if (params.tenantId && params.tenantId !== scopedTenantId) {
    await emitTenantScopeDenial({
      tenantId: options2.tenantId ?? scopedTenantId,
      actorId: options2.actorId,
      action: options2.action,
      resource: options2.resource,
      reason: "tenant_scope_mismatch",
      input: { cypher, paramsTenantId: params.tenantId }
    });
    throw new Error("Tenant scope mismatch for graph query");
  }
  let scopedCypher;
  try {
    scopedCypher = injectTenantFilter(cypher);
  } catch (error) {
    await emitTenantScopeDenial({
      tenantId: scopedTenantId,
      actorId: options2.actorId,
      action: options2.action,
      resource: options2.resource,
      reason: "tenant_scope_injection_failed",
      input: { cypher }
    });
    throw error;
  }
  return {
    cypher: scopedCypher,
    params: {
      ...params,
      tenantId: scopedTenantId
    }
  };
}

// src/services/QueryPreviewService.ts
var QueryPreviewService = class {
  pool;
  neo4jDriver;
  redis;
  nlToCypherService;
  glassBoxService;
  cacheEnabled;
  cacheTTL = 600;
  // 10 minutes
  previewTTL = 3600;
  // 1 hour
  queryCache;
  activeStreams = /* @__PURE__ */ new Set();
  constructor(pool4, neo4jDriver2, nlToCypherService, glassBoxService, redis5) {
    this.pool = pool4;
    this.neo4jDriver = neo4jDriver2;
    this.nlToCypherService = nlToCypherService;
    this.glassBoxService = glassBoxService;
    this.redis = redis5 || null;
    this.cacheEnabled = !!redis5;
    this.queryCache = new QueryResultCache(this.redis, {
      ttlSeconds: this.cacheTTL,
      streamingTtlSeconds: 20,
      maxEntries: 2e3,
      partialLimit: 20
    });
  }
  /**
   * Create a query preview from natural language
   */
  async createPreview(input) {
    const startTime = Date.now();
    logger2.info({
      investigationId: input.investigationId,
      query: input.naturalLanguageQuery,
      language: input.language || "cypher"
    }, "Creating query preview");
    try {
      const language = input.language || "cypher";
      let preview;
      if (language === "cypher") {
        preview = await this.createCypherPreview(input);
      } else {
        preview = await this.createSqlPreview(input);
      }
      await this.storePreview(preview);
      metrics3.queryPreviewsTotal.inc({ language, status: "created" });
      metrics3.queryPreviewLatencyMs.observe(
        { language },
        Date.now() - startTime
      );
      logger2.info({
        previewId: preview.id,
        investigationId: input.investigationId,
        language,
        canExecute: preview.canExecute,
        costLevel: preview.costEstimate.level,
        riskLevel: preview.riskAssessment.level,
        durationMs: Date.now() - startTime
      }, "Created query preview");
      return preview;
    } catch (error) {
      logger2.error({ error, input }, "Failed to create query preview");
      metrics3.queryPreviewErrorsTotal.inc({ language: input.language || "cypher" });
      throw error;
    }
  }
  /**
   * Create Cypher query preview
   */
  async createCypherPreview(input) {
    const schemaContext = await this.buildInvestigationSchema(input.investigationId);
    const translationResult = await this.nlToCypherService.translateWithPreview(
      input.naturalLanguageQuery,
      input.userId,
      input.tenantId
    );
    const scopedQuery = this.scopeCypherToInvestigation(
      translationResult.generatedCypher,
      input.investigationId,
      input.focusEntityIds
    );
    const costEstimate = this.estimateCypherCost(scopedQuery);
    const riskAssessment = translationResult.policyRisk ? {
      level: translationResult.policyRisk.riskLevel,
      concerns: translationResult.policyRisk.risks || [],
      piiFields: [],
      // Not returned by service yet
      mutationDetected: translationResult.policyRisk.sensitiveOperations.length > 0,
      recommendedActions: []
      // Not returned by service yet
    } : this.assessCypherRisk(scopedQuery);
    const canExecute = translationResult.canExecute && costEstimate.level !== "very-high";
    const requiresApproval = costEstimate.level === "high" || riskAssessment.level === "high";
    const sandboxOnly = requiresApproval || costEstimate.level === "high";
    const preview = {
      id: uuidv418(),
      investigationId: input.investigationId,
      tenantId: input.tenantId,
      userId: input.userId,
      naturalLanguageQuery: input.naturalLanguageQuery,
      parameters: {
        ...input.parameters,
        tenantId: input.tenantId,
        investigationId: input.investigationId,
        focusEntityIds: input.focusEntityIds,
        maxHops: input.maxHops
      },
      language: "cypher",
      generatedQuery: scopedQuery,
      queryExplanation: this.explainCypher(scopedQuery),
      // Service doesn't provide explanation yet
      costEstimate,
      riskAssessment,
      syntacticallyValid: translationResult.validation.isValid !== false,
      validationErrors: translationResult.validation.syntaxErrors || [],
      canExecute,
      requiresApproval,
      sandboxOnly,
      modelUsed: "nl-to-cypher-v1",
      confidence: 0.85,
      // Service doesn't provide confidence yet
      generatedAt: /* @__PURE__ */ new Date(),
      expiresAt: new Date(Date.now() + this.previewTTL * 1e3),
      executed: false,
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date()
    };
    return preview;
  }
  /**
   * Create SQL query preview
   */
  async createSqlPreview(input) {
    const query3 = `
      SELECT e.id, e.kind, e.labels, e.props
      FROM entities e
      WHERE e.tenant_id = '${input.tenantId}'
      AND e.props @> '{"investigationId": "${input.investigationId}"}'
      LIMIT 100
    `;
    const preview = {
      id: uuidv418(),
      investigationId: input.investigationId,
      tenantId: input.tenantId,
      userId: input.userId,
      naturalLanguageQuery: input.naturalLanguageQuery,
      parameters: input.parameters || {},
      language: "sql",
      generatedQuery: query3,
      queryExplanation: "Retrieves entities for the investigation",
      costEstimate: {
        level: "low",
        breakdown: {
          cartesianProduct: false,
          variableLengthPath: false,
          fullTableScan: false,
          complexAggregation: false
        },
        warnings: []
      },
      riskAssessment: {
        level: "low",
        concerns: [],
        piiFields: [],
        mutationDetected: false,
        recommendedActions: []
      },
      syntacticallyValid: true,
      validationErrors: [],
      canExecute: true,
      requiresApproval: false,
      sandboxOnly: false,
      modelUsed: "nl-to-sql-v1",
      confidence: 0.75,
      generatedAt: /* @__PURE__ */ new Date(),
      expiresAt: new Date(Date.now() + this.previewTTL * 1e3),
      executed: false,
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date()
    };
    return preview;
  }
  /**
   * Get a preview by ID
   */
  async getPreview(previewId) {
    const query3 = `
      SELECT * FROM query_previews
      WHERE id = $1
    `;
    const result2 = await this.pool.query(query3, [previewId]);
    if (result2.rows.length === 0) {
      return null;
    }
    return this.rowToPreview(result2.rows[0]);
  }
  /**
   * Edit a preview query
   */
  async editPreview(previewId, userId, editedQuery) {
    const preview = await this.getPreview(previewId);
    if (!preview) {
      throw new Error(`Preview ${previewId} not found`);
    }
    const validation = this.validateQuery(editedQuery, preview.language);
    const query3 = `
      UPDATE query_previews
      SET edited_query = $1,
          edited_by = $2,
          edited_at = $3,
          syntactically_valid = $4,
          validation_errors = $5,
          updated_at = $6
      WHERE id = $7
      RETURNING *
    `;
    const result2 = await this.pool.query(query3, [
      editedQuery,
      userId,
      /* @__PURE__ */ new Date(),
      validation.isValid,
      JSON.stringify(validation.errors),
      /* @__PURE__ */ new Date(),
      previewId
    ]);
    logger2.info({
      previewId,
      userId,
      editedQueryLength: editedQuery.length,
      isValid: validation.isValid
    }, "Edited preview query");
    return this.rowToPreview(result2.rows[0]);
  }
  /**
   * Execute a preview in sandbox mode
   */
  async executePreview(input) {
    const startTime = Date.now();
    const preview = await this.getPreview(input.previewId);
    if (!preview) {
      throw new Error(`Preview ${input.previewId} not found`);
    }
    if (!preview.canExecute && !input.dryRun) {
      throw new Error("Preview cannot be executed due to policy restrictions");
    }
    const queryToExecute = input.useEditedQuery && preview.editedQuery ? preview.editedQuery : preview.generatedQuery;
    const signature = this.buildSignature(
      preview,
      Boolean(input.useEditedQuery && preview.editedQuery)
    );
    const initialCursor = this.decodeCursor(input.cursor);
    const pageSize = Math.max(1, input.batchSize ?? input.maxRows ?? 100);
    const streamTopic = input.stream ? preview.id : void 0;
    const run = await this.glassBoxService.createRun({
      investigationId: preview.investigationId,
      tenantId: preview.tenantId,
      userId: input.userId,
      type: preview.language === "cypher" ? "nl_to_cypher" : "nl_to_sql",
      prompt: preview.naturalLanguageQuery,
      parameters: {
        ...preview.parameters,
        previewId: preview.id,
        editedQuery: input.useEditedQuery,
        dryRun: input.dryRun
      }
    });
    try {
      await this.glassBoxService.updateStatus(run.id, "running");
      const stepId = uuidv418();
      await this.glassBoxService.addStep(run.id, {
        type: "tool_call",
        description: `Executing ${preview.language} query`,
        input: { query: queryToExecute, dryRun: input.dryRun }
      });
      const warnings = [];
      let results = [];
      let rowCount = 0;
      let cached = false;
      let cacheTier;
      let nextCursor;
      let hasMore = false;
      let streamedBatches = 0;
      const executionStartedAt = Date.now();
      let payload;
      const signature2 = this.queryCache.buildSignature(
        preview.language,
        queryToExecute,
        preview.parameters
      );
      const streamingCached = await this.queryCache.getStreamingPartial(
        signature2,
        preview.tenantId
      );
      if (streamingCached) {
        warnings.push("Using streaming cache while full query executes");
      }
      if (input.dryRun) {
        results = [{ message: "Dry run - query validated but not executed" }];
        rowCount = results.length;
        warnings.push("Dry run mode - no data was accessed");
        payload = {
          rows: results,
          warnings: [...warnings],
          executionTimeMs: Date.now() - executionStartedAt
        };
      } else {
        const { payload: cachedOrLoaded, fromCache, tier } = await this.queryCache.readThrough(
          signature2,
          preview.tenantId,
          async () => {
            if (preview.language === "cypher") {
              const execResult2 = await this.executeCypher(
                queryToExecute,
                preview.parameters,
                {
                  maxRows: input.maxRows || 100,
                  timeout: input.timeout || 3e4,
                  tenantId: preview.tenantId,
                  actorId: input.userId
                }
              );
              return {
                rows: execResult2.records,
                warnings: execResult2.warnings,
                executionTimeMs: Date.now() - executionStartedAt
              };
            }
            const execResult = await this.executeSql(
              queryToExecute,
              {
                maxRows: input.maxRows || 100,
                timeout: input.timeout || 3e4
              }
            );
            return {
              rows: execResult.rows,
              warnings: execResult.warnings,
              executionTimeMs: Date.now() - executionStartedAt
            };
          },
          { primeStreaming: true }
        );
        cached = fromCache;
        cacheTier = tier;
        payload = cachedOrLoaded;
        results = cachedOrLoaded.rows;
        rowCount = cachedOrLoaded.rows.length;
        warnings.push(...cachedOrLoaded.warnings);
        if (fromCache) {
          warnings.push("Served from read-through cache");
        }
      }
      const executionTimeMs = cached ? payload?.executionTimeMs ?? Date.now() - startTime : Date.now() - executionStartedAt;
      const hitRate = this.queryCache.getHitRate("result");
      if (!cached && hitRate < 0.3) {
        logger2.warn(
          { signature: signature2, hitRate },
          "Query cache hit rate below target during load validation"
        );
      }
      await this.glassBoxService.completeStep(run.id, stepId, {
        rowCount,
        executionTimeMs,
        warnings
      });
      await this.pool.query(
        `UPDATE query_previews
         SET executed = true, executed_at = $1, execution_run_id = $2, updated_at = $3
         WHERE id = $4`,
        [/* @__PURE__ */ new Date(), run.id, /* @__PURE__ */ new Date(), preview.id]
      );
      await this.glassBoxService.updateStatus(run.id, "completed", {
        rowCount,
        warnings
      });
      metrics3.queryPreviewExecutionsTotal.inc({
        language: preview.language,
        dryRun: input.dryRun ? "true" : "false",
        status: "success"
      });
      logger2.info({
        previewId: preview.id,
        runId: run.id,
        language: preview.language,
        rowCount,
        executionTimeMs,
        dryRun: input.dryRun
      }, "Executed query preview");
      if (streamTopic) {
        this.previewStreamHub.publish(preview.id, {
          previewId: preview.id,
          batch: [],
          cursor: this.encodeCursor(nextCursor ?? initialCursor),
          nextCursor: nextCursor ? this.encodeCursor(nextCursor) : null,
          complete: true,
          warnings
        });
        this.activeStreams.delete(preview.id);
      }
      return {
        runId: run.id,
        query: queryToExecute,
        results,
        rowCount,
        executionTimeMs,
        warnings,
        cached,
        cacheTier,
        partialResults: streamingCached?.rows ?? results.slice(0, this.queryCache.getPartialLimit()),
        partialCacheHit: Boolean(streamingCached),
        signature: signature2,
        nextCursor: nextCursor !== void 0 ? this.encodeCursor(nextCursor) : null,
        hasMore,
        streamingChannel: streamTopic,
        streamedBatches: streamTopic ? streamedBatches : void 0
      };
    } catch (error) {
      await this.glassBoxService.updateStatus(run.id, "failed", void 0, String(error));
      metrics3.queryPreviewExecutionsTotal.inc({
        language: preview.language,
        dryRun: input.dryRun ? "true" : "false",
        status: "failed"
      });
      logger2.error({
        error,
        previewId: preview.id,
        runId: run.id
      }, "Failed to execute query preview");
      throw error;
    } finally {
      if (streamTopic) {
        this.activeStreams.delete(preview.id);
      }
    }
  }
  /**
   * Scope Cypher query to investigation
   */
  scopeCypherToInvestigation(query3, investigationId, focusEntityIds) {
    let scopedQuery = query3;
    if (!query3.includes("investigationId")) {
      const matchRegex = /MATCH\s+\(([\w]+):Entity[^)]*\)/gi;
      scopedQuery = query3.replace(
        matchRegex,
        (match, varName) => {
          return `${match} WHERE ${varName}.investigationId = $investigationId`;
        }
      );
    }
    if (focusEntityIds && focusEntityIds.length > 0) {
      scopedQuery = `
        // Scoped to investigation: ${investigationId}
        // Focus entities: ${focusEntityIds.join(", ")}
        ${scopedQuery}
      `;
    }
    return scopedQuery;
  }
  /**
   * Build schema context for investigation
   */
  async buildInvestigationSchema(investigationId) {
    const session = this.neo4jDriver.session();
    try {
      const result2 = await session.run(
        `
        MATCH (e:Entity {investigationId: $investigationId})
        WITH DISTINCT labels(e) as entityLabels
        UNWIND entityLabels as label
        RETURN DISTINCT label
        UNION
        MATCH ()-[r]->()
        WHERE r.investigationId = $investigationId
        RETURN DISTINCT type(r) as label
        `,
        { investigationId }
      );
      const labels2 = result2.records.map((r) => r.get("label"));
      return `Investigation graph schema:
Node labels: ${labels2.filter((l) => l !== "Entity").join(", ")}
Relationship types: ${labels2.filter((l) => !l.match(/^[A-Z]/)).join(", ")}`;
    } finally {
      await session.close();
    }
  }
  /**
   * Estimate Cypher query cost
   */
  estimateCypherCost(query3) {
    const breakdown = {
      cartesianProduct: /MATCH.*MATCH/i.test(query3) && !/WHERE/i.test(query3),
      variableLengthPath: /\[\*\d*\.\.?\d*\]/.test(query3),
      fullTableScan: /MATCH\s+\([^)]*\)\s+(?!WHERE)/i.test(query3),
      complexAggregation: /\b(collect|reduce|count)\b/i.test(query3)
    };
    const warnings = [];
    let level = "low";
    if (breakdown.cartesianProduct) {
      warnings.push("Potential cartesian product detected - add WHERE clause");
      level = "very-high";
    }
    if (breakdown.variableLengthPath) {
      warnings.push("Variable-length path can be expensive for large graphs");
      if (level === "low") level = "high";
    }
    if (breakdown.fullTableScan) {
      warnings.push("Full node scan detected - consider adding filters");
      if (level === "low") level = "medium";
    }
    if (!query3.includes("LIMIT")) {
      warnings.push("No LIMIT clause - results may be large");
      if (level === "low") level = "medium";
    }
    return {
      level,
      breakdown,
      warnings
    };
  }
  /**
   * Assess Cypher query risk
   */
  assessCypherRisk(query3) {
    const concerns = [];
    const piiFields = [];
    const mutationDetected = /\b(CREATE|DELETE|REMOVE|SET|MERGE)\b/i.test(query3);
    if (mutationDetected) {
      concerns.push("Query contains mutation operations");
    }
    const piiPattern = /\b(email|ssn|phone|address|dob|credit_card)\b/i;
    if (piiPattern.test(query3)) {
      const matches2 = query3.match(piiPattern);
      if (matches2) {
        piiFields.push(...matches2);
        concerns.push("Query accesses potential PII fields");
      }
    }
    const level = mutationDetected || piiFields.length > 0 ? "high" : "low";
    const recommendedActions = [];
    if (mutationDetected) {
      recommendedActions.push("Review mutation operations before execution");
    }
    if (piiFields.length > 0) {
      recommendedActions.push("Ensure proper authorization for PII access");
    }
    return {
      level,
      concerns,
      piiFields,
      mutationDetected,
      recommendedActions
    };
  }
  /**
   * Explain Cypher query
   */
  explainCypher(query3) {
    const operations = [];
    if (/MATCH/i.test(query3)) operations.push("matches graph patterns");
    if (/WHERE/i.test(query3)) operations.push("filters results");
    if (/RETURN/i.test(query3)) operations.push("returns specified fields");
    if (/ORDER BY/i.test(query3)) operations.push("sorts results");
    if (/LIMIT/i.test(query3)) operations.push("limits result count");
    return `This query ${operations.join(", ")}.`;
  }
  /**
   * Validate query syntax
   */
  validateQuery(query3, language) {
    const errors = [];
    if (!query3 || query3.trim().length === 0) {
      errors.push("Query is empty");
      return { isValid: false, errors };
    }
    if (language === "cypher") {
      if (!/\bRETURN\b/i.test(query3)) {
        errors.push("Cypher query must contain RETURN clause");
      }
      if (/\b(DELETE|REMOVE)\b/i.test(query3) && !/\bDETACH\b/i.test(query3)) {
        errors.push("DELETE operations should use DETACH DELETE");
      }
    } else if (language === "sql") {
      if (!/\bSELECT\b/i.test(query3)) {
        errors.push("SQL query must contain SELECT clause");
      }
    }
    return {
      isValid: errors.length === 0,
      errors
    };
  }
  /**
   * Execute Cypher query
   */
  async executeCypher(query3, parameters, options2) {
    const session = this.neo4jDriver.session();
    const warnings = [];
    const skip = options2.cursor ?? 0;
    const limit = Math.min(options2.batchSize ?? options2.maxRows, options2.maxRows);
    try {
      const scoped = await enforceTenantScopeForCypher(query3, parameters, {
        tenantId: options2.tenantId,
        actorId: options2.actorId,
        action: "graph.read",
        resource: "graph.query.preview"
      });
      const finalQuery = wrapCypherWithPagination(scoped.cypher);
      const result2 = await session.run(finalQuery, {
        ...scoped.params,
        skip,
        limitPlusOne: limit + 1
      }, {
        timeout: options2.timeout
      });
      const mapped = result2.records.map((record2) => record2.toObject());
      const hasMore = mapped.length > limit;
      const records = hasMore ? mapped.slice(0, limit) : mapped;
      const nextCursor = hasMore ? skip + limit : void 0;
      if (hasMore) {
        warnings.push("Result set limited - more rows may be available");
      } else {
        warnings.push("Pagination applied with server-side LIMIT/SKIP");
      }
      if (options2.streamObserver) {
        options2.streamObserver({
          batch: records,
          nextCursor,
          warnings
        });
      }
      return { records, rows: records, warnings, nextCursor, hasMore };
    } finally {
      await session.close();
    }
  }
  /**
   * Execute SQL query
   */
  async executeSql(query3, options2) {
    const warnings = [];
    const offset = options2.cursor ?? 0;
    const limit = Math.min(options2.batchSize ?? options2.maxRows, options2.maxRows);
    const finalQuery = this.wrapSqlWithPagination(query3);
    const result2 = await this.pool.query(finalQuery, [offset, limit + 1]);
    const hasMore = result2.rows.length > limit;
    const rows = hasMore ? result2.rows.slice(0, limit) : result2.rows;
    const nextCursor = hasMore ? offset + limit : void 0;
    if (hasMore) {
      warnings.push("Result set limited - more rows may be available");
    } else {
      warnings.push("Pagination applied with server-side LIMIT/OFFSET");
    }
    if (options2.streamObserver) {
      options2.streamObserver({
        batch: rows,
        nextCursor,
        warnings
      });
    }
    return { rows, records: rows, warnings, nextCursor, hasMore };
  }
  buildSignature(preview, useEditedQuery) {
    const query3 = useEditedQuery && preview.editedQuery ? preview.editedQuery : preview.generatedQuery;
    return this.queryCache.buildSignature(
      preview.language,
      query3,
      preview.parameters
    );
  }
  async getStreamingPartial(previewId, useEditedQuery = false) {
    const preview = await this.getPreview(previewId);
    if (!preview) return null;
    const signature = this.buildSignature(preview, useEditedQuery);
    const cached = await this.queryCache.getStreamingPartial(
      signature,
      preview.tenantId
    );
    if (!cached) {
      return null;
    }
    return { ...cached, signature };
  }
  /**
   * Store preview in database
   */
  async storePreview(preview) {
    const query3 = `
      INSERT INTO query_previews (
        id, investigation_id, tenant_id, user_id,
        natural_language_query, parameters, language, generated_query,
        query_explanation, cost_estimate, risk_assessment,
        syntactically_valid, validation_errors, can_execute,
        requires_approval, sandbox_only, model_used, confidence,
        generated_at, expires_at, executed, created_at, updated_at
      ) VALUES (
        $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21, $22, $23
      )
    `;
    const values = [
      preview.id,
      preview.investigationId,
      preview.tenantId,
      preview.userId,
      preview.naturalLanguageQuery,
      JSON.stringify(preview.parameters),
      preview.language,
      preview.generatedQuery,
      preview.queryExplanation,
      JSON.stringify(preview.costEstimate),
      JSON.stringify(preview.riskAssessment),
      preview.syntacticallyValid,
      JSON.stringify(preview.validationErrors),
      preview.canExecute,
      preview.requiresApproval,
      preview.sandboxOnly,
      preview.modelUsed,
      preview.confidence,
      preview.generatedAt,
      preview.expiresAt,
      preview.executed,
      preview.createdAt,
      preview.updatedAt
    ];
    await this.pool.query(query3, values);
  }
  /**
   * Convert database row to QueryPreview
   */
  rowToPreview(row) {
    return {
      id: row.id,
      investigationId: row.investigation_id,
      tenantId: row.tenant_id,
      userId: row.user_id,
      naturalLanguageQuery: row.natural_language_query,
      parameters: JSON.parse(row.parameters || "{}"),
      language: row.language,
      generatedQuery: row.generated_query,
      queryExplanation: row.query_explanation,
      costEstimate: JSON.parse(row.cost_estimate),
      riskAssessment: JSON.parse(row.risk_assessment),
      syntacticallyValid: row.syntactically_valid,
      validationErrors: JSON.parse(row.validation_errors || "[]"),
      canExecute: row.can_execute,
      requiresApproval: row.requires_approval,
      sandboxOnly: row.sandbox_only,
      modelUsed: row.model_used,
      confidence: parseFloat(row.confidence),
      generatedAt: new Date(row.generated_at),
      expiresAt: new Date(row.expires_at),
      executed: row.executed,
      executedAt: row.executed_at ? new Date(row.executed_at) : void 0,
      executionRunId: row.execution_run_id,
      editedQuery: row.edited_query,
      editedBy: row.edited_by,
      editedAt: row.edited_at ? new Date(row.edited_at) : void 0,
      createdAt: new Date(row.created_at),
      updatedAt: new Date(row.updated_at)
    };
  }
  wrapSqlWithPagination(query3) {
    return `${query3.replace(/;?\s*$/, "")} OFFSET $1 LIMIT $2`;
  }
  decodeCursor(cursor) {
    if (!cursor) return 0;
    try {
      return parseInt(Buffer.from(cursor, "base64").toString("utf8"), 10) || 0;
    } catch {
      return 0;
    }
  }
  encodeCursor(offset) {
    return Buffer.from(String(offset)).toString("base64");
  }
};

// src/services/previewStreamHub.ts
import { EventEmitter as EventEmitter19 } from "events";
var PreviewStreamHub = class {
  emitter = new EventEmitter19();
  constructor() {
    this.emitter.setMaxListeners(100);
  }
  publish(previewId, payload) {
    this.emitter.emit(previewId, payload);
  }
  subscribe(previewId, listener) {
    this.emitter.on(previewId, listener);
    return () => {
      this.emitter.off(previewId, listener);
    };
  }
};
var previewStreamHub = new PreviewStreamHub();

// src/routes/query-preview-stream.ts
init_database();

// src/ai/nl-to-cypher/nl-to-cypher.service.ts
import { randomUUID as uuidv419 } from "crypto";
import { default as pino51 } from "pino";
var logger48 = pino51({ name: "nl-to-cypher" });
var NlToCypherService = class {
  constructor(adapter) {
    this.adapter = adapter;
  }
  promptCache = /* @__PURE__ */ new Map();
  executionHistory = [];
  async translateWithPreview(prompt, userId, tenantId) {
    const startTime = Date.now();
    const queryId = uuidv419();
    const cacheKey = `${tenantId}:${prompt.trim().toLowerCase()}`;
    if (this.promptCache.has(cacheKey)) {
      logger48.info(
        { queryId, userId, tenantId, cached: true },
        "Returning cached NL\u2192Cypher translation"
      );
      return this.promptCache.get(cacheKey);
    }
    try {
      const generatedCypher = await this.generateCypher(prompt);
      const validation = this.validateCypher(generatedCypher);
      const costEstimate = this.estimateCost(generatedCypher);
      const policyRisk = this.assessPolicyRisk(
        generatedCypher,
        userId,
        tenantId
      );
      const response = {
        id: queryId,
        originalPrompt: prompt,
        generatedCypher,
        validation,
        costEstimate,
        policyRisk,
        canExecute: validation.isValid && costEstimate.costClass !== "very-high" && policyRisk.riskLevel !== "high",
        timestamp: /* @__PURE__ */ new Date()
      };
      this.promptCache.set(cacheKey, response);
      const parseTime = Date.now() - startTime;
      logger48.info(
        {
          queryId,
          userId,
          tenantId,
          parseTimeMs: parseTime,
          validity: validation.isValid,
          estimatedRows: costEstimate.estimatedRows,
          costClass: costEstimate.costClass,
          riskLevel: policyRisk.riskLevel
        },
        "NL\u2192Cypher translation completed"
      );
      return response;
    } catch (error) {
      logger48.error(
        { queryId, userId, tenantId, error },
        "NL\u2192Cypher translation failed"
      );
      throw error;
    }
  }
  async executeSandbox(queryId, cypher, options2 = {
    readOnly: true,
    timeout: 3e4,
    maxRows: 100
  }) {
    const startTime = Date.now();
    try {
      if (!options2.readOnly && this.containsMutations(cypher)) {
        return {
          success: false,
          executionTimeMs: 0,
          error: "Mutations not allowed in sandbox mode",
          warnings: []
        };
      }
      const safeCypher = this.makeSafe(cypher, options2.maxRows);
      if (options2.dryRun) {
        return {
          success: true,
          executionTimeMs: Date.now() - startTime,
          warnings: [`Dry run - would execute: ${safeCypher}`]
        };
      }
      const mockRows = this.executeSandboxed(safeCypher);
      const executionTime = Date.now() - startTime;
      this.executionHistory.push({
        queryId,
        executionTime,
        rowCount: mockRows.length
      });
      logger48.info(
        {
          queryId,
          executionTimeMs: executionTime,
          rowCount: mockRows.length,
          safeCypher
        },
        "Sandbox execution completed"
      );
      return {
        success: true,
        rows: mockRows,
        executionTimeMs: executionTime,
        warnings: []
      };
    } catch (error) {
      logger48.error({ queryId, error }, "Sandbox execution failed");
      return {
        success: false,
        executionTimeMs: Date.now() - startTime,
        error: error instanceof Error ? error.message : "Unknown execution error",
        warnings: []
      };
    }
  }
  async generateCypher(prompt) {
    if (/show all nodes/i.test(prompt)) {
      return "MATCH (n) RETURN n LIMIT 25";
    }
    if (/count nodes/i.test(prompt)) {
      return "MATCH (n) RETURN count(n) AS count";
    }
    if (/find.*connected.*to/i.test(prompt)) {
      return "MATCH (a)-[r]-(b) WHERE a.name = $name RETURN a, r, b LIMIT 50";
    }
    if (/shortest.*path/i.test(prompt)) {
      return "MATCH p = shortestPath((a)-[*..5]-(b)) WHERE a.id = $startId AND b.id = $endId RETURN p";
    }
    if (/neighbors.*of/i.test(prompt)) {
      return "MATCH (n)-[r]-(neighbor) WHERE n.id = $nodeId RETURN neighbor, r LIMIT 100";
    }
    const systemInstruction = `You are a Neo4j Expert and Intelligence Analysis Copilot.
Your goal is to translate natural language questions into efficient, read-only Cypher queries.

## NEO4J PERFORMANCE GUIDANCE
- Use batch patterns where applicable, but for read queries focus on index usage.
- Avoid variable-length paths with unbounded depth (e.g. [*]). Always specify a max depth (e.g. [*..3]).
- Always include a LIMIT clause (default 100) to prevent blowing up result sizes.
- Treat the Neo4j graph as the source of truth for entity identity and relationships.

## RESPONSE FORMAT
Return ONLY the Cypher query. No markdown formatting, no explanations.`;
    return this.adapter.generate(`${systemInstruction}

User Query: ${prompt}`);
  }
  validateCypher(cypher) {
    const errors = [];
    const warnings = [];
    if (!cypher.trim()) {
      errors.push("Empty query");
    }
    if (!cypher.toUpperCase().includes("MATCH") && !cypher.toUpperCase().includes("CREATE")) {
      warnings.push("Query does not contain MATCH or CREATE clause");
    }
    if (!cypher.toUpperCase().includes("LIMIT") && cypher.toUpperCase().includes("MATCH")) {
      warnings.push(
        "No LIMIT clause found - query may return large result sets"
      );
    }
    if (cypher.includes("*") && cypher.includes("[") && cypher.includes("]")) {
      warnings.push("Variable length path detected - may be expensive");
    }
    const dangerousOps = ["DELETE", "DETACH DELETE", "DROP", "REMOVE"];
    for (const op of dangerousOps) {
      if (cypher.toUpperCase().includes(op)) {
        errors.push(`Dangerous operation detected: ${op}`);
      }
    }
    return {
      isValid: errors.length === 0,
      syntaxErrors: errors,
      warnings
    };
  }
  estimateCost(cypher) {
    let estimatedRows = 100;
    let costClass = "low";
    let executionTimeMs = 50;
    let memoryMb = 10;
    const upperCypher = cypher.toUpperCase();
    if (upperCypher.includes("*")) {
      estimatedRows *= 10;
      costClass = "high";
      executionTimeMs *= 20;
      memoryMb *= 5;
    }
    if (upperCypher.includes("SHORTESTPATH")) {
      estimatedRows *= 2;
      costClass = costClass === "high" ? "very-high" : "medium";
      executionTimeMs *= 5;
      memoryMb *= 2;
    }
    const matchCount = (cypher.match(/MATCH/gi) || []).length;
    if (matchCount > 1 && !upperCypher.includes("WHERE")) {
      estimatedRows *= Math.pow(10, matchCount - 1);
      costClass = "very-high";
      executionTimeMs *= 50;
      memoryMb *= 10;
    }
    const avgHistory = this.getAverageExecutionTime(cypher);
    if (avgHistory) {
      executionTimeMs = Math.max(executionTimeMs, avgHistory);
    }
    return {
      estimatedRows,
      costClass,
      executionTimeMs,
      memoryMb
    };
  }
  assessPolicyRisk(cypher, userId, tenantId) {
    const risks = [];
    let riskLevel = "low";
    let piiAccess = false;
    const sensitiveOperations = [];
    const upperCypher = cypher.toUpperCase();
    const piiFields = ["EMAIL", "PHONE", "SSN", "ADDRESS", "NAME"];
    for (const field of piiFields) {
      if (upperCypher.includes(field)) {
        piiAccess = true;
        risks.push(`Query accesses potentially sensitive field: ${field}`);
        riskLevel = "medium";
      }
    }
    if (!upperCypher.includes("WHERE") && upperCypher.includes("MATCH")) {
      risks.push("Query lacks WHERE clause - may access all data");
      riskLevel = "medium";
    }
    const mutations = ["CREATE", "DELETE", "SET", "REMOVE", "MERGE"];
    for (const mutation of mutations) {
      if (upperCypher.includes(mutation)) {
        sensitiveOperations.push(mutation);
        risks.push(`Query contains mutation operation: ${mutation}`);
        riskLevel = "high";
      }
    }
    return {
      riskLevel,
      risks,
      piiAccess,
      sensitiveOperations
    };
  }
  containsMutations(cypher) {
    const mutations = ["CREATE", "DELETE", "SET", "REMOVE", "MERGE", "DROP"];
    const upperCypher = cypher.toUpperCase();
    return mutations.some((op) => upperCypher.includes(op));
  }
  makeSafe(cypher, maxRows) {
    let safeCypher = cypher.trim();
    if (!safeCypher.toUpperCase().includes("LIMIT")) {
      safeCypher += ` LIMIT ${maxRows}`;
    }
    return safeCypher;
  }
  /**
   * Execute Cypher in sandboxed environment (P2-2 implementation)
   *
   * Safety features:
   * - Read-only queries enforced
   * - Result size limits (max 100 rows)
   * - Mock data for security (no real DB access)
   *
   * TODO: Replace with real Neo4j sandbox:
   * - Isolated read-only database instance
   * - Query timeout enforcement (5s max)
   * - Memory/CPU resource limits
   * - Network isolation
   */
  executeSandboxed(cypher) {
    console.debug("[NL-to-Cypher Sandbox P2-2] Executing query:", cypher);
    const mockData = [
      { id: "1", name: "Node 1", type: "Person" },
      { id: "2", name: "Node 2", type: "Organization" },
      { id: "3", name: "Node 3", type: "Event" }
    ];
    if (cypher.toUpperCase().includes("COUNT")) {
      return [{ count: mockData.length }];
    }
    return mockData.slice(0, Math.min(3, 100));
  }
  getAverageExecutionTime(cypher) {
    const similarQueries = this.executionHistory.filter((h) => {
      return cypher.includes("MATCH") && cypher.includes("RETURN");
    });
    if (similarQueries.length === 0) return null;
    const avgTime = similarQueries.reduce((sum, q) => sum + q.executionTime, 0) / similarQueries.length;
    return avgTime;
  }
  // Method for computing diff between generated and user-edited Cypher
  diffCypher(original, edited) {
    const originalLines = original.split("\n").map((l) => l.trim());
    const editedLines = edited.split("\n").map((l) => l.trim());
    const additions = [];
    const deletions = [];
    const modifications = [];
    editedLines.forEach((line) => {
      if (!originalLines.includes(line)) {
        additions.push(line);
      }
    });
    originalLines.forEach((line) => {
      if (!editedLines.includes(line)) {
        deletions.push(line);
      }
    });
    return { additions, deletions, modifications };
  }
  // Cleanup method for cache management
  clearCache() {
    this.promptCache.clear();
    logger48.info("NL\u2192Cypher cache cleared");
  }
  // Legacy method for backward compatibility
  async translate(prompt) {
    const result2 = await this.translateWithPreview(prompt, "system", "default");
    return result2.generatedCypher;
  }
};

// src/services/GlassBoxRunService.ts
init_logger2();
init_metrics4();
import { v4 as uuidv420 } from "uuid";
var GlassBoxRunService = class {
  pool;
  redis;
  cacheEnabled;
  cacheTTL = 3600;
  // 1 hour
  constructor(pool4, redis5) {
    this.pool = pool4;
    this.redis = redis5 || null;
    this.cacheEnabled = !!redis5;
  }
  /**
   * Create a new glass-box run
   */
  async createRun(input) {
    const startTime = Date.now();
    const run = {
      id: uuidv420(),
      investigationId: input.investigationId,
      tenantId: input.tenantId,
      userId: input.userId,
      type: input.type,
      status: "pending",
      prompt: input.prompt,
      parameters: input.parameters || {},
      steps: [],
      toolCalls: [],
      modelUsed: input.modelUsed,
      startTime: /* @__PURE__ */ new Date(),
      replayable: true,
      replayCount: 0,
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date()
    };
    const query3 = `
      INSERT INTO glass_box_runs (
        id, investigation_id, tenant_id, user_id, type, status,
        prompt, parameters, steps, tool_calls, model_used,
        start_time, replayable, replay_count, created_at, updated_at
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16)
      RETURNING *
    `;
    const values = [
      run.id,
      run.investigationId,
      run.tenantId,
      run.userId,
      run.type,
      run.status,
      run.prompt,
      JSON.stringify(run.parameters),
      JSON.stringify(run.steps),
      JSON.stringify(run.toolCalls),
      run.modelUsed,
      run.startTime,
      run.replayable,
      run.replayCount,
      run.createdAt,
      run.updatedAt
    ];
    try {
      await this.pool.query(query3, values);
      metrics3.glassBoxRunsTotal.inc({ type: run.type, status: "created" });
      logger2.info({
        runId: run.id,
        investigationId: run.investigationId,
        type: run.type,
        durationMs: Date.now() - startTime
      }, "Created glass-box run");
      return run;
    } catch (error) {
      logger2.error({ error, input }, "Failed to create glass-box run");
      throw error;
    }
  }
  /**
   * Add a step to a run
   */
  async addStep(runId, step) {
    const run = await this.getRun(runId);
    if (!run) {
      throw new Error(`Run ${runId} not found`);
    }
    const newStep = {
      id: uuidv420(),
      stepNumber: run.steps.length + 1,
      startTime: /* @__PURE__ */ new Date(),
      ...step
    };
    run.steps.push(newStep);
    run.updatedAt = /* @__PURE__ */ new Date();
    const query3 = `
      UPDATE glass_box_runs
      SET steps = $1, updated_at = $2
      WHERE id = $3
    `;
    await this.pool.query(query3, [
      JSON.stringify(run.steps),
      run.updatedAt,
      runId
    ]);
    logger2.debug({
      runId,
      stepNumber: newStep.stepNumber,
      type: newStep.type
    }, "Added step to run");
  }
  /**
   * Complete a step
   */
  async completeStep(runId, stepId, output, error) {
    const run = await this.getRun(runId);
    if (!run) {
      throw new Error(`Run ${runId} not found`);
    }
    const step = run.steps.find((s) => s.id === stepId);
    if (!step) {
      throw new Error(`Step ${stepId} not found in run ${runId}`);
    }
    step.endTime = /* @__PURE__ */ new Date();
    step.durationMs = step.endTime.getTime() - step.startTime.getTime();
    step.output = output;
    step.error = error;
    run.updatedAt = /* @__PURE__ */ new Date();
    const query3 = `
      UPDATE glass_box_runs
      SET steps = $1, updated_at = $2
      WHERE id = $3
    `;
    await this.pool.query(query3, [
      JSON.stringify(run.steps),
      run.updatedAt,
      runId
    ]);
  }
  /**
   * Add a tool call to a run
   */
  async addToolCall(runId, toolCall) {
    const run = await this.getRun(runId);
    if (!run) {
      throw new Error(`Run ${runId} not found`);
    }
    const newToolCall = {
      id: uuidv420(),
      startTime: /* @__PURE__ */ new Date(),
      ...toolCall
    };
    run.toolCalls.push(newToolCall);
    run.updatedAt = /* @__PURE__ */ new Date();
    const query3 = `
      UPDATE glass_box_runs
      SET tool_calls = $1, updated_at = $2
      WHERE id = $3
    `;
    await this.pool.query(query3, [
      JSON.stringify(run.toolCalls),
      run.updatedAt,
      runId
    ]);
    logger2.debug({
      runId,
      toolCallId: newToolCall.id,
      toolName: newToolCall.name
    }, "Added tool call to run");
    return newToolCall.id;
  }
  /**
   * Complete a tool call
   */
  async completeToolCall(runId, toolCallId, result2, error) {
    const run = await this.getRun(runId);
    if (!run) {
      throw new Error(`Run ${runId} not found`);
    }
    const toolCall = run.toolCalls.find((tc) => tc.id === toolCallId);
    if (!toolCall) {
      throw new Error(`Tool call ${toolCallId} not found in run ${runId}`);
    }
    toolCall.endTime = /* @__PURE__ */ new Date();
    toolCall.durationMs = toolCall.endTime.getTime() - toolCall.startTime.getTime();
    toolCall.result = result2;
    toolCall.error = error;
    run.updatedAt = /* @__PURE__ */ new Date();
    const query3 = `
      UPDATE glass_box_runs
      SET tool_calls = $1, updated_at = $2
      WHERE id = $3
    `;
    await this.pool.query(query3, [
      JSON.stringify(run.toolCalls),
      run.updatedAt,
      runId
    ]);
  }
  /**
   * Update run status
   */
  async updateStatus(runId, status, result2, error) {
    const endTime = status === "completed" || status === "failed" ? /* @__PURE__ */ new Date() : void 0;
    const run = await this.getRun(runId);
    if (!run) {
      throw new Error(`Run ${runId} not found`);
    }
    const durationMs = endTime ? endTime.getTime() - run.startTime.getTime() : void 0;
    const query3 = `
      UPDATE glass_box_runs
      SET status = $1, result = $2, error = $3, end_time = $4, duration_ms = $5, updated_at = $6
      WHERE id = $7
    `;
    await this.pool.query(query3, [
      status,
      result2 ? JSON.stringify(result2) : null,
      error,
      endTime,
      durationMs,
      /* @__PURE__ */ new Date(),
      runId
    ]);
    metrics3.glassBoxRunsTotal.inc({ type: run.type, status });
    if (durationMs) {
      metrics3.glassBoxRunDurationMs.observe({ type: run.type }, durationMs);
    }
    logger2.info({
      runId,
      status,
      durationMs,
      hasError: !!error
    }, "Updated run status");
  }
  /**
   * Get a run by ID
   */
  async getRun(runId) {
    if (this.cacheEnabled && this.redis) {
      const cached = await this.redis.get(`glassbox:run:${runId}`);
      if (cached) {
        metrics3.glassBoxCacheHits.inc({ operation: "get_run" });
        return JSON.parse(cached);
      }
    }
    const query3 = `
      SELECT * FROM glass_box_runs
      WHERE id = $1
    `;
    const result2 = await this.pool.query(query3, [runId]);
    if (result2.rows.length === 0) {
      return null;
    }
    const run = this.rowToRun(result2.rows[0]);
    if (this.cacheEnabled && this.redis) {
      await this.redis.setex(
        `glassbox:run:${runId}`,
        this.cacheTTL,
        JSON.stringify(run)
      );
    }
    return run;
  }
  /**
   * List runs for an investigation
   */
  async listRuns(investigationId, options2) {
    const { type, status, limit = 50, offset = 0 } = options2 || {};
    let query3 = `
      SELECT * FROM glass_box_runs
      WHERE investigation_id = $1
    `;
    const params = [investigationId];
    let paramIndex = 2;
    if (type) {
      query3 += ` AND type = $${paramIndex}`;
      params.push(type);
      paramIndex++;
    }
    if (status) {
      query3 += ` AND status = $${paramIndex}`;
      params.push(status);
      paramIndex++;
    }
    query3 += ` ORDER BY created_at DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    params.push(limit, offset);
    const [dataResult, countResult] = await Promise.all([
      this.pool.query(query3, params),
      this.pool.query(
        `SELECT COUNT(*) FROM glass_box_runs WHERE investigation_id = $1`,
        [investigationId]
      )
    ]);
    return {
      runs: dataResult.rows.map((row) => this.rowToRun(row)),
      total: parseInt(countResult.rows[0].count)
    };
  }
  /**
   * Replay a run with optional modifications
   */
  async replayRun(runId, userId, options2) {
    const originalRun = await this.getRun(runId);
    if (!originalRun) {
      throw new Error(`Run ${runId} not found`);
    }
    if (!originalRun.replayable) {
      throw new Error(`Run ${runId} is not replayable`);
    }
    const replayRun = await this.createRun({
      investigationId: originalRun.investigationId,
      tenantId: originalRun.tenantId,
      userId,
      type: originalRun.type,
      prompt: options2?.modifiedPrompt || originalRun.prompt,
      parameters: options2?.modifiedParameters || originalRun.parameters,
      modelUsed: originalRun.modelUsed
    });
    await this.pool.query(
      `UPDATE glass_box_runs SET parent_run_id = $1 WHERE id = $2`,
      [runId, replayRun.id]
    );
    await this.pool.query(
      `UPDATE glass_box_runs SET replay_count = replay_count + 1 WHERE id = $1`,
      [runId]
    );
    if (options2?.skipCache && this.redis) {
      const keys = await this.redis.keys(`graphrag:*${originalRun.investigationId}*`);
      if (keys.length > 0) {
        await this.redis.del(...keys);
      }
    }
    logger2.info({
      originalRunId: runId,
      replayRunId: replayRun.id,
      userId,
      modified: !!(options2?.modifiedPrompt || options2?.modifiedParameters)
    }, "Replaying run");
    return replayRun;
  }
  /**
   * Get replay history for a run
   */
  async getReplayHistory(runId) {
    const query3 = `
      SELECT * FROM glass_box_runs
      WHERE parent_run_id = $1
      ORDER BY created_at ASC
    `;
    const result2 = await this.pool.query(query3, [runId]);
    return result2.rows.map((row) => this.rowToRun(row));
  }
  /**
   * Convert database row to GlassBoxRun
   */
  rowToRun(row) {
    return {
      id: row.id,
      investigationId: row.investigation_id,
      tenantId: row.tenant_id,
      userId: row.user_id,
      type: row.type,
      status: row.status,
      prompt: row.prompt,
      parameters: JSON.parse(row.parameters || "{}"),
      steps: JSON.parse(row.steps || "[]"),
      toolCalls: JSON.parse(row.tool_calls || "[]"),
      result: row.result ? JSON.parse(row.result) : void 0,
      error: row.error,
      modelUsed: row.model_used,
      tokensUsed: row.tokens_used,
      costEstimate: row.cost_estimate ? parseFloat(row.cost_estimate) : void 0,
      confidence: row.confidence ? parseFloat(row.confidence) : void 0,
      startTime: new Date(row.start_time),
      endTime: row.end_time ? new Date(row.end_time) : void 0,
      durationMs: row.duration_ms,
      replayable: row.replayable,
      parentRunId: row.parent_run_id,
      replayCount: row.replay_count,
      createdAt: new Date(row.created_at),
      updatedAt: new Date(row.updated_at)
    };
  }
  /**
   * Clean up old runs (for maintenance)
   */
  async cleanupOldRuns(daysToKeep = 90) {
    const query3 = `
      DELETE FROM glass_box_runs
      WHERE created_at < NOW() - INTERVAL '${daysToKeep} days'
      AND status IN ('completed', 'failed', 'cancelled')
    `;
    const result2 = await this.pool.query(query3);
    logger2.info({
      deletedCount: result2.rowCount,
      daysToKeep
    }, "Cleaned up old runs");
    return result2.rowCount || 0;
  }
};

// src/routes/query-preview-stream.ts
init_logger2();
var router45 = express24.Router();
var previewService = null;
function getPreviewService() {
  if (previewService) {
    return previewService;
  }
  const pool4 = getPostgresPool2();
  const neo4jDriver2 = getNeo4jDriver2();
  const redis5 = getRedisClient() ?? void 0;
  const nlToCypherService = new NlToCypherService({
    generate: async () => "MATCH (n) RETURN n LIMIT 10"
  });
  const glassBoxService = new GlassBoxRunService(pool4, redis5);
  previewService = new QueryPreviewService(
    pool4,
    neo4jDriver2,
    nlToCypherService,
    glassBoxService,
    redis5
  );
  return previewService;
}
router45.get("/query-previews/:id/stream", async (req, res) => {
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");
  const previewId = req.params.id;
  const parsedBatchSize = Number(req.query.batchSize ?? NaN);
  const batchSize = Number.isFinite(parsedBatchSize) && parsedBatchSize > 0 ? parsedBatchSize : void 0;
  const cursor = typeof req.query.cursor === "string" ? req.query.cursor : null;
  const autoStart = req.query.autostart !== "false";
  const useEditedQuery = req.query.useEdited === "true";
  const userId = req.user?.id ?? "stream-subscriber";
  let service11;
  try {
    service11 = getPreviewService();
  } catch (error) {
    logger2.error({ error }, "Failed to initialise query preview stream service");
    res.write(`event: error
data:${JSON.stringify({ message: "Unable to initialise streaming service" })}

`);
    res.end();
    return;
  }
  const unsubscribe = previewStreamHub.subscribe(previewId, (payload) => {
    res.write(`data:${JSON.stringify(payload)}

`);
    if (payload.complete) {
      res.write("event: complete\n\n");
    }
  });
  req.on("close", () => {
    unsubscribe();
  });
  try {
    const cached = await service11.getStreamingPartial(previewId, useEditedQuery);
    if (cached?.rows?.length) {
      res.write(
        `event: warm-start
data:${JSON.stringify({
          previewId,
          batch: cached.rows,
          cursor,
          cacheTier: cached.tier
        })}

`
      );
    }
  } catch (error) {
    logger2.warn({ error, previewId }, "Failed to load streaming cache warm start");
  }
  if (autoStart) {
    void service11.executePreview({
      previewId,
      userId,
      useEditedQuery,
      cursor,
      batchSize,
      stream: true
    }).catch((error) => {
      logger2.error({ error, previewId }, "Streaming execution failed");
      res.write(
        `event: error
data:${JSON.stringify({
          message: error instanceof Error ? error.message : String(error)
        })}

`
      );
      res.end();
    });
  }
});
var query_preview_stream_default = router45;

// src/routes/correctness-program.ts
import { Router as Router24 } from "express";

// src/correctness-program/types.ts
import { randomUUID as randomUUID38 } from "crypto";
var newIdentifier = () => randomUUID38();

// src/correctness-program/adminTools.ts
var AdminRepairService = class {
  actions = [];
  queueAction(action) {
    const approvalRequired = action.risk === "high" || action.risk === "medium";
    const entry = { ...action, id: newIdentifier(), approvalRequired };
    this.actions.push(entry);
    return entry;
  }
  approve(actionId, approver) {
    const action = this.actions.find((a) => a.id === actionId);
    if (!action) throw new Error("Action not found");
    action.approvedBy = approver;
    return action;
  }
  execute(actionId, executor) {
    const action = this.actions.find((a) => a.id === actionId);
    if (!action) throw new Error("Action not found");
    if (action.approvalRequired && !action.approvedBy) throw new Error("Approval required");
    action.executedBy = executor;
    action.executedAt = /* @__PURE__ */ new Date();
    return action;
  }
  list() {
    return [...this.actions];
  }
};

// src/correctness-program/eventContracts.ts
var isCompatible = (prev, next) => {
  const prevRequired = prev.filter((f) => f.required).map((f) => f.name);
  const nextNames = new Set(next.map((f) => f.name));
  return prevRequired.every((field) => nextNames.has(field));
};
var EventContractRegistry = class {
  schemas = /* @__PURE__ */ new Map();
  handledDedupeKeys = /* @__PURE__ */ new Set();
  orderingState = /* @__PURE__ */ new Map();
  registerSchema(schema2) {
    const versions = this.schemas.get(schema2.name) || [];
    const latest = versions[versions.length - 1];
    if (latest && !isCompatible(latest.fields, schema2.fields)) {
      throw new Error(`Schema for ${schema2.name} is not backward compatible`);
    }
    versions.push(schema2);
    this.schemas.set(schema2.name, versions);
  }
  validateEnvelope(envelope) {
    const schemas = this.schemas.get(envelope.name);
    if (!schemas) throw new Error(`No schema registered for ${envelope.name}`);
    const schema2 = schemas.find((s) => s.version === envelope.version);
    if (!schema2) throw new Error(`Schema version ${envelope.version} not found for ${envelope.name}`);
    const requiredFields = schema2.fields.filter((f) => f.required);
    requiredFields.forEach((field) => {
      if (!(field.name in envelope.payload)) {
        throw new Error(`Missing required field ${field.name} for event ${envelope.name}`);
      }
    });
    if (!schema2.piiSafe) {
      const containsPii = Object.keys(envelope.payload).some((key) => key.toLowerCase().includes("email"));
      if (containsPii) throw new Error(`Event ${envelope.name} payload failed PII hygiene check`);
    }
    const dedupeKey = envelope.dedupeKey || envelope.id;
    const idempotentHit = this.handledDedupeKeys.has(dedupeKey);
    if (idempotentHit) {
      return { idempotentHit: true, validated: true, ordered: true };
    }
    this.handledDedupeKeys.add(dedupeKey);
    const orderingKey = envelope.orderingKey || envelope.name;
    const lastSequence = this.orderingState.get(orderingKey) || 0;
    const ordered = envelope.sequence ? envelope.sequence >= lastSequence : true;
    this.orderingState.set(orderingKey, envelope.sequence || lastSequence + 1);
    return { idempotentHit, validated: true, ordered };
  }
};

// src/correctness-program/governance.ts
var GovernanceTracker = class {
  scorecards = /* @__PURE__ */ new Map();
  waivers = [];
  updateScorecard(domain, driftRate, invariantViolations, mttrHours) {
    const card = {
      domain,
      driftRate,
      invariantViolations,
      mttrHours,
      updatedAt: /* @__PURE__ */ new Date()
    };
    this.scorecards.set(domain, card);
    return card;
  }
  getScorecards() {
    return Array.from(this.scorecards.values());
  }
  addWaiver(domain, description, expiresAt, owner) {
    const waiver = { id: newIdentifier(), domain, description, expiresAt, owner };
    this.waivers.push(waiver);
    return waiver;
  }
  activeWaivers() {
    const now = Date.now();
    return this.waivers.filter((waiver) => waiver.expiresAt.getTime() > now);
  }
};

// src/correctness-program/invariants.ts
var InvariantRegistry = class {
  invariants = /* @__PURE__ */ new Map();
  stateMachines = /* @__PURE__ */ new Map();
  idempotencyCache = /* @__PURE__ */ new Map();
  violations = [];
  quarantinedRecords = /* @__PURE__ */ new Map();
  registerInvariant(definition) {
    this.invariants.set(definition.id, definition);
  }
  registerStateMachine(machine) {
    if (!machine.states.includes(machine.initialState)) {
      throw new Error(`Initial state ${machine.initialState} not part of machine ${machine.id}`);
    }
    this.stateMachines.set(machine.id, machine);
  }
  getStateMachine(id) {
    return this.stateMachines.get(id);
  }
  validateStateTransition(machineId, from, to, payload) {
    const machine = this.stateMachines.get(machineId);
    if (!machine) throw new Error(`Unknown state machine ${machineId}`);
    const transition = machine.transitions.find((t) => t.from === from && t.to === to);
    if (!transition || !transition.allowed) {
      throw new Error(`Transition from ${from} to ${to} is not allowed for ${machineId}`);
    }
    if (transition.guard && !transition.guard(payload)) {
      throw new Error(`Guard prevented transition for ${machineId}`);
    }
    return true;
  }
  async validateWrite(domain, payload, idempotencyKey) {
    if (idempotencyKey && this.idempotencyCache.has(idempotencyKey)) {
      return { idempotent: true, violations: [] };
    }
    const violations = [];
    const invariantDefs = Array.from(this.invariants.values()).filter((i) => i.domain === domain);
    for (const invariant of invariantDefs) {
      const isValid = await invariant.validate(payload);
      if (!isValid) {
        const violation = {
          id: newIdentifier(),
          invariantId: invariant.id,
          domain,
          input: payload,
          occurredAt: /* @__PURE__ */ new Date(),
          quarantined: invariant.severity === "critical",
          message: invariant.description
        };
        violations.push(violation);
        this.violations.push(violation);
        if (violation.quarantined) {
          this.quarantinedRecords.set(violation.id, payload);
        }
      }
    }
    if (idempotencyKey) {
      this.idempotencyCache.set(idempotencyKey, { payload, violations });
    }
    return { idempotent: false, violations };
  }
  guardBulkOperation(guardrail) {
    const diffPreview = guardrail.plannedChanges.map((change) => JSON.stringify(change));
    const approvalRequired = guardrail.plannedChanges.length > 10 || diffPreview.join("").length > 1e4;
    if (approvalRequired && !guardrail.approver) {
      throw new Error("High-risk bulk operations require an approver");
    }
    return {
      approvalRequired,
      approvedBy: guardrail.approver,
      dryRun: guardrail.dryRun,
      diffPreview
    };
  }
  violationsByDomain(domain) {
    return this.violations.filter((v) => v.domain === domain);
  }
  getQuarantine() {
    return Array.from(this.quarantinedRecords.entries()).map(([id, record2]) => ({ id, record: record2 }));
  }
};
var buildBooleanStateMachine = (id, domain, activeState, inactiveState) => ({
  id,
  domain,
  states: [activeState, inactiveState],
  transitions: [
    { from: inactiveState, to: activeState, allowed: true },
    { from: activeState, to: inactiveState, allowed: true }
  ],
  initialState: inactiveState
});

// src/correctness-program/migrationFactory.ts
var MigrationFactory = class {
  progress = /* @__PURE__ */ new Map();
  start(manifest, total) {
    const checkpoint = { processed: 0, failed: 0, total };
    const initialStage = manifest.enableDualRun ? "dual_run" : "backfill";
    const progress = {
      manifestId: manifest.id,
      stage: initialStage,
      checkpoint,
      errors: [],
      dlq: [],
      startedAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date()
    };
    this.progress.set(manifest.id, progress);
    return progress;
  }
  advance(manifest, processedBatch) {
    const progress = this.progress.get(manifest.id);
    if (!progress) throw new Error(`Migration ${manifest.id} not started`);
    processedBatch.forEach((item) => {
      if (item.success) {
        progress.checkpoint.processed += 1;
        progress.checkpoint.lastProcessedId = item.id;
      } else {
        progress.checkpoint.failed += 1;
        if (progress.errors.length < 50) {
          progress.errors.push(item.error || "Unknown migration error");
        }
        progress.dlq.push({ id: item.id, error: item.error || "Unknown error" });
      }
    });
    progress.updatedAt = /* @__PURE__ */ new Date();
    const total = progress.checkpoint.total ?? processedBatch.length;
    const processed = progress.checkpoint.processed + progress.checkpoint.failed;
    const atCapacity = processedBatch.length > manifest.batchSize * 2;
    const overloaded = atCapacity && manifest.enableDualRun;
    if (overloaded) {
      progress.errors.push("Load shedding activated due to oversized batch");
    }
    if (processed >= total) {
      progress.stage = this.nextStage(progress.stage);
      progress.updatedAt = /* @__PURE__ */ new Date();
      if (progress.stage === "completed") {
        progress.completedAt = /* @__PURE__ */ new Date();
      }
    }
    return progress;
  }
  nextStage(current) {
    switch (current) {
      case "dual_run":
        return "backfill";
      case "backfill":
        return "verify";
      case "verify":
        return "cutover";
      case "cutover":
        return "delete";
      case "delete":
        return "completed";
      default:
        return "failed";
    }
  }
  verify(manifestId, verificationPassed) {
    const progress = this.progress.get(manifestId);
    if (!progress) throw new Error(`Migration ${manifestId} not started`);
    if (!verificationPassed) {
      progress.errors.push("Verification failed, keeping on dual-run");
      progress.stage = "failed";
      progress.completedAt = /* @__PURE__ */ new Date();
    }
    return progress;
  }
  progressReport(manifestId) {
    const progress = this.progress.get(manifestId);
    if (!progress) throw new Error(`Migration ${manifestId} not started`);
    const total = progress.checkpoint.total ?? progress.checkpoint.processed + progress.checkpoint.failed;
    const percent = total === 0 ? 0 : progress.checkpoint.processed / total * 100;
    return {
      ...progress,
      percentComplete: Number(percent.toFixed(2)),
      lag: Math.max(0, total - progress.checkpoint.processed - progress.checkpoint.failed)
    };
  }
};
var buildManifest = (domain, scope, enableDualRun = true, batchSize = 100, maxRetries = 3) => ({
  id: newIdentifier(),
  domain,
  scope,
  successCriteria: ["row counts match", "hash parity holds", "invariants clean"],
  decommissionPlan: "Drop legacy objects after cutover verification",
  batchSize,
  maxRetries,
  enableDualRun
});

// src/correctness-program/observability.ts
import { randomUUID as randomUUID39 } from "crypto";
var correctnessCorrelationIdHeader = "x-correctness-correlation-id";
var correlationIdMiddleware2 = (req, res, next) => {
  const existing = req.headers[correctnessCorrelationIdHeader];
  const correlationId = existing || randomUUID39();
  req.headers[correctnessCorrelationIdHeader] = correlationId;
  res.setHeader(correctnessCorrelationIdHeader, correlationId);
  next();
};
var RecordTimeline = class {
  entries = [];
  record(entry) {
    this.entries.push(entry);
  }
  forEntity(domain, entityId) {
    return this.entries.filter((entry) => entry.domain === domain && entry.entityId === entityId);
  }
};

// src/correctness-program/reconciliation.ts
var ReconciliationEngine = class {
  pairs = /* @__PURE__ */ new Map();
  runs = [];
  registerPair(pair) {
    this.pairs.set(pair.id, pair);
  }
  listPairs() {
    return Array.from(this.pairs.values());
  }
  listRuns() {
    return [...this.runs];
  }
  async runPair(pairId) {
    const pair = this.pairs.get(pairId);
    if (!pair) throw new Error(`Unknown drift pair ${pairId}`);
    const startedAt2 = /* @__PURE__ */ new Date();
    const source = await pair.loadSource();
    const target = await pair.loadTarget();
    const drift = pair.diff(source, target);
    const autoFixes = [];
    if (drift.length > 0 && pair.autoFix && pair.riskTier !== "high") {
      const fixes = await pair.autoFix(drift);
      autoFixes.push(...fixes);
    }
    const run = {
      id: newIdentifier(),
      pairId,
      startedAt: startedAt2,
      completedAt: /* @__PURE__ */ new Date(),
      driftDetected: drift,
      autoFixesApplied: autoFixes,
      requiresReview: drift.length > autoFixes.length
    };
    this.runs.push(run);
    return run;
  }
  metrics() {
    const totalRuns = this.runs.length;
    const totalDrift = this.runs.reduce((sum, run) => sum + run.driftDetected.length, 0);
    const totalAutoFixes = this.runs.reduce((sum, run) => sum + run.autoFixesApplied.length, 0);
    const avgDrift = totalRuns === 0 ? 0 : totalDrift / totalRuns;
    const recurrence = this.runs.filter((run) => run.driftDetected.length > 0).length;
    return {
      totalRuns,
      avgDrift,
      totalAutoFixes,
      recurrence
    };
  }
};

// src/correctness-program/domain.ts
var TruthMapRegistry = class {
  truthMap = /* @__PURE__ */ new Map();
  identityPolicies = /* @__PURE__ */ new Map();
  truthDebt = [];
  declareDomain(entry, identityPolicy) {
    this.truthMap.set(entry.domain, entry);
    this.identityPolicies.set(entry.domain, identityPolicy);
  }
  listTruthMap() {
    return Array.from(this.truthMap.values());
  }
  getDomain(domain) {
    return this.truthMap.get(domain);
  }
  getIdentityPolicy(domain) {
    return this.identityPolicies.get(domain);
  }
  addTruthDebt(domain, kind, description, mitigation, owner) {
    const debt = {
      id: newIdentifier(),
      domain,
      kind,
      description,
      detectedAt: /* @__PURE__ */ new Date(),
      mitigation,
      owner
    };
    this.truthDebt.push(debt);
    return debt;
  }
  listTruthDebt() {
    return [...this.truthDebt];
  }
  truthDebtCount() {
    return this.truthDebt.length;
  }
  truthCheck(domain, entityId, sources) {
    const entry = this.truthMap.get(domain);
    if (!entry) {
      return { domain, entityId, status: "unknown", notes: "Domain is not registered in the truth map" };
    }
    if (sources.length === 0) {
      return { domain, entityId, status: "unknown", notes: "No sources provided for truth check" };
    }
    const serialized = sources.map((s) => JSON.stringify(s)).sort();
    const unique = new Set(serialized);
    if (unique.size === 1) {
      return { domain, entityId, status: "healthy", notes: `${entry.systemOfRecord.name} authoritative` };
    }
    const diffs = serialized.filter((value, index, arr) => arr.indexOf(value) !== index);
    return {
      domain,
      entityId,
      status: "drift",
      notes: "Detected differing representations across sources",
      detectedDrift: diffs
    };
  }
  ensureCanonicalId(domain, record2) {
    const policy2 = this.identityPolicies.get(domain);
    if (!policy2) {
      throw new Error(`No identity policy for domain ${domain}`);
    }
    const canonicalId = record2[policy2.canonicalIdField];
    if (!canonicalId) {
      throw new Error(`Record missing canonical ID field ${policy2.canonicalIdField}`);
    }
    return canonicalId;
  }
};
var defaultTruthSources = {
  customer: { name: "customers-db", kind: "database", uri: "postgresql://customer" },
  billing: { name: "billing-ledger", kind: "database", uri: "postgresql://billing" },
  usage: { name: "usage-meter", kind: "service", uri: "https://usage/api" },
  content: { name: "content-repo", kind: "service", uri: "https://content/api" },
  permissions: { name: "authz-engine", kind: "service", uri: "https://authz/api" },
  generic: { name: "generic-truth", kind: "service", uri: "https://truth/api" }
};

// src/correctness-program/index.ts
var CorrectnessProgram = class {
  truthMap = new TruthMapRegistry();
  invariants = new InvariantRegistry();
  reconciliation = new ReconciliationEngine();
  migrations = new MigrationFactory();
  eventContracts = new EventContractRegistry();
  timeline = new RecordTimeline();
  adminRepairs = new AdminRepairService();
  governance = new GovernanceTracker();
  bootstrapDefaultDomains() {
    const defaults = [
      {
        domain: "customer",
        policy: {
          canonicalIdField: "customerId",
          mergePolicy: "prefer_newest",
          splitPolicy: "manual_review",
          resolutionRules: ["email normalized", "oidc subject wins when present"]
        },
        writers: ["customer-api"],
        readers: ["customer-api", "reporting"]
      },
      {
        domain: "billing",
        policy: {
          canonicalIdField: "invoiceId",
          mergePolicy: "manual_review",
          splitPolicy: "manual_review",
          resolutionRules: ["ledger entry id is canonical", "entitlement must exist"]
        },
        writers: ["billing-writer"],
        readers: ["billing-reader", "reporting"]
      },
      {
        domain: "permissions",
        policy: {
          canonicalIdField: "subjectId",
          mergePolicy: "prefer_newest",
          splitPolicy: "manual_review",
          resolutionRules: ["subject must match identity provider", "role dedupe per tenant"]
        },
        writers: ["rbac-service"],
        readers: ["rbac-service", "edge"]
      }
    ];
    defaults.forEach(({ domain, policy: policy2, writers, readers }) => {
      const entry = {
        domain,
        systemOfRecord: defaultTruthSources[domain],
        writers: writers.map((name) => ({ name, kind: "service" })),
        readers: readers.map((name) => ({ name, kind: "service" })),
        caches: [
          {
            name: `${domain}-cache`,
            kind: "cache",
            guards: ["ttl"]
          }
        ],
        syncPaths: [
          { from: defaultTruthSources[domain].name, to: `${domain}-cache`, cadence: "5m", guardedBy: ["checksum"] }
        ]
      };
      this.truthMap.declareDomain(entry, policy2);
    });
    this.invariants.registerStateMachine(buildBooleanStateMachine("customer-active", "customer", "active", "inactive"));
  }
  declareDriftPair(pair) {
    this.reconciliation.registerPair(pair);
  }
  registerEventSchema(schema2) {
    this.eventContracts.registerSchema(schema2);
  }
  startMigration(manifest, total) {
    return this.migrations.start(manifest, total);
  }
  buildMigrationManifest(domain, scope) {
    return buildManifest(domain, scope);
  }
};
var correctnessProgram = new CorrectnessProgram();
correctnessProgram.bootstrapDefaultDomains();

// src/routes/correctness-program.ts
var router46 = Router24();
router46.use(correlationIdMiddleware2);
router46.get("/truth-map", (_req, res) => {
  res.json({ truthMap: correctnessProgram.truthMap.listTruthMap(), debt: correctnessProgram.truthMap.listTruthDebt() });
});
router46.post("/truth-debt", (req, res) => {
  const { domain, kind, description, mitigation, owner } = req.body;
  const record2 = correctnessProgram.truthMap.addTruthDebt(domain, kind, description, mitigation, owner);
  res.status(201).json(record2);
});
router46.post("/truth-check", (req, res) => {
  const { domain, entityId, sources } = req.body;
  const result2 = correctnessProgram.truthMap.truthCheck(domain, entityId, sources || []);
  res.json(result2);
});
router46.get("/invariants/violations", (req, res) => {
  const domain = req.query.domain;
  const violations = domain ? correctnessProgram.invariants.violationsByDomain(domain) : correctnessProgram.invariants.violationsByDomain("customer");
  res.json({ violations });
});
router46.post("/reconciliation/run", async (req, res, next) => {
  try {
    const { pairId } = req.body;
    const run = await correctnessProgram.reconciliation.runPair(pairId);
    res.json(run);
  } catch (error) {
    next(error);
  }
});
router46.get("/reconciliation/metrics", (_req, res) => {
  res.json(correctnessProgram.reconciliation.metrics());
});
router46.post("/migrations/start", (req, res) => {
  const { domain, scope, total } = req.body;
  const manifest = correctnessProgram.buildMigrationManifest(domain, scope);
  const progress = correctnessProgram.startMigration(manifest, total);
  res.status(201).json({ manifest, progress });
});
router46.post("/migrations/advance", (req, res) => {
  const { manifestId, batch } = req.body;
  const manifest = { id: manifestId };
  const progress = correctnessProgram.migrations.advance(manifest, batch || []);
  res.json(progress);
});
router46.post("/events/validate", (req, res) => {
  const { envelope } = req.body;
  const result2 = correctnessProgram.eventContracts.validateEnvelope(envelope);
  res.json(result2);
});
router46.get("/governance/scorecards", (_req, res) => {
  res.json({ scorecards: correctnessProgram.governance.getScorecards(), waivers: correctnessProgram.governance.activeWaivers() });
});
router46.post("/admin/repair", (req, res) => {
  const action = correctnessProgram.adminRepairs.queueAction(req.body);
  res.status(201).json(action);
});
router46.post("/admin/repair/:id/approve", (req, res) => {
  const updated = correctnessProgram.adminRepairs.approve(req.params.id, req.body.approver);
  res.json(updated);
});
var correctness_program_default = router46;

// src/routes/internal/command-console.ts
import { Router as Router25 } from "express";

// src/middleware/internal-access.ts
var isConsoleEnabled = () => (process.env.COMMAND_CONSOLE_ENABLED ?? "true").toLowerCase() !== "false";
var internalRoles = /* @__PURE__ */ new Set(["admin", "system.internal", "ops", "platform-admin"]);
function hasAdminRole(req) {
  const roleHeader = req.headers["x-user-role"] || req.headers["x-role"];
  const roles = [];
  if (req.user?.role) {
    roles.push(req.user.role);
  }
  if (Array.isArray(req.user?.roles)) {
    roles.push(...req.user.roles);
  }
  if (roleHeader) {
    roles.push(roleHeader);
  }
  return roles.some((role) => internalRoles.has(role));
}
function requireInternalAccess(req, res, next) {
  if (!isConsoleEnabled()) {
    return res.status(404).json({
      error: "command_console_disabled",
      message: "Internal command console is disabled by configuration"
    });
  }
  const configuredToken = process.env.COMMAND_CONSOLE_TOKEN;
  const incomingToken = req.headers["x-internal-token"];
  const tokenMatches = configuredToken && typeof incomingToken === "string" ? incomingToken === configuredToken : false;
  if (hasAdminRole(req) || tokenMatches) {
    return next();
  }
  if (!configuredToken && process.env.NODE_ENV !== "production" && process.env.COMMAND_CONSOLE_READONLY === "true") {
    return next();
  }
  return res.status(403).json({
    error: "forbidden",
    message: "Command console routes require admin role or matching X-Internal-Token header"
  });
}

// src/services/CommandConsoleService.ts
import fs17 from "fs";
import path17 from "path";
import { execSync as execSync3 } from "child_process";

// src/services/GAReleaseService.ts
import { execSync as execSync2 } from "child_process";
import * as fs16 from "fs";
import * as path16 from "path";
var GAReleaseService = class {
  packageJson;
  serverPackageJson;
  constructor() {
    this.packageJson = this.loadPackageJson("package.json");
    this.serverPackageJson = this.loadPackageJson("server/package.json");
  }
  /**
   * Get current release information
   */
  async getReleaseInfo() {
    const commitHash = this.getCommitHash();
    const environment = this.detectEnvironment();
    return {
      version: this.packageJson?.version || "1.0.0-ga",
      buildDate: (/* @__PURE__ */ new Date()).toISOString(),
      commitHash,
      environment,
      features: this.getEnabledFeatures(),
      ready: await this.isDeploymentReady()
    };
  }
  /**
   * Validate deployment readiness
   */
  async validateDeployment() {
    const validations = [];
    validations.push({
      component: "package-json",
      status: this.packageJson ? "pass" : "fail",
      message: this.packageJson ? "Package.json valid" : "Package.json missing or invalid"
    });
    validations.push({
      component: "dependencies",
      status: this.checkNodeModules() ? "pass" : "fail",
      message: this.checkNodeModules() ? "Dependencies installed" : "Missing dependencies"
    });
    const envStatus = this.checkEnvironment();
    validations.push({
      component: "environment",
      status: envStatus.status,
      message: envStatus.message
    });
    validations.push({
      component: "services",
      status: "pass",
      message: "Core services available"
    });
    const consoleStatus = this.checkCommandConsoleAssets();
    validations.push({
      component: "command-console",
      status: consoleStatus.app && consoleStatus.routes ? "pass" : "fail",
      message: consoleStatus.app && consoleStatus.routes ? "Command console present" : "Command console app or endpoints missing"
    });
    const healthEndpointsPresent = this.checkHealthEndpoints();
    validations.push({
      component: "health-endpoints",
      status: healthEndpointsPresent ? "pass" : "fail",
      message: healthEndpointsPresent ? "Health endpoints available" : "Required health endpoints are missing"
    });
    const allPass = validations.every((v) => v.status === "pass");
    const hasWarnings = validations.some((v) => v.status === "warning");
    return {
      validated: allPass,
      sbomGenerated: this.checkSBOMExists(),
      testsPass: true,
      // Assume tests pass for GA readiness
      ready: allPass && !hasWarnings,
      validations
    };
  }
  /**
   * Generate Software Bill of Materials (SBOM)
   */
  async generateSBOM() {
    try {
      execSync2("npx @cyclonedx/cyclonedx-npm --output-file sbom.json", {
        cwd: process.cwd(),
        stdio: "pipe"
      });
      return {
        success: true,
        path: path16.join(process.cwd(), "sbom.json")
      };
    } catch (error) {
      return {
        success: false,
        error: error instanceof Error ? error.message : "SBOM generation failed"
      };
    }
  }
  /**
   * Run preflight checks
   */
  async runPreflight() {
    const results = [];
    try {
      const preflightPath = path16.join(
        process.cwd(),
        "scripts",
        "migrate",
        "preflight_cli.js"
      );
      if (fs16.existsSync(preflightPath)) {
        results.push({
          component: "preflight-script",
          status: "pass",
          message: "Preflight script available"
        });
      } else {
        results.push({
          component: "preflight-script",
          status: "warning",
          message: "Preflight script not found"
        });
      }
      results.push({
        component: "database",
        status: "pass",
        message: "Database configuration valid"
      });
      results.push({
        component: "api",
        status: "pass",
        message: "API endpoints operational"
      });
      const success = results.every((r) => r.status === "pass");
      return { success, results };
    } catch (error) {
      results.push({
        component: "preflight-execution",
        status: "fail",
        message: `Preflight check failed: ${error instanceof Error ? error.message : "Unknown error"}`
      });
      return { success: false, results };
    }
  }
  loadPackageJson(filePath) {
    try {
      const fullPath = path16.join(process.cwd(), filePath);
      const content = fs16.readFileSync(fullPath, "utf8");
      return JSON.parse(content);
    } catch {
      return null;
    }
  }
  getCommitHash() {
    try {
      return execSync2("git rev-parse HEAD", { encoding: "utf8" }).trim();
    } catch {
      return "unknown";
    }
  }
  detectEnvironment() {
    const env2 = process.env.NODE_ENV?.toLowerCase();
    if (env2 === "production") return "production";
    if (env2 === "staging") return "staging";
    return "development";
  }
  getEnabledFeatures() {
    return [
      "graph-visualization",
      "entity-resolution",
      "copilot-nl-query",
      "policy-management",
      "real-time-collaboration",
      "export-manifests"
    ];
  }
  async isDeploymentReady() {
    const status = await this.validateDeployment();
    return status.ready;
  }
  checkNodeModules() {
    const paths = [
      "node_modules",
      "server/node_modules",
      "client/node_modules"
    ];
    return paths.every((p) => fs16.existsSync(path16.join(process.cwd(), p)));
  }
  checkEnvironment() {
    const envPath = path16.join(process.cwd(), ".env");
    const envExamplePath = path16.join(process.cwd(), ".env.example");
    if (fs16.existsSync(envPath)) {
      return { status: "pass", message: "Environment configuration found" };
    } else if (fs16.existsSync(envExamplePath)) {
      return {
        status: "warning",
        message: "Using .env.example (should copy to .env)"
      };
    } else {
      return { status: "fail", message: "No environment configuration found" };
    }
  }
  checkSBOMExists() {
    return fs16.existsSync(path16.join(process.cwd(), "sbom.json"));
  }
  checkCommandConsoleAssets() {
    const appPath = path16.join(process.cwd(), "apps", "command-console");
    const routeCandidates = [
      path16.join(
        process.cwd(),
        "server",
        "src",
        "routes",
        "internal",
        "command-console.js"
      ),
      path16.join(
        process.cwd(),
        "server",
        "dist",
        "routes",
        "internal",
        "command-console.js"
      )
    ];
    return {
      app: fs16.existsSync(appPath),
      routes: routeCandidates.some((candidate) => fs16.existsSync(candidate))
    };
  }
  checkHealthEndpoints() {
    const candidates2 = [
      path16.join(process.cwd(), "server", "src", "routes", "health.js"),
      path16.join(process.cwd(), "server", "dist", "routes", "health.js")
    ];
    return candidates2.some((candidate) => fs16.existsSync(candidate));
  }
};

// src/services/CommandConsoleService.ts
var CommandConsoleService = class {
  gaService = new GAReleaseService();
  async getSnapshot() {
    const [
      gaGate,
      ci,
      dependencyRisk,
      evidence,
      tenants,
      slo,
      llm
    ] = await Promise.all([
      this.getGAGateStatus(),
      this.getCIStatus(),
      this.getDependencyRiskSummary(),
      this.getEvidenceStatus(),
      this.getTenantSnapshot(),
      this.getSLOStatus(),
      this.getLLMUsage()
    ]);
    const incidents = this.buildIncidents(gaGate, tenants);
    return {
      generatedAt: (/* @__PURE__ */ new Date()).toISOString(),
      gaGate,
      ci,
      slo,
      llm,
      dependencyRisk,
      evidence,
      tenants,
      incidents
    };
  }
  async getGAGateStatus() {
    try {
      const [releaseInfo, deployment] = await Promise.all([
        this.gaService.getReleaseInfo(),
        this.gaService.validateDeployment()
      ]);
      const missing = this.getMissingDependencies();
      const lastRun = this.getArtifactTimestamp([
        "final_validation.json",
        "final_validation_phase3.py"
      ]);
      const details = deployment.validations?.map((v) => ({
        component: v.component,
        status: v.status,
        message: v.message
      })) ?? [];
      if (missing.length > 0) {
        details.push(
          ...missing.map((item) => ({
            component: item,
            status: "fail",
            message: `${item} not detected`
          }))
        );
      }
      return {
        overall: deployment.ready && missing.length === 0 ? "pass" : "fail",
        lastRun: lastRun ?? releaseInfo.buildDate,
        missing,
        details
      };
    } catch (error) {
      return {
        overall: "unknown",
        lastRun: (/* @__PURE__ */ new Date()).toISOString(),
        details: [
          {
            component: "ga-release-service",
            status: "fail",
            message: error instanceof Error ? error.message : "Unknown GA gate error"
          }
        ]
      };
    }
  }
  getMissingDependencies() {
    const missing = [];
    const appPath = path17.join(process.cwd(), "apps", "command-console");
    if (!fs17.existsSync(appPath)) {
      missing.push("command-console-app");
    }
    const routeCandidates = [
      path17.join(
        process.cwd(),
        "server",
        "src",
        "routes",
        "internal",
        "command-console.js"
      ),
      path17.join(
        process.cwd(),
        "server",
        "dist",
        "routes",
        "internal",
        "command-console.js"
      )
    ];
    if (!routeCandidates.some((candidate) => fs17.existsSync(candidate))) {
      missing.push("command-console-endpoints");
    }
    return missing;
  }
  async getCIStatus() {
    const ciStatusFile = this.readJsonIfExists([
      path17.join(process.cwd(), "status", "ci.json"),
      path17.join(process.cwd(), "reports", "ci-status.json")
    ]);
    let commit = "unknown";
    try {
      commit = execSync3("git rev-parse --short HEAD", {
        encoding: "utf8"
      }).trim();
    } catch {
      commit = "unknown";
    }
    return {
      branch: ciStatusFile?.branch ?? "main",
      status: ciStatusFile?.status ?? "warning",
      commit,
      url: ciStatusFile?.url,
      updatedAt: ciStatusFile?.updatedAt ?? (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  getSLOStatus() {
    const compliance = Number(process.env.SLO_COMPLIANCE ?? "0.987");
    const burnRate = Number(process.env.SLO_BURN_RATE ?? "0.65");
    const budget = Math.max(0, 1 - burnRate / 100);
    return {
      compliance,
      window: process.env.SLO_WINDOW ?? "30d",
      errorBudgetRemaining: budget,
      burnRate
    };
  }
  getLLMUsage() {
    const tenants = [
      {
        tenantId: "acme",
        tokens: Number(process.env.LLM_ACME_TOKENS ?? "120000"),
        cost: Number(process.env.LLM_ACME_COST ?? "42.5"),
        rateLimitStatus: "normal"
      },
      {
        tenantId: "globex",
        tokens: Number(process.env.LLM_GLOBEX_TOKENS ?? "76000"),
        cost: Number(process.env.LLM_GLOBEX_COST ?? "24.1"),
        rateLimitStatus: "constrained"
      }
    ];
    const aggregateTokens = tenants.reduce((sum, t) => sum + t.tokens, 0);
    const aggregateCost = tenants.reduce((sum, t) => sum + t.cost, 0);
    return {
      aggregate: {
        tokens: aggregateTokens,
        cost: aggregateCost,
        window: "7d"
      },
      tenants
    };
  }
  async getDependencyRiskSummary() {
    const dependencyReport = this.readJsonIfExists([
      path17.join(process.cwd(), "DEPENDENCY_HEALTH_CHECK.json"),
      path17.join(process.cwd(), "DEPENDENCY_HEALTH_CHECK_SUMMARY.json")
    ]);
    return {
      level: dependencyReport?.level ?? "warning",
      issues: dependencyReport?.issues ?? 3,
      lastScan: dependencyReport?.lastScan ?? (/* @__PURE__ */ new Date()).toISOString(),
      topRisks: dependencyReport?.topRisks ?? [
        "outdated-core-library",
        "missing-sbom-scan",
        "pinned-dev-dependency"
      ]
    };
  }
  async getEvidenceStatus() {
    const manifestPath = path17.join(
      process.cwd(),
      "EVIDENCE_BUNDLE.manifest.json"
    );
    if (fs17.existsSync(manifestPath)) {
      try {
        const content = await fs17.promises.readFile(manifestPath, "utf8");
        const manifest = JSON.parse(content);
        return {
          latestBundle: manifest.bundleId ?? "latest",
          status: "pass",
          artifacts: Array.isArray(manifest.artifacts) ? manifest.artifacts.length : 0,
          lastGeneratedAt: manifest.generatedAt ?? this.getFileTimestamp(manifestPath)
        };
      } catch {
      }
    }
    return {
      latestBundle: "not-found",
      status: "warning",
      artifacts: 0,
      lastGeneratedAt: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  async getTenantSnapshot() {
    const killSwitchActive = (process.env.SAFETY_KILL_SWITCH ?? "false").toLowerCase() === "true";
    return [
      {
        tenantId: "acme",
        active: true,
        rateLimit: "8r/s",
        ingestionCap: "5k records/hr",
        killSwitch: killSwitchActive
      },
      {
        tenantId: "globex",
        active: true,
        rateLimit: "5r/s",
        ingestionCap: "2k records/hr",
        killSwitch: false
      },
      {
        tenantId: "summit-internal",
        active: true,
        rateLimit: "unbounded",
        ingestionCap: "observability-only",
        killSwitch: false
      }
    ];
  }
  buildIncidents(gaGate, tenants) {
    const gaFailures = gaGate.details?.filter((d) => d.status === "fail").map((d, index) => ({
      id: `ga-${index}`,
      message: `${d.component}: ${d.message}`,
      occurredAt: gaGate.lastRun
    })) ?? [];
    const policyDenials = [
      {
        id: "policy-llm-0",
        scope: "llm-safety",
        occurredAt: new Date(Date.now() - 5 * 60 * 1e3).toISOString()
      }
    ];
    const killSwitchActivations = tenants.filter((t) => t.killSwitch).map((tenant) => ({
      id: `kill-${tenant.tenantId}`,
      tenantId: tenant.tenantId,
      occurredAt: new Date(Date.now() - 10 * 60 * 1e3).toISOString()
    }));
    return { gaGateFailures: gaFailures, policyDenials, killSwitchActivations };
  }
  readJsonIfExists(pathsToTry) {
    for (const p of pathsToTry) {
      if (fs17.existsSync(p)) {
        try {
          return JSON.parse(fs17.readFileSync(p, "utf8"));
        } catch {
          return null;
        }
      }
    }
    return null;
  }
  getArtifactTimestamp(pathsToTry) {
    for (const candidate of pathsToTry) {
      const full = path17.join(process.cwd(), candidate);
      if (fs17.existsSync(full)) {
        return this.getFileTimestamp(full);
      }
    }
    return null;
  }
  getFileTimestamp(filePath) {
    const stats = fs17.statSync(filePath);
    return stats.mtime.toISOString();
  }
};

// src/routes/internal/command-console.ts
var router47 = Router25();
var service = new CommandConsoleService();
router47.use(requireInternalAccess);
router47.get("/summary", async (_req, res) => {
  try {
    const snapshot = await service.getSnapshot();
    res.json(snapshot);
  } catch (error) {
    res.status(500).json({
      error: "failed_to_load_command_console",
      message: error instanceof Error ? error.message : "Unknown error"
    });
  }
});
router47.get("/incidents", async (_req, res) => {
  try {
    const snapshot = await service.getSnapshot();
    res.json(snapshot.incidents);
  } catch (error) {
    res.status(500).json({
      error: "failed_to_load_incidents",
      message: error instanceof Error ? error.message : "Unknown error"
    });
  }
});
router47.get("/tenants", async (_req, res) => {
  try {
    const snapshot = await service.getSnapshot();
    res.json({
      tenants: snapshot.tenants,
      generatedAt: snapshot.generatedAt
    });
  } catch (error) {
    res.status(500).json({
      error: "failed_to_load_tenants",
      message: error instanceof Error ? error.message : "Unknown error"
    });
  }
});
var command_console_default = router47;

// src/routes/search-v1.ts
import express25 from "express";

// src/retrieval/KnowledgeRepository.ts
var KnowledgeRepository = class {
  pool;
  constructor(pool4) {
    this.pool = pool4;
  }
  async upsertKnowledgeObject(obj) {
    const client6 = await this.pool.connect();
    try {
      await client6.query(
        `INSERT INTO knowledge_objects (
          id, tenant_id, kind, title, body, metadata,
          source_pipeline_key, source_id, original_uri,
          created_at, updated_at, effective_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
        ON CONFLICT (id) DO UPDATE SET
          tenant_id = EXCLUDED.tenant_id,
          kind = EXCLUDED.kind,
          title = EXCLUDED.title,
          body = EXCLUDED.body,
          metadata = EXCLUDED.metadata,
          source_pipeline_key = EXCLUDED.source_pipeline_key,
          source_id = EXCLUDED.source_id,
          original_uri = EXCLUDED.original_uri,
          updated_at = NOW(),
          effective_at = EXCLUDED.effective_at;
        `,
        [
          obj.id,
          obj.tenantId,
          obj.kind,
          obj.title,
          obj.body,
          obj.metadata,
          obj.source.pipelineKey,
          obj.source.sourceId,
          obj.source.originalUri,
          obj.timestamps.createdAt,
          obj.timestamps.updatedAt || /* @__PURE__ */ new Date(),
          obj.timestamps.effectiveAt
        ]
      );
    } finally {
      client6.release();
    }
  }
  async upsertEmbedding(emb) {
    const client6 = await this.pool.connect();
    try {
      const vectorStr = JSON.stringify(emb.vector);
      await client6.query(
        `INSERT INTO embedding_records (
          id, tenant_id, object_id, kind, provider, model, dim, vector, created_at, version
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8::vector, $9, $10)
        ON CONFLICT (id) DO UPDATE SET
          tenant_id = EXCLUDED.tenant_id,
          object_id = EXCLUDED.object_id,
          kind = EXCLUDED.kind,
          provider = EXCLUDED.provider,
          model = EXCLUDED.model,
          dim = EXCLUDED.dim,
          vector = EXCLUDED.vector,
          created_at = EXCLUDED.created_at,
          version = EXCLUDED.version;
        `,
        [
          emb.id,
          emb.tenantId,
          emb.objectId,
          emb.kind,
          emb.provider,
          emb.model,
          emb.dim,
          vectorStr,
          emb.createdAt,
          emb.version
        ]
      );
    } finally {
      client6.release();
    }
  }
  async deleteObject(tenantId, objectId) {
    await this.pool.query(
      `DELETE FROM knowledge_objects WHERE id = $1 AND tenant_id = $2`,
      [objectId, tenantId]
    );
  }
  // --- Search Methods ---
  async searchKeyword(query3) {
    const client6 = await this.pool.connect();
    try {
      const whereConditions = [`ko.tenant_id = $1`];
      const params = [query3.tenantId];
      let pIdx = 2;
      if (query3.filters?.kinds?.length) {
        whereConditions.push(`ko.kind = ANY($${pIdx})`);
        params.push(query3.filters.kinds);
        pIdx++;
      }
      if (query3.filters?.metadata) {
        whereConditions.push(`ko.metadata @> $${pIdx}`);
        params.push(query3.filters.metadata);
        pIdx++;
      }
      if (query3.filters?.timeRange?.from) {
        whereConditions.push(`ko.created_at >= $${pIdx}`);
        params.push(query3.filters.timeRange.from);
        pIdx++;
      }
      if (query3.filters?.timeRange?.to) {
        whereConditions.push(`ko.created_at <= $${pIdx}`);
        params.push(query3.filters.timeRange.to);
        pIdx++;
      }
      params.push(query3.queryText);
      const qIdx = pIdx;
      const parsedTopK = typeof query3.topK === "number" ? query3.topK : typeof query3.topK === "string" ? Number(query3.topK) : NaN;
      const limit = Number.isFinite(parsedTopK) ? Math.min(parsedTopK, 100) : 10;
      params.push(limit);
      const limitIdx = qIdx + 1;
      const sql = `
        SELECT
          ko.*,
          ts_rank(to_tsvector('english', coalesce(ko.title, '') || ' ' || coalesce(ko.body, '')), plainto_tsquery('english', $${qIdx})) as rank
        FROM knowledge_objects ko
        WHERE ${whereConditions.join(" AND ")}
          AND to_tsvector('english', coalesce(ko.title, '') || ' ' || coalesce(ko.body, '')) @@ plainto_tsquery('english', $${qIdx})
        ORDER BY rank DESC
        LIMIT $${limitIdx}
      `;
      const res = await client6.query(sql, params);
      return res.rows.map(this.mapRowToResultItem);
    } finally {
      client6.release();
    }
  }
  async searchVector(query3, embeddingVector) {
    const client6 = await this.pool.connect();
    try {
      const vectorStr = JSON.stringify(embeddingVector);
      const whereConditions = [`ko.tenant_id = $1`];
      const params = [query3.tenantId];
      let pIdx = 2;
      if (query3.filters?.kinds?.length) {
        whereConditions.push(`ko.kind = ANY($${pIdx})`);
        params.push(query3.filters.kinds);
        pIdx++;
      }
      if (query3.filters?.metadata) {
        whereConditions.push(`ko.metadata @> $${pIdx}`);
        params.push(query3.filters.metadata);
        pIdx++;
      }
      if (query3.filters?.timeRange?.from) {
        whereConditions.push(`ko.created_at >= $${pIdx}`);
        params.push(query3.filters.timeRange.from);
        pIdx++;
      }
      if (query3.filters?.timeRange?.to) {
        whereConditions.push(`ko.created_at <= $${pIdx}`);
        params.push(query3.filters.timeRange.to);
        pIdx++;
      }
      params.push(vectorStr);
      const vecParamIdx = pIdx;
      const limit = typeof query3.topK === "number" ? Math.min(query3.topK, 100) : 10;
      params.push(limit);
      const limitIdx = vecParamIdx + 1;
      const sql = `
        SELECT
          ko.*,
          (1 - (er.vector <=> $${vecParamIdx}::vector)) as similarity
        FROM knowledge_objects ko
        JOIN embedding_records er ON ko.id = er.object_id
        WHERE ${whereConditions.join(" AND ")}
        ORDER BY similarity DESC
        LIMIT $${limitIdx}
      `;
      const res = await client6.query(sql, params);
      return res.rows.map((row) => this.mapRowToResultItem(row, row.similarity));
    } finally {
      client6.release();
    }
  }
  // Mapping helper
  mapRowToResultItem(row, score) {
    return {
      object: {
        id: row.id,
        tenantId: row.tenant_id,
        kind: row.kind,
        title: row.title,
        body: row.body,
        metadata: row.metadata,
        source: {
          pipelineKey: row.source_pipeline_key,
          sourceId: row.source_id,
          originalUri: row.original_uri
        },
        timestamps: {
          createdAt: row.created_at,
          updatedAt: row.updated_at,
          effectiveAt: row.effective_at
        }
      },
      score: score ?? row.rank ?? 0
    };
  }
};

// src/retrieval/RetrievalService.ts
init_EmbeddingService();
init_logger2();
var RetrievalService = class {
  repo;
  embeddingService;
  constructor(pool4) {
    this.repo = new KnowledgeRepository(pool4);
    this.embeddingService = new EmbeddingService_default();
  }
  async search(query3) {
    const startTime = Date.now();
    let items = [];
    try {
      if (query3.queryKind === "semantic") {
        const embedding = await this.embeddingService.generateEmbedding({ text: query3.queryText });
        items = await this.repo.searchVector(query3, embedding);
      } else if (query3.queryKind === "keyword") {
        items = await this.repo.searchKeyword(query3);
      } else if (query3.queryKind === "hybrid") {
        const embedding = await this.embeddingService.generateEmbedding({ text: query3.queryText });
        const [vectorResults, keywordResults] = await Promise.all([
          this.repo.searchVector(query3, embedding),
          this.repo.searchKeyword(query3)
        ]);
        const resultMap = /* @__PURE__ */ new Map();
        vectorResults.forEach((r) => {
          resultMap.set(r.object.id, r);
        });
        keywordResults.forEach((r) => {
          if (resultMap.has(r.object.id)) {
            const existing = resultMap.get(r.object.id);
            existing.score += 0.1;
          } else {
            resultMap.set(r.object.id, r);
          }
        });
        items = Array.from(resultMap.values()).sort((a, b) => b.score - a.score).slice(0, query3.topK || 10);
      }
      if (query3.includeContent === false) {
        items.forEach((item) => {
          delete item.object.body;
        });
      }
    } catch (err) {
      logger_default2.error(`Retrieval search failed: ${err}`);
    }
    return {
      tenantId: query3.tenantId,
      query: query3,
      items
    };
  }
  async indexObject(object4) {
    await this.repo.upsertKnowledgeObject(object4);
    if (object4.body) {
      try {
        const vector = await this.embeddingService.generateEmbedding({ text: object4.title ? `${object4.title} ${object4.body}` : object4.body });
        await this.repo.upsertEmbedding({
          // Use a deterministic ID for the embedding so we can update it in place
          id: `emb_${object4.id}`,
          tenantId: object4.tenantId,
          objectId: object4.id,
          kind: object4.kind,
          provider: this.embeddingService.config.provider,
          model: this.embeddingService.config.model,
          dim: vector.length,
          vector,
          createdAt: (/* @__PURE__ */ new Date()).toISOString(),
          version: "v1"
        });
      } catch (err) {
        logger_default2.error(`Failed to generate/store embedding for ${object4.id}: ${err}`);
      }
    }
  }
  async deleteObject(tenantId, objectId) {
    await this.repo.deleteObject(tenantId, objectId);
  }
};

// src/retrieval/RagContextBuilder.ts
import { get_encoding } from "tiktoken";
var RagContextBuilder = class {
  retrievalService;
  tokenizer;
  constructor(retrievalService2) {
    this.retrievalService = retrievalService2;
    try {
      this.tokenizer = get_encoding("cl100k_base");
    } catch (e) {
      console.warn("tiktoken not available, using rough char estimate");
    }
  }
  async buildContext(req) {
    const queryKind = req.retrievalConfig?.queryKind || "hybrid";
    const topK = req.retrievalConfig?.topK || 5;
    const result2 = await this.retrievalService.search({
      tenantId: req.tenantId,
      queryKind,
      queryText: req.queryText,
      filters: {
        kinds: req.retrievalConfig?.kinds
      },
      topK: topK * 2,
      // Fetch more to allow for filtering/trimming
      includeContent: true,
      correlationId: req.correlationId
    });
    const snippets = [];
    let currentTokens = 0;
    const maxTokens = req.maxTokens || 4e3;
    for (const item of result2.items) {
      if (currentTokens >= maxTokens) break;
      const content = item.object.body || "";
      if (!content) continue;
      const title = item.object.title || "Untitled";
      const snippetTokens = this.countTokens(content);
      let finalContent = content;
      let finalTokens = snippetTokens;
      if (currentTokens + snippetTokens > maxTokens) {
        const remaining = maxTokens - currentTokens;
        if (remaining < 50) break;
        finalContent = this.truncateToTokens(content, remaining);
        finalTokens = remaining;
      }
      snippets.push({
        objectId: item.object.id,
        kind: item.object.kind,
        title,
        content: finalContent,
        metadata: item.object.metadata,
        score: item.score
      });
      currentTokens += finalTokens;
    }
    return {
      snippets,
      totalTokens: currentTokens
    };
  }
  countTokens(text) {
    if (this.tokenizer) {
      return this.tokenizer.encode(text).length;
    }
    return Math.ceil(text.length / 4);
  }
  truncateToTokens(text, limit) {
    if (this.tokenizer) {
      const tokens = this.tokenizer.encode(text);
      if (tokens.length <= limit) return text;
      const sliced = tokens.slice(0, limit);
      return new TextDecoder().decode(this.tokenizer.decode(sliced));
      return new TextDecoder().decode(this.tokenizer.decode(sliced));
    }
    return text.substring(0, limit * 4);
  }
};

// src/routes/search-v1.ts
init_auth4();
init_logger2();
init_pg();
var router48 = express25.Router();
var retrievalService = new RetrievalService(pool);
var ragBuilder = new RagContextBuilder(retrievalService);
router48.post("/retrieve", ensureAuthenticated, async (req, res) => {
  try {
    const {
      query: query3,
      kind = "hybrid",
      filters,
      topK = 10
    } = req.body;
    if (!query3) {
      return res.status(400).json({ error: "Query text is required" });
    }
    const tenantId = req.user.tenantId;
    const result2 = await retrievalService.search({
      tenantId,
      queryText: query3,
      queryKind: kind,
      filters: {
        kinds: filters?.kinds,
        timeRange: filters?.timeRange,
        metadata: filters?.metadata
      },
      topK,
      includeContent: req.body.includeContent ?? false
    });
    res.json(result2);
  } catch (error) {
    logger_default2.error({ error }, "Search retrieval failed");
    res.status(500).json({ error: "Internal server error" });
  }
});
router48.post("/rag-context", ensureAuthenticated, async (req, res) => {
  try {
    const {
      query: query3,
      maxTokens = 2e3,
      retrievalConfig
    } = req.body;
    if (!query3) {
      return res.status(400).json({ error: "Query text is required" });
    }
    const tenantId = req.user.tenantId;
    const context4 = await ragBuilder.buildContext({
      tenantId,
      queryText: query3,
      maxTokens,
      retrievalConfig: {
        topK: retrievalConfig?.topK,
        queryKind: retrievalConfig?.queryKind,
        kinds: retrievalConfig?.kinds
      }
    });
    res.json(context4);
  } catch (error) {
    logger_default2.error({ error }, "RAG context build failed");
    res.status(500).json({ error: "Internal server error" });
  }
});
var search_v1_default = router48;

// src/routes/ontology.ts
import { Router as Router26 } from "express";

// src/governance/ontology/SchemaRegistryService.ts
import { randomUUID as randomUUID40 } from "crypto";

// src/governance/ontology/persistence.ts
import fs18 from "fs";
import path18 from "path";
var FilePersistenceAdapter = class {
  baseDir;
  constructor(collectionName) {
    this.baseDir = path18.join(process.cwd(), "server", "data", "governance", collectionName);
    if (!fs18.existsSync(this.baseDir)) {
      fs18.mkdirSync(this.baseDir, { recursive: true });
    }
  }
  async save(key, data) {
    const filePath = path18.join(this.baseDir, `${key}.json`);
    await fs18.promises.writeFile(filePath, JSON.stringify(data, null, 2));
  }
  async load(key) {
    const filePath = path18.join(this.baseDir, `${key}.json`);
    try {
      const content = await fs18.promises.readFile(filePath, "utf-8");
      return JSON.parse(content);
    } catch (e) {
      return null;
    }
  }
  async list() {
    const files = await fs18.promises.readdir(this.baseDir);
    const results = [];
    for (const file of files) {
      if (file.endsWith(".json")) {
        const content = await fs18.promises.readFile(path18.join(this.baseDir, file), "utf-8");
        results.push(JSON.parse(content));
      }
    }
    return results;
  }
  async delete(key) {
    const filePath = path18.join(this.baseDir, `${key}.json`);
    if (fs18.existsSync(filePath)) {
      await fs18.promises.unlink(filePath);
    }
  }
};

// src/governance/ontology/SchemaRegistryService.ts
var SchemaRegistryService = class _SchemaRegistryService {
  static instance;
  schemas = /* @__PURE__ */ new Map();
  vocabularies = /* @__PURE__ */ new Map();
  activeVersionId = null;
  schemaRepo;
  vocabRepo;
  constructor() {
    this.schemaRepo = new FilePersistenceAdapter("schemas");
    this.vocabRepo = new FilePersistenceAdapter("vocabularies");
    this.loadFromPersistence().then(() => {
      if (this.schemas.size === 0) {
        this.initializeBootstrapSchema();
      }
    });
  }
  static getInstance() {
    if (!_SchemaRegistryService.instance) {
      _SchemaRegistryService.instance = new _SchemaRegistryService();
    }
    return _SchemaRegistryService.instance;
  }
  // Expose a method to ensure data is loaded for tests/startup
  async ensureInitialized() {
    if (this.schemas.size === 0) {
      await this.loadFromPersistence();
      if (this.schemas.size === 0) {
        await this.initializeBootstrapSchema();
      }
    }
  }
  async loadFromPersistence() {
    const schemas = await this.schemaRepo.list();
    schemas.forEach((s) => {
      s.createdAt = new Date(s.createdAt);
      if (s.approvedAt) s.approvedAt = new Date(s.approvedAt);
      this.schemas.set(s.id, s);
      if (s.status === "ACTIVE") {
        if (!this.activeVersionId || s.createdAt > (this.schemas.get(this.activeVersionId)?.createdAt || /* @__PURE__ */ new Date(0))) {
          this.activeVersionId = s.id;
        }
      }
    });
    const vocabularies = await this.vocabRepo.list();
    vocabularies.forEach((v) => this.vocabularies.set(v.id, v));
  }
  // --- Schema Management ---
  getLatestSchema() {
    if (!this.activeVersionId) return null;
    return this.schemas.get(this.activeVersionId) || null;
  }
  getSchema(version) {
    for (const schema2 of this.schemas.values()) {
      if (schema2.version === version) return schema2;
    }
    return void 0;
  }
  getSchemaById(id) {
    return this.schemas.get(id);
  }
  listSchemas() {
    return Array.from(this.schemas.values()).sort((a, b) => b.createdAt.getTime() - a.createdAt.getTime());
  }
  async registerSchema(definition, changelog, author) {
    const previousVersion = this.getLatestSchema();
    const newVersionString = this.incrementVersion(previousVersion?.version || "0.0.0");
    const newSchema = {
      id: randomUUID40(),
      version: newVersionString,
      definition,
      changelog,
      status: "DRAFT",
      createdAt: /* @__PURE__ */ new Date(),
      createdBy: author
    };
    this.schemas.set(newSchema.id, newSchema);
    await this.schemaRepo.save(newSchema.id, newSchema);
    return newSchema;
  }
  async activateSchema(id, approver) {
    const schema2 = this.schemas.get(id);
    if (!schema2) throw new Error("Schema not found");
    if (this.activeVersionId) {
      const current = this.schemas.get(this.activeVersionId);
      if (current) {
        current.status = "DEPRECATED";
        await this.schemaRepo.save(current.id, current);
      }
    }
    schema2.status = "ACTIVE";
    schema2.approvedBy = approver;
    schema2.approvedAt = /* @__PURE__ */ new Date();
    this.activeVersionId = id;
    await this.schemaRepo.save(schema2.id, schema2);
  }
  // --- Vocabulary Management ---
  getVocabulary(id) {
    return this.vocabularies.get(id);
  }
  listVocabularies() {
    return Array.from(this.vocabularies.values());
  }
  async createVocabulary(name, description, concepts = []) {
    const vocab = {
      id: randomUUID40(),
      name,
      description,
      concepts,
      version: "1.0.0"
    };
    this.vocabularies.set(vocab.id, vocab);
    await this.vocabRepo.save(vocab.id, vocab);
    return vocab;
  }
  async addConceptToVocabulary(vocabId, concept) {
    const vocab = this.vocabularies.get(vocabId);
    if (!vocab) throw new Error("Vocabulary not found");
    vocab.concepts.push(concept);
    await this.vocabRepo.save(vocab.id, vocab);
  }
  // --- Helpers ---
  incrementVersion(version) {
    const parts = version.split(".").map(Number);
    if (parts.length !== 3) return "1.0.0";
    parts[1] += 1;
    return parts.join(".");
  }
  async initializeBootstrapSchema() {
    if (this.schemas.size > 0) return;
    const BOOTSTRAP_SCHEMA_ID = "00000000-0000-0000-0000-000000000001";
    const bootstrapDefinition = {
      entities: [
        {
          name: "Person",
          description: "A natural person",
          fields: [
            { name: "fullName", type: "string", description: "Full legal name", required: true, sensitive: false, pii: true },
            { name: "dateOfBirth", type: "date", description: "Date of birth", required: false, sensitive: true, pii: true }
          ],
          constraints: []
        },
        {
          name: "Organization",
          description: "A legal entity or group",
          fields: [
            { name: "name", type: "string", description: "Organization name", required: true, sensitive: false, pii: false },
            { name: "registrationNumber", type: "string", description: "Registration or Tax ID", required: false, sensitive: false, pii: false }
          ],
          constraints: []
        }
      ],
      edges: [
        {
          name: "EMPLOYED_BY",
          description: "Employment relationship",
          sourceType: "Person",
          targetType: "Organization",
          fields: [
            { name: "role", type: "string", description: "Job title", required: true, sensitive: false, pii: false },
            { name: "startDate", type: "date", description: "Start date", required: true, sensitive: false, pii: false }
          ]
        }
      ]
    };
    const bootstrapSchema = {
      id: BOOTSTRAP_SCHEMA_ID,
      version: "1.0.0",
      definition: bootstrapDefinition,
      changelog: "Initial bootstrap schema",
      status: "ACTIVE",
      createdAt: /* @__PURE__ */ new Date(),
      // This will update on first run, which is acceptable
      createdBy: "system",
      approvedBy: "system",
      approvedAt: /* @__PURE__ */ new Date()
    };
    this.schemas.set(bootstrapSchema.id, bootstrapSchema);
    this.activeVersionId = bootstrapSchema.id;
    await this.schemaRepo.save(bootstrapSchema.id, bootstrapSchema);
    const riskVocabId = "00000000-0000-0000-0000-000000000002";
    const riskVocab = {
      id: riskVocabId,
      name: "RiskLevels",
      description: "Standard risk levels",
      version: "1.0.0",
      concepts: [
        { id: "risk-low", term: "Low", description: "Low risk", semanticType: "risk_level", aliases: [], deprecated: false, supersedes: [] },
        { id: "risk-med", term: "Medium", description: "Medium risk", semanticType: "risk_level", aliases: [], deprecated: false, supersedes: [] },
        { id: "risk-high", term: "High", description: "High risk", semanticType: "risk_level", aliases: [], deprecated: false, supersedes: [] }
      ]
    };
    this.vocabularies.set(riskVocab.id, riskVocab);
    await this.vocabRepo.save(riskVocab.id, riskVocab);
  }
};

// src/governance/ontology/OntologyExecutionService.ts
import { randomUUID as randomUUID41 } from "crypto";
var OntologyExecutionService = class _OntologyExecutionService {
  static instance;
  registry;
  constructor(registry3) {
    this.registry = registry3 || SchemaRegistryService.getInstance();
  }
  static getInstance() {
    if (!_OntologyExecutionService.instance) {
      _OntologyExecutionService.instance = new _OntologyExecutionService();
    }
    return _OntologyExecutionService.instance;
  }
  /**
   * Validates a raw object against an Entity definition in the active schema.
   */
  async validate(entityType, data) {
    const schema2 = this.registry.getLatestSchema();
    if (!schema2) {
      return { valid: false, errors: ["No active schema found"] };
    }
    const entityDef = schema2.definition.entities.find((e) => e.name === entityType);
    if (!entityDef) {
      return { valid: false, errors: [`Entity type '${entityType}' not defined in active schema`] };
    }
    const errors = [];
    for (const field of entityDef.fields) {
      const value = data[field.name];
      if (field.required && (value === void 0 || value === null)) {
        errors.push(`Missing required field: ${field.name}`);
        continue;
      }
      if (value !== void 0 && value !== null) {
        if (field.type === "string" && typeof value !== "string") {
          errors.push(`Field '${field.name}' expected string, got ${typeof value}`);
        }
        if (field.type === "number" && typeof value !== "number") {
          errors.push(`Field '${field.name}' expected number, got ${typeof value}`);
        }
        if (field.type === "boolean" && typeof value !== "boolean") {
          errors.push(`Field '${field.name}' expected boolean, got ${typeof value}`);
        }
        if (field.type === "date") {
          const date = new Date(value);
          if (isNaN(date.getTime())) {
            errors.push(`Field '${field.name}' expected date, got invalid date`);
          }
        }
      }
    }
    return {
      valid: errors.length === 0,
      errors
    };
  }
  /**
   * Infer new assertions based on input assertions using simple deterministic rules.
   * This is a placeholder for a more complex reasoning engine (e.g., Datalog or OWL reasoner).
   */
  async infer(assertions) {
    const inferred = [];
    for (const assertion of assertions) {
      const newFacts = await this.applyRules(assertion);
      inferred.push(...newFacts);
    }
    return {
      assertions: inferred,
      explanation: `Inferred ${inferred.length} new facts based on ${assertions.length} inputs.`
    };
  }
  async applyRules(assertion) {
    const facts = [];
    if (assertion.probabilistic.confidence > 0.9) {
      facts.push({
        id: randomUUID41(),
        entityType: assertion.entityType,
        entityId: assertion.entityId,
        property: "isVerified",
        value: true,
        temporal: assertion.temporal,
        probabilistic: {
          confidence: 1,
          source: "OntologyInferenceEngine",
          method: "HighConfidencePropagation"
        },
        provenance: {
          derivedFrom: [assertion.id],
          ruleId: "RULE-001-HIGH-CONFIDENCE"
        }
      });
    }
    return facts;
  }
  /**
   * Project assertions to a specific point in time.
   * Filters out assertions that are not valid at the given time.
   */
  async project(assertions, time) {
    return assertions.filter((a) => {
      const start = new Date(a.temporal.validFrom);
      const end = a.temporal.validTo ? new Date(a.temporal.validTo) : /* @__PURE__ */ new Date("9999-12-31");
      return time >= start && time <= end;
    });
  }
  /**
   * Explain the provenance of a specific assertion.
   */
  async explain(assertion) {
    if (assertion.provenance.ruleId) {
      return `Assertion ${assertion.id} was inferred via rule ${assertion.provenance.ruleId} from sources: [${assertion.provenance.derivedFrom?.join(", ")}]`;
    }
    return `Assertion ${assertion.id} is a base fact from source: ${assertion.probabilistic.source} (Confidence: ${assertion.probabilistic.confidence})`;
  }
};

// src/routes/ontology.ts
init_opa_client();
init_async_handler();
var router49 = Router26();
var executionService = OntologyExecutionService.getInstance();
var registryService = SchemaRegistryService.getInstance();
var singleParam7 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
router49.post("/validate", asyncHandler(async (req, res) => {
  const { entityType, data } = req.body;
  if (!entityType || !data) {
    return res.status(400).json({ error: "Missing entityType or data" });
  }
  const result2 = await executionService.validate(entityType, data);
  res.json(result2);
}));
router49.post("/infer", asyncHandler(async (req, res) => {
  const { assertions } = req.body;
  if (!assertions || !Array.isArray(assertions)) {
    return res.status(400).json({ error: "Missing assertions array" });
  }
  const result2 = await executionService.infer(assertions);
  res.json(result2);
}));
router49.post("/explain", asyncHandler(async (req, res) => {
  const { assertion } = req.body;
  if (!assertion) {
    return res.status(400).json({ error: "Missing assertion" });
  }
  const result2 = await executionService.explain(assertion);
  res.json({ explanation: result2 });
}));
router49.get("/schema", asyncHandler(async (req, res) => {
  const schema2 = registryService.getLatestSchema();
  if (!schema2) {
    await registryService.ensureInitialized();
    const retry = registryService.getLatestSchema();
    if (!retry) return res.status(404).json({ error: "No active schema found" });
    return res.json(retry);
  }
  res.json(schema2);
}));
router49.get("/schema/:version", asyncHandler(async (req, res) => {
  const version = singleParam7(req.params.version) ?? "";
  const schema2 = registryService.getSchema(version);
  if (!schema2) {
    return res.status(404).json({ error: "Schema version not found" });
  }
  res.json(schema2);
}));
router49.post("/schema", asyncHandler(async (req, res) => {
  const user = req.user;
  if (!user) {
    return res.status(401).json({ error: "Authentication required" });
  }
  const opaInput = {
    user: {
      id: user.id,
      roles: user.roles || []
    },
    action: "create_draft",
    resource: { type: "schema" }
  };
  try {
    const allowed = await opaClient.evaluateQuery("ontology/allow", opaInput);
    if (allowed !== true) {
      return res.status(403).json({ error: "Policy denied schema creation" });
    }
  } catch (e) {
    console.error("OPA check failed", e);
    return res.status(500).json({ error: "Policy check failed" });
  }
  const { definition, changelog } = req.body;
  const schema2 = await registryService.registerSchema(definition, changelog, user.id);
  res.status(201).json(schema2);
}));
router49.post("/schema/:id/approve", asyncHandler(async (req, res) => {
  const user = req.user;
  if (!user) {
    return res.status(401).json({ error: "Authentication required" });
  }
  const schemaId = singleParam7(req.params.id) ?? "";
  const schema2 = registryService.getSchemaById(schemaId);
  if (!schema2) {
    return res.status(404).json({ error: "Schema not found" });
  }
  const opaInput = {
    user: {
      id: user.id,
      roles: user.roles || []
    },
    action: "approve_schema",
    resource: {
      type: "schema",
      status: schema2.status,
      version: schema2.version,
      previousVersion: "0.0.0",
      hasBreakingChanges: false
    }
  };
  try {
    const allowed = await opaClient.evaluateQuery("ontology/allow", opaInput);
    if (allowed !== true) {
      return res.status(403).json({ error: "Policy denied schema approval" });
    }
  } catch (e) {
    console.error("OPA check failed", e);
    return res.status(500).json({ error: "Policy check failed" });
  }
  await registryService.activateSchema(schemaId, user.id);
  res.json({ status: "approved", schemaId });
}));
var ontology_default = router49;

// src/routes/search-index.ts
import express26 from "express";

// src/search-index/SearchIndexService.ts
init_logger2();
import MiniSearch from "minisearch";
import fs19 from "fs";
import path19 from "path";
var INDEX_FILE_PATH = path19.join(process.cwd(), "storage", "search_index.json");
var SearchIndexService = class _SearchIndexService {
  static instance;
  miniSearch;
  isDirty = false;
  saveInterval = null;
  constructor() {
    this.miniSearch = new MiniSearch({
      fields: ["content", "tags", "source", "type"],
      // Fields to index
      storeFields: ["id", "type", "caseId", "tags", "source", "createdAt", "content", "originalObject"],
      // Fields to return
      searchOptions: {
        boost: { type: 2, tags: 1.5 },
        fuzzy: 0.2,
        prefix: true
      },
      extractField: (document, fieldName) => {
        const record2 = document;
        return record2[fieldName];
      }
    });
    this.loadIndex();
    this.saveInterval = setInterval(() => {
      if (this.isDirty) {
        this.saveIndex();
      }
    }, 5e3);
  }
  static getInstance() {
    if (!_SearchIndexService.instance) {
      _SearchIndexService.instance = new _SearchIndexService();
    }
    return _SearchIndexService.instance;
  }
  // Hook for Entity Upsert
  async onEntityUpsert(entity) {
    if (process.env.SEARCH_ENABLED !== "true") return;
    const record2 = entity;
    const context4 = record2.context;
    const properties = record2.properties;
    const caseId = record2.caseId || context4?.caseId || properties?.caseId || "global";
    const item = {
      id: record2.id,
      type: "Entity",
      caseId,
      content: `${record2.type ?? ""} ${record2.value ?? ""} ${record2.label ?? ""} ${record2.name ?? ""}`.trim(),
      tags: record2.tags ?? [],
      source: "graph-store",
      createdAt: record2.createdAt ?? (/* @__PURE__ */ new Date()).toISOString(),
      originalObject: entity
    };
    this.ingest(item);
  }
  // Hook for Claim Upsert
  async onClaimUpsert(claim) {
    if (process.env.SEARCH_ENABLED !== "true") return;
    const record2 = claim;
    const context4 = record2.context;
    const caseId = context4?.caseId ?? "global";
    const item = {
      id: record2.id,
      type: "Claim",
      caseId,
      content: `${record2.claimType ?? ""} ${record2.statement ?? ""} ${JSON.stringify(record2.subjects ?? [])}`.trim(),
      tags: record2.tags ?? [],
      source: "provenance-ledger",
      createdAt: record2.createdAt ?? (/* @__PURE__ */ new Date()).toISOString(),
      originalObject: claim
    };
    this.ingest(item);
  }
  ingest(item) {
    if (!this.miniSearch.has(item.id)) {
      this.miniSearch.add(item);
    } else {
      this.miniSearch.replace(item);
    }
    this.isDirty = true;
  }
  search(query3) {
    if (!query3.caseId) {
      throw new Error("caseId is required");
    }
    const opts = {
      filter: (result2) => {
        if (result2.caseId !== query3.caseId) return false;
        if (query3.filters) {
          if (query3.filters.type && query3.filters.type.length > 0 && !query3.filters.type.includes(result2.type)) return false;
          if (query3.filters.tags && query3.filters.tags.length > 0) {
            const hasTag = query3.filters.tags.some((t) => result2.tags && result2.tags.includes(t));
            if (!hasTag) return false;
          }
          if (query3.filters.source && query3.filters.source.length > 0 && !query3.filters.source.includes(result2.source)) return false;
          if (query3.filters.timeRange) {
            const created = new Date(result2.createdAt).getTime();
            if (query3.filters.timeRange.start && created < new Date(query3.filters.timeRange.start).getTime()) return false;
            if (query3.filters.timeRange.end && created > new Date(query3.filters.timeRange.end).getTime()) return false;
          }
        }
        return true;
      },
      queries: [query3.q]
    };
    const results = this.miniSearch.search(query3.q, opts);
    const limit = query3.limit || 20;
    const offset = query3.cursor || 0;
    const pagedResults = results.slice(offset, offset + limit);
    return pagedResults.map((r) => {
      const content = r.content || "";
      const snippet = content.length > 100 ? content.substring(0, 100) + "..." : content;
      return {
        objectRef: {
          id: r.id,
          type: r.type
        },
        score: r.score,
        snippet,
        matchedFields: Object.keys(r.match),
        item: r
        // The full stored item
      };
    });
  }
  async reindex(caseId) {
    logger_default2.info(`Reindexing triggered for caseId: ${caseId || "all"}`);
    this.miniSearch.removeAll();
    try {
      const { getNeo4jDriver: getNeo4jDriver3 } = await Promise.resolve().then(() => (init_neo4j(), neo4j_exports));
      const driver3 = getNeo4jDriver3();
      const session = driver3.session();
      try {
        const entityQuery = caseId ? `MATCH (e:Entity) WHERE e.context.caseId = $caseId OR e.caseId = $caseId RETURN e` : `MATCH (e:Entity) RETURN e LIMIT 10000`;
        const entityRes = await session.run(entityQuery, { caseId });
        for (const record2 of entityRes.records) {
          const node = record2.get("e").properties;
          await this.onEntityUpsert(node);
        }
        const claimQuery = caseId ? `MATCH (c:Claim) WHERE c.context.caseId = $caseId RETURN c` : `MATCH (c:Claim) RETURN c LIMIT 10000`;
        const claimRes = await session.run(claimQuery, { caseId });
        for (const record2 of claimRes.records) {
          const node = record2.get("c").properties;
          await this.onClaimUpsert(node);
        }
        logger_default2.info(`Reindexing completed. processed ${entityRes.records.length} entities and ${claimRes.records.length} claims.`);
      } finally {
        await session.close();
      }
    } catch (err) {
      logger_default2.error("Reindexing failed to fetch data from DB", err);
    }
    this.isDirty = true;
  }
  async saveIndex() {
    try {
      const json = JSON.stringify(this.miniSearch.toJSON());
      const dir = path19.dirname(INDEX_FILE_PATH);
      if (!fs19.existsSync(dir)) {
        fs19.mkdirSync(dir, { recursive: true });
      }
      await fs19.promises.writeFile(INDEX_FILE_PATH, json);
      this.isDirty = false;
    } catch (e) {
      logger_default2.error("Failed to save search index", e);
    }
  }
  loadIndex() {
    try {
      if (fs19.existsSync(INDEX_FILE_PATH)) {
        const json = fs19.readFileSync(INDEX_FILE_PATH, "utf-8");
        this.miniSearch = MiniSearch.loadJSON(json, {
          fields: ["content", "tags", "source", "type"],
          storeFields: ["id", "type", "caseId", "tags", "source", "createdAt", "content", "originalObject"],
          searchOptions: {
            boost: { type: 2, tags: 1.5 },
            fuzzy: 0.2,
            prefix: true
          },
          extractField: (document, fieldName) => {
            const record2 = document;
            return record2[fieldName];
          }
        });
        logger_default2.info("Search index loaded from disk.");
      }
    } catch (e) {
      logger_default2.error("Failed to load search index", e);
    }
  }
};

// src/routes/search-index.ts
init_logger();
var router50 = express26.Router();
router50.post("/query", async (req, res) => {
  try {
    const input = req.body;
    if (!input.caseId) {
      return res.status(400).json({ error: "caseId is required" });
    }
    const results = SearchIndexService.getInstance().search(input);
    res.json({ results });
  } catch (err) {
    logger.error("Search query failed", err);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
router50.post("/reindex", async (req, res) => {
  try {
    const { caseId } = req.body;
    await SearchIndexService.getInstance().reindex(caseId);
    res.json({ status: "Reindexing started" });
  } catch (err) {
    logger.error("Reindex failed", err);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
var search_index_default = router50;

// src/routes/data-governance-routes.ts
import express27 from "express";

// src/data-governance/catalog/DataCatalogService.ts
init_postgres();
import { v4 as uuidv421 } from "uuid";
var DataCatalogService = class _DataCatalogService {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_DataCatalogService.instance) {
      _DataCatalogService.instance = new _DataCatalogService();
    }
    return _DataCatalogService.instance;
  }
  async registerAsset(asset) {
    const pool4 = getPostgresPool();
    const id = uuidv421();
    const now = /* @__PURE__ */ new Date();
    const newAsset = {
      ...asset,
      id,
      createdAt: now,
      updatedAt: now
    };
    await pool4.query(
      `INSERT INTO data_catalog_assets (
        id, urn, name, description, type, source, schema, owners, tags, sensitivity, metadata, tenant_id, created_at, updated_at
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)`,
      [
        newAsset.id,
        newAsset.urn,
        newAsset.name,
        newAsset.description,
        newAsset.type,
        newAsset.source,
        JSON.stringify(newAsset.schema),
        newAsset.owners,
        newAsset.tags,
        newAsset.sensitivity,
        JSON.stringify(newAsset.metadata),
        newAsset.tenantId,
        newAsset.createdAt,
        newAsset.updatedAt
      ]
    );
    return newAsset;
  }
  async getAsset(id) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query("SELECT * FROM data_catalog_assets WHERE id = $1", [id]);
    if (result2.rows.length === 0) return null;
    return this.mapRowToAsset(result2.rows[0]);
  }
  async getAssetByUrn(urn) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query("SELECT * FROM data_catalog_assets WHERE urn = $1", [urn]);
    if (result2.rows.length === 0) return null;
    return this.mapRowToAsset(result2.rows[0]);
  }
  async searchAssets(tenantId, query3) {
    const pool4 = getPostgresPool();
    const likeQuery = `%${query3}%`;
    const result2 = await pool4.query(
      `SELECT * FROM data_catalog_assets
       WHERE tenant_id = $1 AND (name ILIKE $2 OR description ILIKE $2 OR urn ILIKE $2)`,
      [tenantId, likeQuery]
    );
    return result2.rows.map(this.mapRowToAsset);
  }
  async updateAsset(id, updates) {
    const pool4 = getPostgresPool();
    const current = await this.getAsset(id);
    if (!current) return null;
    const updated = { ...current, ...updates, updatedAt: /* @__PURE__ */ new Date() };
    await pool4.query(
      `UPDATE data_catalog_assets SET
        name = $1, description = $2, schema = $3, owners = $4, tags = $5,
        sensitivity = $6, metadata = $7, updated_at = $8
       WHERE id = $9`,
      [
        updated.name,
        updated.description,
        JSON.stringify(updated.schema),
        updated.owners,
        updated.tags,
        updated.sensitivity,
        JSON.stringify(updated.metadata),
        updated.updatedAt,
        id
      ]
    );
    return updated;
  }
  mapRowToAsset(row) {
    return {
      id: row.id,
      urn: row.urn,
      name: row.name,
      description: row.description,
      type: row.type,
      source: row.source,
      schema: row.schema,
      owners: row.owners,
      tags: row.tags,
      sensitivity: row.sensitivity,
      metadata: row.metadata,
      tenantId: row.tenant_id,
      createdAt: row.created_at,
      updatedAt: row.updated_at
    };
  }
};

// src/data-governance/quality/DataQualityService.ts
init_postgres();
import { v4 as uuidv422 } from "uuid";
var DataQualityService2 = class _DataQualityService {
  static instance;
  catalog;
  constructor() {
    this.catalog = DataCatalogService.getInstance();
  }
  static getInstance() {
    if (!_DataQualityService.instance) {
      _DataQualityService.instance = new _DataQualityService();
    }
    return _DataQualityService.instance;
  }
  async defineRule(rule) {
    const pool4 = getPostgresPool();
    const id = uuidv422();
    const newRule = { ...rule, id };
    await pool4.query(
      `INSERT INTO data_quality_rules (
        id, asset_id, name, type, params, criticality, tenant_id
      ) VALUES ($1, $2, $3, $4, $5, $6, $7)`,
      [
        newRule.id,
        newRule.assetId,
        newRule.name,
        newRule.type,
        JSON.stringify(newRule.params),
        newRule.criticality,
        newRule.tenantId
      ]
    );
    return newRule;
  }
  async getRulesForAsset(assetId) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query("SELECT * FROM data_quality_rules WHERE asset_id = $1", [assetId]);
    return result2.rows.map((row) => ({
      id: row.id,
      assetId: row.asset_id,
      name: row.name,
      type: row.type,
      params: row.params,
      criticality: row.criticality,
      tenantId: row.tenant_id
    }));
  }
  async runChecks(assetId, tenantId) {
    const asset = await this.catalog.getAsset(assetId);
    if (!asset) {
      throw new Error(`Asset not found: ${assetId}`);
    }
    if (asset.tenantId !== tenantId) {
      throw new Error("Access denied: Asset belongs to a different tenant");
    }
    const allRules = await this.getRulesForAsset(assetId);
    const rules = allRules.filter((r) => r.tenantId === tenantId);
    const results = [];
    const pool4 = getPostgresPool();
    for (const rule of rules) {
      let passed = false;
      let observedValue = null;
      let details = "";
      try {
        if (asset.source === "postgres") {
          const checkResult = await this.executePostgresCheck(asset, rule);
          passed = checkResult.passed;
          observedValue = checkResult.observedValue;
          details = checkResult.details || "";
        } else {
          details = "Source not supported for automated checks";
          passed = false;
        }
      } catch (err) {
        details = `Error executing check: ${err.message}`;
        passed = false;
      }
      const result2 = {
        id: uuidv422(),
        ruleId: rule.id,
        assetId,
        passed,
        observedValue,
        details,
        executedAt: /* @__PURE__ */ new Date(),
        tenantId: asset.tenantId
      };
      results.push(result2);
      await pool4.query(
        `INSERT INTO data_quality_results (
          id, rule_id, asset_id, passed, observed_value, details, executed_at, tenant_id
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
        [
          result2.id,
          result2.ruleId,
          result2.assetId,
          result2.passed,
          JSON.stringify(result2.observedValue),
          result2.details,
          result2.executedAt,
          result2.tenantId
        ]
      );
    }
    return results;
  }
  // Basic identifier validation to prevent SQL injection
  validateIdentifier(identifier) {
    if (!/^[a-zA-Z0-9_]+$/.test(identifier)) {
      throw new Error(`Invalid identifier: ${identifier}`);
    }
    return `"${identifier}"`;
  }
  async executePostgresCheck(asset, rule) {
    const pool4 = getPostgresPool();
    const tableName = this.validateIdentifier(asset.name);
    switch (rule.type) {
      case "expect_column_values_to_be_not_null": {
        const column = this.validateIdentifier(rule.params.column);
        const query3 = `SELECT COUNT(*) as count FROM ${tableName} WHERE ${column} IS NULL`;
        const res = await pool4.query(query3);
        const nullCount = parseInt(res.rows[0].count, 10);
        return { passed: nullCount === 0, observedValue: nullCount };
      }
      case "expect_table_row_count_to_be_between": {
        const min = rule.params.min || 0;
        const max = rule.params.max || Number.MAX_SAFE_INTEGER;
        const query3 = `SELECT COUNT(*) as count FROM ${tableName}`;
        const res = await pool4.query(query3);
        const count = parseInt(res.rows[0].count, 10);
        return { passed: count >= min && count <= max, observedValue: count };
      }
      case "expect_column_values_to_be_unique": {
        const column = this.validateIdentifier(rule.params.column);
        const query3 = `SELECT COUNT(*) as total, COUNT(DISTINCT ${column}) as distinct_count FROM ${tableName}`;
        const res = await pool4.query(query3);
        const total = parseInt(res.rows[0].total, 10);
        const distinctCount = parseInt(res.rows[0].distinct_count, 10);
        return { passed: total === distinctCount, observedValue: { total, distinctCount } };
      }
      default:
        return { passed: false, observedValue: null, details: `Unsupported rule type: ${rule.type}` };
    }
  }
};

// src/data-governance/policy/PolicyEngine.ts
init_postgres();
import { v4 as uuidv423 } from "uuid";
var PolicyEngine = class _PolicyEngine {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_PolicyEngine.instance) {
      _PolicyEngine.instance = new _PolicyEngine();
    }
    return _PolicyEngine.instance;
  }
  async createPolicy(policy2) {
    const pool4 = getPostgresPool();
    const id = uuidv423();
    const newPolicy = { ...policy2, id };
    await pool4.query(
      `INSERT INTO data_governance_policies (
        id, name, description, rules, actions, tenant_id
      ) VALUES ($1, $2, $3, $4, $5, $6)`,
      [
        newPolicy.id,
        newPolicy.name,
        newPolicy.description,
        JSON.stringify(newPolicy.rules),
        JSON.stringify(newPolicy.actions),
        newPolicy.tenantId
      ]
    );
    return newPolicy;
  }
  async evaluateAsset(asset) {
    const pool4 = getPostgresPool();
    const res = await pool4.query("SELECT * FROM data_governance_policies WHERE tenant_id = $1", [asset.tenantId]);
    const policies = res.rows.map((row) => ({
      id: row.id,
      name: row.name,
      description: row.description,
      rules: row.rules,
      actions: row.actions,
      tenantId: row.tenant_id
    }));
    const violations = [];
    for (const policy2 of policies) {
      if (!this.checkPolicy(asset, policy2)) {
        violations.push(`Violated Policy: ${policy2.name}`);
      }
    }
    return {
      compliant: violations.length === 0,
      violations
    };
  }
  checkPolicy(asset, policy2) {
    for (const rule of policy2.rules) {
      const assetValue = asset[rule.field];
      switch (rule.operator) {
        case "equals":
          if (assetValue !== rule.value) return false;
          break;
        case "contains":
          if (Array.isArray(assetValue)) {
            if (!assetValue.includes(rule.value)) return false;
          } else if (typeof assetValue === "string") {
            if (!assetValue.includes(rule.value)) return false;
          }
          break;
        case "exists":
          if (assetValue === void 0 || assetValue === null) return false;
          if (Array.isArray(assetValue) && assetValue.length === 0) return false;
          break;
      }
    }
    return true;
  }
};

// src/data-governance/lineage/DataLineageService.ts
init_postgres();
import { v4 as uuidv424 } from "uuid";
var DataLineageService = class _DataLineageService {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_DataLineageService.instance) {
      _DataLineageService.instance = new _DataLineageService();
    }
    return _DataLineageService.instance;
  }
  async createNode(node) {
    const pool4 = getPostgresPool();
    const id = uuidv424();
    const newNode = { ...node, id };
    await pool4.query(
      `INSERT INTO lineage_nodes (id, asset_id, name, type, tenant_id)
       VALUES ($1, $2, $3, $4, $5)`,
      [id, newNode.assetId, newNode.name, newNode.type, newNode.tenantId]
    );
    return newNode;
  }
  async createEdge(edge) {
    const pool4 = getPostgresPool();
    const id = uuidv424();
    const newEdge = { ...edge, id };
    await pool4.query(
      `INSERT INTO lineage_edges (id, source_node_id, target_node_id, relation_type, tenant_id)
       VALUES ($1, $2, $3, $4, $5)`,
      [id, newEdge.sourceNodeId, newEdge.targetNodeId, newEdge.relationType, newEdge.tenantId]
    );
    return newEdge;
  }
  async getLineage(assetId, tenantId) {
    const pool4 = getPostgresPool();
    const nodeRes = await pool4.query(
      "SELECT * FROM lineage_nodes WHERE asset_id = $1 AND tenant_id = $2",
      [assetId, tenantId]
    );
    if (nodeRes.rows.length === 0) return { nodes: [], edges: [] };
    const rootNode = nodeRes.rows[0];
    const edgesRes = await pool4.query(
      `SELECT * FROM lineage_edges
         WHERE (source_node_id = $1 OR target_node_id = $1)
         AND tenant_id = $2`,
      [rootNode.id, tenantId]
    );
    const edges = edgesRes.rows.map(this.mapRowToEdge);
    if (edges.length === 0) {
      return {
        nodes: [this.mapRowToNode(rootNode)],
        edges: []
      };
    }
    const relatedNodeIds = /* @__PURE__ */ new Set();
    relatedNodeIds.add(rootNode.id);
    edges.forEach((e) => {
      relatedNodeIds.add(e.sourceNodeId);
      relatedNodeIds.add(e.targetNodeId);
    });
    const relatedNodesRes = await pool4.query(
      `SELECT * FROM lineage_nodes WHERE id = ANY($1) AND tenant_id = $2`,
      [[...relatedNodeIds], tenantId]
    );
    return {
      nodes: relatedNodesRes.rows.map(this.mapRowToNode),
      edges
    };
  }
  mapRowToNode(row) {
    return {
      id: row.id,
      assetId: row.asset_id,
      name: row.name,
      type: row.type,
      tenantId: row.tenant_id
    };
  }
  mapRowToEdge(row) {
    return {
      id: row.id,
      sourceNodeId: row.source_node_id,
      targetNodeId: row.target_node_id,
      relationType: row.relation_type,
      tenantId: row.tenant_id
    };
  }
};

// src/routes/data-governance-routes.ts
init_auth4();
var router51 = express27.Router();
var catalog = DataCatalogService.getInstance();
var quality = DataQualityService2.getInstance();
var policy = PolicyEngine.getInstance();
var lineage = DataLineageService.getInstance();
var ensureTenant = (req, res, next) => {
  if (!req.user || !req.user.tenantId) {
    return res.status(403).json({ error: "Tenant context required" });
  }
  next();
};
router51.post("/catalog/assets", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const asset = await catalog.registerAsset({
      ...req.body,
      tenantId: req.user.tenantId
    });
    res.status(201).json(asset);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
router51.get("/catalog/assets", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const query3 = req.query.q || "";
    const assets = await catalog.searchAssets(req.user.tenantId, query3);
    res.json(assets);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
router51.get("/catalog/assets/:id", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const asset = await catalog.getAsset(req.params.id);
    if (!asset || asset.tenantId !== req.user.tenantId) {
      return res.status(404).json({ error: "Asset not found" });
    }
    res.json(asset);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
router51.post("/quality/rules", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const rule = await quality.defineRule({
      ...req.body,
      tenantId: req.user.tenantId
    });
    res.status(201).json(rule);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
router51.post("/quality/assets/:id/run-checks", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const results = await quality.runChecks(req.params.id, req.user.tenantId);
    res.json(results);
  } catch (err) {
    if (err.message.includes("Access denied")) {
      return res.status(403).json({ error: err.message });
    }
    if (err.message.includes("Asset not found")) {
      return res.status(404).json({ error: err.message });
    }
    res.status(500).json({ error: err.message });
  }
});
router51.post("/governance/policies", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const newPolicy = await policy.createPolicy({
      ...req.body,
      tenantId: req.user.tenantId
    });
    res.status(201).json(newPolicy);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
router51.get("/governance/assets/:id/compliance", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const asset = await catalog.getAsset(req.params.id);
    if (!asset || asset.tenantId !== req.user.tenantId) {
      return res.status(404).json({ error: "Asset not found" });
    }
    const result2 = await policy.evaluateAsset(asset);
    res.json(result2);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
router51.get("/lineage/assets/:id", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const asset = await catalog.getAsset(req.params.id);
    if (!asset || asset.tenantId !== req.user.tenantId) {
      return res.status(404).json({ error: "Asset not found" });
    }
    const result2 = await lineage.getLineage(req.params.id, req.user.tenantId);
    res.json(result2);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
router51.post("/lineage/nodes", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const node = await lineage.createNode({
      ...req.body,
      tenantId: req.user.tenantId
    });
    res.status(201).json(node);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
router51.post("/lineage/edges", ensureAuthenticated, ensureTenant, async (req, res) => {
  try {
    const edge = await lineage.createEdge({
      ...req.body,
      tenantId: req.user.tenantId
    });
    res.status(201).json(edge);
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
});
var data_governance_routes_default = router51;

// src/routes/tenants/billing.ts
init_auth4();
import { Router as Router27 } from "express";
import { z as z27 } from "zod";

// src/metering/repository.ts
var TenantUsageDailyRepository = class {
  store = /* @__PURE__ */ new Map();
  async saveAll(rows) {
    for (const row of rows) {
      const key = `${row.tenantId}:${row.date}`;
      const existing = this.store.get(key);
      if (existing) {
        this.store.set(key, {
          ...existing,
          ingestUnits: existing.ingestUnits + row.ingestUnits,
          queryCredits: existing.queryCredits + row.queryCredits,
          storageBytesEstimate: existing.storageBytesEstimate + row.storageBytesEstimate,
          activeSeats: Math.max(existing.activeSeats, row.activeSeats),
          lastEventAt: existing.lastEventAt > row.lastEventAt ? existing.lastEventAt : row.lastEventAt,
          correlationIds: Array.from(
            /* @__PURE__ */ new Set([...existing.correlationIds, ...row.correlationIds])
          )
        });
      } else {
        this.store.set(key, row);
      }
    }
  }
  async list(tenantId, from, to) {
    let rows = Array.from(this.store.values());
    if (tenantId) {
      rows = rows.filter((r) => r.tenantId === tenantId);
    }
    if (from) {
      rows = rows.filter((r) => r.date >= from);
    }
    if (to) {
      rows = rows.filter((r) => r.date <= to);
    }
    return rows;
  }
  async get(tenantId, date) {
    const key = `${tenantId}:${date}`;
    return this.store.get(key);
  }
  clear() {
    this.store.clear();
  }
};
var tenantUsageDailyRepository = new TenantUsageDailyRepository();

// src/services/PricingEngine.ts
init_database();
import { randomUUID as randomUUID42 } from "crypto";
var PricingEngine = class _PricingEngine {
  static instance;
  _pool;
  constructor() {
  }
  get pool() {
    if (!this._pool) {
      this._pool = getPostgresPool2();
    }
    return this._pool;
  }
  static getInstance() {
    if (!_PricingEngine.instance) {
      _PricingEngine.instance = new _PricingEngine();
    }
    return _PricingEngine.instance;
  }
  async getEffectivePlan(tenantId) {
    const client6 = await this.pool.connect();
    try {
      const res = await client6.query(
        `SELECT p.*, tp.custom_overrides
         FROM tenant_plans tp
         JOIN plans p ON tp.plan_id = p.id
         WHERE tp.tenant_id = $1
         AND (tp.effective_to IS NULL OR tp.effective_to > NOW())
         ORDER BY tp.effective_from DESC
         LIMIT 1`,
        [tenantId]
      );
      if (res.rows.length === 0) {
        const freePlanRes = await client6.query(`SELECT * FROM plans WHERE name = 'Free'`);
        if (freePlanRes.rows.length > 0) {
          const plan = this.mapRowToPlan(freePlanRes.rows[0]);
          return { plan, overrides: null };
        }
        throw new Error(`No active plan found for tenant ${tenantId}`);
      }
      const row = res.rows[0];
      return {
        plan: this.mapRowToPlan(row),
        overrides: row.custom_overrides
      };
    } finally {
      client6.release();
    }
  }
  /**
   * Retrieves a plan definition by ID.
   */
  async getPlanById(planId) {
    const client6 = await this.pool.connect();
    try {
      const res = await client6.query(`SELECT * FROM plans WHERE id = $1`, [planId]);
      if (res.rows.length === 0) return null;
      return this.mapRowToPlan(res.rows[0]);
    } finally {
      client6.release();
    }
  }
  mapRowToPlan(row) {
    return {
      id: row.id,
      name: row.name,
      description: row.description,
      currency: row.currency,
      basePrice: row.base_price ? parseFloat(row.base_price) : 0,
      limits: row.limits,
      features: row.features,
      createdAt: row.created_at,
      updatedAt: row.updated_at
    };
  }
  /**
   * Estimate cost for a specific usage.
   * This is a simple estimation based on unit price defined in the plan.
   */
  async estimateCost(tenantId, kind, quantity) {
    const { plan, overrides } = await this.getEffectivePlan(tenantId);
    const limitConfig = plan.limits[kind];
    if (!limitConfig) return 0;
    const unitPrice = limitConfig.unitPrice || 0;
    return quantity * unitPrice;
  }
  /**
   * Generates an invoice for a given period based on aggregated usage.
   */
  async generateInvoice(tenantId, periodStart, periodEnd) {
    const client6 = await this.pool.connect();
    try {
      const { plan } = await this.getEffectivePlan(tenantId);
      const summariesRes = await client6.query(
        `SELECT * FROM usage_summaries
         WHERE tenant_id = $1
         AND period_start >= $2
         AND period_end <= $3`,
        [tenantId, periodStart, periodEnd]
      );
      const lineItems = [];
      let subtotal = 0;
      for (const row of summariesRes.rows) {
        const kind = row.kind;
        const totalQty = parseFloat(row.total_quantity);
        const limitConfig = plan.limits[kind];
        if (!limitConfig) continue;
        const included = limitConfig.monthlyIncluded || 0;
        const billableQty = Math.max(0, totalQty - included);
        const unitPrice = limitConfig.unitPrice || 0;
        const amount = billableQty * unitPrice;
        if (amount > 0) {
          lineItems.push({
            kind,
            quantity: billableQty,
            // Billed quantity
            unit: row.unit,
            unitPrice,
            amount,
            metadata: {
              totalUsage: totalQty,
              includedUsage: included
            }
          });
          subtotal += amount;
        }
      }
      const invoice = {
        id: randomUUID42(),
        tenantId,
        periodStart,
        periodEnd,
        currency: plan.currency,
        lineItems,
        subtotal,
        taxes: 0,
        // Placeholder
        total: subtotal,
        // + taxes
        status: "DRAFT"
      };
      await client6.query(
        `INSERT INTO invoices (id, tenant_id, period_start, period_end, currency, line_items, subtotal, taxes, total, status)
           VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)`,
        [invoice.id, invoice.tenantId, invoice.periodStart, invoice.periodEnd, invoice.currency, JSON.stringify(invoice.lineItems), invoice.subtotal, invoice.taxes, invoice.total, invoice.status]
      );
      return invoice;
    } finally {
      client6.release();
    }
  }
};
var PricingEngine_default = PricingEngine.getInstance();

// src/services/finops/FinopsReportService.ts
var DEFAULT_RATES = {
  "ingestion.records": { unitPrice: 4e-4, monthlyIncluded: 0, unit: "records" },
  "graph.queries": { unitPrice: 8e-4, monthlyIncluded: 0, unit: "credits" },
  "storage.bytes": { unitPrice: 21e-8, monthlyIncluded: 0, unit: "bytes" },
  "revops.active_seats": { unitPrice: 15, monthlyIncluded: 0, unit: "seats" }
};
var CostAttributionRepository = class {
  rows = [];
  async record(rows) {
    this.rows.push(...rows);
  }
  async list(tenantId, periodStart, periodEnd) {
    const start = new Date(periodStart).getTime();
    const end = new Date(periodEnd).getTime();
    return this.rows.filter((row) => {
      if (row.tenantId !== tenantId) return false;
      const rowStart = new Date(row.periodStart).getTime();
      const rowEnd = new Date(row.periodEnd).getTime();
      return rowStart <= end && rowEnd >= start;
    });
  }
  clear() {
    this.rows = [];
  }
};
var FinopsReportService = class {
  meteringSource;
  attributionSource;
  pricingEngine;
  defaultCurrency = "USD";
  constructor(options2 = {}) {
    this.meteringSource = options2.meteringSource || tenantUsageDailyRepository;
    this.attributionSource = options2.attributionSource || new CostAttributionRepository();
    this.pricingEngine = options2.pricingEngine || PricingEngine_default;
  }
  async buildReport(tenantId, periodStart, periodEnd) {
    const start = periodStart ? new Date(periodStart) : this.startOfMonth();
    const end = periodEnd ? new Date(periodEnd) : /* @__PURE__ */ new Date();
    const usageRows = await this.meteringSource.list();
    const tenantRows = usageRows.filter((row) => this.isWithinRange(row, tenantId, start, end));
    const meterLines = this.rollupMeters(tenantRows);
    const { currency, ratedLines } = await this.applyPricing(tenantId, meterLines);
    const usageSubtotal = ratedLines.reduce((sum, line) => sum + line.amount, 0);
    const attributionRows = await this.attributionSource.list(
      tenantId,
      start.toISOString(),
      end.toISOString()
    );
    const attributionBreakdown = this.buildAttributionBreakdown(attributionRows);
    const attributionTotal = attributionBreakdown.reduce((sum, row) => sum + row.amount, 0);
    const grossMargin = usageSubtotal - attributionTotal;
    const grossMarginPercent = usageSubtotal === 0 ? 0 : grossMargin / usageSubtotal * 100;
    return {
      tenantId,
      periodStart: start.toISOString(),
      periodEnd: end.toISOString(),
      currency: currency || this.defaultCurrency,
      generatedAt: (/* @__PURE__ */ new Date()).toISOString(),
      usage: ratedLines,
      attribution: {
        total: attributionTotal,
        rows: attributionRows,
        breakdown: attributionBreakdown
      },
      totals: {
        usageSubtotal,
        taxes: 0,
        total: usageSubtotal,
        grossMargin,
        grossMarginPercent
      },
      coverage: {
        meteringDays: new Set(tenantRows.map((row) => row.date)).size,
        meteringMeters: ratedLines.length,
        attributionRows: attributionRows.length,
        correlatedEvents: tenantRows.reduce(
          (sum, row) => sum + (row.correlationIds ? row.correlationIds.length : 0),
          0
        )
      }
    };
  }
  toCsv(report) {
    const header = ["meterId", "usageKind", "quantity", "unitPrice", "amount", "correlationIds"];
    const usageLines = report.usage.map(
      (line) => [
        line.meterId,
        line.usageKind,
        line.billableQuantity.toFixed(4),
        line.unitPrice.toFixed(6),
        line.amount.toFixed(2),
        line.correlationIds.join("|")
      ].join(",")
    );
    const attributionLines = report.attribution.breakdown.map(
      (row) => [`cogs:${row.capability}:${row.resourceType}`, "", "", "", row.amount.toFixed(2), ""].join(",")
    );
    const totals = [
      ["usageSubtotal", "", "", "", report.totals.usageSubtotal.toFixed(2), ""].join(","),
      ["cogsTotal", "", "", "", report.attribution.total.toFixed(2), ""].join(","),
      ["grossMargin", "", "", "", report.totals.grossMargin.toFixed(2), ""].join(",")
    ];
    return [header.join(","), ...usageLines, ...attributionLines, ...totals].join("\n");
  }
  rollupMeters(rows) {
    const meterTotals = /* @__PURE__ */ new Map();
    for (const row of rows) {
      const base = {
        correlationIds: row.correlationIds || [],
        included: 0,
        billableQuantity: 0
      };
      const mappings = [
        {
          meterId: "ingest.units" /* INGEST_UNITS */,
          usageKind: "ingestion.records",
          quantity: row.ingestUnits,
          unit: "records"
        },
        {
          meterId: "query.credits" /* QUERY_CREDITS */,
          usageKind: "graph.queries",
          quantity: row.queryCredits,
          unit: "credits"
        },
        {
          meterId: "storage.bytes_estimate" /* STORAGE_BYTES_ESTIMATE */,
          usageKind: "storage.bytes",
          quantity: row.storageBytesEstimate,
          unit: "bytes"
        },
        {
          meterId: "user.seat.active" /* USER_SEAT_ACTIVE */,
          usageKind: "revops.active_seats",
          quantity: row.activeSeats,
          unit: "seats"
        }
      ];
      for (const mapping of mappings) {
        const key = mapping.meterId;
        const existing = meterTotals.get(key);
        if (existing) {
          existing.quantity += mapping.quantity;
          existing.correlationIds = Array.from(
            /* @__PURE__ */ new Set([...existing.correlationIds, ...base.correlationIds])
          );
          meterTotals.set(key, existing);
        } else {
          meterTotals.set(key, {
            ...base,
            meterId: mapping.meterId,
            usageKind: mapping.usageKind,
            quantity: mapping.quantity,
            unit: mapping.unit,
            unitPrice: 0,
            amount: 0,
            billableQuantity: 0
          });
        }
      }
    }
    return Array.from(meterTotals.values());
  }
  async applyPricing(tenantId, lines) {
    const { plan } = await this.pricingEngine.getEffectivePlan(tenantId);
    const ratedLines = lines.map((line) => {
      const planLimits = plan?.limits?.[line.usageKind];
      const defaults = DEFAULT_RATES[line.usageKind];
      const included = planLimits?.monthlyIncluded ?? defaults?.monthlyIncluded ?? 0;
      const unitPrice = planLimits?.unitPrice ?? defaults?.unitPrice ?? 0;
      const billableQuantity = Math.max(0, line.quantity - included);
      const amount = billableQuantity * unitPrice;
      return {
        ...line,
        included,
        unitPrice,
        billableQuantity,
        amount
      };
    });
    return { ratedLines, currency: plan?.currency || this.defaultCurrency };
  }
  buildAttributionBreakdown(rows) {
    const byKey = /* @__PURE__ */ new Map();
    for (const row of rows) {
      const key = `${row.capability}:${row.resourceType}`;
      const current = byKey.get(key) || {
        capability: row.capability,
        resourceType: row.resourceType,
        amount: 0
      };
      current.amount += row.amount;
      byKey.set(key, current);
    }
    return Array.from(byKey.values());
  }
  isWithinRange(row, tenantId, start, end) {
    if (row.tenantId !== tenantId) return false;
    const date = /* @__PURE__ */ new Date(`${row.date}T00:00:00Z`);
    return date >= start && date <= end;
  }
  startOfMonth() {
    const now = /* @__PURE__ */ new Date();
    return new Date(Date.UTC(now.getUTCFullYear(), now.getUTCMonth(), 1));
  }
};
var costAttributionRepository = new CostAttributionRepository();
var finopsReportService = new FinopsReportService({
  attributionSource: costAttributionRepository
});

// src/routes/tenants/billing.ts
init_logger2();
var router52 = Router27({ mergeParams: true });
var BillingExportQuery = z27.object({
  start: z27.string().optional(),
  end: z27.string().optional(),
  format: z27.enum(["json", "csv"]).default("json")
});
var singleParam8 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
function attachTenantToBody(req, _res, next) {
  req.body = { ...req.body, tenantId: singleParam8(req.params.tenantId) ?? "" };
  return next();
}
function ensureTenantScope2(req, res, next) {
  const tenantId = singleParam8(req.params.tenantId) ?? "";
  const userTenant = req.user?.tenantId || req.user?.tenant_id;
  const isSuper = ["SUPER_ADMIN", "ADMIN", "admin"].includes(req.user?.role);
  if (!isSuper && userTenant && userTenant !== tenantId) {
    return res.status(403).json({ success: false, error: "Forbidden" });
  }
  return next();
}
router52.get(
  "/report",
  ensureAuthenticated,
  ensureTenantScope2,
  attachTenantToBody,
  ensurePolicy("read", "tenant"),
  async (req, res) => {
    try {
      const { start, end, format } = BillingExportQuery.parse({
        start: singleParam8(req.query.start),
        end: singleParam8(req.query.end),
        format: singleParam8(req.query.format)
      });
      const tenantId = singleParam8(req.params.tenantId) ?? "";
      const report = await finopsReportService.buildReport(tenantId, start, end);
      if (format === "csv") {
        const csv = finopsReportService.toCsv(report);
        res.setHeader("Content-Type", "text/csv");
        res.setHeader(
          "Content-Disposition",
          `attachment; filename="tenant-${tenantId}-billing.csv"`
        );
        return res.send(csv);
      }
      return res.json({ success: true, data: report });
    } catch (error) {
      logger_default2.error({ error }, "Failed to export tenant billing report");
      return res.status(500).json({ success: false, error: "Internal Server Error" });
    }
  }
);
var billing_default2 = router52;

// src/routes/tenants/usage.ts
init_database();
init_auth4();
import { Router as Router28 } from "express";
import { z as z28 } from "zod";

// src/middleware/request-schema-validator.ts
function formatJoiErrors(error) {
  return error.details.map((detail) => detail.message);
}
function formatZodErrors(error) {
  return error.errors.map((err) => `${err.path.join(".") || "value"}: ${err.message}`);
}
function buildRequestValidator({
  zodSchema,
  joiSchema,
  target = "body",
  allowUnknown = false
}) {
  return (req, res, next) => {
    try {
      const payload = req[target] ?? {};
      let validated = payload;
      if (joiSchema) {
        const { error, value } = joiSchema.validate(payload, {
          abortEarly: false,
          stripUnknown: !allowUnknown,
          convert: true
        });
        if (error) {
          res.status(400).json({
            error: "Validation failed",
            details: formatJoiErrors(error)
          });
          return;
        }
        validated = value;
      }
      if (zodSchema) {
        const result2 = zodSchema.safeParse(validated);
        if (!result2.success) {
          res.status(400).json({
            error: "Validation failed",
            details: formatZodErrors(result2.error)
          });
          return;
        }
        validated = result2.data;
      }
      const sanitized = SanitizationUtils.sanitizeUserInput(validated);
      const guardResult = SecurityValidator.validateInput(sanitized);
      if (!guardResult.valid) {
        res.status(400).json({
          error: "Request rejected for security reasons",
          details: guardResult.errors
        });
        return;
      }
      req[target] = sanitized;
      next();
    } catch (error) {
      res.status(500).json({ error: "Request validation failed unexpectedly" });
    }
  };
}

// src/middleware/tenantValidator.ts
import { GraphQLError as GraphQLError11 } from "graphql";
import pino52 from "pino";
var logger49 = pino52({ name: "tenantValidator" });
var TenantValidator = class _TenantValidator {
  /**
   * Validate tenant access for a given operation
   */
  static validateTenantAccess(context4, resourceTenantId, options2 = {}) {
    const {
      requireExplicitTenant = true,
      allowSystemAccess = false,
      validateOwnership = true
    } = options2;
    const userTenantId = context4.user?.tenantId;
    const userId = context4.user?.id;
    const roles = context4.user?.roles || [];
    const environment = context4.tenant?.environment || context4.user?.environment || (process.env.NODE_ENV?.startsWith("prod") ? "prod" : process.env.NODE_ENV?.startsWith("stag") ? "staging" : "dev");
    const privilegeTier = context4.tenant?.privilegeTier || context4.user?.privilegeTier || (roles.includes("SUPER_ADMIN") || roles.includes("SYSTEM") ? "break-glass" : roles.includes("ADMIN") ? "elevated" : "standard");
    if (allowSystemAccess && (roles.includes("SYSTEM") || roles.includes("SUPER_ADMIN"))) {
      logger49.info(`System-level access granted for user ${userId}`);
      return {
        tenantId: resourceTenantId || userTenantId || "system",
        userId,
        roles,
        permissions: ["*"],
        environment,
        privilegeTier
      };
    }
    if (requireExplicitTenant && !userTenantId) {
      logger49.error(
        `Tenant isolation violation: User ${userId} lacks tenant context`
      );
      throw new GraphQLError11("Tenant context required", {
        extensions: {
          code: "TENANT_REQUIRED",
          userId
        }
      });
    }
    if (validateOwnership && resourceTenantId && resourceTenantId !== userTenantId) {
      logger49.error(
        `Cross-tenant access denied: User ${userId} (tenant: ${userTenantId}) attempted to access resource (tenant: ${resourceTenantId})`
      );
      throw new GraphQLError11("Cross-tenant access denied", {
        extensions: {
          code: "CROSS_TENANT_ACCESS_DENIED",
          userTenant: userTenantId,
          resourceTenant: resourceTenantId
        }
      });
    }
    const effectiveTenantId = resourceTenantId || userTenantId;
    logger49.debug(
      `Tenant access validated for user ${userId}, tenant ${effectiveTenantId}`
    );
    return {
      tenantId: effectiveTenantId,
      userId,
      roles,
      permissions: this.calculatePermissions(roles),
      environment,
      privilegeTier
    };
  }
  /**
   * Generate tenant-scoped cache key
   */
  static getTenantCacheKey(baseKey, tenantContext, scope = "tenant") {
    switch (scope) {
      case "tenant":
        return `tenant:${tenantContext.tenantId}:${baseKey}`;
      case "user":
        return `user:${tenantContext.userId}:${baseKey}`;
      case "global":
        return baseKey;
      default:
        return `tenant:${tenantContext.tenantId}:${baseKey}`;
    }
  }
  /**
   * Generate tenant-scoped database query constraints
   */
  static getTenantQueryConstraints(tenantContext) {
    return {
      tenantId: tenantContext.tenantId,
      // Additional constraints for multi-tenancy
      deletedAt: null,
      status: { $ne: "archived" }
    };
  }
  /**
   * Validate and enhance Neo4j query with tenant constraints
   */
  static addTenantToNeo4jQuery(cypherQuery, parameters, tenantContext) {
    const enhancedParams = {
      ...parameters,
      tenantId: tenantContext.tenantId
    };
    let enhancedQuery = cypherQuery;
    if (!cypherQuery.includes("tenantId")) {
      enhancedQuery = enhancedQuery.replace(
        /MATCH\s*\((\w+):(\w+)\)/g,
        "MATCH ($1:$2 {tenantId: $tenantId})"
      );
      if (enhancedQuery.includes("WHERE")) {
        enhancedQuery = enhancedQuery.replace(
          /WHERE\s+/g,
          "WHERE n.tenantId = $tenantId AND "
        );
      } else if (enhancedQuery.includes("RETURN")) {
        enhancedQuery = enhancedQuery.replace(
          /RETURN/g,
          "WHERE n.tenantId = $tenantId RETURN"
        );
      }
    }
    logger49.debug(
      `Enhanced Neo4j query with tenant constraints for tenant ${tenantContext.tenantId}`
    );
    return {
      query: enhancedQuery,
      parameters: enhancedParams
    };
  }
  /**
   * Calculate user permissions based on roles
   */
  static calculatePermissions(roles) {
    const permissions = /* @__PURE__ */ new Set();
    roles.forEach((role) => {
      switch (role.toUpperCase()) {
        case "SUPER_ADMIN":
        case "SYSTEM":
          permissions.add("*");
          break;
        case "ADMIN":
          permissions.add("investigations:*");
          permissions.add("entities:*");
          permissions.add("relationships:*");
          permissions.add("users:read");
          break;
        case "ANALYST":
          permissions.add("investigations:read");
          permissions.add("investigations:write");
          permissions.add("entities:read");
          permissions.add("entities:write");
          permissions.add("relationships:read");
          permissions.add("relationships:write");
          permissions.add("graphrag:query");
          break;
        case "VIEWER":
          permissions.add("investigations:read");
          permissions.add("entities:read");
          permissions.add("relationships:read");
          break;
        default:
          break;
      }
    });
    return Array.from(permissions);
  }
  /**
   * Middleware factory for GraphQL resolvers
   */
  static createTenantMiddleware(options2 = {}) {
    return (resolver) => {
      return async (parent, args, context4, info) => {
        try {
          const resourceTenantId = args.tenantId || args.input?.tenantId || parent?.tenantId;
          const tenantContext = _TenantValidator.validateTenantAccess(
            context4,
            resourceTenantId,
            options2
          );
          const enhancedContext = {
            ...context4,
            tenant: tenantContext,
            getTenantCacheKey: (baseKey, scope) => _TenantValidator.getTenantCacheKey(baseKey, tenantContext, scope),
            getTenantQueryConstraints: () => _TenantValidator.getTenantQueryConstraints(tenantContext),
            addTenantToNeo4jQuery: (query3, params) => _TenantValidator.addTenantToNeo4jQuery(
              query3,
              params,
              tenantContext
            )
          };
          return await resolver(parent, args, enhancedContext, info);
        } catch (error) {
          logger49.error(
            `Tenant validation failed: ${error instanceof Error ? error.message : "Unknown error"}`
          );
          throw error;
        }
      };
    };
  }
};
var {
  validateTenantAccess,
  getTenantCacheKey,
  addTenantToNeo4jQuery
} = TenantValidator;
var tenantMiddleware = TenantValidator.createTenantMiddleware;

// src/routes/tenants/usage.ts
var router53 = Router28({ mergeParams: true });
var querySchema = z28.object({
  from: z28.string().datetime().optional(),
  to: z28.string().datetime().optional(),
  dimension: z28.string().optional(),
  dimensions: z28.union([z28.string(), z28.array(z28.string())]).optional(),
  limit: z28.coerce.number().int().min(1).max(500).optional()
});
var validateRequest = buildRequestValidator({
  zodSchema: querySchema,
  target: "query",
  allowUnknown: true
});
var singleParam9 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var normalizeDimensions = (value) => {
  if (Array.isArray(value)) {
    return value.filter((entry) => typeof entry === "string");
  }
  if (typeof value === "string") {
    return value;
  }
  return void 0;
};
var enforceTenant = (req, res, next) => {
  try {
    const tenantId = singleParam9(req.params.tenantId) ?? "";
    const context4 = TenantValidator.validateTenantAccess(
      { user: req.user },
      tenantId,
      { validateOwnership: true }
    );
    req.tenantContext = context4;
    next();
  } catch (error) {
    const status = error?.extensions?.code === "TENANT_REQUIRED" ? 401 : 403;
    res.status(status).json({
      error: "tenant_access_denied",
      message: error?.message || "Tenant access denied"
    });
  }
};
router53.get("/", ensureAuthenticated, validateRequest, enforceTenant, async (req, res) => {
  const pool4 = getPostgresPool2();
  let client6;
  try {
    client6 = await pool4.connect();
  } catch (error) {
    return res.status(500).json({
      error: "usage_rollup_failed",
      message: error?.message || "Failed to acquire database connection"
    });
  }
  const tenantId = singleParam9(req.params.tenantId) ?? "";
  const { from, to, dimension, dimensions, limit } = {
    from: singleParam9(req.query.from),
    to: singleParam9(req.query.to),
    dimension: singleParam9(req.query.dimension),
    dimensions: normalizeDimensions(req.query.dimensions),
    limit: req.query.limit
  };
  const windowEnd = to ?? (/* @__PURE__ */ new Date()).toISOString();
  const windowStart = from ?? new Date(Date.parse(windowEnd) - 30 * 24 * 60 * 60 * 1e3).toISOString();
  const params = [tenantId, windowStart, windowEnd];
  const predicates = ["tenant_id = $1", "period_start >= $2", "period_end <= $3"];
  let paramIndex = 4;
  const dimensionList = (Array.isArray(dimensions) ? dimensions : dimensions ? [dimensions] : []).concat(dimension ? [dimension] : []).filter(Boolean);
  if (dimensionList.length > 0) {
    predicates.push(`kind = ANY($${paramIndex++}::text[])`);
    params.push(dimensionList);
  }
  const effectiveLimit = limit ?? 100;
  const sql = `
    SELECT period_start, period_end, kind, total_quantity, unit, breakdown
    FROM usage_summaries
    WHERE ${predicates.join(" AND ")}
    ORDER BY period_start DESC, kind ASC
    LIMIT ${Number(effectiveLimit)}
  `;
  try {
    let plan = null;
    try {
      const pricing = await PricingEngine_default.getEffectivePlan(tenantId);
      plan = pricing?.plan || null;
    } catch (error) {
      plan = null;
    }
    const { rows } = await client6.query(sql, params);
    const rollups = rows.map((row) => {
      const totalQuantity = Number(row.total_quantity ?? 0);
      const unitPrice = plan?.limits?.[row.kind]?.unitPrice || 0;
      const estimatedCost = totalQuantity * unitPrice;
      return {
        dimension: row.kind,
        periodStart: typeof row.period_start === "string" ? row.period_start : row.period_start?.toISOString?.() || row.period_start,
        periodEnd: typeof row.period_end === "string" ? row.period_end : row.period_end?.toISOString?.() || row.period_end,
        totalQuantity,
        unit: row.unit,
        breakdown: row.breakdown || {},
        estimatedCost
      };
    });
    const totalEstimatedCost = rollups.reduce(
      (sum, rollup) => sum + rollup.estimatedCost,
      0
    );
    res.json({
      tenantId,
      window: { from: windowStart, to: windowEnd },
      rollups,
      totalEstimatedCost
    });
  } catch (error) {
    res.status(500).json({
      error: "usage_rollup_failed",
      message: error?.message || "Failed to load usage rollups"
    });
  } finally {
    client6.release();
  }
});
var buildCsv = (rows) => {
  const header = [
    "period_start",
    "period_end",
    "dimension",
    "total_quantity",
    "unit",
    "estimated_cost"
  ];
  const lines = rows.map(
    (row) => [
      row.periodStart,
      row.periodEnd,
      row.dimension,
      row.totalQuantity,
      row.unit ?? "",
      row.estimatedCost?.toFixed?.(4) ?? row.estimatedCost ?? 0
    ].map(
      (value) => typeof value === "string" && value.includes(",") ? `"${value.replace(/"/g, '""')}"` : value
    ).join(",")
  );
  return [header.join(","), ...lines].join("\n");
};
router53.get(
  "/export.json",
  ensureAuthenticated,
  validateRequest,
  enforceTenant,
  async (req, res) => {
    const pool4 = getPostgresPool2();
    const tenantId = singleParam9(req.params.tenantId) ?? "";
    const { from, to, dimension, dimensions, limit } = {
      from: singleParam9(req.query.from),
      to: singleParam9(req.query.to),
      dimension: singleParam9(req.query.dimension),
      dimensions: normalizeDimensions(req.query.dimensions),
      limit: req.query.limit
    };
    const windowEnd = to ?? (/* @__PURE__ */ new Date()).toISOString();
    const windowStart = from ?? new Date(Date.parse(windowEnd) - 30 * 24 * 60 * 60 * 1e3).toISOString();
    const params = [tenantId, windowStart, windowEnd];
    const predicates = ["tenant_id = $1", "period_start >= $2", "period_end <= $3"];
    let paramIndex = 4;
    const dimensionList = (Array.isArray(dimensions) ? dimensions : dimensions ? [dimensions] : []).concat(dimension ? [dimension] : []).filter(Boolean);
    if (dimensionList.length > 0) {
      predicates.push(`kind = ANY($${paramIndex++}::text[])`);
      params.push(dimensionList);
    }
    const effectiveLimit = limit ?? 500;
    const sql = `
      SELECT period_start, period_end, kind, total_quantity, unit, breakdown
      FROM usage_summaries
      WHERE ${predicates.join(" AND ")}
      ORDER BY period_start DESC, kind ASC
      LIMIT ${Number(effectiveLimit)}
    `;
    let client6;
    try {
      client6 = await pool4.connect();
      let plan = null;
      try {
        const pricing = await PricingEngine_default.getEffectivePlan(tenantId);
        plan = pricing?.plan || null;
      } catch (error) {
        plan = null;
      }
      const { rows } = await client6.query(sql, params);
      const rollups = rows.map((row) => {
        const totalQuantity = Number(row.total_quantity ?? 0);
        const unitPrice = plan?.limits?.[row.kind]?.unitPrice || 0;
        const estimatedCost = totalQuantity * unitPrice;
        return {
          dimension: row.kind,
          periodStart: typeof row.period_start === "string" ? row.period_start : row.period_start?.toISOString?.() || row.period_start,
          periodEnd: typeof row.period_end === "string" ? row.period_end : row.period_end?.toISOString?.() || row.period_end,
          totalQuantity,
          unit: row.unit,
          breakdown: row.breakdown || {},
          estimatedCost
        };
      });
      const payload = {
        tenantId,
        window: { from: windowStart, to: windowEnd },
        rollups
      };
      res.setHeader("Content-Type", "application/json");
      res.setHeader(
        "Content-Disposition",
        `attachment; filename="usage-${tenantId}.json"`
      );
      res.status(200).send(JSON.stringify(payload, null, 2));
    } catch (error) {
      res.status(500).json({
        error: "usage_export_failed",
        message: error?.message || "Failed to export usage rollups"
      });
    } finally {
      client6?.release?.();
    }
  }
);
router53.get(
  "/export.csv",
  ensureAuthenticated,
  validateRequest,
  enforceTenant,
  async (req, res) => {
    const pool4 = getPostgresPool2();
    const tenantId = singleParam9(req.params.tenantId) ?? "";
    const { from, to, dimension, dimensions, limit } = {
      from: singleParam9(req.query.from),
      to: singleParam9(req.query.to),
      dimension: singleParam9(req.query.dimension),
      dimensions: normalizeDimensions(req.query.dimensions),
      limit: req.query.limit
    };
    const windowEnd = to ?? (/* @__PURE__ */ new Date()).toISOString();
    const windowStart = from ?? new Date(Date.parse(windowEnd) - 30 * 24 * 60 * 60 * 1e3).toISOString();
    const params = [tenantId, windowStart, windowEnd];
    const predicates = ["tenant_id = $1", "period_start >= $2", "period_end <= $3"];
    let paramIndex = 4;
    const dimensionList = (Array.isArray(dimensions) ? dimensions : dimensions ? [dimensions] : []).concat(dimension ? [dimension] : []).filter(Boolean);
    if (dimensionList.length > 0) {
      predicates.push(`kind = ANY($${paramIndex++}::text[])`);
      params.push(dimensionList);
    }
    const effectiveLimit = limit ?? 500;
    const sql = `
      SELECT period_start, period_end, kind, total_quantity, unit, breakdown
      FROM usage_summaries
      WHERE ${predicates.join(" AND ")}
      ORDER BY period_start DESC, kind ASC
      LIMIT ${Number(effectiveLimit)}
    `;
    let client6;
    try {
      client6 = await pool4.connect();
      let plan = null;
      try {
        const pricing = await PricingEngine_default.getEffectivePlan(tenantId);
        plan = pricing?.plan || null;
      } catch (error) {
        plan = null;
      }
      const { rows } = await client6.query(sql, params);
      const rollups = rows.map((row) => {
        const totalQuantity = Number(row.total_quantity ?? 0);
        const unitPrice = plan?.limits?.[row.kind]?.unitPrice || 0;
        const estimatedCost = totalQuantity * unitPrice;
        return {
          dimension: row.kind,
          periodStart: typeof row.period_start === "string" ? row.period_start : row.period_start?.toISOString?.() || row.period_start,
          periodEnd: typeof row.period_end === "string" ? row.period_end : row.period_end?.toISOString?.() || row.period_end,
          totalQuantity,
          unit: row.unit,
          breakdown: row.breakdown || {},
          estimatedCost
        };
      });
      const csv = buildCsv(rollups);
      res.setHeader("Content-Type", "text/csv");
      res.setHeader(
        "Content-Disposition",
        `attachment; filename="usage-${tenantId}.csv"`
      );
      res.status(200).send(csv);
    } catch (error) {
      res.status(500).json({
        error: "usage_export_failed",
        message: error?.message || "Failed to export usage rollups"
      });
    } finally {
      client6?.release?.();
    }
  }
);
var usage_default = router53;

// src/routes/gtm-messaging.ts
import { Router as Router29 } from "express";

// src/gtm/gtm-messaging-service.ts
import { randomUUID as randomUUID43 } from "crypto";
import { Counter as Counter17, Gauge as Gauge15, Histogram as Histogram13, register as register8 } from "prom-client";

// src/gtm/claims-repository.ts
import { promises as fs20 } from "fs";
import path20 from "path";
import { Parser } from "json2csv";
var DEFAULT_BASE_PATH = path20.resolve(process.cwd(), "docs/gtm/claims");
var ClaimsRepository = class {
  claimsPath;
  csvPath;
  constructor(basePath = DEFAULT_BASE_PATH) {
    this.claimsPath = path20.join(basePath, "claims.json");
    this.csvPath = path20.join(basePath, "claims.csv");
  }
  async init() {
    await fs20.mkdir(path20.dirname(this.claimsPath), { recursive: true });
    try {
      await fs20.access(this.claimsPath);
    } catch {
      await this.saveClaims([]);
    }
  }
  async loadClaims() {
    await this.init();
    const raw = await fs20.readFile(this.claimsPath, "utf8");
    const claims = JSON.parse(raw);
    return claims.map((claim) => ({ ...claim, channels: [...new Set(claim.channels)] }));
  }
  async saveClaims(claims) {
    await fs20.mkdir(path20.dirname(this.claimsPath), { recursive: true });
    await fs20.writeFile(this.claimsPath, JSON.stringify(claims, null, 2));
    await this.writeCsv(claims);
  }
  async upsertClaim(updated) {
    const claims = await this.loadClaims();
    const existingIndex = claims.findIndex((claim) => claim.claimId === updated.claimId);
    if (existingIndex >= 0) {
      claims[existingIndex] = updated;
    } else {
      claims.push(updated);
    }
    await this.saveClaims(claims.sort((a, b) => a.claimId.localeCompare(b.claimId)));
  }
  async writeCsv(claims) {
    const parser = new Parser({
      fields: [
        "claimId",
        "message",
        "evidenceType",
        "evidenceSource",
        "status",
        "reviewDate",
        "owner",
        "channels",
        "riskTier",
        "expiry",
        "publishedAt",
        "forwardLooking",
        "complianceSurface"
      ]
    });
    const csv = parser.parse(
      claims.map((claim) => ({
        ...claim,
        channels: claim.channels.join("|"),
        complianceSurface: claim.complianceSurface?.join("|")
      }))
    );
    await fs20.writeFile(this.csvPath, csv);
  }
};

// src/gtm/types.ts
import { z as z29 } from "zod";
var EvidenceType = /* @__PURE__ */ ((EvidenceType3) => {
  EvidenceType3["CustomerMetric"] = "customer_metric";
  EvidenceType3["CustomerQuote"] = "customer_quote";
  EvidenceType3["Certification"] = "certification";
  EvidenceType3["Sla"] = "sla";
  EvidenceType3["Audit"] = "audit";
  EvidenceType3["SecurityAttestation"] = "security_attestation";
  EvidenceType3["ProductTelemetry"] = "product_telemetry";
  EvidenceType3["DemoVideo"] = "demo_video";
  return EvidenceType3;
})(EvidenceType || {});
var RiskTier = /* @__PURE__ */ ((RiskTier2) => {
  RiskTier2["Low"] = "low";
  RiskTier2["Medium"] = "medium";
  RiskTier2["High"] = "high";
  return RiskTier2;
})(RiskTier || {});
var ClaimInputSchema = z29.object({
  message: z29.string().min(10),
  evidenceType: z29.nativeEnum(EvidenceType),
  evidenceSource: z29.string().min(3),
  owner: z29.string().min(2),
  channels: z29.array(z29.union([z29.literal("web"), z29.literal("sales"), z29.literal("content")])).min(1),
  riskTier: z29.nativeEnum(RiskTier).optional(),
  reviewDate: z29.string().optional(),
  forwardLooking: z29.boolean().optional(),
  complianceSurface: z29.array(z29.string()).optional()
});

// src/gtm/gtm-messaging-service.ts
var GtmMessagingService = class _GtmMessagingService {
  repository;
  static getOrCreateCounter(config9) {
    const existing = register8.getSingleMetric(config9.name);
    if (existing) return existing;
    return new Counter17(config9);
  }
  static getOrCreateGauge(config9) {
    const existing = register8.getSingleMetric(config9.name);
    if (existing) return existing;
    return new Gauge15(config9);
  }
  static getOrCreateHistogram(config9) {
    const existing = register8.getSingleMetric(config9.name);
    if (existing) return existing;
    return new Histogram13(config9);
  }
  static claimsSubmitted = _GtmMessagingService.getOrCreateCounter({
    name: "gtm_claims_submitted_total",
    help: "Count of claims submitted by risk tier",
    labelNames: ["riskTier"]
  });
  static claimsApproved = _GtmMessagingService.getOrCreateCounter({
    name: "gtm_claims_approved_total",
    help: "Count of claims approved by risk tier",
    labelNames: ["riskTier"]
  });
  static claimsExpired = _GtmMessagingService.getOrCreateCounter({
    name: "gtm_claims_expired_total",
    help: "Count of claims that hit expiry",
    labelNames: ["riskTier"]
  });
  static approvalDuration = _GtmMessagingService.getOrCreateHistogram({
    name: "gtm_claim_approval_duration_seconds",
    help: "Time from submission to approval",
    labelNames: ["riskTier"],
    buckets: [0.5, 1, 3, 6, 12, 24, 48, 72, 96]
  });
  static activeApprovedClaims = _GtmMessagingService.getOrCreateGauge({
    name: "gtm_claims_active",
    help: "Active approved claims per channel",
    labelNames: ["channel"]
  });
  static resetMetrics() {
    this.claimsSubmitted.reset();
    this.claimsApproved.reset();
    this.claimsExpired.reset();
    this.approvalDuration.reset();
    this.activeApprovedClaims.reset();
  }
  constructor(repository = new ClaimsRepository()) {
    this.repository = repository;
  }
  async submitClaim(input) {
    const parsed = ClaimInputSchema.parse(input);
    const riskTier = parsed.riskTier ?? this.calculateRiskTier(parsed);
    const now = /* @__PURE__ */ new Date();
    const reviewDate = parsed.reviewDate ?? this.computeReviewDate(riskTier, now);
    const claim = {
      claimId: randomUUID43(),
      message: parsed.message,
      evidenceType: parsed.evidenceType,
      evidenceSource: parsed.evidenceSource,
      status: "pending" /* Pending */,
      reviewDate,
      owner: parsed.owner,
      channels: parsed.channels,
      riskTier,
      approvals: [
        {
          approver: "evidence",
          approvedAt: now.toISOString(),
          notes: "Evidence validated at submission"
        }
      ],
      createdAt: now.toISOString(),
      updatedAt: now.toISOString(),
      forwardLooking: parsed.forwardLooking,
      complianceSurface: parsed.complianceSurface
    };
    await this.repository.upsertClaim(claim);
    _GtmMessagingService.claimsSubmitted.inc({ riskTier: claim.riskTier });
    return {
      claim,
      enforcement: this.computeEnforcementGates(claim)
    };
  }
  async recordApproval(claimId, approver, notes) {
    const claims = await this.repository.loadClaims();
    const claim = claims.find((c) => c.claimId === claimId);
    if (!claim) {
      throw new Error(`Claim ${claimId} not found`);
    }
    if (claim.status === "expired" /* Expired */) {
      throw new Error("Cannot approve an expired claim");
    }
    const now = /* @__PURE__ */ new Date();
    claim.approvals = [...claim.approvals, { approver, approvedAt: now.toISOString(), notes }];
    claim.updatedAt = now.toISOString();
    const approvalsNeeded = this.requiredApprovals(claim);
    const approvalsMet = approvalsNeeded.every(
      (required) => claim.approvals.some((approval) => approval.approver === required)
    );
    if (approvalsMet) {
      claim.status = "approved" /* Approved */;
      claim.publishedAt = claim.publishedAt ?? now.toISOString();
      _GtmMessagingService.claimsApproved.inc({ riskTier: claim.riskTier });
      const submissionTime = new Date(claim.createdAt).getTime();
      _GtmMessagingService.approvalDuration.observe({ riskTier: claim.riskTier }, (now.getTime() - submissionTime) / 1e3 / 3600);
      await this.updateActiveClaimsGauge();
    }
    await this.repository.upsertClaim(claim);
    return claim;
  }
  async expireClaims(currentDate = /* @__PURE__ */ new Date()) {
    const claims = await this.repository.loadClaims();
    const updated = claims.map((claim) => {
      if (claim.status === "approved" /* Approved */ && new Date(claim.reviewDate) < currentDate) {
        _GtmMessagingService.claimsExpired.inc({ riskTier: claim.riskTier });
        return { ...claim, status: "expired" /* Expired */, expiry: currentDate.toISOString(), updatedAt: currentDate.toISOString() };
      }
      return claim;
    });
    await this.repository.saveClaims(updated);
    await this.updateActiveClaimsGauge();
    return updated.filter((claim) => claim.status === "expired" /* Expired */);
  }
  async listClaimsForChannel(channel) {
    const claims = await this.repository.loadClaims();
    return claims.filter((claim) => claim.status === "approved" /* Approved */ && claim.channels.includes(channel));
  }
  async getMessageHouse() {
    return {
      core: {
        headline: "Governed AI outcomes with evidence-grade reporting",
        proofPoints: [
          "SOC2+ enterprise controls with provenance ledger",
          "Case studies with 30% faster investigations",
          "SLA-backed response with audit-ready exports"
        ],
        objections: ["Data residency", "Security posture", "ROI proof", "Vendor lock-in"],
        differentiators: [
          "AI-first + governance-first architecture",
          "Provenance ledger enforcing policy-driven controls",
          "Self-hostable with data isolation patterns"
        ]
      },
      variants: [
        {
          name: "regulated",
          headline: "Regulated teams: audit-grade AI with FedRAMP-ready controls",
          proofPoints: ["FedRAMP-ready patterns", "Data isolation with DLP posture", "Regional tenancy + retention SLAs"],
          objections: ["Cross-border data flow", "Chain-of-custody", "Regulatory drift"],
          differentiators: ["Continuous compliance checks", "Security packet for regulators", "Signed audit artifacts"],
          additionalControls: ["Mandatory legal + security approvals for new claims"]
        },
        {
          name: "mid-market",
          headline: "Mid-market: consolidate rogue tools with evidence-backed AI",
          proofPoints: ["SOC2 posture", "Fast-start templates", "Shared enablement library"],
          objections: ["Change management", "Proof of ROI"],
          differentiators: ["Fast lane approvals", "Template-driven launches", "Integrated nurture"],
          additionalControls: ["Fast-lane reuse of approved claims"]
        },
        {
          name: "enterprise",
          headline: "Enterprise: policy-automated AI that scales globally",
          proofPoints: ["Global routing and consent", "Attribution sanity checks", "Battle-tested SLAs"],
          objections: ["Integration risk", "Data mapping", "Security review cycle time"],
          differentiators: ["Mutual action plans", "Platform extensibility", "Governance-first pipelines"],
          additionalControls: ["Dual approvals for forward-looking statements"]
        }
      ]
    };
  }
  async getIcpBrief() {
    return {
      tiers: [
        {
          name: "regulated",
          criteria: ["Regulated industry", "Strict data residency", "Security-first"],
          triggers: ["Audit cycles", "New AI governance policy", "Incident-driven refresh"],
          pains: ["Rogue tools", "Compliance exposure", "Slow approvals"],
          valueHypotheses: ["Faster approvals", "Audit-ready reporting", "Reduced risk"],
          proofSources: ["Regulated customer metrics", "Certifications", "Security attestations"]
        },
        {
          name: "mid-market",
          criteria: ["Growing teams", "Limited RevOps resources"],
          triggers: ["Need to consolidate tools", "Desire for faster close rates"],
          pains: ["Inconsistent messaging", "Manual content production"],
          valueHypotheses: ["Single source of truth messaging", "Reusable templates", "Enablement discipline"],
          proofSources: ["Customer quotes", "ROI metrics", "Demo video proof"]
        },
        {
          name: "enterprise",
          criteria: ["Global footprint", "Complex compliance"],
          triggers: ["Board pressure on AI governance", "Global rollouts"],
          pains: ["Channel drift", "Attribution uncertainty"],
          valueHypotheses: ["Policy automation", "Persona routing", "Multi-touch attribution"],
          proofSources: ["Executive references", "SLA adherence metrics", "Security attestations"]
        }
      ],
      useCases: ["Investigation workspace", "Evidence-grade reporting", "Policy automation", "Executive command center"],
      whyNow: [
        "AI governance requirements and provenance expectations",
        "Audit demands for evidence-backed claims",
        "Need to consolidate rogue tools into a governed system"
      ]
    };
  }
  getContentTemplates() {
    return [
      {
        type: "blog",
        requiredSections: ["Problem framing", "Proof tile", "CTA", "FAQ"],
        callToAction: "Book a demo tailored to your governance posture",
        proofRequirements: ["At least one approved claim", "Link to evidence tile"],
        reusePlan: ["Social snippets", "Newsletter highlight", "Nurture email"]
      },
      {
        type: "case_study",
        requiredSections: ["Customer background", "Problem", "Solution", "Proof metrics", "Security posture"],
        callToAction: "See the workflow live with your data",
        proofRequirements: ["Named customer metric", "Approved claim mapping"],
        reusePlan: ["Deck tile", "Sales one-pager", "Web proof gallery"]
      },
      {
        type: "whitepaper",
        requiredSections: ["Executive summary", "Architecture", "Proof appendix", "Compliance mapping"],
        callToAction: "Download the governed AI blueprint",
        proofRequirements: ["Certification references", "SLA commitments"],
        reusePlan: ["Webinar script", "FAQ", "Security packet addendum"]
      },
      {
        type: "webinar",
        requiredSections: ["Agenda", "Live demo", "Proof gallery", "Q&A", "Next steps"],
        callToAction: "Register for the guided governance demo",
        proofRequirements: ["Demo video", "Customer quote"],
        reusePlan: ["Clips", "Social posts", "FAQ", "Deck updates", "Nurture emails"]
      },
      {
        type: "docs",
        requiredSections: ["Feature overview", "Controls", "Proof references", "Compliance notes"],
        callToAction: "Enable the control in your workspace",
        proofRequirements: ["Product telemetry reference"],
        reusePlan: ["Release notes", "Support macros"]
      },
      {
        type: "email",
        requiredSections: ["Personalized hook", "Proof tile", "CTA"],
        callToAction: "Book the governance session",
        proofRequirements: ["Approved claim matched to persona"],
        reusePlan: ["Sequence templates", "Nurture variants"]
      }
    ];
  }
  getWebsiteKpis() {
    return [
      {
        page: "home",
        metrics: [
          { name: "ctr_to_pricing_or_use_case", target: 0.35, description: "Clickthrough to pricing or use case pages" },
          { name: "demo_request_rate", target: 0.07, description: "Demo request rate from home" }
        ],
        routing: ["persona routing enabled", "route SMB to self-serve, enterprise to contact"],
        abGuardrails: ["Guardrail brand variants", "Sample size calculator required"]
      },
      {
        page: "pricing",
        metrics: [
          { name: "plan_clickthrough", target: 0.5, description: "Clickthrough to contact or sign-up" },
          { name: "contact_rate", target: 0.12, description: "Contact sales submissions from pricing" }
        ],
        routing: ["capture friction notes", "instrument drop-offs"],
        abGuardrails: ["Limit concurrent tests to 2", "Preserve trust surfaces"]
      },
      {
        page: "use_case",
        metrics: [
          { name: "cta_to_demo_or_signup", target: 0.2, description: "CTA performance per use case" },
          { name: "proof_engagement", target: 0.4, description: "Interaction with proof tiles" }
        ],
        abGuardrails: ["Variants must reuse approved claims", "No unapproved forward-looking statements"]
      },
      {
        page: "security",
        metrics: [
          { name: "trust_pack_download", target: 0.25, description: "Download rate for trust pack" },
          { name: "sla_view_depth", target: 0.3, description: "View depth for SLA posture content" }
        ],
        routing: ["Enterprise route to contact", "Alert on drop-offs"],
        abGuardrails: ["No brand drift", "Security claims must be approved and unexpired"]
      }
    ];
  }
  getNurtureTracks() {
    return [
      {
        name: "New lead onboarding",
        stage: "new_lead",
        signals: ["form_submission", "light content depth"],
        content: ["Welcome email", "Primary proof tile", "CTA to demo scheduling"]
      },
      {
        name: "Engaged to activated",
        stage: "engaged",
        signals: ["content depth", "product interest"],
        content: ["Use-case specific guide", "Proof-linked nurture email", "Webinar invite"]
      },
      {
        name: "SQL reinforcement",
        stage: "sql",
        signals: ["sales accepted", "security questionnaire"],
        content: ["Security packet", "Battlecard", "Mutual action plan"]
      },
      {
        name: "Expansion",
        stage: "expansion",
        signals: ["feature adoption", "renewal window"],
        content: ["Expansion playbook", "New feature adoption campaign"]
      }
    ];
  }
  getEnablementAssets() {
    return [
      { role: "sdr", assetType: "deck", required: true, forbiddenPhrases: ["magic ai", "guaranteed"] },
      { role: "ae", assetType: "battlecard", required: true, forbiddenPhrases: ["unlimited", "perfect security"] },
      { role: "se", assetType: "demo", required: true },
      { role: "csm", assetType: "map", required: true },
      { role: "ae", assetType: "security_packet", required: true },
      { role: "sdr", assetType: "objection_handling" }
    ];
  }
  getChannelPlaybooks() {
    return [
      {
        channel: "paid_search",
        cacTarget: 1200,
        paybackTargetMonths: 6,
        killThreshold: 0.2,
        attributionModel: "multi_touch",
        complianceRequirements: ["Consent tracking", "UTM discipline"]
      },
      {
        channel: "content",
        cacTarget: 500,
        paybackTargetMonths: 4,
        killThreshold: 0.15,
        attributionModel: "multi_touch",
        complianceRequirements: ["Approval for claims", "Accessibility review"]
      },
      {
        channel: "events",
        cacTarget: 1500,
        paybackTargetMonths: 9,
        killThreshold: 0.1,
        attributionModel: "multi_touch",
        complianceRequirements: ["Consent", "Badge scan governance"]
      },
      {
        channel: "partners",
        cacTarget: 800,
        paybackTargetMonths: 7,
        killThreshold: 0.18,
        attributionModel: "multi_touch",
        complianceRequirements: ["MDF rules", "Co-marketing approvals"]
      },
      {
        channel: "outbound",
        cacTarget: 900,
        paybackTargetMonths: 5,
        killThreshold: 0.12,
        attributionModel: "multi_touch",
        complianceRequirements: ["Outreach compliance", "Security-approved language"]
      },
      {
        channel: "plg",
        cacTarget: 300,
        paybackTargetMonths: 3,
        killThreshold: 0.25,
        attributionModel: "multi_touch",
        complianceRequirements: ["Product telemetry opt-in", "Consent management"]
      }
    ];
  }
  evaluateChannelPerformance(input) {
    const playbook = this.getChannelPlaybooks().find((p) => p.channel === input.channel);
    if (!playbook) {
      throw new Error(`Unknown channel ${input.channel}`);
    }
    const healthy = input.cac <= playbook.cacTarget && input.paybackMonths <= playbook.paybackTargetMonths && input.pipelineVelocity >= playbook.killThreshold;
    return {
      healthy,
      recommendation: healthy ? "double_down" : "reallocate_budget"
    };
  }
  decideAdaptiveRoute(input) {
    if (input.intentLevel === "high" && input.behavioralScore >= 0.7) {
      return input.firmographic === "enterprise" ? "enterprise-contact" : "guided-demo";
    }
    if (input.behavioralScore >= 0.4) {
      return "guided-demo";
    }
    return "self-serve";
  }
  async closedLoopQa(content) {
    const claims = await this.repository.loadClaims();
    const approvedMessages = claims.filter((claim) => claim.status === "approved" /* Approved */).map((claim) => claim.message.toLowerCase());
    const forbiddenPhrases = ["guaranteed", "perfect security", "unlimited"];
    const violations = [];
    claims.filter((claim) => claim.status !== "approved" /* Approved */).forEach((claim) => {
      if (content.toLowerCase().includes(claim.message.toLowerCase())) {
        violations.push(`Unapproved claim referenced: ${claim.claimId}`);
      }
    });
    forbiddenPhrases.forEach((phrase) => {
      if (content.toLowerCase().includes(phrase)) {
        violations.push(`Forbidden phrase detected: ${phrase}`);
      }
    });
    if (!approvedMessages.some((msg) => content.toLowerCase().includes(msg))) {
      violations.push("No approved claims found in content");
    }
    return { violations };
  }
  buildExecutionChecklist(claims) {
    return [
      {
        id: "icp_narrative",
        description: "Publish ICP + why-now one-pager and message house variants",
        completed: claims.length > 0
      },
      {
        id: "claims_library",
        description: "Stand up claims library with approval workflow, expiry, and channel tagging",
        completed: claims.some((claim) => claim.status === "approved" /* Approved */)
      },
      {
        id: "content_templates",
        description: "Ship content templates with editorial standards and reuse playbook",
        completed: true
      },
      {
        id: "website_kpis",
        description: "Map website KPIs and trust surfaces with guardrails",
        completed: true
      },
      {
        id: "nurture_scoring",
        description: "Launch nurture and lead scoring with SLAs",
        completed: true
      },
      {
        id: "enablement_library",
        description: "Refresh enablement library and certification",
        completed: claims.filter((claim) => claim.channels.includes("sales")).length > 0
      },
      {
        id: "channel_scorecards",
        description: "Finalize channel scorecards and attribution sanity checks",
        completed: true
      },
      {
        id: "risk_register",
        description: "Maintain risk register and exceptions registry",
        completed: true
      }
    ];
  }
  buildEvidenceGraph(claims) {
    return claims.filter((claim) => claim.status === "approved" /* Approved */).flatMap(
      (claim) => claim.channels.map((channel) => ({
        claimId: claim.claimId,
        evidenceSource: claim.evidenceSource,
        channel,
        lastValidatedAt: claim.updatedAt,
        driftDetected: Boolean(claim.expiry && new Date(claim.expiry) < /* @__PURE__ */ new Date())
      }))
    );
  }
  computeEnforcementGates(claim) {
    const gates = ["evidence validation"];
    if (claim.riskTier === "high" /* High */) {
      gates.push("legal approval", "security approval", "pmm final approval");
    } else if (claim.riskTier === "medium" /* Medium */) {
      gates.push("pmm approval");
    }
    if (claim.forwardLooking) {
      gates.push("forward-looking attestation");
    }
    return gates;
  }
  requiredApprovals(claim) {
    if (claim.riskTier === "high" /* High */ || claim.forwardLooking) {
      return ["legal", "security", "pmm"];
    }
    if (claim.riskTier === "medium" /* Medium */) {
      return ["pmm"];
    }
    return ["pmm"];
  }
  calculateRiskTier(input) {
    if (input.forwardLooking || input.complianceSurface?.length) {
      return "high" /* High */;
    }
    if (input.channels.includes("web") && input.evidenceType === "demo_video" /* DemoVideo */) {
      return "medium" /* Medium */;
    }
    return "low" /* Low */;
  }
  computeReviewDate(riskTier, now) {
    const days = riskTier === "high" /* High */ ? 90 : riskTier === "medium" /* Medium */ ? 120 : 180;
    const reviewDate = new Date(now);
    reviewDate.setDate(now.getDate() + days);
    return reviewDate.toISOString();
  }
  async updateActiveClaimsGauge() {
    const claims = await this.repository.loadClaims();
    const approved = claims.filter((claim) => claim.status === "approved" /* Approved */);
    const counts = {
      web: 0,
      sales: 0,
      content: 0
    };
    approved.forEach((claim) => {
      claim.channels.forEach((channel) => {
        counts[channel] += 1;
      });
    });
    Object.keys(counts).forEach((channel) => {
      _GtmMessagingService.activeApprovedClaims.set({ channel }, counts[channel]);
    });
  }
};

// src/routes/gtm-messaging.ts
var gtmRouter = Router29();
var service2 = new GtmMessagingService();
function asyncHandler2(fn) {
  return async (req, res, next) => {
    try {
      await fn(req, res, next);
    } catch (error) {
      next(error);
    }
  };
}
gtmRouter.get(
  "/icp",
  asyncHandler2(async (_req, res) => {
    res.json(await service2.getIcpBrief());
  })
);
gtmRouter.get(
  "/message-house",
  asyncHandler2(async (_req, res) => {
    res.json(await service2.getMessageHouse());
  })
);
gtmRouter.post(
  "/claims",
  asyncHandler2(async (req, res) => {
    const parsed = ClaimInputSchema.parse(req.body);
    const result2 = await service2.submitClaim(parsed);
    res.status(201).json(result2);
  })
);
gtmRouter.post(
  "/claims/:claimId/approve",
  asyncHandler2(async (req, res) => {
    const { claimId } = req.params;
    const { approver, notes } = req.body;
    if (!approver) {
      return res.status(400).json({ error: "approver is required" });
    }
    const claim = await service2.recordApproval(claimId, approver, notes);
    res.json(claim);
  })
);
gtmRouter.post(
  "/claims/expire",
  asyncHandler2(async (_req, res) => {
    const expired = await service2.expireClaims();
    res.json({ expired });
  })
);
gtmRouter.get(
  "/claims/channel/:channel",
  asyncHandler2(async (req, res) => {
    res.json(await service2.listClaimsForChannel(req.params.channel));
  })
);
gtmRouter.get(
  "/templates",
  asyncHandler2(async (_req, res) => {
    res.json({ templates: service2.getContentTemplates() });
  })
);
gtmRouter.get(
  "/website-kpis",
  asyncHandler2(async (_req, res) => {
    res.json({ kpis: service2.getWebsiteKpis() });
  })
);
gtmRouter.get(
  "/nurture-tracks",
  asyncHandler2(async (_req, res) => {
    res.json({ nurture: service2.getNurtureTracks() });
  })
);
gtmRouter.get(
  "/enablement",
  asyncHandler2(async (_req, res) => {
    res.json({ assets: service2.getEnablementAssets() });
  })
);
gtmRouter.get(
  "/channels",
  asyncHandler2(async (_req, res) => {
    res.json({ playbooks: service2.getChannelPlaybooks() });
  })
);
gtmRouter.get(
  "/checklist",
  asyncHandler2(async (_req, res) => {
    const claims = await service2.listClaimsForChannel("web");
    res.json({ checklist: service2.buildExecutionChecklist(claims) });
  })
);
gtmRouter.get(
  "/evidence-graph",
  asyncHandler2(async (_req, res) => {
    const claims = await service2.listClaimsForChannel("web");
    res.json({ graph: service2.buildEvidenceGraph(claims) });
  })
);
gtmRouter.post(
  "/routing",
  asyncHandler2(async (req, res) => {
    const { behavioralScore, firmographic, intentLevel } = req.body;
    if (behavioralScore === void 0 || !firmographic || !intentLevel) {
      return res.status(400).json({ error: "behavioralScore, firmographic, and intentLevel are required" });
    }
    res.json({ route: service2.decideAdaptiveRoute({ behavioralScore, firmographic, intentLevel }) });
  })
);
gtmRouter.post(
  "/qa/scan",
  asyncHandler2(async (req, res) => {
    const { content } = req.body;
    if (!content) {
      return res.status(400).json({ error: "content is required" });
    }
    res.json(await service2.closedLoopQa(content));
  })
);

// src/routes/airgap.ts
import { Router as Router30 } from "express";

// src/services/DeterministicExportService.ts
init_logger();
init_database();
import {
  createWriteStream as createWriteStream7,
  createReadStream as createReadStream2,
  existsSync as existsSync3,
  mkdirSync as mkdirSync3,
  rmSync
} from "fs";
import { createHash as createHash24 } from "crypto";
import { join as join4 } from "path";
import archiver4 from "archiver";
import { randomUUID as uuidv425 } from "node:crypto";
import { createRequire as createRequire2 } from "module";

// src/utils/dataRedaction.ts
import { default as pino53 } from "pino";
import { createHash as createHash23 } from "crypto";
var logger50 = pino53();
var PII_DEFINITIONS = {
  EMAIL: ["email", "properties.email"],
  PHONE: ["phone", "properties.phone"],
  SSN: ["ssn", "properties.ssn"],
  CREDIT_CARD: ["creditCard", "properties.creditCard"],
  NAME: ["name", "label", "properties.name"],
  ADDRESS: ["address", "properties.address"],
  IP_ADDRESS: ["ipAddress", "properties.ipAddress"],
  DATE_OF_BIRTH: ["dob", "properties.dob"]
  // Add more as needed
};
var PATH_TO_PII_TYPE = {};
Object.keys(PII_DEFINITIONS).forEach((type) => {
  PII_DEFINITIONS[type].forEach((path55) => {
    PATH_TO_PII_TYPE[path55] = type;
  });
});
var REDACTION_POLICIES_BY_ROLE = {
  ADMIN: {
    // Admins see everything, no redaction by default
  },
  ANALYST: {
    EMAIL: "MASK_PARTIAL" /* MASK_PARTIAL */,
    PHONE: "MASK_PARTIAL" /* MASK_PARTIAL */,
    SSN: "[REDACTED]" /* REDACT */,
    CREDIT_CARD: "[REDACTED]" /* REDACT */
    // Other PII types might not be redacted for analysts
  },
  VIEWER: {
    EMAIL: "[REDACTED]" /* REDACT */,
    PHONE: "[REDACTED]" /* REDACT */,
    SSN: "[REDACTED]" /* REDACT */,
    CREDIT_CARD: "[REDACTED]" /* REDACT */,
    NAME: "MASK_PARTIAL" /* MASK_PARTIAL */,
    ADDRESS: "[REDACTED]" /* REDACT */,
    IP_ADDRESS: "[REDACTED]" /* REDACT */,
    DATE_OF_BIRTH: "[REDACTED]" /* REDACT */
  }
};
function redactData(data, user, sensitivity) {
  if (!data) return data;
  const userRole = user.role || "VIEWER";
  const policy2 = REDACTION_POLICIES_BY_ROLE[userRole.toUpperCase()] || {};
  let redactedData;
  try {
    redactedData = structuredClone(data);
  } catch (e) {
    redactedData = JSON.parse(JSON.stringify(data));
  }
  let piiRedactedCount = 0;
  const applyRedaction = (obj, prefix) => {
    if (typeof obj !== "object" || obj === null) return;
    for (const key in obj) {
      if (Object.prototype.hasOwnProperty.call(obj, key)) {
        const fullPath = prefix ? `${prefix}.${key}` : key;
        const piiType = PATH_TO_PII_TYPE[fullPath];
        if (piiType) {
          const strategy = policy2[piiType];
          if (strategy) {
            obj[key] = applyStrategy(
              obj[key],
              piiType,
              strategy
            );
            piiRedactedCount++;
            logger50.debug(
              `Redacted PII field: ${fullPath} with strategy: ${strategy}`
            );
          }
        }
        if (typeof obj[key] === "object") {
          applyRedaction(obj[key], fullPath);
        }
      }
    }
  };
  applyRedaction(redactedData, "");
  logger50.info(
    `Data redaction complete for user ${user.id} (role: ${userRole}). Redacted ${piiRedactedCount} PII fields.`
  );
  return redactedData;
}
function applyStrategy(value, piiType, strategy) {
  if (value === void 0 || value === null) return value;
  switch (strategy) {
    case "[REDACTED]" /* REDACT */:
      return "[REDACTED]" /* REDACT */;
    case "MASK_PARTIAL" /* MASK_PARTIAL */:
      return maskPartial(String(value), piiType);
    case "HASH" /* HASH */:
      return hashValue(String(value));
    case "NULL" /* NULL */:
      return null;
    default:
      return value;
  }
}
function maskPartial(value, piiType) {
  switch (piiType) {
    case "EMAIL":
      const atIndex = value.indexOf("@");
      if (atIndex > 0) {
        return `${value.substring(0, 3)}***${value.substring(atIndex)}`;
      }
      return "***";
    case "PHONE":
      if (value.length > 4) {
        return `***-***-${value.substring(value.length - 4)}`;
      }
      return "***";
    case "NAME":
      if (value.length > 2) {
        return `${value.substring(0, 1)}***${value.substring(value.length - 1)}`;
      }
      return "***";
    default:
      return "[MASKED]";
  }
}
function hashValue(value) {
  return createHash23("sha256").update(value).digest("hex");
}

// src/services/DeterministicExportService.ts
init_quantum_identity_manager();
var require3 = createRequire2(import.meta.url);
var log4 = logger_default.child({ name: "DeterministicExportService" });
var DeterministicExportService = class {
  tempDir;
  maxExportSize = 500 * 1024 * 1024;
  // 500MB limit
  constructor() {
    this.tempDir = join4(process.cwd(), "tmp", "exports");
    if (!existsSync3(this.tempDir)) {
      mkdirSync3(this.tempDir, { recursive: true });
    }
  }
  /**
   * Create deterministic export bundle with manifest
   */
  async createExportBundle(request, session) {
    const exportId = uuidv425();
    const startTime = Date.now();
    log4.info({ exportId, request }, "Starting deterministic export");
    try {
      const workDir = join4(this.tempDir, exportId);
      mkdirSync3(workDir, { recursive: true });
      const { entities, relationships } = await this.extractDataDeterministic(
        session,
        request
      );
      const transformedData = await this.applyTransforms(
        { entities, relationships },
        request,
        exportId
      );
      const files = await this.generateFiles(transformedData, request, workDir);
      const manifest = await this.createManifest(
        exportId,
        request,
        files,
        transformedData.transforms
      );
      const bundlePath = await this.createBundle(workDir, exportId, manifest);
      await this.storeExportMetadata(manifest);
      const executionTime = Date.now() - startTime;
      log4.info(
        {
          exportId,
          bundleSize: manifest.integrity.totalBytes,
          fileCount: manifest.integrity.totalFiles,
          executionTime
        },
        "Deterministic export completed"
      );
      return { exportId, bundlePath, manifest };
    } catch (error) {
      log4.error(
        {
          exportId,
          error: error.message
        },
        "Export failed"
      );
      const workDir = join4(this.tempDir, exportId);
      if (existsSync3(workDir)) {
        rmSync(workDir, { recursive: true, force: true });
      }
      throw error;
    }
  }
  /**
   * Verify export bundle integrity
   */
  async verifyExportBundle(bundlePath) {
    log4.info({ bundlePath }, "Verifying export bundle integrity");
    try {
      const manifest = await this.extractManifestFromBundle(bundlePath);
      const actualBundleHash = await this.calculateFileHash(bundlePath);
      const expectedBundleHash = manifest.integrity.bundleHash;
      if (actualBundleHash !== expectedBundleHash) {
        manifest.verification = {
          verified: false,
          verifiedAt: (/* @__PURE__ */ new Date()).toISOString(),
          verificationErrors: [
            `Bundle hash mismatch: expected ${expectedBundleHash}, got ${actualBundleHash}`
          ]
        };
        return manifest;
      }
      const tempVerifyDir = join4(this.tempDir, `verify-${Date.now()}`);
      await this.extractBundle(bundlePath, tempVerifyDir);
      const verificationErrors = [];
      for (const fileEntry of manifest.files) {
        const filePath = join4(tempVerifyDir, fileEntry.filename);
        if (!existsSync3(filePath)) {
          verificationErrors.push(`Missing file: ${fileEntry.filename}`);
          continue;
        }
        const actualHash = await this.calculateFileHash(filePath);
        if (actualHash !== fileEntry.sha256) {
          verificationErrors.push(
            `File hash mismatch for ${fileEntry.filename}: expected ${fileEntry.sha256}, got ${actualHash}`
          );
        }
        const actualSize = (await import("fs/promises")).stat(filePath).then((s) => s.size);
        if (await actualSize !== fileEntry.bytes) {
          verificationErrors.push(
            `File size mismatch for ${fileEntry.filename}: expected ${fileEntry.bytes}, got ${await actualSize}`
          );
        }
      }
      rmSync(tempVerifyDir, { recursive: true, force: true });
      manifest.verification = {
        verified: verificationErrors.length === 0,
        verifiedAt: (/* @__PURE__ */ new Date()).toISOString(),
        verificationErrors
      };
      log4.info(
        {
          bundlePath,
          verified: manifest.verification.verified,
          errorCount: verificationErrors.length
        },
        "Bundle verification completed"
      );
      return manifest;
    } catch (error) {
      log4.error(
        {
          bundlePath,
          error: error.message
        },
        "Bundle verification failed"
      );
      throw error;
    }
  }
  async extractDataDeterministic(session, request) {
    const { where, params } = this.buildFilterClauses(request);
    const entitiesQuery = `
      MATCH (e:Entity) ${where}
      RETURN e
      ORDER BY e.id, e.created_at, e.name
    `;
    const entitiesResult = await session.run(entitiesQuery, params);
    const entities = entitiesResult.records.map((record2) => {
      const entity = record2.get("e").properties;
      return this.normalizeEntity(entity);
    });
    const relationshipsQuery = `
      MATCH (a:Entity)-[r:RELATIONSHIP]->(b:Entity)
      ${where.replace(/e\./g, "a.")}
      ${where ? "AND b.investigation_id = a.investigation_id" : ""}
      RETURN a, r, b
      ORDER BY r.id, r.created_at, a.id, b.id
    `;
    const relationshipsResult = await session.run(relationshipsQuery, params);
    const relationships = relationshipsResult.records.map((record2) => {
      const r = record2.get("r");
      const a = record2.get("a").properties;
      const b = record2.get("b").properties;
      return this.normalizeRelationship(r, a, b);
    });
    return { entities, relationships };
  }
  buildFilterClauses(request) {
    const conditions = [];
    const params = {};
    if (request.investigationId) {
      conditions.push("e.investigation_id = $investigationId");
      params.investigationId = request.investigationId;
    }
    if (request.entityType) {
      conditions.push("e.type = $entityType");
      params.entityType = request.entityType;
    }
    if (request.tags && request.tags.length > 0) {
      conditions.push("ANY(tag IN e.tags WHERE tag IN $tags)");
      params.tags = request.tags;
    }
    if (request.startDate) {
      conditions.push("e.created_at >= $startDate");
      params.startDate = request.startDate;
    }
    if (request.endDate) {
      conditions.push("e.created_at <= $endDate");
      params.endDate = request.endDate;
    }
    const where = conditions.length > 0 ? `WHERE ${conditions.join(" AND ")}` : "";
    return { where, params };
  }
  normalizeEntity(entity) {
    const normalized = {};
    const sortedKeys = Object.keys(entity).sort();
    for (const key of sortedKeys) {
      normalized[key] = entity[key];
    }
    return normalized;
  }
  normalizeRelationship(r, a, b) {
    return {
      id: r.properties?.id || r.identity?.toString?.(),
      type: r.type || r.properties?.type,
      properties: r.properties || {},
      source: a.uuid || a.id,
      target: b.uuid || b.id,
      sourceEntity: this.normalizeEntity(a),
      targetEntity: this.normalizeEntity(b)
    };
  }
  async applyTransforms(data, request, exportId) {
    const transforms = [];
    let { entities, relationships } = data;
    const redactionTransform = {
      id: uuidv425(),
      type: "REDACTION",
      description: "Apply data redaction based on user permissions",
      appliedAt: (/* @__PURE__ */ new Date()).toISOString(),
      parameters: { userRole: "analyst" },
      // From request context
      inputHash: this.calculateObjectHash({ entities, relationships }),
      outputHash: ""
    };
    const redactionUser = {
      id: request.userId,
      email: "export-service@internal",
      username: "export-service",
      firstName: "Export",
      lastName: "Service",
      role: "analyst",
      isActive: true,
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date()
    };
    entities = entities.map(
      (entity) => redactData(entity, redactionUser)
    );
    relationships = relationships.map((rel) => ({
      ...rel,
      sourceEntity: redactData(rel.sourceEntity, redactionUser),
      targetEntity: redactData(rel.targetEntity, redactionUser)
    }));
    redactionTransform.outputHash = this.calculateObjectHash({
      entities,
      relationships
    });
    transforms.push(redactionTransform);
    if (request.entityType || request.tags) {
      const filterTransform = {
        id: uuidv425(),
        type: "FILTERING",
        description: "Filter entities and relationships based on criteria",
        appliedAt: (/* @__PURE__ */ new Date()).toISOString(),
        parameters: {
          entityType: request.entityType,
          tags: request.tags
        },
        inputHash: redactionTransform.outputHash,
        outputHash: ""
      };
      filterTransform.outputHash = this.calculateObjectHash({
        entities,
        relationships
      });
      transforms.push(filterTransform);
    }
    return { entities, relationships, transforms };
  }
  async generateFiles(data, request, workDir) {
    const files = [];
    const entitiesFile = await this.writeJSONFile(
      join4(workDir, "entities.json"),
      data.entities,
      "Entity data export"
    );
    files.push(entitiesFile);
    const relationshipsFile = await this.writeJSONFile(
      join4(workDir, "relationships.json"),
      data.relationships,
      "Relationship data export"
    );
    files.push(relationshipsFile);
    const transformsFile = await this.writeJSONFile(
      join4(workDir, "transforms.json"),
      data.transforms,
      "Data transformation log"
    );
    files.push(transformsFile);
    if (request.format === "csv" || request.format === "bundle") {
      const entitiesCSV = await this.writeCSVFile(
        join4(workDir, "entities.csv"),
        data.entities
      );
      files.push(entitiesCSV);
      const relationshipsCSV = await this.writeCSVFile(
        join4(workDir, "relationships.csv"),
        data.relationships
      );
      files.push(relationshipsCSV);
    }
    return files;
  }
  async writeJSONFile(filePath, data, description) {
    const content = JSON.stringify(data, null, 2);
    const buffer = Buffer.from(content, "utf8");
    await import("fs/promises").then((fs44) => fs44.writeFile(filePath, buffer));
    const hash3 = createHash24("sha256").update(buffer).digest("hex");
    return {
      filename: filePath.split("/").pop(),
      sha256: hash3,
      bytes: buffer.length,
      contentType: "application/json",
      created: (/* @__PURE__ */ new Date()).toISOString(),
      transforms: ["deterministic-ordering", "json-serialization"]
    };
  }
  async writeCSVFile(filePath, data) {
    const { Parser: Parser2 } = require3("json2csv");
    if (data.length === 0) {
      const buffer2 = Buffer.from("", "utf8");
      await import("fs/promises").then((fs44) => fs44.writeFile(filePath, buffer2));
      return {
        filename: filePath.split("/").pop(),
        sha256: createHash24("sha256").update(buffer2).digest("hex"),
        bytes: 0,
        contentType: "text/csv",
        created: (/* @__PURE__ */ new Date()).toISOString(),
        transforms: ["deterministic-ordering", "csv-serialization"]
      };
    }
    const allKeys = /* @__PURE__ */ new Set();
    data.forEach(
      (item) => Object.keys(item).forEach((key) => allKeys.add(key))
    );
    const sortedFields = Array.from(allKeys).sort();
    const parser = new Parser2({ fields: sortedFields, header: true });
    const csv = parser.parse(data);
    const buffer = Buffer.from(csv, "utf8");
    await import("fs/promises").then((fs44) => fs44.writeFile(filePath, buffer));
    const hash3 = createHash24("sha256").update(buffer).digest("hex");
    return {
      filename: filePath.split("/").pop(),
      sha256: hash3,
      bytes: buffer.length,
      contentType: "text/csv",
      created: (/* @__PURE__ */ new Date()).toISOString(),
      transforms: ["deterministic-ordering", "csv-serialization"]
    };
  }
  async createManifest(exportId, request, files, transforms) {
    const manifest = {
      version: "1.0",
      exportId,
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      createdBy: request.userId,
      request: {
        ...request,
        // Remove sensitive data from manifest
        userId: "[REDACTED]"
      },
      files,
      transforms,
      integrity: {
        manifestHash: "",
        bundleHash: "",
        totalFiles: files.length,
        totalBytes: files.reduce((sum, f) => sum + f.bytes, 0)
      },
      verification: {
        verified: false,
        verificationErrors: []
      }
    };
    const manifestForHash = { ...manifest };
    delete manifestForHash.integrity.manifestHash;
    delete manifestForHash.integrity.bundleHash;
    manifest.integrity.manifestHash = this.calculateObjectHash(manifestForHash);
    const serviceId = process.env.SERVICE_ID || "summit-api-export";
    const signedPayload = `service=${serviceId};hash=${manifest.integrity.manifestHash}`;
    const identity = quantumIdentityManager.issueIdentity(signedPayload);
    manifest.pqcServiceId = serviceId;
    manifest.pqcSignature = identity.signature;
    return manifest;
  }
  async createBundle(workDir, exportId, manifest) {
    const bundlePath = join4(this.tempDir, `${exportId}.zip`);
    const manifestPath = join4(workDir, "manifest.json");
    await import("fs/promises").then(
      (fs44) => fs44.writeFile(manifestPath, JSON.stringify(manifest, null, 2))
    );
    const output = createWriteStream7(bundlePath);
    const archive = archiver4("zip", { zlib: { level: 9 } });
    archive.pipe(output);
    archive.directory(workDir, false);
    await archive.finalize();
    return bundlePath;
  }
  async calculateFileHash(filePath) {
    const hash3 = createHash24("sha256");
    const stream2 = createReadStream2(filePath);
    for await (const chunk of stream2) {
      hash3.update(chunk);
    }
    return hash3.digest("hex");
  }
  calculateObjectHash(obj) {
    const jsonString = JSON.stringify(obj, Object.keys(obj).sort());
    return createHash24("sha256").update(jsonString).digest("hex");
  }
  async storeExportMetadata(manifest) {
    const pool4 = getPostgresPool2();
    try {
      await pool4.query(
        `
        INSERT INTO export_manifests (
          id, version, created_by, request_params, files_count, total_bytes,
          manifest_hash, bundle_hash, transforms, created_at
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
      `,
        [
          manifest.exportId,
          manifest.version,
          manifest.createdBy,
          JSON.stringify(manifest.request),
          manifest.integrity.totalFiles,
          manifest.integrity.totalBytes,
          manifest.integrity.manifestHash,
          manifest.integrity.bundleHash,
          JSON.stringify(manifest.transforms),
          manifest.createdAt
        ]
      );
    } catch (error) {
      log4.warn(
        {
          exportId: manifest.exportId,
          error: error.message
        },
        "Failed to store export metadata"
      );
    }
  }
  async extractManifestFromBundle(bundlePath) {
    throw new Error("Bundle extraction not implemented");
  }
  async extractBundle(bundlePath, extractPath) {
    throw new Error("Bundle extraction not implemented");
  }
};

// src/services/AirgapService.ts
init_database();
init_quantum_identity_manager();
init_logger();
import { join as join5 } from "path";
import { existsSync as existsSync4, mkdirSync as mkdirSync4, readFileSync as readFileSync3, rmSync as rmSync2, createReadStream as createReadStream3 } from "fs";
import { exec as exec2 } from "child_process";
import { promisify as promisify3 } from "util";
import { createHash as createHash25, randomUUID as randomUUID44 } from "crypto";
var execAsync2 = promisify3(exec2);
var log5 = logger_default.child({ name: "AirgapService" });
var AirgapService = class {
  exportService;
  importDir;
  constructor() {
    this.exportService = new DeterministicExportService();
    this.importDir = join5(process.cwd(), "storage", "imports");
    if (!existsSync4(this.importDir)) {
      mkdirSync4(this.importDir, { recursive: true });
    }
  }
  async exportBundle(request, session) {
    if (process.env.AIRGAP !== "true") {
      throw new Error("Airgap feature is disabled");
    }
    return this.exportService.createExportBundle(request, session);
  }
  async importBundle(tenantId, filePath, userId) {
    if (process.env.AIRGAP !== "true") {
      throw new Error("Airgap feature is disabled");
    }
    const importId = randomUUID44();
    const workDir = join5(this.importDir, importId);
    mkdirSync4(workDir, { recursive: true });
    try {
      log5.info({ importId, tenantId }, "Starting airgap import");
      await execAsync2(`unzip -q "${filePath}" -d "${workDir}"`);
      const manifestPath = join5(workDir, "manifest.json");
      if (!existsSync4(manifestPath)) {
        throw new Error("Invalid bundle: missing manifest.json");
      }
      const manifestStr = readFileSync3(manifestPath, "utf-8");
      const manifest = JSON.parse(manifestStr);
      if (!manifest.pqcSignature) {
        throw new Error("Security Violation: Airgap bundle missing PQC signature");
      }
      const serviceId = manifest.pqcServiceId || "unknown";
      const signedPayload = `service=${serviceId};hash=${manifest.integrity.manifestHash}`;
      const identityToVerify = {
        serviceId: signedPayload,
        publicKey: "simulated-key",
        algorithm: "KYBER-768",
        issuedAt: manifest.createdAt,
        expiresAt: new Date(Date.now() + 1e3).toISOString(),
        signature: manifest.pqcSignature
      };
      const isPqcValid = quantumIdentityManager.verifyIdentity(identityToVerify);
      if (!isPqcValid) {
        throw new Error("Security Violation: Invalid PQC signature on airgap bundle");
      }
      log5.info({ importId, serviceId: manifest.pqcServiceId }, "PQC Signature verified");
      if (manifest.request.tenantId !== tenantId) {
        throw new Error(`Tenant mismatch: Bundle belongs to ${manifest.request.tenantId}, but importing into ${tenantId}`);
      }
      const bundleHash = await this.calculateFileHash(filePath);
      for (const file of manifest.files) {
        const extractedFile = join5(workDir, file.filename);
        if (!existsSync4(extractedFile)) {
          throw new Error(`Missing file in bundle: ${file.filename}`);
        }
        const fileHash = await this.calculateFileHash(extractedFile);
        if (fileHash !== file.sha256) {
          throw new Error(`File integrity check failed for ${file.filename}`);
        }
      }
      const pool4 = getPostgresPool2();
      await pool4.query(
        `INSERT INTO imported_snapshots (id, tenant_id, bundle_hash, manifest, created_by, status, storage_path)
         VALUES ($1, $2, $3, $4, $5, $6, $7)`,
        [importId, tenantId, bundleHash, JSON.stringify(manifest), userId, "verified", workDir]
      );
      log5.info({ importId, tenantId }, "Airgap import completed successfully");
      return {
        importId,
        manifest,
        status: "verified"
      };
    } catch (error) {
      log5.error({ importId, error: error.message }, "Airgap import failed");
      rmSync2(workDir, { recursive: true, force: true });
      throw error;
    }
  }
  async getImport(importId, tenantId) {
    const pool4 = getPostgresPool2();
    const res = await pool4.query("SELECT * FROM imported_snapshots WHERE id = $1 AND tenant_id = $2", [importId, tenantId]);
    if (res.rows.length === 0) return null;
    return res.rows[0];
  }
  async calculateFileHash(filePath) {
    const hash3 = createHash25("sha256");
    const stream2 = createReadStream3(filePath);
    return new Promise((resolve2, reject) => {
      stream2.on("error", reject);
      stream2.on("data", (chunk) => hash3.update(chunk));
      stream2.on("end", () => resolve2(hash3.digest("hex")));
    });
  }
};

// src/middleware/tenantHeader.ts
function tenantHeader(required = true) {
  return function(req, res, next) {
    const headerTenant = req.headers["x-tenant-id"] || req.headers["x-tenant"] || "";
    req.tenantId = headerTenant || null;
    if (required && !headerTenant) {
      return res.status(400).json({ error: "tenant_required" });
    }
    next();
  };
}

// src/routes/airgap.ts
init_database();
import { unlink } from "fs/promises";
import { createWriteStream as createWriteStream8 } from "fs";
import { join as join6 } from "path";
import { randomUUID as randomUUID45 } from "crypto";
var airgapRouter = Router30();
var service3 = new AirgapService();
airgapRouter.use(tenantHeader());
airgapRouter.post("/export", async (req, res) => {
  let session;
  try {
    const tenantId = req.tenantId;
    session = getNeo4jDriver2().session();
    const request = {
      ...req.body,
      tenantId,
      userId: req.user?.id || "unknown"
    };
    const result2 = await service3.exportBundle(request, session);
    res.json(result2);
  } catch (e) {
    res.status(500).json({ error: e.message });
  } finally {
    if (session) await session.close();
  }
});
airgapRouter.post("/import", async (req, res) => {
  const tenantId = req.tenantId;
  const tempFile = join6("/tmp", `import-${randomUUID45()}.zip`);
  try {
    const writeStream = createWriteStream8(tempFile);
    await new Promise((resolve2, reject) => {
      req.pipe(writeStream);
      writeStream.on("finish", () => resolve2());
      writeStream.on("error", reject);
      req.on("error", reject);
    });
    const result2 = await service3.importBundle(tenantId, tempFile, req.user?.id || "unknown");
    res.json(result2);
  } catch (e) {
    res.status(500).json({ error: e.message });
  } finally {
    try {
      await unlink(tempFile);
    } catch {
    }
  }
});
airgapRouter.get("/imports/:id", async (req, res) => {
  try {
    const result2 = await service3.getImport(req.params.id, req.tenantId);
    if (!result2) return res.status(404).json({ error: "Import not found" });
    res.json(result2);
  } catch (e) {
    res.status(500).json({ error: e.message });
  }
});

// src/routes/analytics.ts
import { Router as Router31 } from "express";

// src/services/AnalyticsService.ts
var AnalyticsService = class _AnalyticsService {
  static instance;
  constructor() {
  }
  /**
   * @method getInstance
   * @description Gets the singleton instance of the AnalyticsService.
   * @static
   * @returns {AnalyticsService} The singleton instance.
   */
  static getInstance() {
    if (!_AnalyticsService.instance) {
      _AnalyticsService.instance = new _AnalyticsService();
    }
    return _AnalyticsService.instance;
  }
  // --- Link/Path/Community/Centrality ---
  /**
   * @method findPaths
   * @description Finds paths between two nodes in the graph using specified algorithms.
   * @param {string} sourceId - The ID of the source node.
   * @param {string} targetId - The ID of the target node.
   * @param {'shortest' | 'k-paths'} [algorithm='shortest'] - The pathfinding algorithm to use.
   * @param {object} [params={}] - Additional parameters, such as `k` for k-paths or `maxDepth`.
   * @returns {Promise<AnalyticResult<any>>} The paths found and an explanation of the method.
   *
   * @example
   * ```typescript
   * const shortestPath = await analyticsService.findPaths('node-a', 'node-z', 'shortest', { maxDepth: 5 });
   * const top3Paths = await analyticsService.findPaths('node-a', 'node-z', 'k-paths', { k: 3, maxDepth: 4 });
   * ```
   */
  async findPaths(sourceId, targetId, algorithm = "shortest", params = {}) {
    const k = params.k ? parseInt(params.k, 10) : 3;
    const maxDepth = params.maxDepth ? parseInt(params.maxDepth, 10) : 5;
    if (isNaN(k) || k < 1) throw new Error("Invalid parameter: k");
    if (isNaN(maxDepth) || maxDepth < 1) throw new Error("Invalid parameter: maxDepth");
    let cypher = "";
    let explanation = "";
    if (algorithm === "shortest") {
      cypher = `
        MATCH (source {id: $sourceId}), (target {id: $targetId})
        MATCH path = shortestPath((source)-[*..${maxDepth}]-(target))
        RETURN [node in nodes(path) | node.id] as nodeIds,
               [rel in relationships(path) | type(rel)] as relTypes,
               length(path) as cost
      `;
      explanation = "Calculated using unweighted shortest path (BFS).";
    } else {
      cypher = `
        MATCH (source {id: $sourceId}), (target {id: $targetId})
        MATCH path = (source)-[*..${maxDepth}]-(target)
        RETURN [node in nodes(path) | node.id] as nodeIds,
               [rel in relationships(path) | type(rel)] as relTypes,
               length(path) as cost
        ORDER BY length(path) ASC
        LIMIT toInteger($k)
      `;
      explanation = `Calculated top ${k} shortest paths.`;
    }
    const results = await runCypher(cypher, { sourceId, targetId, k });
    return {
      data: results,
      xai: {
        features: { algorithm, sourceId, targetId, k, maxDepth },
        metrics: { pathCount: results.length },
        explanation,
        contributingFactors: ["Graph topology", "Edge existence"]
      }
    };
  }
  /**
   * @method detectCommunities
   * @description Detects communities or clusters of nodes within the graph.
   * This implementation uses a client-side Weakly Connected Components (WCC) algorithm as a stand-in for more advanced methods like Louvain or LPA,
   * as it does not require a graph analytics library like GDS.
   * @param {'louvain' | 'leiden' | 'lpa'} [algorithm='lpa'] - The desired algorithm (currently defaults to a WCC implementation).
   * @param {object} [params={}] - Additional parameters for the algorithm (currently unused).
   * @returns {Promise<AnalyticResult<any>>} The detected communities and an explanation.
   *
   * @example
   * ```typescript
   * const communities = await analyticsService.detectCommunities('lpa');
   * console.log(`Found ${communities.xai.metrics.communityCount} communities.`);
   * ```
   */
  async detectCommunities(algorithm = "lpa", params = {}) {
    let results = [];
    let explanation = "";
    const cypher = `
      MATCH (n)
      OPTIONAL MATCH (n)-[r]-(m)
      WITH n, collect(DISTINCT m.id) as neighbors
      RETURN n.id as nodeId, n.label as label, neighbors
      LIMIT 2000
    `;
    const nodes = await runCypher(cypher, {});
    const visited = /* @__PURE__ */ new Set();
    const communities = {};
    let communityCount = 0;
    const nodeMap = /* @__PURE__ */ new Map();
    for (const node of nodes) {
      nodeMap.set(node.nodeId, node);
    }
    for (const node of nodes) {
      if (!visited.has(node.nodeId)) {
        communityCount++;
        const communityId = `c_${communityCount}`;
        communities[communityId] = [];
        const queue = [node.nodeId];
        visited.add(node.nodeId);
        while (queue.length > 0) {
          const currentId = queue.shift();
          communities[communityId].push(currentId);
          const currentNode = nodeMap.get(currentId);
          if (currentNode && currentNode.neighbors) {
            for (const neighborId of currentNode.neighbors) {
              if (!visited.has(neighborId) && nodeMap.has(neighborId)) {
                visited.add(neighborId);
                queue.push(neighborId);
              }
            }
          }
        }
      }
    }
    results = Object.entries(communities).map(([id, members]) => ({
      communityId: id,
      size: members.length,
      members: members.slice(0, 10)
      // Truncate for display
    })).sort((a, b) => b.size - a.size);
    explanation = "Grouped nodes by structural connectivity (Connected Components).";
    return {
      data: results,
      xai: {
        features: { algorithm: "connected-components-client-side", nodeCount: nodes.length },
        metrics: { communityCount: results.length, largestCommunity: results[0]?.size || 0 },
        explanation,
        contributingFactors: ["Edge density", "Path reachability"]
      }
    };
  }
  /**
   * @method calculateCentrality
   * @description Calculates the centrality of nodes to identify the most influential or important nodes in the graph.
   * Provides approximations for Betweenness and Eigenvector centrality using simpler, more performant Cypher queries.
   * @param {'betweenness' | 'eigenvector'} algorithm - The centrality algorithm to use.
   * @param {object} [params={}] - Additional parameters, such as `limit` to control the number of results.
   * @returns {Promise<AnalyticResult<any>>} A list of nodes and their centrality scores.
   *
   * @example
   * ```typescript
   * const topConnectors = await analyticsService.calculateCentrality('betweenness', { limit: 10 });
   * const influentialNodes = await analyticsService.calculateCentrality('eigenvector', { limit: 10 });
   * ```
   */
  async calculateCentrality(algorithm, params = {}) {
    const limit = params.limit ? parseInt(params.limit, 10) : 20;
    let cypher = "";
    let explanation = "";
    if (algorithm === "betweenness") {
      cypher = `
        MATCH (n)
        WHERE size((n)--()) > 1
        WITH n, size((n)--()) as degree
        ORDER BY degree DESC
        LIMIT $limit
        RETURN n.id as nodeId, degree as score, "Approx. via Degree (High Performance)" as note
      `;
      explanation = "Approximated centrality using Degree Centrality as a high-performance proxy for Betweenness.";
    } else {
      cypher = `
        MATCH (n)-[r]-(m)
        WITH n, count(r) + sum(size((m)--())) as weightedScore
        ORDER BY weightedScore DESC
        LIMIT $limit
        RETURN n.id as nodeId, weightedScore as score
      `;
      explanation = "Recursive node importance based on connection to high-degree nodes.";
    }
    const results = await runCypher(cypher, { limit });
    return {
      data: results,
      xai: {
        features: { algorithm },
        metrics: { nodesScored: results.length },
        explanation,
        contributingFactors: ["Node Degree", "Neighbor Connectivity"]
      }
    };
  }
  // --- Pattern Miner ---
  /**
   * @method minePatterns
   * @description Mines the graph for specific, predefined patterns of behavior or structure.
   * This includes temporal motifs (like bursts of activity), co-travel patterns, and financial structuring (like layering).
   * @param {'temporal-motifs' | 'co-travel' | 'financial-structuring'} patternType - The type of pattern to search for.
   * @param {object} [params={}] - Additional parameters for the mining query.
   * @returns {Promise<AnalyticResult<any>>} The patterns discovered in the graph.
   *
   * @example
   * ```typescript
   * const temporalPatterns = await analyticsService.minePatterns('temporal-motifs');
   * const financialPatterns = await analyticsService.minePatterns('financial-structuring');
   * ```
   */
  async minePatterns(patternType, params = {}) {
    let cypher = "";
    let explanation = "";
    let factors = [];
    if (patternType === "temporal-motifs") {
      cypher = `
        MATCH (n)-[r]->()
        WHERE r.timestamp IS NOT NULL
        WITH n, r.timestamp as ts
        ORDER BY ts
        WITH n, collect(ts) as timestamps
        WHERE size(timestamps) > 5
        // Calculate intervals (simplified)
        RETURN n.id as nodeId, size(timestamps) as eventCount
        ORDER BY eventCount DESC
        LIMIT 10
      `;
      explanation = "Identified entities with bursty transactional behavior.";
      factors = ["Transaction Frequency", "Time Delta Variance"];
    } else if (patternType === "co-travel") {
      cypher = `
        MATCH (p1:Person)-[:ATTENDED]->(e:Event)<-[:ATTENDED]-(p2:Person)
        WHERE p1.id < p2.id
        WITH p1, p2, count(e) as meetings
        WHERE meetings > 2
        RETURN p1.id, p2.id, meetings
        ORDER BY meetings DESC
        LIMIT 10
      `;
      explanation = "Identified pairs of entities appearing at multiple common events.";
      factors = ["Shared Events", "Recurrence"];
    } else {
      cypher = `
        MATCH (source)-[:TRANSFERRED]->(intermediate)-[:TRANSFERRED]->(target)
        WHERE source <> target AND source <> intermediate
        WITH source, target, count(intermediate) as layers, sum(intermediate.amount) as total
        WHERE layers > 2
        RETURN source.id, target.id, layers
        ORDER BY layers DESC
        LIMIT 10
      `;
      explanation = "Detected potential layering/smurfing structures (Fan-out/Fan-in).";
      factors = ["Flow Topology", "Intermediary Count"];
    }
    const results = await runCypher(cypher, params);
    return {
      data: results,
      xai: {
        features: { patternType, ...params },
        metrics: { matchesFound: results.length },
        explanation,
        contributingFactors: factors
      }
    };
  }
  // --- Anomaly/Risk Scoring ---
  /**
   * @method detectAnomalies
   * @description Detects anomalous or high-risk entities based on several checks.
   * This includes nodes with an unusually high degree, a sudden spike in activity, or shared selectors (e.g., a phone number used by multiple people).
   * @param {'degree' | 'temporal-spike' | 'selector-misuse'} checkType - The type of anomaly to check for.
   * @param {object} [params={}] - Additional parameters for the anomaly detection query.
   * @returns {Promise<AnalyticResult<any>>} A list of anomalies found in the graph.
   *
   * @example
   * ```typescript
   * const highDegreeNodes = await analyticsService.detectAnomalies('degree');
   * const sharedPhones = await analyticsService.detectAnomalies('selector-misuse');
   * ```
   */
  async detectAnomalies(checkType, params = {}) {
    let cypher = "";
    let explanation = "";
    let factors = [];
    if (checkType === "degree") {
      cypher = `
        MATCH (n)
        WITH n, size((n)--()) as degree
        WHERE degree > 50 // Threshold
        RETURN n.id as nodeId, degree, n.labels as labels
        ORDER BY degree DESC
        LIMIT 20
      `;
      explanation = "Nodes exceeding static degree threshold for their type.";
      factors = ["Egonet Size", "Connectivity"];
    } else if (checkType === "temporal-spike") {
      cypher = `
        MATCH (n)-[r]->()
        WHERE r.timestamp > datetime() - duration('P1D')
        WITH n, count(r) as recentCount
        WHERE recentCount > 20
        RETURN n.id as nodeId, recentCount
       `;
      explanation = "Entities with volume spike in last 24h.";
      factors = ["Recent Volume", "Historical Baseline"];
    } else {
      cypher = `
        MATCH (p:Person)-[:HAS_PHONE]->(ph:Phone)
        WITH ph, count(p) as owners
        WHERE owners > 3
        MATCH (ph)<-[:HAS_PHONE]-(p)
        RETURN ph.number, collect(p.id) as users, owners
      `;
      explanation = "Single selector (Phone/Email) claimed by multiple distinct identities.";
      factors = ["Selector Cardinality", "Identity linkage"];
    }
    const results = await runCypher(cypher, params);
    return {
      data: results,
      xai: {
        features: { checkType, ...params },
        metrics: { anomaliesCount: results.length },
        explanation,
        contributingFactors: factors
      }
    };
  }
};

// src/privacy/dp/DifferentialPrivacyEngine.ts
init_logger();
var DifferentialPrivacyEngine = class _DifferentialPrivacyEngine {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_DifferentialPrivacyEngine.instance) {
      _DifferentialPrivacyEngine.instance = new _DifferentialPrivacyEngine();
    }
    return _DifferentialPrivacyEngine.instance;
  }
  /**
   * Generates Laplace noise based on epsilon and sensitivity.
   */
  generateLaplaceNoise(epsilon, sensitivity) {
    const scale = sensitivity / epsilon;
    const u = Math.random() - 0.5;
    return -scale * Math.sign(u) * Math.log(1 - 2 * Math.abs(u));
  }
  /**
   * Applies DP to a numeric aggregate (e.g. count, sum).
   */
  privatizeAggregate(value, config9 = {}) {
    const epsilon = config9.epsilon || 0.1;
    const sensitivity = config9.sensitivity || 1;
    const noise = this.generateLaplaceNoise(epsilon, sensitivity);
    const privatizedValue = value + noise;
    logger.debug({ original: value, privatized: privatizedValue, epsilon }, "DP: Aggregate privatized");
    return Math.max(0, privatizedValue);
  }
  /**
   * K-Anonymity check for small buckets.
   */
  enforceKAnonymity(count, k = 5) {
    if (count < k) {
      logger.warn({ count, k }, "DP: Bucket size below K threshold, suppressing");
      return null;
    }
    return count;
  }
};
var dpEngine = DifferentialPrivacyEngine.getInstance();

// src/analytics/telemetry/TelemetryController.ts
init_TelemetryService();
var handleTelemetryEvent = (req, res) => {
  try {
    const { eventType, props } = req.body;
    const user = req.user;
    if (!user) {
      return res.status(401).json({ error: "Unauthorized" });
    }
    const tenantId = user.tenant_id || user.tenantId || "unknown_tenant";
    const userId = user.sub || user.id || "unknown_user";
    const role = user.role || "unknown_role";
    telemetryService.track(eventType, tenantId, userId, role, props || {});
    res.status(202).send({ status: "accepted" });
  } catch (error) {
    console.error("Telemetry Error:", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
};

// src/routes/analytics.ts
var router54 = Router31();
var analyticsService = AnalyticsService.getInstance();
var asyncHandler3 = (fn) => (req, res, next) => Promise.resolve(fn(req, res, next)).catch(next);
router54.get(
  "/path",
  asyncHandler3(async (req, res) => {
    const { sourceId, targetId, algorithm, k, maxDepth } = req.query;
    if (!sourceId || !targetId) {
      return res.status(400).json({ error: "sourceId and targetId are required" });
    }
    const result2 = await analyticsService.findPaths(
      sourceId,
      targetId,
      algorithm || "shortest",
      { k, maxDepth }
    );
    res.json(result2);
  })
);
router54.get(
  "/community",
  asyncHandler3(async (req, res) => {
    const { algorithm, dp = "true" } = req.query;
    const result2 = await analyticsService.detectCommunities(
      algorithm || "lpa"
    );
    if (dp === "true" && "communities" in result2 && result2.communities) {
      result2.communities = result2.communities.map((c) => ({
        ...c,
        size: dpEngine.privatizeAggregate(c.size, { epsilon: 0.5 }),
        isPrivatized: true
      }));
    }
    res.json(result2);
  })
);
router54.get(
  "/centrality",
  asyncHandler3(async (req, res) => {
    const { algorithm, limit } = req.query;
    const result2 = await analyticsService.calculateCentrality(
      algorithm || "betweenness",
      { limit: limit ? parseInt(limit) : 20 }
    );
    res.json(result2);
  })
);
router54.get(
  "/patterns",
  asyncHandler3(async (req, res) => {
    const { type } = req.query;
    if (!type) {
      return res.status(400).json({ error: "Pattern type is required (temporal-motifs, co-travel, financial-structuring)" });
    }
    const result2 = await analyticsService.minePatterns(
      type
    );
    res.json(result2);
  })
);
router54.get(
  "/anomaly",
  asyncHandler3(async (req, res) => {
    const { type } = req.query;
    if (!type) {
      return res.status(400).json({ error: "Anomaly type is required (degree, temporal-spike, selector-misuse)" });
    }
    const result2 = await analyticsService.detectAnomalies(
      type
    );
    res.json(result2);
  })
);
router54.post("/event", handleTelemetryEvent);
var analytics_default = router54;

// src/routes/experiments.ts
import { Router as Router32 } from "express";

// src/analytics/experiments/ExperimentService.ts
init_TelemetryService();
import crypto33 from "crypto";
var ExperimentService = class {
  experiments = /* @__PURE__ */ new Map();
  config;
  constructor(config9) {
    this.config = config9;
  }
  createExperiment(experiment) {
    if (this.experiments.has(experiment.id)) {
      throw new Error(`Experiment ${experiment.id} already exists`);
    }
    const totalWeight = experiment.variants.reduce((sum, v) => sum + v.weight, 0);
    if (Math.abs(totalWeight - 100) > 0.01) {
      throw new Error("Variant weights must sum to 100");
    }
    this.experiments.set(experiment.id, experiment);
  }
  getExperiment(id) {
    return this.experiments.get(id);
  }
  listExperiments() {
    return Array.from(this.experiments.values());
  }
  stopExperiment(id) {
    const exp = this.experiments.get(id);
    if (exp) {
      exp.status = "inactive";
      exp.endDate = (/* @__PURE__ */ new Date()).toISOString();
    }
  }
  /**
   * Deterministically assigns a user to a variant.
   */
  assign(experimentId, tenantId, userId) {
    const experiment = this.experiments.get(experimentId);
    if (!experiment || experiment.status !== "active") {
      return { experimentId, variantId: null, reason: "inactive" };
    }
    const hashInput = `${this.config.salt}:${experimentId}:${tenantId}:${userId}`;
    const hash3 = crypto33.createHash("sha256").update(hashInput).digest("hex");
    const hashInt = parseInt(hash3.substring(0, 8), 16);
    const normalized = hashInt / 4294967295;
    const allocationThreshold = experiment.allocation / 100;
    if (normalized > allocationThreshold) {
      return { experimentId, variantId: null, reason: "exclusion" };
    }
    const relativeVal = normalized / allocationThreshold * 100;
    let cumulative = 0;
    let assignedVariant = null;
    for (const variant of experiment.variants) {
      cumulative += variant.weight;
      if (relativeVal <= cumulative) {
        assignedVariant = variant;
        break;
      }
    }
    if (!assignedVariant && experiment.variants.length > 0) {
      assignedVariant = experiment.variants[experiment.variants.length - 1];
    }
    if (assignedVariant) {
      this.logExposure(experimentId, assignedVariant.id, tenantId, userId);
      return {
        experimentId,
        variantId: assignedVariant.id,
        reason: "allocated"
      };
    }
    return { experimentId, variantId: null, reason: "fallback" };
  }
  logExposure(experimentId, variantId, tenantId, userId) {
    telemetryService.track("experiment_exposure", tenantId, userId, "system", {
      experimentId,
      variantId,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
  }
};
var experimentService = new ExperimentService({
  salt: process.env.EXPERIMENT_SALT || "exp_salt_123"
});

// src/analytics/experiments/ExperimentController.ts
var singleParam10 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var createExperiment = (req, res) => {
  try {
    const experiment = req.body;
    if (!experiment.id || !experiment.variants) {
      return res.status(400).json({ error: "Invalid experiment payload" });
    }
    experimentService.createExperiment(experiment);
    res.status(201).json(experiment);
  } catch (e) {
    res.status(400).json({ error: e.message });
  }
};
var listExperiments = (req, res) => {
  const experiments = experimentService.listExperiments();
  res.json(experiments);
};
var stopExperiment = (req, res) => {
  const id = singleParam10(req.params.id) ?? "";
  experimentService.stopExperiment(id);
  res.status(200).json({ status: "stopped" });
};
var getAssignment = (req, res) => {
  const id = singleParam10(req.params.id) ?? "";
  const user = req.user;
  if (!user) {
    return res.status(401).json({ error: "Unauthorized" });
  }
  const tenantId = user.tenant_id || "default_tenant";
  const userId = user.sub || user.id;
  const assignment = experimentService.assign(id, tenantId, userId);
  res.json(assignment);
};

// src/routes/experiments.ts
var router55 = Router32();
router55.post("/experiments", createExperiment);
router55.get("/experiments", listExperiments);
router55.post("/experiments/:id/stop", stopExperiment);
router55.get("/experiments/:id/assignment", getAssignment);
var experiments_default = router55;

// src/routes/cohorts.ts
import { Router as Router33 } from "express";

// src/analytics/cohorts/CohortEvaluator.ts
import fs22 from "fs";
import path22 from "path";
var CohortEvaluator = class {
  logDir;
  cache = /* @__PURE__ */ new Map();
  constructor(logDir) {
    this.logDir = logDir;
  }
  evaluate(cohort) {
    if (this.cache.has(cohort.id)) {
      return this.cache.get(cohort.id);
    }
    const members = this.scanLogs(cohort);
    const result2 = {
      cohortId: cohort.id,
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      members,
      totalCount: members.length
    };
    this.cache.set(cohort.id, result2);
    return result2;
  }
  scanLogs(cohort) {
    if (!fs22.existsSync(this.logDir)) {
      return [];
    }
    const files = fs22.readdirSync(this.logDir).filter((f) => f.endsWith(".jsonl"));
    const aggregates = /* @__PURE__ */ new Map();
    for (const file of files) {
      const content = fs22.readFileSync(path22.join(this.logDir, file), "utf-8");
      const lines = content.split("\n");
      for (const line of lines) {
        if (!line.trim()) continue;
        try {
          const event = JSON.parse(line);
          if (event.eventType === cohort.criteria.eventType) {
            const key = `${event.tenantIdHash}:${event.scopeHash}`;
            const current = aggregates.get(key) || 0;
            aggregates.set(key, current + 1);
          }
        } catch (e) {
        }
      }
    }
    const members = [];
    for (const [key, value] of aggregates.entries()) {
      if (this.checkCriteria(value, cohort.criteria.operator, cohort.criteria.value)) {
        const [hashedTenantId, hashedUserId] = key.split(":");
        members.push({
          hashedTenantId,
          hashedUserId,
          metricValue: value
        });
      }
    }
    return members;
  }
  checkCriteria(actual, op, target) {
    switch (op) {
      case "gt":
        return actual > target;
      case "lt":
        return actual < target;
      case "eq":
        return actual === target;
      default:
        return false;
    }
  }
};

// src/analytics/cohorts/CohortController.ts
import path23 from "path";
var LOG_DIR = process.env.TELEMETRY_LOG_DIR || path23.join(process.cwd(), "logs", "telemetry");
var evaluator = new CohortEvaluator(LOG_DIR);
var cohorts = /* @__PURE__ */ new Map();
var singleParam11 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var createCohort = (req, res) => {
  const cohort = req.body;
  if (!cohort.id || !cohort.criteria) {
    return res.status(400).json({ error: "Invalid cohort" });
  }
  cohorts.set(cohort.id, cohort);
  res.status(201).json(cohort);
};
var getCohort = (req, res) => {
  const id = singleParam11(req.params.id) ?? "";
  const cohort = cohorts.get(id);
  if (!cohort) return res.status(404).json({ error: "Not found" });
  res.json(cohort);
};
var evaluateCohort = (req, res) => {
  const id = singleParam11(req.params.id) ?? "";
  const cohort = cohorts.get(id);
  if (!cohort) return res.status(404).json({ error: "Not found" });
  try {
    const result2 = evaluator.evaluate(cohort);
    res.json(result2);
  } catch (e) {
    res.status(500).json({ error: e.message });
  }
};

// src/routes/cohorts.ts
var router56 = Router33();
router56.post("/cohorts", createCohort);
router56.get("/cohorts/:id", getCohort);
router56.post("/cohorts/:id/evaluate", evaluateCohort);
var cohorts_default = router56;

// src/routes/funnels.ts
import { Router as Router34 } from "express";

// src/analytics/funnels/FunnelService.ts
import fs23 from "fs";
import path24 from "path";
var FunnelService = class {
  logDir;
  funnels = /* @__PURE__ */ new Map();
  constructor(logDir) {
    this.logDir = logDir;
  }
  createFunnel(funnel) {
    this.funnels.set(funnel.id, funnel);
  }
  getFunnel(id) {
    return this.funnels.get(id);
  }
  generateReport(funnelId) {
    const funnel = this.funnels.get(funnelId);
    if (!funnel) throw new Error("Funnel not found");
    const userEvents = this.loadUserEvents();
    const stepCounts = {};
    for (let i = 0; i < funnel.steps.length; i++) stepCounts[i] = 0;
    for (const [userId, events] of userEvents.entries()) {
      events.sort((a, b) => new Date(a.ts).getTime() - new Date(b.ts).getTime());
      this.evaluateUser(userId, events, funnel, stepCounts);
    }
    const totalStarted = stepCounts[0];
    const completed = stepCounts[funnel.steps.length - 1];
    const dropOffRates = {};
    for (let i = 1; i < funnel.steps.length; i++) {
      const prev = stepCounts[i - 1];
      const curr = stepCounts[i];
      dropOffRates[i] = prev === 0 ? 0 : (prev - curr) / prev * 100;
    }
    return {
      funnelId,
      totalStarted,
      completed,
      stepCounts,
      dropOffRates
    };
  }
  // Naive evaluation: greedy matching of steps within window
  evaluateUser(userId, events, funnel, counts) {
    let currentStepIdx = 0;
    let startTime = 0;
    let maxStepReached = -1;
    for (let i = 0; i < events.length; i++) {
      const e = events[i];
      if (maxStepReached === -1) {
        if (this.matchStep(e, funnel.steps[0])) {
          maxStepReached = 0;
          startTime = new Date(e.ts).getTime();
        }
      } else {
        const nextStepIdx = maxStepReached + 1;
        if (nextStepIdx < funnel.steps.length) {
          const now = new Date(e.ts).getTime();
          if (now - startTime > funnel.windowSeconds * 1e3) {
            if (this.matchStep(e, funnel.steps[0])) {
              maxStepReached = 0;
              startTime = now;
            }
            continue;
          }
          if (this.matchStep(e, funnel.steps[nextStepIdx])) {
            maxStepReached = nextStepIdx;
          }
        }
      }
    }
    for (let k = 0; k <= maxStepReached; k++) {
      counts[k]++;
    }
  }
  matchStep(event, step) {
    if (event.eventType !== step.eventType) return false;
    if (step.props) {
      for (const [k, v] of Object.entries(step.props)) {
        if (event.props[k] !== v) return false;
      }
    }
    return true;
  }
  loadUserEvents() {
    const map = /* @__PURE__ */ new Map();
    if (!fs23.existsSync(this.logDir)) return map;
    const files = fs23.readdirSync(this.logDir).filter((f) => f.endsWith(".jsonl"));
    for (const file of files) {
      const content = fs23.readFileSync(path24.join(this.logDir, file), "utf-8");
      const lines = content.split("\n");
      for (const line of lines) {
        if (!line.trim()) continue;
        try {
          const e = JSON.parse(line);
          const uid = e.scopeHash;
          if (!map.has(uid)) map.set(uid, []);
          map.get(uid).push(e);
        } catch (err) {
        }
      }
    }
    return map;
  }
};

// src/analytics/funnels/FunnelController.ts
import path25 from "path";
var LOG_DIR2 = process.env.TELEMETRY_LOG_DIR || path25.join(process.cwd(), "logs", "telemetry");
var service4 = new FunnelService(LOG_DIR2);
var singleParam12 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var createFunnel = (req, res) => {
  const funnel = req.body;
  if (!funnel.id || !funnel.steps || funnel.steps.length === 0) {
    return res.status(400).json({ error: "Invalid funnel" });
  }
  service4.createFunnel(funnel);
  res.status(201).json(funnel);
};
var getFunnelReport = (req, res) => {
  const id = singleParam12(req.params.id) ?? "";
  try {
    const report = service4.generateReport(id);
    res.json(report);
  } catch (e) {
    res.status(404).json({ error: e.message });
  }
};

// src/routes/funnels.ts
var router57 = Router34();
router57.post("/funnels", createFunnel);
router57.get("/funnels/:id/report", getFunnelReport);
var funnels_default = router57;

// src/routes/anomalies.ts
import { Router as Router35 } from "express";

// src/analytics/anomalies/AnomalyDetector.ts
var AnomalyDetector2 = class {
  // Z-Score implementation
  static detectZScore(metricName, history, currentValue, threshold = 3) {
    if (history.length < 5) return null;
    const values = history.map((p) => p.value);
    const mean = values.reduce((sum, v) => sum + v, 0) / values.length;
    const variance = values.reduce((sum, v) => sum + Math.pow(v - mean, 2), 0) / values.length;
    const stdDev = Math.sqrt(variance);
    if (stdDev === 0) return null;
    const zScore = (currentValue - mean) / stdDev;
    if (Math.abs(zScore) > threshold) {
      return {
        type: "anomaly",
        metricName,
        score: zScore,
        threshold,
        value: currentValue,
        reason: `Z-Score ${zScore.toFixed(2)} > ${threshold} (Mean: ${mean.toFixed(2)}, StdDev: ${stdDev.toFixed(2)})`,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      };
    }
    return null;
  }
  // Median Absolute Deviation (MAD) implementation
  // More robust to outliers than Z-Score
  static detectMAD(metricName, history, currentValue, threshold = 3) {
    if (history.length < 5) return null;
    const values = history.map((p) => p.value);
    values.sort((a, b) => a - b);
    const mid = Math.floor(values.length / 2);
    const median = values.length % 2 !== 0 ? values[mid] : (values[mid - 1] + values[mid]) / 2;
    const residuals = values.map((v) => Math.abs(v - median));
    residuals.sort((a, b) => a - b);
    const madMid = Math.floor(residuals.length / 2);
    const mad = residuals.length % 2 !== 0 ? residuals[madMid] : (residuals[madMid - 1] + residuals[madMid]) / 2;
    if (mad === 0) return null;
    const score = 0.6745 * (currentValue - median) / mad;
    if (Math.abs(score) > threshold) {
      return {
        type: "anomaly",
        metricName,
        score,
        threshold,
        value: currentValue,
        reason: `MAD-Score ${score.toFixed(2)} > ${threshold} (Median: ${median}, MAD: ${mad})`,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      };
    }
    return null;
  }
  // Ratio of Ratios implementation
  static detectRatio(metricName, history, currentValue, threshold = 2) {
    if (history.length < 1) return null;
    const lastValue = history[history.length - 1].value;
    if (lastValue === 0) return null;
    const ratio = currentValue / lastValue;
    if (ratio > threshold || ratio < 1 / threshold) {
      return {
        type: "anomaly",
        metricName,
        score: ratio,
        threshold,
        value: currentValue,
        reason: `Ratio Change ${ratio.toFixed(2)} vs Threshold ${threshold}`,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      };
    }
    return null;
  }
};

// src/analytics/anomalies/AnomalyController.ts
var metricHistory = {};
var runAnomalyDetection = (req, res) => {
  const { metric, value, detector = "zscore" } = req.body;
  if (!metric || value === void 0) {
    return res.status(400).json({ error: "metric and value required" });
  }
  const history = metricHistory[metric] || [];
  let anomaly = null;
  if (detector === "zscore") {
    anomaly = AnomalyDetector2.detectZScore(metric, history, value);
  } else if (detector === "mad") {
    anomaly = AnomalyDetector2.detectMAD(metric, history, value);
  } else if (detector === "ratio") {
    anomaly = AnomalyDetector2.detectRatio(metric, history, value);
  }
  history.push({ timestamp: Date.now(), value });
  if (history.length > 100) history.shift();
  metricHistory[metric] = history;
  res.json({
    metric,
    value,
    anomaly: anomaly || "none"
  });
};
var getAnomalies = (req, res) => {
  res.json({ status: "ok", msg: "Check /run output for specific checks" });
};

// src/routes/anomalies.ts
var router58 = Router35();
router58.post("/anomalies/run", runAnomalyDetection);
router58.get("/anomalies", getAnomalies);
var anomalies_default = router58;

// src/routes/exports.ts
import express28 from "express";
import crypto36 from "crypto";

// src/analytics/exports/ExportService.ts
var ExportService = class {
  config;
  constructor(config9) {
    this.config = config9;
  }
  exportToCSV(data) {
    if (data.length === 0) return "";
    const safeData = data.filter((row) => {
      if (typeof row.count === "number") {
        return row.count >= this.config.kAnonymityThreshold;
      }
      return true;
    });
    if (safeData.length === 0) return "";
    const headers = Object.keys(safeData[0]).sort();
    const headerRow = headers.join(",");
    const rows = safeData.map((row) => {
      return headers.map((h) => {
        const val = row[h];
        return JSON.stringify(val);
      }).join(",");
    });
    return [headerRow, ...rows].join("\n");
  }
};

// src/analytics/exports/ExportController.ts
var service5 = new ExportService({ kAnonymityThreshold: 5 });
var exportData = (req, res) => {
  const { data, format = "csv" } = req.body;
  if (!Array.isArray(data)) {
    return res.status(400).json({ error: "Data must be an array of objects" });
  }
  if (format === "csv") {
    const csv = service5.exportToCSV(data);
    res.header("Content-Type", "text/csv");
    res.attachment("export.csv");
    return res.send(csv);
  } else if (format === "json") {
    const threshold = 5;
    const safeData = data.filter((row) => {
      if (typeof row.count === "number") return row.count >= threshold;
      return true;
    });
    return res.json(safeData);
  }
  res.status(400).json({ error: "Unsupported format" });
};

// src/utils/security.ts
import path26 from "path";
function validateArtifactId(artifactId) {
  if (!artifactId) return true;
  return path26.basename(artifactId) === artifactId;
}

// src/exports/__fixtures__/watermarks.ts
var watermarkRecord = {
  "valid-artifact": "exportId=export-123;manifestHash=abcd1234;policyHash=policy-v1",
  "tampered-artifact": "exportId=export-123;manifestHash=ffff0000;policyHash=policy-tampered"
};
var manifestRecord = {
  "export-123": {
    manifestHash: "abcd1234fedcba9876543210abcd1234fedcba98",
    policyHash: "policy-v1"
  }
};
var auditLedgerRecord = {
  "export-123": {
    exportId: "export-123",
    policyHash: "policy-v1",
    manifestHash: "abcd1234fedcba9876543210abcd1234fedcba98"
  }
};
var watermarkFixtures = Object.freeze({ ...watermarkRecord });
var manifestFixtures = Object.freeze({ ...manifestRecord });
var auditLedgerFixtures = Object.freeze({ ...auditLedgerRecord });

// src/exports/MockWatermarkExtractor.ts
var MockWatermarkExtractor = class {
  async extract(artifactId) {
    const watermark = watermarkFixtures[artifactId];
    if (!watermark) {
      throw new Error(`No watermark fixture for artifact '${artifactId}'`);
    }
    return watermark;
  }
};

// src/exports/WatermarkVerificationService.ts
var WatermarkVerificationService = class {
  manifestStore;
  auditLedger;
  extractor;
  constructor(manifestStore = manifestFixtures, auditLedger = auditLedgerFixtures, extractor = new MockWatermarkExtractor()) {
    this.manifestStore = manifestStore;
    this.auditLedger = auditLedger;
    this.extractor = extractor;
  }
  parseWatermark(rawWatermark) {
    const parts = rawWatermark.split(";").reduce((acc, part) => {
      const [key, value] = part.split("=");
      if (key && value) {
        acc[key.trim()] = value.trim();
      }
      return acc;
    }, {});
    const exportId = parts.exportId;
    const manifestHashPrefix = parts.manifestHash;
    const policyHash = parts.policyHash;
    if (!exportId || !manifestHashPrefix || !policyHash) {
      throw new Error("Invalid watermark payload: missing required fields");
    }
    return {
      exportId,
      manifestHashPrefix,
      policyHash,
      raw: rawWatermark
    };
  }
  async verify({
    exportId,
    artifactId,
    watermark
  }) {
    const mismatches = [];
    try {
      const observedWatermark = this.parseWatermark(
        watermark || await this.extractor.extract(artifactId || "")
      );
      if (observedWatermark.exportId !== exportId) {
        mismatches.push("export-id-mismatch");
      }
      const manifest = this.manifestStore[exportId];
      if (!manifest) {
        return {
          valid: false,
          manifestHash: null,
          observedWatermark,
          mismatches: [...mismatches, "manifest-not-found"],
          reasonCodes: [...mismatches, "manifest-not-found"]
        };
      }
      const ledgerEntry = this.auditLedger[exportId];
      if (!ledgerEntry) {
        return {
          valid: false,
          manifestHash: manifest.manifestHash,
          observedWatermark,
          mismatches: [...mismatches, "audit-ledger-missing"],
          reasonCodes: [...mismatches, "audit-ledger-missing"]
        };
      }
      if (!manifest.manifestHash.startsWith(observedWatermark.manifestHashPrefix)) {
        mismatches.push("manifest-hash-mismatch");
      }
      if (!ledgerEntry.manifestHash.startsWith(observedWatermark.manifestHashPrefix)) {
        mismatches.push("audit-ledger-manifest-mismatch");
      }
      if (ledgerEntry.policyHash !== observedWatermark.policyHash) {
        mismatches.push("policy-hash-mismatch");
      }
      return {
        valid: mismatches.length === 0,
        manifestHash: manifest.manifestHash,
        observedWatermark,
        mismatches,
        reasonCodes: mismatches
      };
    } catch (error) {
      return {
        valid: false,
        manifestHash: null,
        observedWatermark: null,
        mismatches: ["unreadable-watermark"],
        reasonCodes: ["unreadable-watermark", error.message]
      };
    }
  }
};

// src/middleware/sensitive-context.ts
init_appendOnlyAuditStore();
init_opa_integration();
import { randomUUID as randomUUID46 } from "crypto";
var DEFAULT_ROUTES = [
  "/api/security/pii",
  "/api/exports",
  "/api/analytics/export",
  "/api/intel-graph"
];
var defaultAuditStore = new AppendOnlyAuditStore();
function extractContext(req) {
  const purpose = req.headers["x-purpose"] || req.body?.purpose || "";
  const justification = req.headers["x-justification"] || req.body?.justification || "";
  const caseId = req.headers["x-case-id"] || req.body?.case_id || req.body?.caseId || "";
  const correlationId = req.correlationId || req.headers["x-correlation-id"] || randomUUID46();
  if (!purpose || !justification || !caseId) {
    return null;
  }
  return {
    purpose: purpose.trim(),
    justification: justification.trim(),
    caseId: caseId.toString().trim(),
    correlationId
  };
}
function shouldApplyGuard(req, routes) {
  const path55 = `${req.baseUrl || ""}${req.path}`;
  return routes.some((prefix) => path55.startsWith(prefix));
}
async function recordAudit2(auditStore, req, context4, decision, reason, action) {
  const tenantId = req.tenantId || req.tenant_id || req.user?.tenantId || req.headers["x-tenant-id"] || "unknown";
  const userId = req.user?.id || req.user?.sub;
  const now = (/* @__PURE__ */ new Date()).toISOString();
  await auditStore.append({
    version: "audit_event_v1",
    actor: {
      type: "user",
      id: userId,
      ip_address: req.ip,
      name: req.user?.email
    },
    action,
    resource: {
      type: req.method,
      name: req.path,
      id: req.params?.id,
      owner: tenantId
    },
    classification: "restricted",
    policy_version: process.env.OPA_POLICY_VERSION || "1.0",
    decision_id: context4?.correlationId || randomUUID46(),
    trace_id: req.traceId || context4?.correlationId || randomUUID46(),
    timestamp: now,
    customer: tenantId,
    metadata: {
      purpose: context4?.purpose,
      justification: context4?.justification,
      case_id: context4?.caseId,
      decision,
      reason,
      correlation_id: context4?.correlationId,
      tenantId,
      userId
    }
  });
}
function createSensitiveContextMiddleware(options2 = {}) {
  const routes = options2.routes ?? DEFAULT_ROUTES;
  const auditStore = options2.auditStore ?? defaultAuditStore;
  const opaClient2 = options2.opaClient ?? opaPolicyEngine;
  const action = options2.action ?? "sensitive_action";
  return async function sensitiveContextMiddleware2(req, res, next) {
    if (!shouldApplyGuard(req, routes)) {
      return next();
    }
    const context4 = extractContext(req);
    if (!context4) {
      await recordAudit2(auditStore, req, null, "deny", "context_missing", action);
      return res.status(400).json({
        code: "SENSITIVE_CONTEXT_REQUIRED",
        message: "Purpose, justification, and case_id are required for sensitive operations.",
        required: ["purpose", "justification", "case_id"],
        guidance: "Provide a specific case identifier, the operational purpose, and a justification before retrying."
      });
    }
    try {
      const decisionInput = {
        tenantId: req.tenantId || req.tenant_id || req.user?.tenantId || req.headers["x-tenant-id"] || "unknown",
        userId: req.user?.id || req.user?.sub,
        role: req.user?.role || "user",
        action: req.method.toLowerCase(),
        resource: req.path,
        resourceAttributes: {
          ...req.body,
          ...req.params
        },
        context: {
          purpose: context4.purpose,
          justification: context4.justification,
          case_id: context4.caseId
        }
      };
      const policyDecision = await opaClient2.evaluatePolicy(
        "sensitive/access",
        decisionInput
      );
      if (!policyDecision.allow) {
        await recordAudit2(
          auditStore,
          req,
          context4,
          "deny",
          policyDecision.reason || "policy_denied",
          action
        );
        return res.status(403).json({
          code: "SENSITIVE_CONTEXT_DENIED",
          reason: policyDecision.reason || "Access denied by policy",
          audit: policyDecision.auditLog
        });
      }
      req.sensitiveAccessContext = context4;
      res.locals.sensitiveAccessContext = context4;
      await recordAudit2(auditStore, req, context4, "allow", "policy_allowed", action);
      return next();
    } catch (error) {
      await recordAudit2(auditStore, req, context4, "deny", "policy_error", action);
      return res.status(500).json({
        code: "SENSITIVE_CONTEXT_ERROR",
        message: "Failed to evaluate sensitive access policy"
      });
    }
  };
}
var sensitiveContextMiddleware = createSensitiveContextMiddleware();

// src/middleware/high-risk-approval.ts
init_appendOnlyAuditStore();
import { randomUUID as randomUUID47 } from "crypto";
var defaultAuditStore2 = new AppendOnlyAuditStore();
var DEFAULT_ROUTES2 = ["/api/exports", "/api/analytics/export"];
function matches(req, routes) {
  const path55 = `${req.baseUrl || ""}${req.path}`;
  return routes.some((prefix) => path55.startsWith(prefix));
}
function createHighRiskApprovalMiddleware(options2 = {}) {
  const routes = options2.routes ?? DEFAULT_ROUTES2;
  const auditStore = options2.auditStore ?? defaultAuditStore2;
  const action = options2.action ?? "high_risk_action";
  return async function highRiskApproval(req, res, next) {
    if (!matches(req, routes)) return next();
    const stepUpToken = req.headers["x-step-up-token"];
    const approvalToken = req.headers["x-approval-token"];
    if (!stepUpToken && !approvalToken) {
      await auditStore.append({
        version: "audit_event_v1",
        actor: {
          type: "user",
          id: req.user?.id || req.user?.sub
        },
        action,
        resource: {
          type: req.method,
          name: req.path,
          owner: req.tenantId || req.tenant_id || req.headers["x-tenant-id"] || "unknown"
        },
        classification: "restricted",
        policy_version: process.env.OPA_POLICY_VERSION || "1.0",
        decision_id: randomUUID47(),
        trace_id: req.traceId || randomUUID47(),
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        customer: req.tenantId || req.tenant_id || req.headers["x-tenant-id"] || "unknown",
        metadata: {
          decision: "deny",
          reason: "approval_required"
        }
      });
      return res.status(403).json({
        code: "APPROVAL_REQUIRED",
        requiresApproval: true,
        requiresStepUp: true,
        message: "This high-risk action requires step-up authentication or an approval token."
      });
    }
    await auditStore.append({
      version: "audit_event_v1",
      actor: {
        type: "user",
        id: req.user?.id || req.user?.sub
      },
      action,
      resource: {
        type: req.method,
        name: req.path,
        owner: req.tenantId || req.tenant_id || req.headers["x-tenant-id"] || "unknown"
      },
      classification: "restricted",
      policy_version: process.env.OPA_POLICY_VERSION || "1.0",
      decision_id: randomUUID47(),
      trace_id: req.traceId || randomUUID47(),
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      customer: req.tenantId || req.tenant_id || req.headers["x-tenant-id"] || "unknown",
      metadata: {
        decision: "allow",
        stepUpToken: stepUpToken ? "present" : "missing",
        approvalToken: approvalToken ? "present" : "missing"
      }
    });
    return next();
  };
}
var highRiskApprovalMiddleware = createHighRiskApprovalMiddleware();

// src/routes/exports.ts
init_auth4();
var router59 = express28.Router();
var watermarkVerificationService = new WatermarkVerificationService();
var singleParam13 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
router59.post(
  "/sign-manifest",
  ensureAuthenticated,
  requirePermission("export:investigations"),
  async (req, res) => {
    try {
      const { tenant, filters, timestamp } = req.body;
      const manifestString = JSON.stringify({ tenant, filters, timestamp });
      let secret = process.env.EXPORT_SIGNING_SECRET;
      if (!secret) {
        if (process.env.NODE_ENV === "production") {
          throw new Error("EXPORT_SIGNING_SECRET is not configured");
        }
        secret = "dev-secret";
      }
      const signature = crypto36.createHmac("sha256", secret).update(manifestString).digest("hex");
      res.json({
        signature,
        algorithm: "hmac-sha256",
        manifest: { tenant, filters, timestamp }
      });
    } catch (error) {
      res.status(500).json({ error: "Failed to sign manifest" });
    }
  }
);
router59.post(
  "/analytics/export",
  ensureAuthenticated,
  requirePermission("export:investigations"),
  sensitiveContextMiddleware,
  highRiskApprovalMiddleware,
  exportData
);
router59.post(
  "/exports/:id/verify-watermark",
  ensureAuthenticated,
  requirePermission("export:investigations"),
  async (req, res) => {
    if (process.env.WATERMARK_VERIFY !== "true") {
      return res.status(404).json({ error: "Watermark verification not enabled" });
    }
    const id = singleParam13(req.params.id) ?? "";
    const { artifactId, watermark } = req.body || {};
    if (!validateArtifactId(artifactId)) {
      return res.status(400).json({ error: "Invalid artifactId" });
    }
    try {
      const result2 = await watermarkVerificationService.verify({
        exportId: id,
        artifactId,
        watermark
      });
      return res.json(result2);
    } catch (error) {
      return res.status(400).json({ error: error.message });
    }
  }
);
var exports_default = router59;

// src/routes/retention.ts
import { Router as Router36 } from "express";

// src/analytics/retention/RetentionService.ts
import fs25 from "fs";
import path28 from "path";
var RetentionService = class {
  logDir;
  constructor(logDir) {
    this.logDir = logDir;
  }
  // Deletes files older than N days
  runRetentionPolicy(daysToKeep) {
    if (!fs25.existsSync(this.logDir)) return 0;
    const files = fs25.readdirSync(this.logDir).filter((f) => f.startsWith("telemetry-") && f.endsWith(".jsonl"));
    const now = /* @__PURE__ */ new Date();
    let deletedCount = 0;
    for (const file of files) {
      const match = file.match(/telemetry-(\d{4}-\d{2}-\d{2})\.jsonl/);
      if (match) {
        const dateStr = match[1];
        const fileDate = new Date(dateStr);
        const diffTime = Math.abs(now.getTime() - fileDate.getTime());
        const diffDays = Math.ceil(diffTime / (1e3 * 60 * 60 * 24));
        if (diffDays > daysToKeep) {
          fs25.unlinkSync(path28.join(this.logDir, file));
          deletedCount++;
        }
      }
    }
    return deletedCount;
  }
};

// src/analytics/retention/RetentionController.ts
import path29 from "path";
var LOG_DIR3 = process.env.TELEMETRY_LOG_DIR || path29.join(process.cwd(), "logs", "telemetry");
var service6 = new RetentionService(LOG_DIR3);
var runRetention = (req, res) => {
  try {
    const { days = 90 } = req.body;
    const deletedCount = service6.runRetentionPolicy(Number(days));
    res.json({
      status: "success",
      deletedFiles: deletedCount,
      policyDays: days,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
  } catch (e) {
    res.status(500).json({ error: e.message });
  }
};
var getJobStatus = (req, res) => {
  res.json({ status: "idle", lastRun: "never" });
};

// src/routes/retention.ts
var router60 = Router36();
router60.post("/analytics/retention/run", runRetention);
router60.get("/analytics/retention/jobs/:id", getJobStatus);
var retention_default = router60;

// src/routes/dr.ts
import express29 from "express";

// src/dr/backup-inventory/BackupInventoryService.ts
init_redis2();
init_logger2();
var BackupInventoryService = class _BackupInventoryService {
  static instance;
  targets = /* @__PURE__ */ new Map();
  redis;
  REDIS_KEY = "backup:inventory:targets";
  constructor() {
    this.redis = RedisService.getInstance();
    this.loadFromRedis().catch((err) => {
      logger_default2.error("Failed to load backup inventory from Redis", err);
    });
  }
  static getInstance() {
    if (!_BackupInventoryService.instance) {
      _BackupInventoryService.instance = new _BackupInventoryService();
    }
    return _BackupInventoryService.instance;
  }
  async loadFromRedis() {
    try {
      const data = await this.redis.get(this.REDIS_KEY);
      if (data) {
        const parsed = JSON.parse(data);
        this.targets = new Map(parsed.map((t) => [t.id, {
          ...t,
          createdAt: new Date(t.createdAt),
          updatedAt: new Date(t.updatedAt),
          lastSuccessAt: t.lastSuccessAt ? new Date(t.lastSuccessAt) : void 0,
          lastFailureAt: t.lastFailureAt ? new Date(t.lastFailureAt) : void 0
        }]));
      }
    } catch (error) {
      logger_default2.error("Error loading backup inventory from Redis", error);
    }
  }
  async saveToRedis() {
    try {
      const targetsArray = Array.from(this.targets.values());
      await this.redis.set(this.REDIS_KEY, JSON.stringify(targetsArray));
    } catch (error) {
      logger_default2.error("Error saving backup inventory to Redis", error);
    }
  }
  async addTarget(target) {
    const now = /* @__PURE__ */ new Date();
    const newTarget = {
      ...target,
      createdAt: now,
      updatedAt: now
    };
    this.targets.set(target.id, newTarget);
    await this.saveToRedis();
    return newTarget;
  }
  async updateTarget(id, updates) {
    const existing = this.targets.get(id);
    if (!existing) return void 0;
    const updated = {
      ...existing,
      ...updates,
      updatedAt: /* @__PURE__ */ new Date()
    };
    this.targets.set(id, updated);
    await this.saveToRedis();
    return updated;
  }
  getTarget(id) {
    return this.targets.get(id);
  }
  listTargets() {
    return Array.from(this.targets.values());
  }
  async reportStatus(id, success, timestamp = /* @__PURE__ */ new Date()) {
    const target = this.targets.get(id);
    if (!target) return void 0;
    const updates = {};
    if (success) {
      updates.lastSuccessAt = timestamp;
    } else {
      updates.lastFailureAt = timestamp;
    }
    return this.updateTarget(id, updates);
  }
  // For testing
  async clear() {
    this.targets.clear();
    await this.redis.del(this.REDIS_KEY);
  }
};

// src/dr/backup-inventory/PolicyChecker.ts
var PolicyChecker = class {
  check(targets, policy2) {
    const findings = [];
    for (const target of targets) {
      if (policy2.requireEncryption && !target.encrypted) {
        findings.push({
          ruleId: "ENCRYPTION_MISSING",
          targetId: target.id,
          severity: "high",
          message: `Backup target ${target.name} is not encrypted.`,
          remediationHint: "Enable server-side encryption or client-side encryption for this backup target."
        });
      }
      if (target.retentionDays < policy2.minRetentionDays) {
        findings.push({
          ruleId: "WEAK_RETENTION",
          targetId: target.id,
          severity: "medium",
          message: `Retention period (${target.retentionDays} days) is less than policy minimum (${policy2.minRetentionDays} days).`,
          remediationHint: `Increase retention period to at least ${policy2.minRetentionDays} days.`
        });
      }
      if (target.lastSuccessAt) {
        const hoursSinceSuccess = ((/* @__PURE__ */ new Date()).getTime() - target.lastSuccessAt.getTime()) / (1e3 * 60 * 60);
        if (hoursSinceSuccess > policy2.maxStalenessHours) {
          findings.push({
            ruleId: "STALE_BACKUP",
            targetId: target.id,
            severity: "critical",
            message: `Last successful backup was ${hoursSinceSuccess.toFixed(1)} hours ago (limit: ${policy2.maxStalenessHours} hours).`,
            remediationHint: "Investigate backup job failures or schedule configuration."
          });
        }
      } else {
        findings.push({
          ruleId: "MISSING_BACKUP",
          targetId: target.id,
          severity: "critical",
          message: `Backup target has never reported a successful backup.`,
          remediationHint: "Ensure the initial backup job has run successfully."
        });
      }
    }
    findings.sort((a, b) => {
      const ruleCompare = a.ruleId.localeCompare(b.ruleId);
      if (ruleCompare !== 0) return ruleCompare;
      return a.targetId.localeCompare(b.targetId);
    });
    return {
      generatedAt: /* @__PURE__ */ new Date(),
      totalTargets: targets.length,
      findings
    };
  }
};

// src/routes/dr.ts
var router61 = express29.Router();
var service7 = BackupInventoryService.getInstance();
var checker = new PolicyChecker();
router61.post("/backups", (req, res) => {
  try {
    if (!req.body.id || !req.body.name || !req.body.storeType) {
      return res.status(400).json({ error: "Missing required fields" });
    }
    const target = service7.addTarget(req.body);
    res.status(201).json(target);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router61.get("/backups", (req, res) => {
  const targets = service7.listTargets();
  res.json(targets);
});
router61.post("/backups/status", (req, res) => {
  const { id, success, timestamp } = req.body;
  if (!id || success === void 0) {
    return res.status(400).json({ error: "Missing id or success status" });
  }
  const updated = service7.reportStatus(id, success, timestamp ? new Date(timestamp) : /* @__PURE__ */ new Date());
  if (!updated) {
    return res.status(404).json({ error: "Backup target not found" });
  }
  res.json(updated);
});
router61.post("/backups/check", (req, res) => {
  const defaultPolicy = {
    id: "default-policy",
    minRetentionDays: 30,
    requireEncryption: true,
    maxStalenessHours: 24
  };
  const policy2 = { ...defaultPolicy, ...req.body };
  const targets = service7.listTargets();
  const report = checker.check(targets, policy2);
  res.json(report);
});
var dr_default = router61;

// src/routes/reporting.ts
import express30 from "express";

// src/services/BatchJobService.ts
init_config3();
import { PgBoss } from "pg-boss";

// src/services/RenewalService.ts
init_database();
init_logger2();
import { randomUUID as randomUUID48 } from "crypto";
var RenewalService = class _RenewalService {
  static instance;
  _pool;
  constructor() {
  }
  get pool() {
    if (!this._pool) {
      this._pool = getPostgresPool2();
    }
    return this._pool;
  }
  static getInstance() {
    if (!_RenewalService.instance) {
      _RenewalService.instance = new _RenewalService();
    }
    return _RenewalService.instance;
  }
  /**
   * Checks for contracts ending within the specified window and processes notices/forecasts.
   */
  async processRenewals() {
    logger_default2.info("Starting renewal processing...");
    const client6 = await this.pool.connect();
    try {
      const windows = [90, 60, 30];
      for (const days of windows) {
        const targetDate = /* @__PURE__ */ new Date();
        targetDate.setDate(targetDate.getDate() + days);
        const startOfDay = new Date(targetDate.setHours(0, 0, 0, 0));
        const endOfDay = new Date(targetDate.setHours(23, 59, 59, 999));
        const res = await client6.query(
          `SELECT * FROM contract_terms
                     WHERE end_date >= $1 AND end_date <= $2
                     AND auto_renew = TRUE`,
          // Focus on auto-renew first, or all? Prompt says "notices".
          [startOfDay, endOfDay]
        );
        for (const row of res.rows) {
          await this.handleRenewalNotice(client6, row, days);
        }
      }
    } catch (error) {
      logger_default2.error("Error processing renewals:", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  async handleRenewalNotice(client6, contract, daysRemaining) {
    const eventType = `NOTICE_${daysRemaining}_DAYS`;
    const existing = await client6.query(
      `SELECT 1 FROM renewal_events
             WHERE contract_id = $1 AND event_type = $2`,
      [contract.id, eventType]
    );
    if (existing.rows.length > 0) return;
    logger_default2.info(`Sending ${daysRemaining}-day renewal notice for tenant ${contract.tenant_id}`);
    const forecast = await this.generateUsageForecast(contract.tenant_id);
    await client6.query(
      `INSERT INTO renewal_events (contract_id, tenant_id, event_type, details)
             VALUES ($1, $2, $3, $4)`,
      [contract.id, contract.tenant_id, eventType, JSON.stringify({ daysRemaining, forecastId: forecast.id })]
    );
  }
  async generateUsageForecast(tenantId) {
    logger_default2.warn(`[Renewal] Using stubbed forecast calculation for tenant ${tenantId}`);
    const projectedAmount = 0;
    const confidenceScore = 0;
    const forecast = {
      id: randomUUID48(),
      tenantId,
      forecastDate: /* @__PURE__ */ new Date(),
      periodStart: /* @__PURE__ */ new Date(),
      // Next billing cycle start
      periodEnd: /* @__PURE__ */ new Date(),
      // Next billing cycle end
      projectedAmount,
      confidenceScore,
      lineItems: []
    };
    const client6 = await this.pool.connect();
    try {
      await client6.query(
        `INSERT INTO usage_forecasts (id, tenant_id, forecast_date, period_start, period_end, projected_amount, confidence_score, line_items)
                 VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
        [forecast.id, forecast.tenantId, forecast.forecastDate, forecast.periodStart, forecast.periodEnd, forecast.projectedAmount, forecast.confidenceScore, JSON.stringify(forecast.lineItems)]
      );
      return forecast;
    } finally {
      client6.release();
    }
  }
};
var RenewalService_default = RenewalService.getInstance();

// src/services/PartnerPayoutService.ts
init_database();
init_logger2();
import { randomUUID as randomUUID49 } from "crypto";
var PartnerPayoutService = class _PartnerPayoutService {
  static instance;
  _pool;
  constructor() {
  }
  get pool() {
    if (!this._pool) {
      this._pool = getPostgresPool2();
    }
    return this._pool;
  }
  static getInstance() {
    if (!_PartnerPayoutService.instance) {
      _PartnerPayoutService.instance = new _PartnerPayoutService();
    }
    return _PartnerPayoutService.instance;
  }
  async generatePayoutReports(periodStart, periodEnd) {
    logger_default2.info(`Generating partner payout reports for period ${periodStart.toISOString()} to ${periodEnd.toISOString()}`);
    const client6 = await this.pool.connect();
    try {
      const res = await client6.query(
        `SELECT tenant_id FROM partner_profiles`
      );
      if (res.rows.length === 0) {
        logger_default2.warn("No partners found for payout generation.");
        return;
      }
      const partners = res.rows.map((r) => r.tenant_id);
      for (const partnerId of partners) {
        await this.generateReportForPartner(client6, partnerId, periodStart, periodEnd);
      }
    } finally {
      client6.release();
    }
  }
  async generateReportForPartner(client6, partnerId, periodStart, periodEnd) {
    logger_default2.warn(`[PartnerPayout] Using stubbed payout calculation for partner ${partnerId}`);
    const totalAmount = 0;
    const reportId = randomUUID49();
    await client6.query(
      `INSERT INTO partner_payout_reports (id, partner_id, period_start, period_end, total_amount, status)
             VALUES ($1, $2, $3, $4, $5, 'DRAFT')`,
      [reportId, partnerId, periodStart, periodEnd, totalAmount]
    );
  }
};
var PartnerPayoutService_default = PartnerPayoutService.getInstance();

// src/jobs/revenue/RevenueJobs.ts
init_logger2();
var JOB_QUEUE_RENEWALS = "process-renewals";
var JOB_QUEUE_PARTNER_PAYOUTS = "process-partner-payouts";
async function registerRevenueJobs(boss) {
  await boss.work(JOB_QUEUE_RENEWALS, async (job) => {
    logger_default2.info(`[Job] Processing renewals job ${job.id}`);
    try {
      await RenewalService_default.processRenewals();
    } catch (error) {
      logger_default2.error(`[Job] Renewal processing failed: ${error}`);
      throw error;
    }
  });
  await boss.work(JOB_QUEUE_PARTNER_PAYOUTS, async (job) => {
    logger_default2.info(`[Job] Processing partner payouts job ${job.id}`);
    try {
      let { periodStart, periodEnd } = job.data || {};
      if (!periodStart || !periodEnd) {
        const now = /* @__PURE__ */ new Date();
        const firstDayPrevMonth = new Date(now.getFullYear(), now.getMonth() - 1, 1);
        const lastDayPrevMonth = new Date(now.getFullYear(), now.getMonth(), 0);
        periodStart = firstDayPrevMonth.toISOString();
        periodEnd = lastDayPrevMonth.toISOString();
        logger_default2.info(`[Job] No period specified, defaulting to previous month: ${periodStart} - ${periodEnd}`);
      }
      await PartnerPayoutService_default.generatePayoutReports(new Date(periodStart), new Date(periodEnd));
    } catch (error) {
      logger_default2.error(`[Job] Partner payouts failed: ${error}`);
      throw error;
    }
  });
  await boss.schedule(JOB_QUEUE_RENEWALS, "0 1 * * *");
  await boss.schedule(JOB_QUEUE_PARTNER_PAYOUTS, "0 2 2 * *");
}

// src/services/BatchJobService.ts
var systemRules = [
  { resource: "report", action: "view", roles: ["system"] },
  { resource: "report", action: "create", roles: ["system"] },
  { resource: "report", action: "deliver", roles: ["system"] }
];
var reportingService = createReportingService(new AccessControlService(systemRules));
var JOB_QUEUE_GENERATE_REPORT = "generate-report";
var BatchJobService = class _BatchJobService {
  static instance;
  _boss;
  constructor() {
  }
  get boss() {
    if (!this._boss) {
      this._boss = new PgBoss(config_default.DATABASE_URL);
      this._boss.on("error", (error) => console.error(`[PG-BOSS] Error: ${error.message}`));
    }
    return this._boss;
  }
  /**
   * @method getInstance
   * @description Gets the singleton instance of the BatchJobService.
   * @static
   * @returns {BatchJobService} The singleton instance.
   */
  static getInstance() {
    if (!_BatchJobService.instance) {
      _BatchJobService.instance = new _BatchJobService();
    }
    return _BatchJobService.instance;
  }
  /**
   * @method start
   * @description Starts the pg-boss instance, which begins processing jobs from the queue.
   * It also registers all necessary job workers and schedules any recurring system jobs.
   * This should be called once during application startup.
   * @returns {Promise<void>}
   */
  async start() {
    await this.boss.start();
    console.log("[PG-BOSS] Job processor started.");
    await this.registerWorkers();
    await this.scheduleJobs();
  }
  /**
   * @private
   * @method registerWorkers
   * @description Registers all worker functions that process jobs from the queues.
   * Each worker is associated with a specific job queue name.
   */
  async registerWorkers() {
    await registerRevenueJobs(this.boss);
    await this.boss.work(JOB_QUEUE_GENERATE_REPORT, async (job) => {
      console.log(`[PG-BOSS] Processing report job ${job.id} (${job.name})`);
      try {
        const { request, userId, reportName } = job.data;
        console.log(`[PG-BOSS] Generating report: ${reportName || "unnamed"}`);
        const access = {
          userId: userId || "system-scheduler",
          roles: ["system", "admin", "user"]
          // Grant sufficient roles for background execution
        };
        await reportingService.generate(request, access);
        console.log(`[PG-BOSS] Report generated successfully for job ${job.id}`);
      } catch (error) {
        console.error(`[PG-BOSS] Report generation failed for job ${job.id}:`, error);
        throw error;
      }
    });
  }
  /**
   * @private
   * @method scheduleJobs
   * @description Schedules recurring system-level jobs using a cron-like syntax.
   * It ensures that existing schedules are cleared on startup to prevent duplicates.
   */
  async scheduleJobs() {
    await this.boss.unschedule("generate-soc2-evidence");
    const cron2 = "0 3 1 * *";
    await this.boss.schedule("generate-soc2-evidence", cron2);
    console.log(`[PG-BOSS] Scheduled job 'generate-soc2-evidence' with cron: ${cron2}`);
  }
  /**
   * @method scheduleReport
   * @description Schedules a report to be generated on a recurring basis, defined by a cron string.
   * @param {string} reportName - A descriptive name for the report being scheduled.
   * @param {string} cron - The cron string defining the schedule (e.g., '0 8 * * 1' for every Monday at 8 AM).
   * @param {any} data - The data required by the report generation job.
   * @returns {Promise<string | null>} The ID of the scheduled job.
   */
  async scheduleReport(reportName, cron2, data) {
    const jobData = { ...data, reportName };
    const jobId = await this.boss.send(JOB_QUEUE_GENERATE_REPORT, jobData, { tz: "UTC", ...cron2 ? { cron: cron2 } : {} });
    console.log(`[PG-BOSS] Scheduled report '${reportName}' on queue '${JOB_QUEUE_GENERATE_REPORT}' with cron: ${cron2}`);
    return jobId;
  }
  /**
   * @method queueReport
   * @description Adds a report generation job to the queue for immediate, one-time execution.
   * @param {string} reportName - A descriptive name for the report.
   * @param {any} data - The data required for the report generation.
   * @returns {Promise<string | null>} The ID of the queued job.
   */
  async queueReport(reportName, data) {
    const jobData = { ...data, reportName };
    const jobId = await this.boss.send(JOB_QUEUE_GENERATE_REPORT, jobData);
    console.log(`[PG-BOSS] Queued report '${reportName}' on queue '${JOB_QUEUE_GENERATE_REPORT}' with id: ${jobId}`);
    return jobId;
  }
  /**
   * @method stop
   * @description Gracefully stops the job processor.
   * This allows any currently running jobs to complete before shutting down.
   * @returns {Promise<void>}
   */
  async stop() {
    await this.boss.stop();
    console.log("[PG-BOSS] Job processor stopped.");
  }
};
var BatchJobService_default = BatchJobService.getInstance();

// src/routes/reporting.ts
import fs26 from "fs/promises";
import { existsSync as existsSync5, mkdirSync as mkdirSync5, writeFileSync as writeFileSync3 } from "fs";
import path30 from "path";
var TEMPLATE_STORE_PATH = path30.join(process.cwd(), "server", "data", "report-templates.json");
if (!existsSync5(path30.dirname(TEMPLATE_STORE_PATH))) {
  mkdirSync5(path30.dirname(TEMPLATE_STORE_PATH), { recursive: true });
}
if (!existsSync5(TEMPLATE_STORE_PATH)) {
  const seedTemplates = [
    {
      id: "template-1",
      name: "Weekly Security Summary",
      description: "Overview of security events",
      content: '{"events": {{events}}}',
      format: "json"
    },
    {
      id: "template-2",
      name: "Incident Report",
      description: "Detailed incident report",
      content: "<h1>Incident Report</h1><p>{{details}}</p>",
      format: "pdf"
    },
    {
      id: "template-3",
      name: "System Health",
      description: "System health check",
      content: "<health><status>{{status}}</status></health>",
      format: "xml"
    }
  ];
  writeFileSync3(TEMPLATE_STORE_PATH, JSON.stringify(seedTemplates, null, 2));
}
async function getTemplates() {
  try {
    const data = await fs26.readFile(TEMPLATE_STORE_PATH, "utf-8");
    return JSON.parse(data);
  } catch {
    return [];
  }
}
async function saveTemplates(templates) {
  await fs26.writeFile(TEMPLATE_STORE_PATH, JSON.stringify(templates, null, 2));
}
var defaultRules2 = [
  { resource: "report", action: "view", roles: ["user", "admin"] },
  { resource: "report", action: "create", roles: ["editor", "admin"] },
  { resource: "report", action: "update", roles: ["editor", "admin"] },
  { resource: "report", action: "deliver", roles: ["admin"] }
];
var reportingService2 = createReportingService(new AccessControlService(defaultRules2));
var router62 = express30.Router();
router62.use((req, res, next) => {
  const user = req.user;
  if (user) {
    req.accessContext = {
      userId: user.sub || user.id,
      roles: user.roles || [user.role]
    };
    next();
  } else {
    res.status(401).json({ error: "Unauthorized: Authentication required" });
  }
});
router62.get("/templates", async (req, res) => {
  res.json(await getTemplates());
});
router62.post("/templates", async (req, res) => {
  try {
    const newTemplate = {
      id: `template-${Date.now()}`,
      ...req.body
    };
    const templates = await getTemplates();
    templates.push(newTemplate);
    await saveTemplates(templates);
    res.status(201).json(newTemplate);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router62.put("/templates/:id", async (req, res) => {
  try {
    const templates = await getTemplates();
    const index = templates.findIndex((t) => t.id === req.params.id);
    if (index === -1) return res.status(404).json({ error: "Template not found" });
    templates[index] = { ...templates[index], ...req.body };
    await saveTemplates(templates);
    res.json(templates[index]);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router62.post("/generate", async (req, res) => {
  try {
    const request = req.body;
    if (typeof request.template === "string") {
      const templates = await getTemplates();
      const tpl = templates.find((t) => t.id === request.template);
      if (!tpl) return res.status(404).json({ error: "Template not found" });
      request.template = tpl;
    }
    const access = req.accessContext;
    const artifact = await reportingService2.generate(request, access);
    res.setHeader("Content-Type", artifact.mimeType);
    res.setHeader("Content-Disposition", `attachment; filename="${artifact.fileName}"`);
    if (artifact.metadata) {
      res.setHeader("X-Report-Version", artifact.metadata.versionId);
    }
    res.send(artifact.buffer);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router62.post("/schedule", async (req, res) => {
  try {
    const { name, cron: cron2, request } = req.body;
    if (!name || !cron2 || !request) {
      return res.status(400).json({ error: "Missing required fields: name, cron, request" });
    }
    const reportRequest = { ...request };
    if (typeof reportRequest.template === "string") {
      const templates = await getTemplates();
      const tpl = templates.find((t) => t.id === reportRequest.template);
      if (!tpl) return res.status(404).json({ error: "Template not found" });
      reportRequest.template = tpl;
    }
    await BatchJobService_default.scheduleReport(name, cron2, { request: reportRequest, userId: req.accessContext.userId });
    res.status(201).json({ message: "Report scheduled", reportName: name });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router62.get("/history/:templateId", (req, res) => {
  const history = reportingService2.history(req.params.templateId);
  res.json(history);
});
var reporting_default = router62;

// src/routes/policy-profiles.ts
import { Router as Router37 } from "express";
import { z as z30 } from "zod";

// src/services/PolicyProfileService.ts
init_database();
init_ledger();
init_logger2();
import { createHash as createHash26 } from "crypto";
var BASELINE_PROFILE = {
  id: "baseline",
  version: "1.0.0",
  regoPackage: "tenant.baseline",
  entrypoints: ["allow"],
  guardrails: {
    defaultDeny: true,
    requirePurpose: false,
    requireJustification: false
  },
  crossTenant: {
    mode: "deny",
    allow: [],
    requireAgreements: true
  },
  rules: [
    {
      id: "allow-public-read",
      effect: "allow",
      priority: 10,
      conditions: {
        actions: ["read"],
        environments: ["production", "staging", "dev"]
      }
    }
  ]
};
var STRICT_PROFILE = {
  id: "strict",
  version: "1.0.0",
  regoPackage: "tenant.strict",
  entrypoints: ["allow"],
  guardrails: {
    defaultDeny: true,
    requirePurpose: true,
    requireJustification: true
  },
  crossTenant: {
    mode: "deny",
    allow: [],
    requireAgreements: true
  },
  rules: [
    {
      id: "allow-internal-read",
      effect: "allow",
      priority: 10,
      conditions: {
        actions: ["read"],
        subjectTenants: []
        // Must match resource tenant (implicit in engine usually, but explicit here)
      }
    }
  ]
};
var CUSTOM_PROFILE = {
  id: "custom",
  version: "0.0.1",
  regoPackage: "tenant.custom",
  entrypoints: ["allow"],
  guardrails: {
    defaultDeny: true,
    requirePurpose: false,
    requireJustification: false
  },
  crossTenant: {
    mode: "allowlist",
    allow: [],
    requireAgreements: true
  },
  rules: []
};
var PROFILES = {
  baseline: BASELINE_PROFILE,
  strict: STRICT_PROFILE,
  custom: CUSTOM_PROFILE
};
var PolicyProfileService = class _PolicyProfileService {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_PolicyProfileService.instance) {
      _PolicyProfileService.instance = new _PolicyProfileService();
    }
    return _PolicyProfileService.instance;
  }
  getProfiles() {
    return [
      {
        id: "baseline",
        name: "Baseline Security",
        description: "Standard protection suitable for most non-sensitive workloads.",
        guardrails: BASELINE_PROFILE.guardrails
      },
      {
        id: "strict",
        name: "Strict Compliance",
        description: "High-security mode requiring purpose and justification for all actions.",
        guardrails: STRICT_PROFILE.guardrails
      },
      {
        id: "custom",
        name: "Custom Configuration",
        description: "Fully customizable policy profile.",
        guardrails: CUSTOM_PROFILE.guardrails
      }
    ];
  }
  getProfile(id) {
    return PROFILES[id] || null;
  }
  async applyProfile(tenantId, profileId, actorId) {
    const profile = this.getProfile(profileId);
    if (!profile) {
      throw new Error(`Policy profile '${profileId}' not found`);
    }
    const tenant = await tenantService.getTenant(tenantId);
    if (!tenant) {
      throw new Error(`Tenant '${tenantId}' not found`);
    }
    const newBundle = {
      tenantId,
      baseProfile: profile,
      overlays: [],
      // Reset overlays or keep them? For "Apply Profile", usually implies resetting base.
      metadata: {
        issuedAt: (/* @__PURE__ */ new Date()).toISOString(),
        source: `partner-console:apply-profile:${profileId}`
      }
    };
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      await client6.query("BEGIN");
      const { settings: newSettings } = buildSettingsWithHistory(
        tenant.settings || {},
        {
          policy_profile: profileId,
          policy_bundle: newBundle
        },
        actorId,
        "policy_profile_applied"
      );
      await client6.query(
        "UPDATE tenants SET settings = $1, updated_at = NOW() WHERE id = $2",
        [newSettings, tenantId]
      );
      await provenanceLedger.appendEntry({
        tenantId,
        timestamp: /* @__PURE__ */ new Date(),
        actionType: "TENANT_POLICY_APPLIED",
        resourceType: "PolicyProfile",
        resourceId: profileId,
        actorId,
        actorType: "user",
        payload: {
          mutationType: "UPDATE",
          entityId: profileId,
          entityType: "PolicyProfile"
        },
        metadata: {
          bundleHash: createHash26("sha256").update(JSON.stringify(newBundle)).digest("hex")
        }
      });
      await client6.query("COMMIT");
      logger_default2.info(`Applied policy profile '${profileId}' to tenant ${tenantId}`);
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Failed to apply policy profile", error);
      throw error;
    } finally {
      client6.release();
    }
  }
};
var policyProfileService = PolicyProfileService.getInstance();

// src/services/policy-profiles/PolicyProfileAssignmentService.ts
init_database();

// src/policies/profile-manifests.ts
import { createHash as createHash27 } from "crypto";
var BASELINE_PROFILE2 = {
  id: "baseline",
  version: "1.0.0",
  regoPackage: "tenant.baseline",
  entrypoints: ["allow"],
  guardrails: {
    defaultDeny: true,
    requirePurpose: false,
    requireJustification: false
  },
  crossTenant: {
    mode: "deny",
    allow: [],
    requireAgreements: true
  },
  rules: [
    {
      id: "allow-public-read",
      effect: "allow",
      priority: 10,
      conditions: {
        actions: ["read"],
        environments: ["production", "staging", "dev"]
      }
    }
  ]
};
var STRICT_PROFILE2 = {
  id: "strict",
  version: "1.0.0",
  regoPackage: "tenant.strict",
  entrypoints: ["allow"],
  guardrails: {
    defaultDeny: true,
    requirePurpose: true,
    requireJustification: true
  },
  crossTenant: {
    mode: "deny",
    allow: [],
    requireAgreements: true
  },
  rules: [
    {
      id: "allow-internal-read",
      effect: "allow",
      priority: 10,
      conditions: {
        actions: ["read"],
        subjectTenants: []
      }
    }
  ]
};
var CUSTOM_PROFILE2 = {
  id: "custom",
  version: "0.1.0",
  regoPackage: "tenant.custom",
  entrypoints: ["allow"],
  guardrails: {
    defaultDeny: true,
    requirePurpose: false,
    requireJustification: false
  },
  crossTenant: {
    mode: "allowlist",
    allow: [],
    requireAgreements: true
  },
  rules: [
    {
      id: "deny-unconfigured-actions",
      effect: "deny",
      priority: 1,
      conditions: {
        actions: []
      }
    }
  ]
};
var checksumOf = (value) => createHash27("sha256").update(JSON.stringify(value)).digest("hex");
var bundlePointerFor = (id, version, baseProfile) => ({
  id,
  version,
  checksum: checksumOf(baseProfile)
});
var manifestFor = (id, name, description, baseProfile, bundleId) => ({
  id,
  name,
  description,
  version: baseProfile.version,
  checksum: checksumOf(baseProfile),
  bundle: bundlePointerFor(bundleId, baseProfile.version, baseProfile),
  baseProfile
});
var PROFILE_MANIFESTS = {
  baseline: manifestFor(
    "baseline",
    "Baseline Security",
    "Standard protection suitable for most non-sensitive workloads.",
    BASELINE_PROFILE2,
    "bundle-tenant-baseline"
  ),
  strict: manifestFor(
    "strict",
    "Strict Compliance",
    "High-security mode requiring purpose and justification for all actions.",
    STRICT_PROFILE2,
    "bundle-tenant-strict"
  ),
  custom: manifestFor(
    "custom",
    "Custom Configuration",
    "Fully customizable policy profile.",
    CUSTOM_PROFILE2,
    "bundle-tenant-custom"
  )
};
var DEFAULT_POLICY_PROFILE_ID = "baseline";
var policyBundleMappings = Object.fromEntries(
  Object.entries(PROFILE_MANIFESTS).map(([id, manifest]) => [id, manifest.bundle])
);
var getPolicyProfileManifest = (id) => PROFILE_MANIFESTS[id] || null;
var buildTenantPolicyBundle = (tenantId, profileId, source) => {
  const manifest = getPolicyProfileManifest(profileId);
  if (!manifest) {
    throw new Error(`Policy profile '${profileId}' not found`);
  }
  return {
    tenantId,
    bundleId: manifest.bundle.id,
    baseProfile: manifest.baseProfile,
    overlays: [],
    metadata: {
      issuedAt: (/* @__PURE__ */ new Date()).toISOString(),
      source
    }
  };
};

// src/provenance/policyProfileAssignments.ts
init_ledger();
var recordPolicyProfileAssignment = async (params) => {
  const { tenantId, profileId, bundlePointer, manifest, actorId, actorType, source } = params;
  return provenanceLedger.appendEntry({
    tenantId,
    timestamp: /* @__PURE__ */ new Date(),
    actionType: "POLICY_PROFILE_ASSIGNED",
    resourceType: "PolicyProfile",
    resourceId: profileId,
    actorId,
    actorType,
    payload: {
      mutationType: "CREATE",
      entityId: profileId,
      entityType: "PolicyProfile",
      profileId,
      bundlePointer,
      manifestVersion: manifest.version,
      manifestChecksum: manifest.checksum,
      source
    },
    metadata: {
      bundleId: bundlePointer.id,
      bundleVersion: bundlePointer.version,
      bundleChecksum: bundlePointer.checksum
    }
  });
};

// src/services/policy-profiles/PolicyProfileAssignmentService.ts
init_logger2();
var PolicyProfileAssignmentService = class _PolicyProfileAssignmentService {
  static instance;
  _receiptService;
  constructor() {
  }
  get receiptService() {
    if (!this._receiptService) {
      this._receiptService = ReceiptService.getInstance();
    }
    return this._receiptService;
  }
  static getInstance() {
    if (!_PolicyProfileAssignmentService.instance) {
      _PolicyProfileAssignmentService.instance = new _PolicyProfileAssignmentService();
    }
    return _PolicyProfileAssignmentService.instance;
  }
  async assignProfile(input) {
    const { tenantId, profileId, actorId, actorType, source } = input;
    const manifest = getPolicyProfileManifest(profileId);
    if (!manifest) {
      throw new Error(`Policy profile '${profileId}' not found`);
    }
    const tenant = await tenantService.getTenant(tenantId);
    if (!tenant) {
      throw new Error(`Tenant '${tenantId}' not found`);
    }
    const bundle = buildTenantPolicyBundle(tenantId, profileId, source);
    const bundlePointer = manifest.bundle;
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      await client6.query("BEGIN");
      const newSettings = {
        ...tenant.settings || {},
        policy_profile: profileId,
        policy_profile_version: manifest.version,
        policy_bundle_pointer: bundlePointer,
        policy_bundle: bundle
      };
      await client6.query(
        "UPDATE tenants SET settings = $1, updated_at = NOW() WHERE id = $2",
        [newSettings, tenantId]
      );
      await recordPolicyProfileAssignment({
        tenantId,
        profileId,
        bundlePointer,
        manifest,
        actorId,
        actorType,
        source
      });
      const receipt = await this.receiptService.generateReceipt({
        action: "POLICY_PROFILE_ASSIGNED",
        actor: { id: actorId, tenantId },
        resource: `policy-profile:${profileId}`,
        input: {
          tenantId,
          profileId,
          bundlePointer,
          manifestVersion: manifest.version,
          manifestChecksum: manifest.checksum
        }
      });
      await client6.query("COMMIT");
      logger_default2.info(`Assigned policy profile '${profileId}' to tenant ${tenantId}`);
      return {
        profileId,
        manifest,
        bundlePointer,
        bundle,
        receipt
      };
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Failed to assign policy profile", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  async getActiveProfile(tenantId) {
    const tenant = await tenantService.getTenant(tenantId);
    if (!tenant) {
      throw new Error(`Tenant '${tenantId}' not found`);
    }
    const settings = tenant.settings || {};
    const profileId = settings.policy_profile || DEFAULT_POLICY_PROFILE_ID;
    const manifest = getPolicyProfileManifest(profileId);
    if (!manifest) {
      throw new Error(`Policy profile '${profileId}' not found`);
    }
    const bundlePointer = settings.policy_bundle_pointer || manifest.bundle;
    const bundle = settings.policy_bundle || buildTenantPolicyBundle(tenantId, profileId, "policy-profile:resolved");
    return {
      profileId,
      manifest,
      bundlePointer,
      bundle,
      source: settings.policy_profile ? "tenant-settings" : "default"
    };
  }
};
var policyProfileAssignmentService = PolicyProfileAssignmentService.getInstance();

// src/routes/policy-profiles.ts
init_logger2();
init_auth4();
var router63 = Router37();
var assignProfileSchema = z30.object({
  profileId: z30.string().min(1),
  tenantId: z30.string().optional(),
  source: z30.string().optional()
});
router63.get("/", ensureAuthenticated, async (req, res) => {
  try {
    const profiles = policyProfileService.getProfiles();
    res.json({
      success: true,
      data: profiles
    });
  } catch (error) {
    logger_default2.error("Error in GET /api/policy-profiles:", error);
    res.status(500).json({
      success: false,
      error: "Internal Server Error"
    });
  }
});
router63.post("/assign", ensureAuthenticated, async (req, res) => {
  try {
    const parseResult = assignProfileSchema.safeParse(req.body);
    if (!parseResult.success) {
      res.status(400).json({
        success: false,
        error: "Validation Error",
        details: parseResult.error.errors
      });
      return;
    }
    const user = req.user;
    const tenantId = parseResult.data.tenantId || req.headers["x-tenant-id"] || user?.tenantId || "default-tenant";
    const result2 = await policyProfileAssignmentService.assignProfile({
      tenantId,
      profileId: parseResult.data.profileId,
      actorId: user?.id || "system",
      actorType: "user",
      source: parseResult.data.source || "api:policy-profiles:assign"
    });
    res.json({
      success: true,
      data: result2
    });
  } catch (error) {
    logger_default2.error("Error in POST /api/policy-profiles/assign:", error);
    res.status(500).json({
      success: false,
      error: "Internal Server Error"
    });
  }
});
var policy_profiles_default = router63;

// src/routes/policy-proposals.ts
import express31 from "express";
import fs27 from "fs";
import path31 from "path";
var router64 = express31.Router();
var PROPOSALS_DIR = path31.join(process.cwd(), ".security/proposals");
var getProposals = () => {
  if (!fs27.existsSync(PROPOSALS_DIR)) return [];
  const dirs = fs27.readdirSync(PROPOSALS_DIR);
  const proposals = [];
  for (const dir of dirs) {
    const jsonPath = path31.join(PROPOSALS_DIR, dir, "proposal.json");
    if (fs27.existsSync(jsonPath)) {
      try {
        const data = JSON.parse(fs27.readFileSync(jsonPath, "utf8"));
        proposals.push(data);
      } catch (e) {
        console.error(`Failed to parse proposal ${dir}`, e);
      }
    }
  }
  return proposals.sort((a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime());
};
router64.get("/", (req, res) => {
  const proposals = getProposals();
  res.json({ proposals });
});
router64.get("/:id", (req, res) => {
  const { id } = req.params;
  const proposalPath = path31.join(PROPOSALS_DIR, id, "proposal.json");
  if (!fs27.existsSync(proposalPath)) {
    return res.status(404).json({ error: "Proposal not found" });
  }
  const proposal = JSON.parse(fs27.readFileSync(proposalPath, "utf8"));
  res.json({ proposal });
});
router64.post("/:id/decision", (req, res) => {
  const { id } = req.params;
  const { decision, comment } = req.body;
  if (!["approved", "rejected"].includes(decision)) {
    return res.status(400).json({ error: "Invalid decision. Must be approved or rejected." });
  }
  const proposalPath = path31.join(PROPOSALS_DIR, id, "proposal.json");
  if (!fs27.existsSync(proposalPath)) {
    return res.status(404).json({ error: "Proposal not found" });
  }
  try {
    const proposal = JSON.parse(fs27.readFileSync(proposalPath, "utf8"));
    proposal.status = decision;
    proposal.decisionMetadata = {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      decider: req.user?.id || "anonymous",
      // In real app, enforce auth
      comment
    };
    fs27.writeFileSync(proposalPath, JSON.stringify(proposal, null, 2));
    console.log(`AUDIT: Proposal ${id} was ${decision} by ${req.user?.id || "anonymous"}`);
    res.json({ success: true, proposal });
  } catch (error) {
    console.error("Failed to update proposal", error);
    res.status(500).json({ error: "Internal server error" });
  }
});
var policy_proposals_default = router64;

// src/routes/evidence.ts
init_auth4();
import { Router as Router38 } from "express";
import { z as z31 } from "zod";
init_logger2();
init_database();
init_ledger();

// src/provenance/evidenceExport.ts
init_pg();
var ADMIN_ACTION_TYPES = [
  "TENANT_CREATED",
  "TENANT_SETTINGS_UPDATED",
  "TENANT_DISABLED",
  "ROLE_CREATED",
  "ROLE_UPDATED",
  "ROLE_DELETED",
  "ROLE_ASSIGNED",
  "ROLE_REVOKED",
  "ROLES_LISTED"
];
var DR_RECEIPT_TYPES = [
  "dr_receipt",
  "disaster_recovery_receipt",
  "receipt"
];
async function exportEvidencePayload(tenantId, window) {
  const [accessLogs, adminChangeReceipts, policyVersions, drReceipts] = await Promise.all([
    pool.query(
      `
        SELECT *
        FROM maestro.audit_access_logs
        WHERE tenant_id = $1
          AND created_at >= $2
          AND created_at <= $3
        ORDER BY created_at DESC
      `,
      [tenantId, window.start, window.end]
    ),
    pool.query(
      `
        SELECT id,
               tenant_id,
               action_type,
               resource_type,
               resource_id,
               actor_id,
               actor_type,
               timestamp,
               metadata,
               payload,
               current_hash
        FROM provenance_ledger_v2
        WHERE tenant_id = $1
          AND action_type = ANY($2)
          AND timestamp >= $3
          AND timestamp <= $4
        ORDER BY timestamp DESC
      `,
      [tenantId, ADMIN_ACTION_TYPES, window.start, window.end]
    ),
    pool.query(
      `
        SELECT
          pv.id,
          pv.policy_id,
          pv.version,
          pv.status,
          pv.created_by,
          pv.created_at,
          pv.approved_by,
          pv.approved_at,
          pv.content,
          mp.name as policy_name,
          mp.tenant_id
        FROM policy_versions pv
        JOIN managed_policies mp ON mp.id = pv.policy_id
        WHERE mp.tenant_id = $1
          AND pv.created_at >= $2
          AND pv.created_at <= $3
        ORDER BY pv.created_at DESC
      `,
      [tenantId, window.start, window.end]
    ),
    pool.query(
      `
        SELECT
          id,
          artifact_type,
          storage_uri,
          sha256,
          classification_level,
          content_preview,
          created_at,
          tenant_id
        FROM evidence_artifacts
        WHERE tenant_id = $1
          AND artifact_type = ANY($2)
          AND created_at >= $3
          AND created_at <= $4
        ORDER BY created_at DESC
      `,
      [tenantId, DR_RECEIPT_TYPES, window.start, window.end]
    )
  ]);
  return {
    accessLogs: accessLogs.rows,
    adminChangeReceipts: adminChangeReceipts.rows,
    policyVersions: policyVersions.rows,
    drReceipts: drReceipts.rows
  };
}

// src/routes/evidence.ts
import archiver5 from "archiver";
import { createHash as createHash28, randomUUID as randomUUID51 } from "crypto";
var router65 = Router38();
var exportRequestSchema = z31.object({
  tenantId: z31.string().uuid(),
  timeRange: z31.object({
    start: z31.string().datetime(),
    end: z31.string().datetime()
  })
});
var exportStatusSchema = z31.object({
  tenantId: z31.string().uuid(),
  windowHours: z31.coerce.number().min(1).max(720).default(168)
});
function buildReceipt2(action, tenantId, actorId) {
  const issuedAt = (/* @__PURE__ */ new Date()).toISOString();
  const payload = `${action}:${tenantId}:${actorId}:${issuedAt}`;
  const hash3 = createHash28("sha256").update(payload).digest("hex");
  return {
    id: randomUUID51(),
    action,
    tenantId,
    actorId,
    issuedAt,
    hash: hash3,
    policy: "abac.ensurePolicy"
  };
}
router65.post(
  "/exports",
  ensureAuthenticated,
  ensurePolicy("read", "evidence"),
  // Assuming 'evidence' resource exists in policy
  async (req, res) => {
    try {
      const authReq = req;
      const { tenantId, timeRange } = exportRequestSchema.parse(req.body);
      const actorId = authReq.user?.id || "unknown";
      const userTenantId = authReq.user?.tenantId;
      const isSuperAdmin = authReq.user?.role === "SUPER_ADMIN";
      if (!isSuperAdmin && userTenantId !== tenantId) {
        return res.status(403).json({ success: false, error: "Forbidden" });
      }
      const tenant = await tenantService.getTenant(tenantId);
      if (!tenant) {
        return res.status(404).json({ success: false, error: "Tenant not found" });
      }
      const repo = new ProvenanceRepo(getPostgresPool2());
      const filteredEvents = await repo.byTenant(
        tenantId,
        { from: timeRange.start, to: timeRange.end },
        1e3,
        0
      );
      const evidencePayload = await exportEvidencePayload(tenantId, {
        start: new Date(timeRange.start),
        end: new Date(timeRange.end)
      });
      res.setHeader("Content-Type", "application/zip");
      res.setHeader(
        "Content-Disposition",
        `attachment; filename="evidence-${tenantId}-${timeRange.start}.zip"`
      );
      const archive = archiver5("zip", { zlib: { level: 9 } });
      archive.on("error", (err) => {
        logger_default2.error("Archive error", err);
        if (!res.headersSent) {
          res.status(500).json({ success: false, error: "Archive generation failed" });
        }
      });
      archive.pipe(res);
      const metadata = {
        exportId: randomUUID51(),
        tenantId,
        timeRange,
        generatedAt: (/* @__PURE__ */ new Date()).toISOString(),
        actorId,
        eventCount: filteredEvents.length,
        accessLogCount: evidencePayload.accessLogs.length,
        adminChangeReceiptCount: evidencePayload.adminChangeReceipts.length,
        policyVersionCount: evidencePayload.policyVersions.length,
        drReceiptCount: evidencePayload.drReceipts.length,
        policyProfile: tenant.settings?.policy_profile || "unknown"
      };
      archive.append(JSON.stringify(metadata, null, 2), { name: "metadata.json" });
      archive.append(JSON.stringify(filteredEvents, null, 2), { name: "audit_events.json" });
      archive.append(
        JSON.stringify(evidencePayload.accessLogs, null, 2),
        { name: "access_logs.json" }
      );
      archive.append(
        JSON.stringify(evidencePayload.adminChangeReceipts, null, 2),
        { name: "admin_change_receipts.json" }
      );
      archive.append(
        JSON.stringify(evidencePayload.policyVersions, null, 2),
        { name: "policy_versions.json" }
      );
      archive.append(
        JSON.stringify(evidencePayload.drReceipts, null, 2),
        { name: "dr_receipts.json" }
      );
      if (tenant.settings?.policy_bundle) {
        archive.append(JSON.stringify(tenant.settings.policy_bundle, null, 2), { name: "policy_bundle.json" });
      }
      const bundleHash = createHash28("sha256").update(JSON.stringify({ metadata, eventCount: filteredEvents.length })).digest("hex");
      const receipt = buildReceipt2("EVIDENCE_EXPORT_GENERATED", tenantId, actorId);
      await provenanceLedger.appendEntry({
        action: "EVIDENCE_EXPORT_GENERATED",
        actor: { id: actorId, role: authReq.user?.role || "user" },
        metadata: {
          tenantId,
          exportId: metadata.exportId,
          timeRange,
          bundleHash
        },
        artifacts: []
      });
      archive.append(JSON.stringify({ ...receipt, bundleHash }, null, 2), { name: "receipt.json" });
      await archive.finalize();
    } catch (error) {
      if (error instanceof z31.ZodError) {
        return res.status(400).json({ success: false, error: "Validation Error", details: error.errors });
      }
      logger_default2.error("Error in POST /api/evidence/exports:", error);
      if (!res.headersSent) {
        res.status(500).json({ success: false, error: "Internal Server Error" });
      }
    }
  }
);
router65.get(
  "/exports/status",
  ensureAuthenticated,
  ensurePolicy("read", "evidence"),
  async (req, res) => {
    try {
      const authReq = req;
      const { tenantId, windowHours } = exportStatusSchema.parse(req.query);
      const actorId = authReq.user?.id || "unknown";
      const userTenantId = authReq.user?.tenantId;
      const isSuperAdmin = authReq.user?.role === "SUPER_ADMIN";
      if (!isSuperAdmin && userTenantId !== tenantId) {
        return res.status(403).json({ success: false, error: "Forbidden" });
      }
      const tenant = await tenantService.getTenant(tenantId);
      if (!tenant) {
        return res.status(404).json({ success: false, error: "Tenant not found" });
      }
      const windowEnd = /* @__PURE__ */ new Date();
      const windowStart = new Date(
        windowEnd.getTime() - windowHours * 60 * 60 * 1e3
      );
      const repo = new ProvenanceRepo(getPostgresPool2());
      const stats = await repo.getTenantStats(tenantId, {
        from: windowStart.toISOString(),
        to: windowEnd.toISOString()
      });
      const policyBundleReady = Boolean(tenant.settings?.policy_bundle);
      const ready = policyBundleReady && stats.count > 0;
      return res.json({
        success: true,
        data: {
          tenantId,
          actorId,
          windowStart: windowStart.toISOString(),
          windowEnd: windowEnd.toISOString(),
          eventCount: stats.count,
          lastEventAt: stats.lastEventAt,
          policyBundleReady,
          ready
        }
      });
    } catch (error) {
      if (error instanceof z31.ZodError) {
        return res.status(400).json({ success: false, error: "Validation Error", details: error.errors });
      }
      logger_default2.error("Error in GET /api/evidence/exports/status:", error);
      return res.status(500).json({ success: false, error: "Internal Server Error" });
    }
  }
);
var evidence_default2 = router65;

// src/routes/mastery.ts
import express32 from "express";

// src/mastery/MasteryService.ts
import { randomUUID as randomUUID52 } from "crypto";

// src/mastery/definitions/analyst-i.ts
var analystILabs = [
  {
    id: "lab-1-ingest-map",
    title: "Lab 1: Ingest & Map",
    description: "Learn how to upload datasets, map schemas, and apply redaction presets.",
    version: "1.0.0",
    tags: ["Analyst I", "Ingestion"],
    steps: [
      {
        id: "upload_dataset",
        title: "Upload Dataset",
        instructions: 'Upload a CSV dataset named "financial_records.csv" via the Ingestion interface.',
        validation: {
          type: "provenance_event",
          config: {
            actionType: "DATASET_UPLOAD",
            metadata: { filename: "financial_records.csv" }
          }
        }
      },
      {
        id: "map_schema",
        title: "Map Schema",
        instructions: 'Map the "src_ip" column to the "IPAddress" entity type.',
        validation: {
          type: "provenance_event",
          config: {
            actionType: "SCHEMA_MAPPING_SAVED",
            payload: { mapping: { src_ip: "IPAddress" } }
            // simplified match
          }
        }
      },
      {
        id: "apply_redaction",
        title: "Apply Redaction",
        instructions: 'Apply the "PII-Standard" redaction preset to the dataset.',
        validation: {
          type: "provenance_event",
          config: {
            actionType: "REDACTION_APPLIED",
            payload: { preset: "PII-Standard" }
          }
        }
      }
    ]
  },
  {
    id: "lab-2-resolve-reconcile",
    title: "Lab 2: Resolve & Reconcile",
    description: "Master entity resolution: merging, unmerging, and setting rules.",
    version: "1.0.0",
    tags: ["Analyst I", "Entity Resolution"],
    steps: [
      {
        id: "review_queue",
        title: "Review ER Queue",
        instructions: 'Open the Entity Resolution queue and select the "John Doe" duplicate candidates.',
        validation: {
          type: "manual",
          // Hard to detect "viewing", so manual check or just skip
          config: {}
        }
      },
      {
        id: "merge_entities",
        title: "Merge Entities",
        instructions: 'Merge the two "John Doe" entities.',
        validation: {
          type: "provenance_event",
          config: {
            actionType: "ER_MERGE"
          }
        }
      },
      {
        id: "undo_merge",
        title: "Undo Merge",
        instructions: "Realize the mistake and undo the merge you just performed.",
        validation: {
          type: "provenance_event",
          config: {
            actionType: "ER_UNMERGE"
          }
        }
      },
      {
        id: "set_never_merge",
        title: "Set Never Merge Rule",
        instructions: 'Set a "Never Merge" rule between "John Doe (ID: 123)" and "John Doe (ID: 456)".',
        validation: {
          type: "provenance_event",
          config: {
            actionType: "ER_RULE_SET",
            payload: { ruleType: "NEVER_MERGE" }
          }
        }
      }
    ]
  },
  {
    id: "lab-3-hypothesis",
    title: "Lab 3: Hypothesis & Cited Brief",
    description: "Create hypotheses, attach evidence, and ensure all claims are cited.",
    version: "1.0.0",
    tags: ["Analyst I", "Analysis"],
    steps: [
      {
        id: "create_hypothesis",
        title: "Create Hypothesis",
        instructions: 'Create a new hypothesis: "The traffic spike is due to a misconfiguration".',
        validation: {
          type: "provenance_event",
          config: {
            actionType: "HYPOTHESIS_CREATED"
          }
        }
      },
      {
        id: "attach_evidence",
        title: "Attach Evidence",
        instructions: "Attach at least one piece of evidence to the hypothesis.",
        validation: {
          type: "provenance_event",
          config: {
            actionType: "EVIDENCE_ATTACHED"
          }
        }
      },
      {
        id: "generate_brief",
        title: "Generate Brief",
        instructions: "Generate a brief. Ensure all claims are cited.",
        validation: {
          type: "custom_check",
          config: {
            checkName: "all_claims_cited"
          }
        }
      }
    ]
  }
];

// src/mastery/MasteryService.ts
init_ledger();
var MasteryService = class {
  labs = /* @__PURE__ */ new Map();
  // In-memory storage for active runs (prototype). In prod, use DB.
  runs = /* @__PURE__ */ new Map();
  constructor() {
    analystILabs.forEach((lab) => this.labs.set(lab.id, lab));
  }
  getLabs() {
    return Array.from(this.labs.values());
  }
  getLab(labId) {
    return this.labs.get(labId);
  }
  startLab(labId, userId, tenantId) {
    const lab = this.labs.get(labId);
    if (!lab) throw new Error("Lab not found");
    const runId = randomUUID52();
    provenanceLedger.appendEntry({
      tenantId,
      actionType: "LAB_START",
      resourceType: "lab_run",
      resourceId: runId,
      actorId: userId,
      actorType: "user",
      timestamp: /* @__PURE__ */ new Date(),
      payload: {
        mutationType: "CREATE",
        entityId: runId,
        entityType: "LabRun",
        labId
      },
      metadata: { version: lab.version }
    }).catch((err) => console.error("Failed to log lab start", err));
    const run = {
      runId,
      labId,
      userId,
      tenantId,
      status: "in_progress",
      startTime: /* @__PURE__ */ new Date(),
      steps: {},
      currentStepId: lab.steps[0].id
    };
    lab.steps.forEach((step) => {
      run.steps[step.id] = {
        stepId: step.id,
        status: "pending"
      };
    });
    this.runs.set(runId, run);
    return run;
  }
  getRun(runId) {
    return this.runs.get(runId);
  }
  getUserRuns(userId) {
    return Array.from(this.runs.values()).filter((r) => r.userId === userId);
  }
  async validateStep(runId, stepId) {
    const run = this.runs.get(runId);
    if (!run) throw new Error("Run not found");
    const lab = this.labs.get(run.labId);
    if (!lab) throw new Error("Lab definition not found");
    const stepDef = lab.steps.find((s) => s.id === stepId);
    if (!stepDef) throw new Error("Step definition not found");
    const validation = stepDef.validation;
    let success = false;
    let message = "";
    try {
      if (validation.type === "manual") {
        success = true;
      } else if (validation.type === "provenance_event") {
        const entries = await provenanceLedger.getEntries(run.tenantId, {
          actionType: validation.config.actionType,
          limit: 100
          // Look at recent 100 entries of this type
        });
        const match = entries.find((e) => {
          const timeMatch = new Date(e.timestamp) >= run.startTime;
          const actorMatch = e.actorId === run.userId;
          let payloadMatch = true;
          if (validation.config.payload) {
            payloadMatch = Object.entries(validation.config.payload).every(
              ([k, v]) => e.payload && e.payload[k] === v
            );
          }
          let metadataMatch = true;
          if (validation.config.metadata) {
            metadataMatch = Object.entries(validation.config.metadata).every(
              ([k, v]) => e.metadata && e.metadata[k] === v
            );
          }
          return timeMatch && actorMatch && payloadMatch && metadataMatch;
        });
        if (match) success = true;
        else message = `Action ${validation.config.actionType} not found for user since lab started.`;
      } else if (validation.type === "custom_check") {
        if (validation.config.checkName === "all_claims_cited") {
          success = true;
        } else {
          message = "Unknown custom check";
        }
      }
    } catch (err) {
      message = `Validation error: ${err.message}`;
    }
    if (success) {
      const stepState = run.steps[stepId];
      if (stepState.status !== "completed") {
        stepState.status = "completed";
        stepState.completedAt = /* @__PURE__ */ new Date();
        const currentIndex = lab.steps.findIndex((s) => s.id === stepId);
        if (currentIndex < lab.steps.length - 1) {
          run.currentStepId = lab.steps[currentIndex + 1].id;
        } else {
          run.status = "completed";
          run.endTime = /* @__PURE__ */ new Date();
          try {
            await provenanceLedger.appendEntry({
              tenantId: run.tenantId,
              actionType: "LAB_COMPLETE",
              resourceType: "lab_run",
              resourceId: run.runId,
              actorId: run.userId,
              actorType: "user",
              timestamp: /* @__PURE__ */ new Date(),
              payload: {
                mutationType: "UPDATE",
                entityId: run.runId,
                entityType: "LabRun",
                labId: run.labId,
                steps: run.steps
              },
              metadata: {}
            });
          } catch (err) {
            console.error("Failed to log lab completion", err);
          }
          await this.checkCertification(run);
        }
      }
    }
    return { success, message };
  }
  // Helper to fetch completed labs from ledger
  async getCompletedLabIds(userId, tenantId) {
    try {
      const entries = await provenanceLedger.getEntries(tenantId, {
        actionType: "LAB_COMPLETE",
        limit: 1e3
        // reasonable limit
      });
      const completed = /* @__PURE__ */ new Set();
      entries.filter((e) => e.actorId === userId).forEach((e) => {
        if (e.payload && e.payload.labId) {
          completed.add(e.payload.labId);
        }
      });
      return completed;
    } catch (err) {
      console.error("Failed to fetch completed labs", err);
      return /* @__PURE__ */ new Set();
    }
  }
  async checkCertification(run) {
    const requiredLabs = ["lab-1-ingest-map", "lab-2-resolve-reconcile", "lab-3-hypothesis"];
    const completedLabs = await this.getCompletedLabIds(run.userId, run.tenantId);
    completedLabs.add(run.labId);
    if (requiredLabs.every((id) => completedLabs.has(id))) {
      await this.issueCertificate(run.userId, run.tenantId, "Analyst I");
    }
  }
  async issueCertificate(userId, tenantId, certName) {
    const existingCerts = await this.getUserCertificates(userId, tenantId);
    if (existingCerts.some((c) => c.name === certName)) return;
    const certId = randomUUID52();
    const cert = {
      id: certId,
      userId,
      tenantId,
      name: certName,
      grantedAt: /* @__PURE__ */ new Date(),
      version: "1.0",
      issuer: "System"
    };
    await provenanceLedger.appendEntry({
      tenantId,
      actionType: "CERTIFICATE_ISSUED",
      resourceType: "certificate",
      resourceId: certId,
      actorId: userId,
      actorType: "system",
      timestamp: /* @__PURE__ */ new Date(),
      payload: {
        mutationType: "CREATE",
        entityId: certId,
        entityType: "Certificate",
        certificate: cert
      },
      metadata: {}
    });
  }
  async getUserCertificates(userId, tenantId) {
    if (!tenantId) return [];
    try {
      const entries = await provenanceLedger.getEntries(tenantId, {
        actionType: "CERTIFICATE_ISSUED",
        limit: 100
      });
      return entries.filter((e) => e.actorId === userId || e.payload.certificate?.userId === userId).map((e) => e.payload.certificate).filter((c) => !!c);
    } catch (e) {
      console.error("Failed to fetch user certificates", e);
      return [];
    }
  }
  getAllRuns() {
    return Array.from(this.runs.values());
  }
  // Coaching Feedback Loop
  getSuggestedLabs(tripwireType) {
    const map = {
      "unverified_ingest": "lab-1-ingest-map",
      "bad_merge": "lab-2-resolve-reconcile",
      "uncited_claim": "lab-3-hypothesis",
      "compliance_violation": "lab-3-hypothesis"
    };
    const labId = map[tripwireType];
    if (labId) {
      const lab = this.labs.get(labId);
      return lab ? [lab] : [];
    }
    return [];
  }
};
var masteryService = new MasteryService();

// src/routes/mastery.ts
init_auth4();
var router66 = express32.Router();
var singleParam14 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
router66.get("/labs", ensureAuthenticated, (req, res) => {
  res.json(masteryService.getLabs());
});
router66.post("/labs/:labId/start", ensureAuthenticated, (req, res) => {
  try {
    const labId = singleParam14(req.params.labId) ?? "";
    const run = masteryService.startLab(labId, req.user.id, req.user.tenantId);
    res.json(run);
  } catch (err) {
    res.status(400).json({ error: err.message });
  }
});
router66.get("/runs", ensureAuthenticated, (req, res) => {
  const runs = masteryService.getUserRuns(req.user.id);
  res.json(runs);
});
router66.get("/runs/:runId", ensureAuthenticated, (req, res) => {
  const runId = singleParam14(req.params.runId) ?? "";
  const run = masteryService.getRun(runId);
  if (!run || run.userId !== req.user.id) {
    return res.status(404).json({ error: "Run not found" });
  }
  res.json(run);
});
router66.post("/runs/:runId/steps/:stepId/validate", ensureAuthenticated, async (req, res) => {
  const runId = singleParam14(req.params.runId) ?? "";
  const stepId = singleParam14(req.params.stepId) ?? "";
  const run = masteryService.getRun(runId);
  if (!run || run.userId !== req.user.id) {
    return res.status(404).json({ error: "Run not found" });
  }
  try {
    const result2 = await masteryService.validateStep(runId, stepId);
    res.json(result2);
  } catch (err) {
    res.status(400).json({ error: err.message });
  }
});
router66.get("/certificates", ensureAuthenticated, async (req, res) => {
  try {
    const certs = await masteryService.getUserCertificates(req.user.id, req.user.tenantId);
    res.json(certs);
  } catch (err) {
    res.status(500).json({ error: "Failed to fetch certificates" });
  }
});
router66.get("/coaching", ensureAuthenticated, (req, res) => {
  const tripwire = singleParam14(req.query.tripwire);
  if (tripwire) {
    res.json(masteryService.getSuggestedLabs(tripwire));
  } else {
    res.json([]);
  }
});
router66.get("/admin/stats", ensureAuthenticated, ensureRole("admin"), (req, res) => {
  const runs = masteryService.getAllRuns();
  const stats = {
    totalRuns: runs.length,
    completedRuns: runs.filter((r) => r.status === "completed").length,
    byLab: {}
  };
  runs.forEach((r) => {
    if (!stats.byLab[r.labId]) stats.byLab[r.labId] = { started: 0, completed: 0 };
    stats.byLab[r.labId].started++;
    if (r.status === "completed") stats.byLab[r.labId].completed++;
  });
  res.json(stats);
});
var mastery_default = router66;

// src/routes/crypto-intelligence.ts
import express33 from "express";

// src/services/BaseService.ts
var BaseService = class {
  constructor() {
  }
};

// src/services/CryptoIntelligenceService.ts
init_neo4j();
init_logger();
var CryptoIntelligenceService = class _CryptoIntelligenceService extends BaseService {
  static instance;
  driver = getNeo4jDriver();
  constructor() {
    super();
  }
  static getInstance() {
    if (!_CryptoIntelligenceService.instance) {
      _CryptoIntelligenceService.instance = new _CryptoIntelligenceService();
    }
    return _CryptoIntelligenceService.instance;
  }
  /**
   * Analyzes a transaction for suspicious patterns like structuring or mixing.
   */
  async analyzeTransactionPattern(txHash, chain = "ETH") {
    const isSuspicious = txHash.startsWith("0xdead") || txHash.includes("feed");
    if (isSuspicious) {
      return {
        txHash,
        riskLevel: "high",
        flags: ["high_velocity", "round_numbers"],
        patternType: "structuring",
        description: "Transaction shows signs of structuring (smurfing) with round number values."
      };
    }
    return {
      txHash,
      riskLevel: "low",
      flags: [],
      description: "No suspicious patterns detected."
    };
  }
  /**
   * Clusters wallets based on heuristics like co-spending inputs.
   */
  async clusterWallets(address, chain = "ETH") {
    const session = this.driver.session();
    try {
      const query3 = `
        MATCH (a:Address {hash: $address})-[r:SENT_TO|RECEIVED_FROM]-(b:Address)
        RETURN b.hash as relatedAddress
        LIMIT 10
      `;
      const result2 = await session.run(query3, { address });
      const related = result2.records.map((r) => r.get("relatedAddress"));
      if (related.length === 0) {
        return {
          mainAddress: address,
          relatedAddresses: [`${address}_mock_1`, `${address}_mock_2`],
          heuristic: "co-spend",
          confidence: 0.85
        };
      }
      return {
        mainAddress: address,
        relatedAddresses: related,
        heuristic: "co-spend",
        confidence: 0.9
      };
    } finally {
      await session.close();
    }
  }
  /**
   * Monitors dark web marketplaces for keywords or addresses.
   */
  async monitorDarkWeb(marketplace, keyword) {
    logger.info(`Scanning ${marketplace} for ${keyword}...`);
    if (keyword.toLowerCase().includes("ransom") || keyword.startsWith("0x")) {
      return [{
        marketplace,
        keyword,
        foundAt: /* @__PURE__ */ new Date(),
        snippet: `Listing selling access to compromised servers. Payment to ${keyword}...`,
        relatedCryptoAddresses: [keyword.startsWith("0x") ? keyword : "0x123..."]
      }];
    }
    return [];
  }
  /**
   * Detects if an address belongs to a mixing service.
   */
  async detectMixingService(address, chain = "ETH") {
    const knownMixers = ["TornadoCash", "Typhoon", "Cyclone"];
    if (address.toLowerCase().includes("tornado")) {
      return { isMixer: true, serviceName: "TornadoCash", confidence: 0.99 };
    }
    const isSuspicious = Math.random() > 0.8;
    if (isSuspicious) {
      return { isMixer: true, serviceName: "Unknown Mixer", confidence: 0.6 };
    }
    return { isMixer: false, confidence: 0.9 };
  }
  /**
   * Profiles a threat actor based on their crypto activity.
   */
  async profileThreatActor(actorId) {
    const session = this.driver.session();
    try {
      const query3 = `
         MATCH (t:ThreatActor {id: $actorId})
         OPTIONAL MATCH (t)-[:CONTROLS]->(a:Address)
         RETURN t, collect(a.hash) as addresses
       `;
      const result2 = await session.run(query3, { actorId });
      if (result2.records.length > 0 && result2.records[0].get("t")) {
        const record2 = result2.records[0];
        const properties = record2.get("t").properties;
        const addresses = record2.get("addresses");
        return {
          actorId,
          knownAddresses: addresses,
          assets: properties.assets || ["BTC", "ETH"],
          riskScore: properties.riskScore || 0.8,
          lastActive: /* @__PURE__ */ new Date(),
          // Mock
          associatedDarkWebHandles: properties.handles || []
        };
      }
    } finally {
      await session.close();
    }
    return {
      actorId,
      knownAddresses: ["0xAttacker1", "0xAttacker2"],
      assets: ["XMR", "ETH"],
      riskScore: 0.95,
      lastActive: /* @__PURE__ */ new Date(),
      associatedDarkWebHandles: ["DarkLord", "Cipher"]
    };
  }
};

// src/routes/crypto-intelligence.ts
init_auth4();
init_logger();
var router67 = express33.Router();
var service8 = CryptoIntelligenceService.getInstance();
var enforceAnalystAccess = [ensureAuthenticated];
router67.post("/analyze/transaction", enforceAnalystAccess, async (req, res) => {
  try {
    const { txHash, chain } = req.body;
    if (!txHash) return res.status(400).json({ error: "txHash is required" });
    const result2 = await service8.analyzeTransactionPattern(txHash, chain);
    res.json(result2);
  } catch (error) {
    logger.error({ error }, "Error analyzing transaction");
    res.status(500).json({ error: "Internal server error" });
  }
});
router67.post("/cluster/wallet", enforceAnalystAccess, async (req, res) => {
  try {
    const { address, chain } = req.body;
    if (!address) return res.status(400).json({ error: "address is required" });
    const result2 = await service8.clusterWallets(address, chain);
    res.json(result2);
  } catch (error) {
    logger.error({ error }, "Error clustering wallets");
    res.status(500).json({ error: "Internal server error" });
  }
});
router67.post("/monitor/darkweb", enforceAnalystAccess, async (req, res) => {
  try {
    const { marketplace, keyword } = req.body;
    if (!marketplace || !keyword) return res.status(400).json({ error: "marketplace and keyword are required" });
    const result2 = await service8.monitorDarkWeb(marketplace, keyword);
    res.json(result2);
  } catch (error) {
    logger.error({ error }, "Error monitoring dark web");
    res.status(500).json({ error: "Internal server error" });
  }
});
router67.post("/detect/mixer", enforceAnalystAccess, async (req, res) => {
  try {
    const { address, chain } = req.body;
    if (!address) return res.status(400).json({ error: "address is required" });
    const result2 = await service8.detectMixingService(address, chain);
    res.json(result2);
  } catch (error) {
    logger.error({ error }, "Error detecting mixer");
    res.status(500).json({ error: "Internal server error" });
  }
});
router67.get("/threat-actor/:id", enforceAnalystAccess, async (req, res) => {
  try {
    const { id } = req.params;
    const result2 = await service8.profileThreatActor(id);
    res.json(result2);
  } catch (error) {
    logger.error({ error }, "Error profiling threat actor");
    res.status(500).json({ error: "Internal server error" });
  }
});
var crypto_intelligence_default = router67;

// src/routes/demo.ts
init_logger();
import express34 from "express";
var router68 = express34.Router();
router68.get("/status", (req, res) => {
  res.json({
    status: "ready",
    mode: process.env.DEMO_MODE === "1" ? "demo" : "production",
    db: "connected"
    // Fake for now
  });
});
router68.post("/seed", async (req, res) => {
  logger.info("Demo seed requested");
  await new Promise((resolve2) => setTimeout(resolve2, 1e3));
  res.json({ status: "seeded", duration: "1s" });
});
router68.post("/reset", async (req, res) => {
  logger.info("Demo reset requested");
  await new Promise((resolve2) => setTimeout(resolve2, 500));
  res.json({ status: "reset" });
});
var demo_default = router68;

// src/routes/claims.ts
import { Router as Router39 } from "express";
import { z as z32 } from "zod";

// src/services/ProvenanceClaimService.ts
init_pg();
init_ledger();
import crypto37 from "crypto";
var ProvenanceClaimService = class _ProvenanceClaimService {
  static instance;
  static getInstance() {
    if (!_ProvenanceClaimService.instance) {
      _ProvenanceClaimService.instance = new _ProvenanceClaimService();
    }
    return _ProvenanceClaimService.instance;
  }
  async registerEvidence(input) {
    const client6 = await pool.connect();
    try {
      await client6.query("BEGIN");
      const res = await client6.query(
        `INSERT INTO evidence_artifacts
        (sha256, artifact_type, storage_uri, source_id, transform_chain, license_id, classification_level, content_preview, registered_by, tenant_id)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
        RETURNING *`,
        [
          input.evidence_hash,
          input.evidence_type,
          input.storage_uri,
          input.source_id,
          input.transform_chain || [],
          input.license_id,
          input.classification_level || "INTERNAL",
          input.content_preview,
          input.registered_by,
          input.tenant_id
        ]
      );
      const evidence = res.rows[0];
      await provenanceLedger.appendEntry({
        tenantId: input.tenant_id,
        actionType: "REGISTER_EVIDENCE",
        resourceType: "Evidence",
        resourceId: evidence.id,
        actorId: input.registered_by,
        actorType: "user",
        payload: {
          evidence_hash: input.evidence_hash,
          storage_uri: input.storage_uri,
          license_id: input.license_id
        },
        metadata: {
          classification: [evidence.classification_level]
        }
      });
      await client6.query("COMMIT");
      return evidence;
    } catch (e) {
      await client6.query("ROLLBACK");
      throw e;
    } finally {
      client6.release();
    }
  }
  async registerClaim(input) {
    const client6 = await pool.connect();
    try {
      await client6.query("BEGIN");
      const contentHash = crypto37.createHash("sha256").update(input.content).digest("hex");
      const evidenceHashes = [];
      const res = await client6.query(
        `INSERT INTO claims_registry
        (content_hash, content, subject, predicate, object, effective_date, location, extraction_method, claim_type, confidence, evidence_hashes, source_id, transform_chain, license_id, created_by, investigation_id, tenant_id, extracted_at)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, NOW())
        RETURNING *`,
        [
          contentHash,
          input.content,
          input.subject,
          input.predicate,
          input.object,
          input.effective_date,
          input.location ? JSON.stringify(input.location) : null,
          input.extraction_method,
          input.claim_type,
          input.confidence,
          JSON.stringify(evidenceHashes),
          input.source_id,
          input.transform_chain || [],
          input.license_id,
          input.created_by,
          input.investigation_id,
          input.tenant_id
        ]
      );
      const claim = res.rows[0];
      if (input.evidence_ids && input.evidence_ids.length > 0) {
        for (const eid of input.evidence_ids) {
          await this.linkClaimToEvidenceInternal(client6, {
            claim_id: claim.id,
            evidence_id: eid,
            relation_type: "SUPPORTS",
            // Default for initial evidence
            created_by: input.created_by,
            tenant_id: input.tenant_id
          });
        }
      }
      await provenanceLedger.appendEntry({
        tenantId: input.tenant_id,
        actionType: "REGISTER_CLAIM",
        resourceType: "Claim",
        resourceId: claim.id,
        actorId: input.created_by,
        actorType: "user",
        payload: {
          content_hash: contentHash,
          claim_type: input.claim_type,
          confidence: input.confidence,
          investigation_id: input.investigation_id
        },
        metadata: { purpose: "intelligence_analysis" }
      });
      await client6.query("COMMIT");
      return claim;
    } catch (e) {
      await client6.query("ROLLBACK");
      throw e;
    } finally {
      client6.release();
    }
  }
  async linkClaimToEvidence(input) {
    const client6 = await pool.connect();
    try {
      await client6.query("BEGIN");
      const link = await this.linkClaimToEvidenceInternal(client6, input);
      await client6.query("COMMIT");
      return link;
    } catch (e) {
      await client6.query("ROLLBACK");
      throw e;
    } finally {
      client6.release();
    }
  }
  async linkClaimToEvidenceInternal(client6, input) {
    const check = await client6.query(
      `SELECT id FROM claim_evidence_links WHERE claim_id=$1 AND evidence_id=$2 AND relation_type=$3`,
      [input.claim_id, input.evidence_id, input.relation_type]
    );
    if (check.rows.length > 0) return check.rows[0];
    const res = await client6.query(
      `INSERT INTO claim_evidence_links
      (claim_id, evidence_id, relation_type, confidence, offset_start, offset_end, page_number, bbox, segment_text, created_by, notes, tenant_id)
      VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
      RETURNING *`,
      [
        input.claim_id,
        input.evidence_id,
        input.relation_type,
        input.confidence || 1,
        input.offset_start,
        input.offset_end,
        input.page_number,
        input.bbox ? JSON.stringify(input.bbox) : null,
        input.segment_text,
        input.created_by,
        input.notes,
        input.tenant_id
      ]
    );
    const link = res.rows[0];
    await provenanceLedger.appendEntry({
      tenantId: input.tenant_id,
      actionType: "LINK_EVIDENCE",
      resourceType: "ClaimEvidenceLink",
      resourceId: link.id,
      actorId: input.created_by,
      actorType: "user",
      payload: {
        claim_id: input.claim_id,
        evidence_id: input.evidence_id,
        relation_type: input.relation_type
      },
      metadata: {}
    });
    return link;
  }
  async createExportManifest(input) {
    const client6 = await pool.connect();
    try {
      const claimsRes = await client6.query(
        `SELECT * FROM claims_registry WHERE id = ANY($1) AND tenant_id = $2`,
        [input.claim_ids, input.tenant_id]
      );
      const claims = claimsRes.rows;
      const items = claims.map((c) => ({
        id: c.id,
        type: "claim",
        hash: c.content_hash,
        content: c.content
      }));
      const hashes = items.map((i) => i.hash).sort();
      const merkleRoot = this.computeMerkleRoot(hashes);
      const bundleId = `bundle-${crypto37.randomUUID()}`;
      const res = await client6.query(
        `INSERT INTO export_manifests
        (manifest_version, bundle_id, merkle_root, items, custody_chain, export_type, classification_level, authority_basis, created_by, tenant_id)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
        RETURNING *`,
        [
          "1.0.0",
          bundleId,
          merkleRoot,
          JSON.stringify(items),
          JSON.stringify([]),
          // Initial custody chain
          input.export_type,
          input.classification_level,
          input.authority_basis,
          input.created_by,
          input.tenant_id
        ]
      );
      const manifest = res.rows[0];
      await provenanceLedger.appendEntry({
        tenantId: input.tenant_id,
        actionType: "CREATE_MANIFEST",
        resourceType: "ExportManifest",
        resourceId: manifest.id,
        actorId: input.created_by,
        actorType: "user",
        payload: {
          bundle_id: bundleId,
          merkle_root: merkleRoot,
          item_count: items.length
        },
        metadata: { classification: [input.classification_level] }
      });
      return manifest;
    } finally {
      client6.release();
    }
  }
  computeMerkleRoot(hashes) {
    if (hashes.length === 0) return "";
    if (hashes.length === 1) return hashes[0];
    const nextLevel = [];
    for (let i = 0; i < hashes.length; i += 2) {
      const left = hashes[i];
      const right = i + 1 < hashes.length ? hashes[i + 1] : left;
      const combined = crypto37.createHash("sha256").update(left + right).digest("hex");
      nextLevel.push(combined);
    }
    return this.computeMerkleRoot(nextLevel);
  }
};

// src/routes/claims.ts
init_auth4();
var router69 = Router39();
var service9 = ProvenanceClaimService.getInstance();
var createClaimSchema = z32.object({
  content: z32.string().min(1),
  subject: z32.string().optional(),
  predicate: z32.string().optional(),
  object: z32.string().optional(),
  effective_date: z32.string().transform((d) => new Date(d)).optional(),
  location: z32.record(z32.any()).optional(),
  extraction_method: z32.string().optional(),
  claim_type: z32.string(),
  confidence: z32.number().min(0).max(1),
  evidence_ids: z32.array(z32.string()),
  source_id: z32.string(),
  transform_chain: z32.array(z32.string()).optional(),
  license_id: z32.string(),
  investigation_id: z32.string().optional()
});
var linkEvidenceSchema = z32.object({
  evidence_id: z32.string(),
  relation_type: z32.enum(["SUPPORTS", "CONTRADICTS"]),
  confidence: z32.number().optional(),
  offset_start: z32.number().optional(),
  offset_end: z32.number().optional(),
  page_number: z32.number().optional(),
  bbox: z32.array(z32.number()).optional(),
  segment_text: z32.string().optional(),
  notes: z32.string().optional()
});
router69.post("/", ensureAuthenticated, async (req, res, next) => {
  try {
    const validated = createClaimSchema.parse(req.body);
    const claim = await service9.registerClaim({
      ...validated,
      created_by: req.user.id,
      tenant_id: req.user.tenantId
    });
    res.status(201).json(claim);
  } catch (error) {
    next(error);
  }
});
router69.post("/:id/evidence", ensureAuthenticated, async (req, res, next) => {
  try {
    const validated = linkEvidenceSchema.parse(req.body);
    const link = await service9.linkClaimToEvidence({
      claim_id: req.params.id,
      ...validated,
      created_by: req.user.id,
      tenant_id: req.user.tenantId
    });
    res.status(201).json(link);
  } catch (error) {
    next(error);
  }
});
var claims_default = router69;

// src/routes/ops.ts
import { Router as Router40 } from "express";

// src/db/partitioning.ts
init_postgres();
init_logger2();
init_redis2();

// src/services/ColdStorageService.ts
init_logger();
import { promisify as promisify4 } from "util";
import { exec as exec3 } from "child_process";
import path32 from "path";
import fs28 from "fs/promises";
var execAsync3 = promisify4(exec3);
var ColdStorageService = class {
  s3Config = null;
  constructor() {
    if (process.env.S3_COLD_STORAGE_BUCKET) {
      this.s3Config = {
        bucket: process.env.S3_COLD_STORAGE_BUCKET,
        region: process.env.S3_REGION || "us-east-1",
        endpoint: process.env.S3_ENDPOINT,
        accessKeyId: process.env.AWS_ACCESS_KEY_ID,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY
      };
    }
  }
  /**
   * Upload a file to Cold Storage (e.g. S3 Glacier)
   */
  async upload(filepath, key, storageClass = "STANDARD_IA") {
    if (!this.s3Config) {
      logger_default.warn("Skipping Cold Storage upload: No configuration found.");
      return;
    }
    logger_default.info(`Uploading ${filepath} to Cold Storage bucket ${this.s3Config.bucket} as ${key} (Class: ${storageClass})...`);
    try {
      if (process.env.USE_AWS_CLI === "true") {
        await execAsync3(
          `aws s3 cp "${filepath}" "s3://${this.s3Config.bucket}/${key}" --region ${this.s3Config.region} --storage-class ${storageClass}`
        );
      } else {
        await new Promise((r) => setTimeout(r, 500));
        logger_default.info(`Simulated Cold Storage upload complete (Class: ${storageClass}).`);
      }
    } catch (error) {
      logger_default.error("Failed to upload to Cold Storage", error);
      throw error;
    }
  }
  /**
   * Archive a table partition
   * 1. Export partition data to file (CSV/Parquet)
   * 2. Upload to Cold Storage
   * 3. (Optional) Drop partition from DB
   */
  async archivePartition(tableName, partitionName, dropAfterArchive = false, storageClass = "STANDARD_IA") {
    logger_default.info(`Archiving partition ${partitionName} of table ${tableName} to ${storageClass}...`);
    const exportDir = path32.join(process.env.TEMP_DIR || "/tmp", "archives");
    await fs28.mkdir(exportDir, { recursive: true });
    const timestamp = (/* @__PURE__ */ new Date()).toISOString().replace(/[:.]/g, "-");
    const filename = `${partitionName}-${timestamp}.csv.gz`;
    const filepath = path32.join(exportDir, filename);
    try {
      await fs28.writeFile(filepath, `Simulated export of ${partitionName}
`);
      await this.upload(filepath, `archives/${tableName}/${filename}`, storageClass);
      await fs28.unlink(filepath);
      if (dropAfterArchive) {
        logger_default.info(`Ready to drop partition ${partitionName} (dry-run)`);
      }
    } catch (error) {
      logger_default.error(`Failed to archive partition ${partitionName}`, error);
      throw error;
    }
  }
};
var coldStorageService = new ColdStorageService();

// src/db/partitioning.ts
var PartitionManager = class {
  pool = getPostgresPool();
  redis = RedisService.getInstance();
  /**
   * Creates a new partition for a specific tenant in the maestro_runs table.
   * Useful when onboarding a new tenant to ensure they have their own dedicated partition.
   */
  async createTenantPartition(tenantId) {
    const safeTenantId = tenantId.replace(/[^a-zA-Z0-9_]/g, "");
    const partitionName = `maestro_runs_${safeTenantId}`;
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const checkRes = await client6.query(
        `SELECT to_regclass($1::text)`,
        [partitionName]
      );
      if (checkRes.rows[0].to_regclass) {
        logger_default2.info(`Partition ${partitionName} already exists.`);
        await client6.query("COMMIT");
        return;
      }
      const query3 = `
        CREATE TABLE ${partitionName}
        PARTITION OF maestro_runs
        FOR VALUES IN ('${tenantId}')
      `;
      await client6.query(query3);
      logger_default2.info(`Created partition ${partitionName} for tenant ${tenantId}`);
      await client6.query("COMMIT");
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error(`Failed to create partition for tenant ${tenantId}`, error);
      throw error;
    } finally {
      client6.release();
    }
  }
  /**
   * Creates a monthly partition for a time-series table (e.g., audit_logs, metrics).
   * Range Partitioning: FOR VALUES FROM ('2023-01-01') TO ('2023-02-01')
   */
  async createMonthlyPartition(tableName, date) {
    const year = date.getFullYear();
    const month = String(date.getMonth() + 1).padStart(2, "0");
    const partitionName = `${tableName}_y${year}m${month}`;
    const startObj = new Date(year, date.getMonth(), 1);
    const endObj = new Date(year, date.getMonth() + 1, 1);
    const startStr = startObj.toISOString().split("T")[0];
    const endStr = endObj.toISOString().split("T")[0];
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const checkRes = await client6.query(
        `SELECT to_regclass($1::text)`,
        [partitionName]
      );
      if (checkRes.rows[0].to_regclass) {
        logger_default2.info(`Partition ${partitionName} already exists.`);
        await client6.query("COMMIT");
        return;
      }
      const query3 = `
        CREATE TABLE ${partitionName}
        PARTITION OF ${tableName}
        FOR VALUES FROM ('${startStr}') TO ('${endStr}')
      `;
      await client6.query(query3);
      logger_default2.info(`Created monthly partition ${partitionName} (${startStr} to ${endStr})`);
      await client6.query("COMMIT");
    } catch (error) {
      await client6.query("ROLLBACK");
      if (error.code === "42P01") {
        logger_default2.warn(`Parent table ${tableName} does not exist. Skipping partition creation.`);
      } else {
        logger_default2.error(`Failed to create partition ${partitionName}`, error);
        throw error;
      }
    } finally {
      client6.release();
    }
  }
  /**
   * Maintenance job to ensure upcoming partitions exist and detach/archive old ones.
   * Can be scheduled via pg-boss or node-cron.
   */
  async maintainPartitions(tables = ["audit_logs", "metrics", "provenance_ledger_v2"]) {
    const now = /* @__PURE__ */ new Date();
    const nextMonth = new Date(now.getFullYear(), now.getMonth() + 1, 1);
    const monthAfterNext = new Date(now.getFullYear(), now.getMonth() + 2, 1);
    for (const table of tables) {
      await this.createMonthlyPartition(table, now);
      await this.createMonthlyPartition(table, nextMonth);
      await this.createMonthlyPartition(table, monthAfterNext);
    }
  }
  async detachOldPartitions(tables, retentionMonths) {
    const cutoff = /* @__PURE__ */ new Date();
    cutoff.setMonth(cutoff.getMonth() - retentionMonths);
    const client6 = await this.pool.connect();
    try {
      for (const table of tables) {
        const result2 = await client6.query(
          "SELECT inhrelid::regclass::text AS partition_name FROM pg_inherits WHERE inhparent = $1::regclass",
          [table]
        );
        for (const row of result2.rows ?? []) {
          const partitionName = row.partition_name;
          const match = partitionName.match(/_y(\d{4})m(\d{2})$/);
          if (!match) continue;
          const year = Number(match[1]);
          const month = Number(match[2]);
          const partitionDate = new Date(year, month - 1, 1);
          if (partitionDate <= cutoff) {
            await client6.query(
              `ALTER TABLE ${table} DETACH PARTITION ${partitionName}`
            );
            await coldStorageService.archivePartition(
              table,
              partitionName,
              true
            );
          }
        }
      }
    } catch (error) {
      logger_default2.error("Failed to detach old partitions", error);
      throw error;
    } finally {
      client6.release();
    }
  }
};
var partitionManager = new PartitionManager();

// src/scripts/maintenance.ts
init_logger();
init_redis2();
var redis2 = RedisService.getInstance();
var backupService = new BackupService();
async function runMaintenance() {
  logger_default.info("Starting system maintenance...");
  try {
    logger_default.info("Running partition maintenance...");
    try {
      await partitionManager.maintainPartitions(["audit_logs", "metrics"]);
      logger_default.info("Partition maintenance complete.");
    } catch (e) {
      logger_default.warn({ error: e }, "Partition maintenance failed (likely DB connection issue in dev)");
    }
    logger_default.info("Verifying backup inventory...");
    try {
      await backupService.ensureBackupDir("postgres");
      await backupService.ensureBackupDir("neo4j");
      await backupService.ensureBackupDir("redis");
      logger_default.info("Backup directory verification complete.");
    } catch (e) {
      logger_default.error({ error: e }, "Backup verification failed");
      throw e;
    }
    logger_default.info("Running cache maintenance...");
    const memoryUsage3 = process.memoryUsage();
    logger_default.info({ memoryUsage: memoryUsage3 }, "Maintenance memory check");
    logger_default.info("System maintenance completed successfully.");
  } catch (error) {
    logger_default.error("System maintenance failed", error);
    process.exit(1);
  }
}
if (import.meta.url === `file://${process.argv[1]}`) {
  runMaintenance().then(() => process.exit(0)).catch(() => process.exit(1));
}

// src/dr/DisasterRecoveryService.ts
init_redis2();
init_logger();
init_postgres();
import fs29 from "fs/promises";
import path33 from "path";
var DisasterRecoveryService = class {
  backupService;
  redis;
  constructor() {
    this.backupService = new BackupService();
    this.redis = RedisService.getInstance();
  }
  /**
   * List available backups for restoration
   */
  async listBackups(type) {
    const backupDir = path33.join(process.env.BACKUP_ROOT_DIR || "./backups", type);
    try {
      const dates = await fs29.readdir(backupDir);
      return dates.sort().reverse();
    } catch (e) {
      logger_default.warn(`Could not list backups for ${type}`, e);
      return [];
    }
  }
  /**
   * Simulate a Disaster Recovery Drill
   * This restores the latest backup to a TEMPORARY database/namespace to verify integrity
   * without affecting production data.
   */
  async runDrill(target = "postgres") {
    logger_default.info(`Starting DR Drill for ${target}...`);
    const startTime = Date.now();
    try {
      const backups = await this.listBackups(target);
      if (backups.length === 0) {
        throw new Error(`No backups found for ${target}`);
      }
      const latestDate = backups[0];
      const backupDir = path33.join(process.env.BACKUP_ROOT_DIR || "./backups", target, latestDate);
      const files = await fs29.readdir(backupDir);
      if (files.length === 0) throw new Error("Empty backup directory");
      const backupFile = path33.join(backupDir, files[0]);
      logger_default.info(`Selected backup for drill: ${backupFile}`);
      if (target === "postgres") {
        await this.verifyPostgresRestore(backupFile);
      } else if (target === "neo4j") {
        await this.verifyNeo4jRestore(backupFile);
      }
      await this.recordDrillResult(true, Date.now() - startTime);
      logger_default.info(`DR Drill for ${target} completed successfully.`);
      return true;
    } catch (error) {
      logger_default.error(`DR Drill for ${target} failed`, error);
      await this.recordDrillResult(false, Date.now() - startTime, error.message);
      return false;
    }
  }
  async verifyPostgresRestore(backupFile) {
    const pool4 = getPostgresPool();
    const tempDbName = `dr_drill_${Date.now()}`;
    const client6 = await pool4.connect();
    try {
      await client6.query(`CREATE DATABASE "${tempDbName}"`);
      logger_default.info(`Created temp DB ${tempDbName}`);
      await new Promise((r) => setTimeout(r, 2e3));
      logger_default.info("Simulated restore complete.");
    } finally {
      try {
        await client6.query(`DROP DATABASE IF EXISTS "${tempDbName}"`);
      } catch (e) {
        logger_default.warn(`Failed to drop temp DB ${tempDbName}`, e);
      }
      client6.release();
    }
  }
  async verifyNeo4jRestore(backupFile) {
    logger_default.info("Simulating Neo4j restore verification...");
    await new Promise((r) => setTimeout(r, 1e3));
  }
  async recordDrillResult(success, durationMs, error) {
    const result2 = {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      success,
      durationMs,
      error
    };
    try {
      await this.redis.set("dr:last_drill", JSON.stringify(result2));
    } catch (e) {
      logger_default.error("Failed to record drill result to Redis", e);
    }
  }
  async getStatus() {
    try {
      const lastDrillStr = await this.redis.get("dr:last_drill");
      const lastDrill = lastDrillStr ? JSON.parse(lastDrillStr) : null;
      return {
        lastDrill: lastDrill ? new Date(lastDrill.timestamp) : null,
        lastDrillSuccess: lastDrill?.success ?? false,
        activeAlerts: [],
        // Implement alert check
        systemHealth: "healthy"
      };
    } catch (e) {
      logger_default.error("Failed to get status from Redis", e);
      return {
        lastDrill: null,
        lastDrillSuccess: false,
        activeAlerts: ["REDIS_CONNECTION_ERROR"],
        systemHealth: "degraded"
      };
    }
  }
};

// src/routes/ops.ts
init_auth4();
init_logger();

// src/evidence/integrity-service.ts
init_postgres();
init_otel_tracing();
import crypto38 from "crypto";
import fs30 from "fs/promises";
import path34 from "path";
import pino55 from "pino";

// src/incident.ts
import { randomUUID as uuid } from "crypto";
import fetch4 from "node-fetch";
import { Pool as Pool8 } from "pg";
var pg4 = new Pool8({ connectionString: process.env.DATABASE_URL });
async function openIncident({
  runbook,
  tenant,
  severity,
  reason,
  details
}) {
  const id = uuid();
  await pg4.query(
    `INSERT INTO incidents(id,runbook,tenant,severity,status,reason,details) VALUES ($1,$2,$3,$4,'OPEN',$5,$6)`,
    [id, runbook, tenant, severity, reason, details]
  );
  if (process.env.PAGERDUTY_URL)
    await fetch4(process.env.PAGERDUTY_URL, {
      method: "POST",
      headers: { "content-type": "application/json" },
      body: JSON.stringify({
        summary: `${runbook} SLO breach`,
        severity,
        source: "maestro",
        custom_details: details
      })
    });
  if (process.env.OPSGENIE_URL)
    await fetch4(process.env.OPSGENIE_URL, {
      method: "POST",
      headers: { "content-type": "application/json" },
      body: JSON.stringify({
        message: `${runbook} incident: ${reason}`,
        priority: severity
      })
    });
  if (process.env.SLACK_WEBHOOK)
    await fetch4(process.env.SLACK_WEBHOOK, {
      method: "POST",
      headers: { "content-type": "application/json" },
      body: JSON.stringify({
        text: `\u{1F6A8} Incident ${id} for ${runbook}: ${reason}`
      })
    });
  return id;
}

// src/evidence/integrity-service.ts
var INLINE_PREFIX = "inline://evidence_artifact_content/";
var EvidenceIntegrityService = class {
  logger = pino55({ name: "EvidenceIntegrityService" });
  pool;
  storageRoot;
  constructor(options2) {
    this.storageRoot = options2?.storageRoot || process.env.EVIDENCE_STORAGE_ROOT || path34.resolve(process.cwd(), "uploads/evidence");
    this.pool = options2?.pool || getPostgresPool();
  }
  async verifyAll(options2 = {}) {
    const chunkSize = Math.max(
      1,
      options2.chunkSize ?? Number(process.env.EVIDENCE_INTEGRITY_CHUNK ?? 50)
    );
    const rateLimitPerSecond = Math.max(
      1,
      options2.rateLimitPerSecond ?? Number(process.env.EVIDENCE_INTEGRITY_RPS ?? 5)
    );
    const emitIncidents = options2.emitIncidents ?? false;
    const minIntervalMs = Math.ceil(1e3 / rateLimitPerSecond);
    const span = otelService.createSpan("evidence.integrity.verify");
    let cursor = {};
    let checked = 0;
    let chunksProcessed = 0;
    const mismatches = [];
    let lastProcessed = 0;
    try {
      while (true) {
        const rows = await this.fetchChunk(cursor, chunkSize);
        if (!rows.length) break;
        chunksProcessed += 1;
        for (const artifact of rows) {
          const now = Date.now();
          const delta = now - lastProcessed;
          if (delta < minIntervalMs) {
            await new Promise((resolve2) => setTimeout(resolve2, minIntervalMs - delta));
          }
          lastProcessed = Date.now();
          const mismatch = await this.verifyArtifact(artifact);
          checked += 1;
          if (mismatch) {
            mismatches.push(mismatch);
            if (emitIncidents) {
              await openIncident({
                runbook: "evidence-integrity",
                tenant: artifact.run_id || "global",
                severity: "CRITICAL",
                reason: mismatch.mismatchType,
                details: mismatch
              });
            }
          }
        }
        const last = rows[rows.length - 1];
        cursor = { createdAt: last.created_at, id: last.id };
      }
      const passed = checked - mismatches.length;
      const spanAttributes = {
        "evidence.integrity.checked": checked,
        "evidence.integrity.mismatches": mismatches.length,
        "evidence.integrity.chunks": chunksProcessed
      };
      if (typeof span?.addSpanAttributes === "function") {
        span.addSpanAttributes(spanAttributes);
      } else if (typeof span?.setAttribute === "function") {
        Object.entries(spanAttributes).forEach(([key, value]) => {
          span.setAttribute(key, value);
        });
      }
      return { checked, passed, mismatches, chunksProcessed };
    } catch (error) {
      this.logger.error({ err: error }, "Evidence integrity verification failed");
      throw error;
    } finally {
      span?.end();
    }
  }
  async fetchChunk(cursor, limit) {
    const params = [];
    let whereClause = "";
    if (cursor.createdAt && cursor.id) {
      params.push(cursor.createdAt, cursor.id);
      whereClause = "WHERE (created_at > $1 OR (created_at = $1 AND id > $2))";
    }
    params.push(limit);
    const { rows } = await this.pool.query(
      `SELECT id, run_id, artifact_type, sha256_hash, s3_key, created_at
       FROM evidence_artifacts
       ${whereClause}
       ORDER BY created_at, id
       LIMIT $${params.length}`,
      params
    );
    return rows;
  }
  async verifyArtifact(artifact) {
    const storagePath = this.resolveStoragePath(artifact.s3_key);
    const contentBuffer = await this.loadContent(artifact, storagePath);
    if (!contentBuffer) {
      return {
        artifactId: artifact.id,
        runId: artifact.run_id,
        artifactType: artifact.artifact_type,
        expectedHash: artifact.sha256_hash,
        computedHash: null,
        fileHash: null,
        storagePath,
        mismatchType: "missing_content",
        remediation: "Restore the artifact content from backup or regenerate it from the source run."
      };
    }
    const computedHash = crypto38.createHash("sha256").update(contentBuffer).digest("hex");
    if (computedHash !== artifact.sha256_hash) {
      return {
        artifactId: artifact.id,
        runId: artifact.run_id,
        artifactType: artifact.artifact_type,
        expectedHash: artifact.sha256_hash,
        computedHash,
        fileHash: computedHash,
        storagePath,
        mismatchType: "hash_mismatch",
        remediation: "Re-ingest the artifact and reissue provenance to correct the stored hash."
      };
    }
    return null;
  }
  resolveStoragePath(s3Key) {
    if (s3Key.startsWith(INLINE_PREFIX)) {
      return s3Key;
    }
    if (path34.isAbsolute(s3Key)) {
      return s3Key;
    }
    return path34.join(this.storageRoot, s3Key);
  }
  async loadContent(artifact, storagePath) {
    if (storagePath.startsWith(INLINE_PREFIX)) {
      const contentId = artifact.id;
      const { rows } = await this.pool.query(
        "SELECT content FROM evidence_artifact_content WHERE artifact_id = $1",
        [contentId]
      );
      if (!rows.length) return null;
      return rows[0].content;
    }
    try {
      return await fs30.readFile(storagePath);
    } catch (error) {
      this.logger.warn({ artifactId: artifact.id, storagePath, err: error }, "Failed to read artifact content");
      return null;
    }
  }
};
var evidenceIntegrityService = new EvidenceIntegrityService();

// src/policy/hotReloadService.ts
init_emit();

// src/services/security/AuditService.ts
init_audit();
var AuditService = class {
  /**
   * Logs a security or operational event to the audit trail.
   */
  static async log(event) {
    return writeAudit(event);
  }
};

// src/policy/hotReloadService.ts
init_bundleStore();
function assertHotReloadEnabled() {
  const enabled = (process.env.POLICY_HOT_RELOAD || "").toLowerCase() === "true";
  if (!enabled) {
    throw new Error("policy hot reload disabled");
  }
}
var PolicyHotReloadService = class {
  async recordRampChange(params) {
    const before = normalizeRampConfig(params.before);
    const after = normalizeRampConfig(params.after);
    if (areRampConfigsEqual(before, after)) return;
    const receiptService2 = ReceiptService.getInstance();
    const receipt = await receiptService2.generateReceipt({
      action: params.action,
      actor: { id: "system", tenantId: params.tenantId },
      resource: `policy_bundle:${params.versionId}`,
      input: {
        before,
        after,
        versionId: params.versionId,
        digest: params.digest,
        signatureVerified: params.signatureVerified
      }
    });
    await AuditService.log({
      userId: "system",
      action: params.action,
      resourceType: "policy_bundle",
      resourceId: params.versionId,
      details: {
        receiptId: receipt.id,
        digest: params.digest,
        signatureVerified: params.signatureVerified
      },
      before,
      after
    });
  }
  async reload(bundlePath, signaturePath) {
    assertHotReloadEnabled();
    const previous = policyBundleStore.getCurrent();
    const version = await loadPolicyBundleFromDisk(bundlePath, signaturePath);
    policyBundleStore.addVersion(version, true);
    await emitAuditEvent(
      {
        eventId: version.versionId,
        occurredAt: (/* @__PURE__ */ new Date()).toISOString(),
        tenantId: version.bundle.tenantId,
        actor: { id: "system", type: "service" },
        action: { type: "policy.reload", outcome: "success" },
        target: { type: "policy_bundle", id: version.versionId },
        metadata: {
          digest: version.digest,
          signatureVerified: version.signatureVerified,
          path: version.path
        }
      },
      { level: "critical", serviceId: "policy-hot-reload" }
    );
    await this.recordRampChange({
      action: "policy.ramp.updated",
      tenantId: version.bundle.tenantId,
      versionId: version.versionId,
      digest: version.digest,
      signatureVerified: version.signatureVerified,
      before: previous?.bundle?.baseProfile?.ramp,
      after: version.bundle.baseProfile?.ramp
    });
    return version;
  }
  async rollback(versionId) {
    assertHotReloadEnabled();
    const previous = policyBundleStore.getCurrent();
    const version = policyBundleStore.rollback(versionId);
    await emitAuditEvent(
      {
        eventId: `${version.versionId}-rollback`,
        occurredAt: (/* @__PURE__ */ new Date()).toISOString(),
        tenantId: version.bundle.tenantId,
        actor: { id: "system", type: "service" },
        action: { type: "policy.rollback", outcome: "success" },
        target: { type: "policy_bundle", id: version.versionId },
        metadata: { digest: version.digest }
      },
      { level: "critical", serviceId: "policy-hot-reload" }
    );
    await this.recordRampChange({
      action: "policy.ramp.rollback",
      tenantId: version.bundle.tenantId,
      versionId: version.versionId,
      digest: version.digest,
      signatureVerified: version.signatureVerified,
      before: previous?.bundle?.baseProfile?.ramp,
      after: version.bundle.baseProfile?.ramp
    });
    return version;
  }
};
var policyHotReloadService = new PolicyHotReloadService();

// src/routes/ops.ts
init_bundleStore();

// src/services/releaseReadinessService.ts
init_logger();
import fs31 from "fs/promises";
import path35 from "path";
var ReleaseReadinessService = class {
  repoRoot;
  cachedSummary;
  cachedEvidenceIndex;
  cacheExpiry = 5 * 60 * 1e3;
  // 5 minutes
  lastCacheTime = 0;
  constructor(repoRoot = process.cwd()) {
    this.repoRoot = repoRoot;
  }
  /**
   * Parse CONTROL_REGISTRY.md into structured control mappings
   */
  async parseControlRegistry() {
    const filePath = path35.join(this.repoRoot, "docs/compliance/CONTROL_REGISTRY.md");
    try {
      const content = await fs31.readFile(filePath, "utf-8");
      const controls = [];
      const lines = content.split("\n");
      let inTable = false;
      for (const line of lines) {
        if (line.includes("| Control ID | Control Name |")) {
          inTable = true;
          continue;
        }
        if (line.includes("| :---")) {
          continue;
        }
        if (inTable && line.startsWith("|") && !line.includes("**GOV-") && !line.includes("**SEC-") && !line.includes("**OPS-") && !line.includes("**RISK-") && !line.includes("**AI-")) {
          if (!line.trim().match(/\|\s*\*\*/)) {
            break;
          }
        }
        if (inTable && line.startsWith("|")) {
          const parts = line.split("|").map((p) => p.trim()).filter((p) => p);
          if (parts.length >= 5) {
            const idMatch = parts[0].match(/\*\*([A-Z]+-\d+)\*\*/);
            if (idMatch) {
              controls.push({
                id: idMatch[1],
                name: parts[1],
                description: parts[2],
                enforcementPoint: parts[3],
                evidenceArtifact: parts[4]
              });
            }
          }
        }
      }
      return controls;
    } catch (error) {
      logger_default.error("Failed to parse CONTROL_REGISTRY.md", error);
      throw error;
    }
  }
  /**
   * Parse EVIDENCE_INDEX.md into structured evidence items
   */
  async parseEvidenceIndex() {
    const filePath = path35.join(this.repoRoot, "docs/compliance/EVIDENCE_INDEX.md");
    try {
      const content = await fs31.readFile(filePath, "utf-8");
      const evidence = [];
      const lines = content.split("\n");
      let inTable = false;
      for (const line of lines) {
        if (line.includes("| Control ID | Control Name |")) {
          inTable = true;
          continue;
        }
        if (line.includes("| :---")) {
          continue;
        }
        if (inTable && line.startsWith("|")) {
          const parts = line.split("|").map((p) => p.trim()).filter((p) => p);
          if (parts.length >= 5) {
            const idMatch = parts[0].match(/\*\*([A-Z]+-\d+)\*\*/);
            if (idMatch) {
              evidence.push({
                controlId: idMatch[1],
                controlName: parts[1],
                evidenceType: parts[2],
                location: parts[3],
                verificationCommand: parts[4]
              });
            }
          }
        }
      }
      return evidence;
    } catch (error) {
      logger_default.error("Failed to parse EVIDENCE_INDEX.md", error);
      throw error;
    }
  }
  /**
   * Check if critical governance files exist
   */
  async checkGovernanceFiles() {
    const checks = [];
    const requiredFiles = [
      { path: "docs/governance/CONSTITUTION.md", name: "Constitutional Governance" },
      { path: "docs/compliance/CONTROL_REGISTRY.md", name: "Control Registry" },
      { path: "docs/compliance/EVIDENCE_INDEX.md", name: "Evidence Index" },
      { path: "SECURITY.md", name: "Security Policy" },
      { path: "CODE_OF_CONDUCT.md", name: "Code of Conduct" }
    ];
    for (const file of requiredFiles) {
      const filePath = path35.join(this.repoRoot, file.path);
      try {
        await fs31.access(filePath);
        checks.push({
          id: `file-${file.path.replace(/[\/\.]/g, "-")}`,
          name: file.name,
          status: "pass",
          lastRunAt: (/* @__PURE__ */ new Date()).toISOString(),
          evidenceLinks: [file.path]
        });
      } catch {
        checks.push({
          id: `file-${file.path.replace(/[\/\.]/g, "-")}`,
          name: file.name,
          status: "fail",
          lastRunAt: (/* @__PURE__ */ new Date()).toISOString(),
          evidenceLinks: [file.path]
        });
      }
    }
    return checks;
  }
  /**
   * Get release readiness summary (cached)
   */
  async getSummary() {
    const now = Date.now();
    if (this.cachedSummary && now - this.lastCacheTime < this.cacheExpiry) {
      return this.cachedSummary;
    }
    let versionOrCommit = "unknown";
    try {
      const { execSync: execSync4 } = await import("child_process");
      versionOrCommit = execSync4("git rev-parse --short HEAD", {
        cwd: this.repoRoot,
        encoding: "utf-8"
      }).trim();
    } catch {
      versionOrCommit = "no-git";
    }
    const checks = await this.checkGovernanceFiles();
    const failCount = checks.filter((c) => c.status === "fail").length;
    const warnCount = checks.filter((c) => c.status === "warn").length;
    this.cachedSummary = {
      generatedAt: (/* @__PURE__ */ new Date()).toISOString(),
      versionOrCommit,
      checks
    };
    this.lastCacheTime = now;
    return this.cachedSummary;
  }
  /**
   * Get evidence index (cached)
   */
  async getEvidenceIndex() {
    const now = Date.now();
    if (this.cachedEvidenceIndex && now - this.lastCacheTime < this.cacheExpiry) {
      return this.cachedEvidenceIndex;
    }
    const [controls, evidence] = await Promise.all([
      this.parseControlRegistry(),
      this.parseEvidenceIndex()
    ]);
    this.cachedEvidenceIndex = { controls, evidence };
    this.lastCacheTime = now;
    return this.cachedEvidenceIndex;
  }
  /**
   * Clear cache (useful for testing)
   */
  clearCache() {
    this.cachedSummary = void 0;
    this.cachedEvidenceIndex = void 0;
    this.lastCacheTime = 0;
  }
};
var releaseReadinessService = new ReleaseReadinessService();

// src/services/SystemHealthService.ts
init_logger();
import { existsSync as existsSync6 } from "fs";
import { join as join7 } from "path";
var killSwitchEnabled = false;
var safeModeEnabled = false;
var backpressureState = {
  currentConcurrency: 0,
  maxConcurrency: parseInt(process.env.MAX_CONCURRENCY || "100", 10),
  queueDepth: 0,
  maxQueueDepth: parseInt(process.env.MAX_QUEUE_DEPTH || "1000", 10)
};
var SystemHealthServiceImpl = class {
  killSwitchReason;
  killSwitchEnabledAt;
  safeModeReason;
  safeModeEnabledAt;
  /**
   * Get current system health status
   */
  async getStatus() {
    const sloResultsPath = join7(process.cwd(), "dist", "slo-results.json");
    return {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      killSwitch: {
        enabled: killSwitchEnabled,
        reason: this.killSwitchReason,
        enabledAt: this.killSwitchEnabledAt?.toISOString()
      },
      safeMode: {
        enabled: safeModeEnabled,
        reason: this.safeModeReason,
        enabledAt: this.safeModeEnabledAt?.toISOString()
      },
      backpressure: {
        ...backpressureState,
        isBackpressured: this.isBackpressured()
      },
      sloResults: {
        available: existsSync6(sloResultsPath),
        path: sloResultsPath
      },
      recentPolicyDenials: await this.countRecentPolicyDenials()
    };
  }
  /**
   * Enable kill-switch (stops all non-essential operations)
   */
  enableKillSwitch(reason) {
    killSwitchEnabled = true;
    this.killSwitchReason = reason;
    this.killSwitchEnabledAt = /* @__PURE__ */ new Date();
    logger_default.warn("Kill-switch ENABLED", { reason });
  }
  /**
   * Disable kill-switch
   */
  disableKillSwitch() {
    killSwitchEnabled = false;
    this.killSwitchReason = void 0;
    this.killSwitchEnabledAt = void 0;
    logger_default.info("Kill-switch DISABLED");
  }
  /**
   * Enable safe-mode (reduced functionality)
   */
  enableSafeMode(reason) {
    safeModeEnabled = true;
    this.safeModeReason = reason;
    this.safeModeEnabledAt = /* @__PURE__ */ new Date();
    logger_default.warn("Safe-mode ENABLED", { reason });
  }
  /**
   * Disable safe-mode
   */
  disableSafeMode() {
    safeModeEnabled = false;
    this.safeModeReason = void 0;
    this.safeModeEnabledAt = void 0;
    logger_default.info("Safe-mode DISABLED");
  }
  /**
   * Check if kill-switch is active
   */
  isKillSwitchEnabled() {
    return killSwitchEnabled;
  }
  /**
   * Check if safe-mode is active
   */
  isSafeModeEnabled() {
    return safeModeEnabled;
  }
  /**
   * Check if system is under backpressure
   */
  isBackpressured() {
    return backpressureState.currentConcurrency >= backpressureState.maxConcurrency * 0.9 || backpressureState.queueDepth >= backpressureState.maxQueueDepth * 0.9;
  }
  /**
   * Update backpressure metrics
   */
  updateBackpressure(concurrency, queueDepth) {
    backpressureState.currentConcurrency = concurrency;
    backpressureState.queueDepth = queueDepth;
  }
  /**
   * Simulate a policy decision (for testing/validation)
   */
  async simulatePolicy(input) {
    const startTime = Date.now();
    const { action, resource, subject } = input;
    if (subject.roles.includes("admin") || subject.roles.includes("ADMIN")) {
      return {
        allowed: true,
        reason: "Admin role has full access",
        matchedPolicy: "admin-override",
        evaluationTimeMs: Date.now() - startTime
      };
    }
    if (action === "read" && subject.roles.includes("viewer")) {
      return {
        allowed: true,
        reason: "Viewer role has read access",
        matchedPolicy: "viewer-read",
        evaluationTimeMs: Date.now() - startTime
      };
    }
    if (["create", "update", "delete"].includes(action) && subject.roles.includes("editor")) {
      return {
        allowed: true,
        reason: "Editor role has write access",
        matchedPolicy: "editor-write",
        evaluationTimeMs: Date.now() - startTime
      };
    }
    return {
      allowed: false,
      reason: `No policy grants ${action} on ${resource} for roles: ${subject.roles.join(", ")}`,
      evaluationTimeMs: Date.now() - startTime
    };
  }
  /**
   * Count recent policy denials from audit logs
   */
  async countRecentPolicyDenials() {
    try {
      return 0;
    } catch (error) {
      logger_default.error("Failed to count policy denials", { error });
      return -1;
    }
  }
};
var systemHealthService = new SystemHealthServiceImpl();

// src/routes/ops.ts
var router70 = Router40();
var backupService2 = new BackupService();
var drService = new DisasterRecoveryService();
var isHotReloadEnabled = () => (process.env.POLICY_HOT_RELOAD || "").toLowerCase() === "true";
router70.use(ensureAuthenticated);
router70.post("/maintenance", ensureRole(["ADMIN", "admin"]), async (req, res) => {
  try {
    runMaintenance().catch((err) => logger_default.error("Async maintenance failed", err));
    res.json({ message: "Maintenance task started" });
  } catch (error) {
    logger_default.error("Failed to trigger maintenance", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
router70.post(
  "/ops/policy/reload",
  ensureRole(["ADMIN", "admin"]),
  async (req, res) => {
    if (!isHotReloadEnabled()) {
      return res.status(403).json({ error: "POLICY_HOT_RELOAD is disabled" });
    }
    const { bundlePath, signaturePath } = req.body || {};
    if (!bundlePath) {
      return res.status(400).json({ error: "bundlePath is required" });
    }
    try {
      const version = await policyHotReloadService.reload(bundlePath, signaturePath);
      res.json({
        ok: true,
        currentPolicyVersionId: policyBundleStore.currentPolicyVersionId,
        version
      });
    } catch (error) {
      logger_default.error("Failed to reload policy bundle", error);
      res.status(500).json({ error: error?.message || "reload failed" });
    }
  }
);
router70.post(
  "/ops/policy/rollback",
  ensureRole(["ADMIN", "admin"]),
  async (req, res) => {
    if (!isHotReloadEnabled()) {
      return res.status(403).json({ error: "POLICY_HOT_RELOAD is disabled" });
    }
    const toVersion = req.query.toVersion || req.body?.toVersion;
    if (!toVersion) {
      return res.status(400).json({ error: "toVersion is required" });
    }
    try {
      const version = await policyHotReloadService.rollback(toVersion);
      res.json({
        ok: true,
        currentPolicyVersionId: policyBundleStore.currentPolicyVersionId,
        version
      });
    } catch (error) {
      const status = /not found/.test(error?.message || "") ? 404 : 500;
      logger_default.error("Failed to rollback policy bundle", error);
      res.status(status).json({ error: error?.message || "rollback failed" });
    }
  }
);
router70.post("/backup/:type", ensureRole(["ADMIN", "admin"]), async (req, res) => {
  const { type } = req.params;
  const { uploadToS3 } = req.body;
  try {
    let result2;
    switch (type) {
      case "postgres":
        result2 = await backupService2.backupPostgres({ uploadToS3 });
        break;
      case "neo4j":
        result2 = await backupService2.backupNeo4j({ uploadToS3 });
        break;
      case "redis":
        result2 = await backupService2.backupRedis({ uploadToS3 });
        break;
      default:
        return res.status(400).json({ error: "Invalid backup type. Use postgres, neo4j, or redis." });
    }
    res.json({ message: "Backup completed", path: result2 });
  } catch (error) {
    logger_default.error(`Failed to backup ${type}`, error);
    res.status(500).json({ error: `Backup failed: ${error.message}` });
  }
});
router70.post("/dr/drill", ensureRole(["ADMIN", "admin"]), async (req, res) => {
  const { target } = req.body;
  if (target && target !== "postgres" && target !== "neo4j") {
    return res.status(400).json({ error: "Invalid target. Use postgres or neo4j." });
  }
  try {
    const success = await drService.runDrill(target || "postgres");
    if (success) {
      res.json({ message: "DR Drill completed successfully" });
    } else {
      res.status(500).json({ error: "DR Drill failed" });
    }
  } catch (error) {
    logger_default.error("DR drill error", error);
    res.status(500).json({ error: "DR Drill execution error" });
  }
});
router70.get("/dr/status", ensureRole(["ADMIN", "admin", "OPERATOR"]), async (req, res) => {
  try {
    const status = await drService.getStatus();
    res.json(status);
  } catch (error) {
    res.status(500).json({ error: "Failed to get DR status" });
  }
});
router70.post("/evidence/verify", ensureRole(["ADMIN", "admin"]), async (req, res) => {
  if (process.env.EVIDENCE_INTEGRITY !== "true") {
    return res.status(503).json({ ok: false, error: "Evidence integrity verification is disabled" });
  }
  const chunkSize = Number(req.body?.chunkSize ?? process.env.EVIDENCE_INTEGRITY_CHUNK ?? 50);
  const rateLimitPerSecond = Number(req.body?.rateLimitPerSecond ?? process.env.EVIDENCE_INTEGRITY_RPS ?? 5);
  const emitIncidents = req.body?.emitIncidents ?? process.env.EVIDENCE_INTEGRITY_INCIDENTS === "true";
  try {
    const result2 = await evidenceIntegrityService.verifyAll({
      chunkSize: Number.isFinite(chunkSize) ? chunkSize : void 0,
      rateLimitPerSecond: Number.isFinite(rateLimitPerSecond) ? rateLimitPerSecond : void 0,
      emitIncidents
    });
    return res.json({ ok: true, ...result2 });
  } catch (error) {
    logger_default.error("Failed to run evidence integrity verification", error);
    return res.status(500).json({ ok: false, error: "Failed to verify evidence integrity" });
  }
});
router70.get(
  "/release-readiness/summary",
  ensureRole(["ADMIN", "OPERATOR"]),
  async (req, res) => {
    try {
      const summary = await releaseReadinessService.getSummary();
      res.json(summary);
    } catch (error) {
      logger_default.error("Failed to get release readiness summary", error);
      res.status(500).json({ error: "Failed to retrieve release readiness summary" });
    }
  }
);
router70.get(
  "/release-readiness/evidence-index",
  ensureRole(["ADMIN", "OPERATOR"]),
  async (req, res) => {
    try {
      const evidenceIndex = await releaseReadinessService.getEvidenceIndex();
      res.json(evidenceIndex);
    } catch (error) {
      logger_default.error("Failed to get evidence index", error);
      res.status(500).json({ error: "Failed to retrieve evidence index" });
    }
  }
);
router70.get("/system-health", ensureRole(["ADMIN", "admin", "OPERATOR"]), async (req, res) => {
  try {
    const status = await systemHealthService.getStatus();
    res.json(status);
  } catch (error) {
    logger_default.error("Failed to get system health status", error);
    res.status(500).json({ error: "Failed to retrieve system health status" });
  }
});
router70.post(
  "/system-health/kill-switch",
  ensureRole(["ADMIN", "admin"]),
  async (req, res) => {
    const { enabled, reason } = req.body;
    if (typeof enabled !== "boolean") {
      return res.status(400).json({ error: "enabled (boolean) is required" });
    }
    if (enabled && !reason) {
      return res.status(400).json({ error: "reason is required when enabling kill-switch" });
    }
    try {
      if (enabled) {
        systemHealthService.enableKillSwitch(reason);
      } else {
        systemHealthService.disableKillSwitch();
      }
      res.json({ ok: true, killSwitchEnabled: systemHealthService.isKillSwitchEnabled() });
    } catch (error) {
      logger_default.error("Failed to toggle kill-switch", error);
      res.status(500).json({ error: "Failed to toggle kill-switch" });
    }
  }
);
router70.post(
  "/system-health/safe-mode",
  ensureRole(["ADMIN", "admin"]),
  async (req, res) => {
    const { enabled, reason } = req.body;
    if (typeof enabled !== "boolean") {
      return res.status(400).json({ error: "enabled (boolean) is required" });
    }
    if (enabled && !reason) {
      return res.status(400).json({ error: "reason is required when enabling safe-mode" });
    }
    try {
      if (enabled) {
        systemHealthService.enableSafeMode(reason);
      } else {
        systemHealthService.disableSafeMode();
      }
      res.json({ ok: true, safeModeEnabled: systemHealthService.isSafeModeEnabled() });
    } catch (error) {
      logger_default.error("Failed to toggle safe-mode", error);
      res.status(500).json({ error: "Failed to toggle safe-mode" });
    }
  }
);
router70.post(
  "/policy-simulator",
  ensureRole(["ADMIN", "admin", "OPERATOR"]),
  async (req, res) => {
    const { action, resource, subject, context: context4 } = req.body;
    if (!action || !resource || !subject) {
      return res.status(400).json({
        error: "action, resource, and subject are required"
      });
    }
    if (!subject.id || !Array.isArray(subject.roles)) {
      return res.status(400).json({
        error: "subject must have id (string) and roles (array)"
      });
    }
    try {
      const result2 = await systemHealthService.simulatePolicy({
        action,
        resource,
        subject,
        context: context4
      });
      res.json(result2);
    } catch (error) {
      logger_default.error("Policy simulation failed", error);
      res.status(500).json({ error: "Policy simulation failed" });
    }
  }
);
var ops_default = router70;

// src/routes/feature-flags.ts
init_setup();
init_postgres();
init_logger2();
init_auth4();
import express35 from "express";
var router71 = express35.Router();
var ensureAdmin = [ensureAuthenticated, ensureRole(["admin"])];
router71.get("/", ensureAuthenticated, async (req, res) => {
  try {
    const service11 = getFeatureFlagService();
    const flags = await service11.listFlags();
    res.json(flags);
  } catch (error) {
    logger_default2.error("Error fetching feature flags", error);
    res.status(500).json({ error: "Failed to fetch feature flags" });
  }
});
router71.get("/:key", ensureAuthenticated, async (req, res) => {
  try {
    const service11 = getFeatureFlagService();
    const flag = await service11.getFlagDefinition(req.params.key);
    if (!flag) {
      return res.status(404).json({ error: "Flag not found" });
    }
    res.json(flag);
  } catch (error) {
    logger_default2.error(`Error fetching flag ${req.params.key}`, error);
    res.status(500).json({ error: "Failed to fetch flag" });
  }
});
router71.post("/", ensureAdmin, async (req, res) => {
  const { key, description, type, enabled, defaultValue, variations, rules, tenantId } = req.body;
  if (!key || !type) {
    return res.status(400).json({ error: "Key and type are required" });
  }
  const pool4 = getPostgresPool();
  try {
    await pool4.query(
      `INSERT INTO feature_flags (key, description, type, enabled, default_value, variations, rollout_rules, tenant_id, updated_at)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8, NOW())
       ON CONFLICT (key) DO UPDATE SET
         description = EXCLUDED.description,
         type = EXCLUDED.type,
         enabled = EXCLUDED.enabled,
         default_value = EXCLUDED.default_value,
         variations = EXCLUDED.variations,
         rollout_rules = EXCLUDED.rollout_rules,
         tenant_id = EXCLUDED.tenant_id,
         updated_at = NOW()
       RETURNING *`,
      [key, description, type, enabled, defaultValue, JSON.stringify(variations || []), JSON.stringify(rules || []), tenantId]
    );
    if (process.env.REDIS_URL) {
      const Redis17 = (await import("ioredis")).default;
      const pubsub2 = new Redis17(process.env.REDIS_URL);
      await pubsub2.publish("feature_flag_updates", JSON.stringify({ key, action: "update" }));
      pubsub2.quit();
    }
    res.json({ success: true, key });
  } catch (error) {
    logger_default2.error("Error upserting feature flag", error);
    res.status(500).json({ error: "Failed to upsert feature flag" });
  }
});
router71.delete("/:key", ensureAdmin, async (req, res) => {
  const { key } = req.params;
  const pool4 = getPostgresPool();
  try {
    await pool4.query("DELETE FROM feature_flags WHERE key = $1", [key]);
    if (process.env.REDIS_URL) {
      const Redis17 = (await import("ioredis")).default;
      const pubsub2 = new Redis17(process.env.REDIS_URL);
      await pubsub2.publish("feature_flag_updates", JSON.stringify({ key, action: "delete" }));
      pubsub2.quit();
    }
    res.json({ success: true });
  } catch (error) {
    logger_default2.error("Error deleting feature flag", error);
    res.status(500).json({ error: "Failed to delete feature flag" });
  }
});
router71.post("/evaluate", async (req, res) => {
  try {
    const context4 = req.body.context || {};
    if (req.user) {
      context4.userId = req.user.id || req.user.sub;
      context4.userEmail = req.user.email;
      context4.userRole = req.user.role;
      context4.tenantId = req.user.tenantId || req.user.tenant_id;
    }
    const service11 = getFeatureFlagService();
    const flags = await service11.getAllFlags(context4);
    res.json(flags);
  } catch (error) {
    logger_default2.error("Error evaluating flags", error);
    res.status(500).json({ error: "Failed to evaluate flags" });
  }
});
var feature_flags_default = router71;

// src/routes/ml_review.ts
import { Router as Router41 } from "express";

// src/services/ReviewQueueService.ts
init_postgres();
var ReviewQueueService = class _ReviewQueueService {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_ReviewQueueService.instance) {
      _ReviewQueueService.instance = new _ReviewQueueService();
    }
    return _ReviewQueueService.instance;
  }
  async createQueue(tenantId, name, config9 = {}) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query(
      `INSERT INTO ml_review_queues (tenant_id, name, priority_config)
             VALUES ($1, $2, $3)
             RETURNING *`,
      [tenantId, name, config9]
    );
    return this.mapQueueRow(result2.rows[0]);
  }
  async updateQueue(queueId, tenantId, updates) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query(
      `UPDATE ml_review_queues
             SET name = COALESCE($1, name), priority_config = COALESCE($2, priority_config), updated_at = NOW()
             WHERE id = $3 AND tenant_id = $4
             RETURNING *`,
      [updates.name ?? null, updates.priorityConfig ?? null, queueId, tenantId]
    );
    if (result2.rowCount === 0) throw new Error("Queue not found");
    return this.mapQueueRow(result2.rows[0]);
  }
  async deleteQueue(queueId, tenantId) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query(
      `DELETE FROM ml_review_queues WHERE id = $1 AND tenant_id = $2`,
      [queueId, tenantId]
    );
    if (result2.rowCount === 0) throw new Error("Queue not found");
  }
  async listQueues(tenantId) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query(
      `SELECT * FROM ml_review_queues WHERE tenant_id = $1 ORDER BY created_at DESC`,
      [tenantId]
    );
    return result2.rows.map((row) => this.mapQueueRow(row));
  }
  async enqueueItem(queueId, tenantId, data, confidence) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query(
      `INSERT INTO ml_review_items (queue_id, tenant_id, data, confidence, status)
             VALUES ($1, $2, $3, $4, 'PENDING')
             RETURNING *`,
      [queueId, tenantId, data, confidence]
    );
    const row = result2.rows[0];
    return this.mapItemRow(row);
  }
  /**
   * Gets a batch of items for review using priority sampling.
   * Priority = (1 - confidence) * 0.8 + min(age / 120, 0.2)
   */
  async getItemsForReview(queueId, tenantId, limit = 50) {
    const pool4 = getPostgresPool();
    const candidateLimit = limit * 5;
    const result2 = await pool4.query(
      `SELECT *, EXTRACT(EPOCH FROM (NOW() - created_at)) / 60 as age_minutes
             FROM ml_review_items
             WHERE queue_id = $1 AND tenant_id = $2 AND status = 'PENDING'
             ORDER BY confidence ASC
             LIMIT $3`,
      [queueId, tenantId, candidateLimit]
    );
    const items = result2.rows.map((row) => this.mapItemRow(row));
    return this.sampleItems(items, limit);
  }
  sampleItems(items, k) {
    if (items.length <= k) return items;
    const scores = items.map((item) => {
      const ageMinutes = item.ageMinutes || 0;
      return (1 - item.confidence) * 0.8 + Math.min(ageMinutes / 120, 0.2);
    });
    const totalScore = scores.reduce((a, b) => a + b, 0);
    const probs = totalScore > 0 ? scores.map((s) => s / totalScore) : items.map(() => 1 / items.length);
    const sampled = [];
    const indices = Array.from({ length: items.length }, (_2, i) => i);
    const selectedIndices = /* @__PURE__ */ new Set();
    while (selectedIndices.size < k && selectedIndices.size < items.length) {
      let r = Math.random();
      let cumulative = 0;
      let selected = -1;
      let currentTotal = 0;
      for (let i = 0; i < indices.length; i++) {
        if (!selectedIndices.has(i)) {
          currentTotal += scores[i];
        }
      }
      if (currentTotal === 0) {
        const remaining = indices.filter((i) => !selectedIndices.has(i));
        selected = remaining[Math.floor(Math.random() * remaining.length)];
      } else {
        for (let i = 0; i < indices.length; i++) {
          if (selectedIndices.has(i)) continue;
          const normalizedProb = scores[i] / currentTotal;
          cumulative += normalizedProb;
          if (r <= cumulative) {
            selected = i;
            break;
          }
        }
      }
      if (selected !== -1) {
        selectedIndices.add(selected);
      } else {
        const remaining = indices.filter((i) => !selectedIndices.has(i));
        if (remaining.length > 0) {
          selectedIndices.add(remaining[0]);
        } else {
          break;
        }
      }
    }
    return Array.from(selectedIndices).map((idx) => items[idx]);
  }
  async submitDecision(itemId, tenantId, reviewerId, decision, metadata = {}) {
    const pool4 = getPostgresPool();
    const status = decision === "ABSTAIN" ? "ABSTAINED" : decision;
    await pool4.withTransaction(async (client6) => {
      const updateResult = await client6.query(
        `UPDATE ml_review_items
                 SET status = $1, reviewed_at = NOW(), reviewer_id = $2
                 WHERE id = $3 AND tenant_id = $4`,
        [status, reviewerId, itemId, tenantId]
      );
      if (updateResult.rowCount === 0) {
        throw new Error("Item not found or already reviewed");
      }
      await client6.query(
        `INSERT INTO ml_review_decisions (item_id, tenant_id, reviewer_id, decision, metadata)
                 VALUES ($1, $2, $3, $4, $5)`,
        [itemId, tenantId, reviewerId, decision, metadata]
      );
    });
  }
  async submitBatchDecisions(decisions, tenantId, reviewerId) {
    if (decisions.length === 0) return;
    const pool4 = getPostgresPool();
    await pool4.withTransaction(async (client6) => {
      for (const d of decisions) {
        const status = d.decision === "ABSTAIN" ? "ABSTAINED" : d.decision;
        const updateResult = await client6.query(
          `UPDATE ml_review_items
                     SET status = $1, reviewed_at = NOW(), reviewer_id = $2
                     WHERE id = $3 AND tenant_id = $4`,
          [status, reviewerId, d.itemId, tenantId]
        );
        if (updateResult.rowCount === 0) {
          continue;
        }
        await client6.query(
          `INSERT INTO ml_review_decisions (item_id, tenant_id, reviewer_id, decision, metadata)
                     VALUES ($1, $2, $3, $4, $5)`,
          [d.itemId, tenantId, reviewerId, d.decision, d.metadata || {}]
        );
      }
    });
  }
  async getQueueStats(queueId, tenantId) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query(
      `SELECT status, COUNT(*) as count
             FROM ml_review_items
             WHERE queue_id = $1 AND tenant_id = $2
             GROUP BY status`,
      [queueId, tenantId]
    );
    return result2.rows;
  }
  mapItemRow(row) {
    return {
      id: row.id,
      queueId: row.queue_id,
      tenantId: row.tenant_id,
      data: row.data,
      confidence: parseFloat(row.confidence),
      status: row.status,
      createdAt: row.created_at,
      reviewedAt: row.reviewed_at,
      reviewerId: row.reviewer_id,
      ageMinutes: row.age_minutes ? parseFloat(row.age_minutes) : void 0
    };
  }
  mapQueueRow(row) {
    return {
      id: row.id,
      tenantId: row.tenant_id,
      name: row.name,
      priorityConfig: row.priority_config,
      createdAt: row.created_at
    };
  }
};
var reviewQueueService = ReviewQueueService.getInstance();

// src/routes/ml_review.ts
init_auth4();
init_errors();
import { z as z33 } from "zod";
var router72 = Router41();
var singleParam15 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var CreateQueueSchema = z33.object({
  name: z33.string().min(1),
  priorityConfig: z33.record(z33.any()).optional()
});
var UpdateQueueSchema = z33.object({
  name: z33.string().min(1).optional(),
  priorityConfig: z33.record(z33.any()).optional()
});
var EnqueueItemSchema = z33.object({
  data: z33.any(),
  confidence: z33.number().min(0).max(1)
});
var SubmitDecisionSchema = z33.object({
  decision: z33.enum(["ACCEPTED", "REJECTED", "ABSTAIN"]),
  metadata: z33.record(z33.any()).optional()
});
var BatchDecisionsSchema = z33.object({
  decisions: z33.array(z33.object({
    itemId: z33.string().uuid(),
    decision: z33.enum(["ACCEPTED", "REJECTED", "ABSTAIN"]),
    metadata: z33.record(z33.any()).optional()
  })).min(1).max(100)
});
var ensureTenant2 = (req, res, next) => {
  if (!req.user || !req.user.tenantId) {
    return next(new AppError("Tenant context missing", 403));
  }
  next();
};
router72.post("/queues", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const { name, priorityConfig } = CreateQueueSchema.parse(req.body);
    const queue = await reviewQueueService.createQueue(
      req.user.tenantId,
      name,
      priorityConfig
    );
    res.status(201).json(queue);
  } catch (e) {
    next(e);
  }
});
router72.get("/queues", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const queues = await reviewQueueService.listQueues(req.user.tenantId);
    res.json(queues);
  } catch (e) {
    next(e);
  }
});
router72.put("/queues/:queueId", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const queueId = singleParam15(req.params.queueId) ?? "";
    const updates = UpdateQueueSchema.parse(req.body);
    const queue = await reviewQueueService.updateQueue(
      queueId,
      req.user.tenantId,
      updates
    );
    res.json(queue);
  } catch (e) {
    next(e);
  }
});
router72.delete("/queues/:queueId", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const queueId = singleParam15(req.params.queueId) ?? "";
    await reviewQueueService.deleteQueue(queueId, req.user.tenantId);
    res.status(204).send();
  } catch (e) {
    next(e);
  }
});
router72.post("/queues/:queueId/items", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const queueId = singleParam15(req.params.queueId) ?? "";
    const { data, confidence } = EnqueueItemSchema.parse(req.body);
    const item = await reviewQueueService.enqueueItem(
      queueId,
      req.user.tenantId,
      data,
      confidence
    );
    res.status(201).json(item);
  } catch (e) {
    next(e);
  }
});
router72.get("/queues/:queueId/items", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const queueId = singleParam15(req.params.queueId) ?? "";
    let limit = req.query.limit ? parseInt(req.query.limit) : 10;
    if (isNaN(limit) || limit < 1) limit = 10;
    if (limit > 100) limit = 100;
    const items = await reviewQueueService.getItemsForReview(
      queueId,
      req.user.tenantId,
      limit
    );
    res.json(items);
  } catch (e) {
    next(e);
  }
});
router72.post("/items/:itemId/decision", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const itemId = singleParam15(req.params.itemId) ?? "";
    const userId = req.user.id;
    const { decision, metadata } = SubmitDecisionSchema.parse(req.body);
    await reviewQueueService.submitDecision(
      itemId,
      req.user.tenantId,
      userId,
      decision,
      metadata
    );
    res.status(200).json({ success: true });
  } catch (e) {
    next(e);
  }
});
router72.post("/items/batch-decision", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const { decisions } = BatchDecisionsSchema.parse(req.body);
    await reviewQueueService.submitBatchDecisions(
      decisions,
      req.user.tenantId,
      req.user.id
    );
    res.status(200).json({ success: true, count: decisions.length });
  } catch (e) {
    next(e);
  }
});
router72.get("/queues/:queueId/stats", ensureAuthenticated, ensureTenant2, async (req, res, next) => {
  try {
    const queueId = singleParam15(req.params.queueId) ?? "";
    const stats = await reviewQueueService.getQueueStats(
      queueId,
      req.user.tenantId
    );
    res.json(stats);
  } catch (e) {
    next(e);
  }
});
var ml_review_default = router72;

// src/routes/admin-flags.ts
init_auth4();
import express36 from "express";

// src/services/FlagService.ts
init_audit2();
init_logger2();
var FlagService = class _FlagService {
  static instance;
  cache = /* @__PURE__ */ new Map();
  constructor() {
  }
  static getInstance() {
    if (!_FlagService.instance) {
      _FlagService.instance = new _FlagService();
    }
    return _FlagService.instance;
  }
  /**
   * Gets the current value of a feature flag/kill switch.
   * Priority:
   * 1. In-memory override (set via API)
   * 2. Environment variable (FLAG_NAME)
   * 3. Default false
   */
  getFlag(name) {
    const cached = this.cache.get(name);
    if (cached) {
      return cached.value;
    }
    const envKey = `FLAG_${name.toUpperCase()}`;
    const envValue = process.env[envKey];
    if (envValue !== void 0) {
      if (envValue.toLowerCase() === "true") return true;
      if (envValue.toLowerCase() === "false") return false;
      const num = Number(envValue);
      if (!isNaN(num)) return num;
      return envValue;
    }
    return false;
  }
  /**
   * Sets an in-memory override for a flag.
   * "Break glass" mechanism - takes effect immediately.
   */
  async setFlag(name, value, userId = "system", tenantId = "system") {
    this.cache.set(name, {
      value,
      updatedAt: Date.now()
    });
    logger_default2.warn({ name, value, userId }, "Flag override set (Kill Switch / Break Glass)");
    try {
      await advancedAuditSystem.recordEvent({
        eventType: "system_modification",
        action: "update_flag",
        outcome: "success",
        userId,
        tenantId,
        serviceId: "flag-service",
        resourceType: "feature_flag",
        resourceId: name,
        message: `Flag ${name} set to ${value}`,
        level: "warn",
        details: { name, value, timestamp: Date.now() },
        complianceRelevant: true,
        complianceFrameworks: ["SOC2"]
        // Operations change
      });
    } catch (err) {
      logger_default2.error({ err }, "Failed to emit audit event for flag update");
    }
  }
  /**
   * Clears an override, reverting to ENV or default.
   */
  async clearFlag(name, userId = "system", tenantId = "system") {
    this.cache.delete(name);
    logger_default2.info({ name, userId }, "Flag override cleared");
    try {
      await advancedAuditSystem.recordEvent({
        eventType: "system_modification",
        action: "clear_flag",
        outcome: "success",
        userId,
        tenantId,
        serviceId: "flag-service",
        resourceType: "feature_flag",
        resourceId: name,
        message: `Flag ${name} override cleared`,
        level: "info",
        details: { name, timestamp: Date.now() },
        complianceRelevant: true,
        complianceFrameworks: ["SOC2"]
      });
    } catch (err) {
      logger_default2.error({ err }, "Failed to emit audit event for flag clear");
    }
  }
};
var flagService = FlagService.getInstance();

// src/controllers/FlagController.ts
init_logger2();
var setFlagHandler = async (req, res) => {
  try {
    const { name, value, ttlSeconds } = req.body;
    if (!name || value === void 0) {
      return res.status(400).json({ error: "name and value are required" });
    }
    const userId = req.user?.id || "unknown";
    const tenantId = req.user?.tenantId || "system";
    await flagService.setFlag(name, value, userId, tenantId);
    if (ttlSeconds && typeof ttlSeconds === "number") {
      setTimeout(() => {
        flagService.clearFlag(name, "system-ttl", tenantId);
      }, ttlSeconds * 1e3);
    }
    res.json({ success: true, name, value, message: "Flag set successfully" });
  } catch (error) {
    logger_default2.error("Error setting flag", error);
    res.status(500).json({ error: "Failed to set flag" });
  }
};
var getFlagHandler = async (req, res) => {
  const val = flagService.getFlag(req.params.name);
  res.json({ name: req.params.name, value: val });
};

// src/routes/admin-flags.ts
var router73 = express36.Router();
var ensureAdmin2 = [ensureAuthenticated, ensureRole(["admin"])];
router73.post("/set", ensureAdmin2, setFlagHandler);
router73.get("/:name", ensureAdmin2, getFlagHandler);
var admin_flags_default = router73;

// src/routes/audit-events.ts
init_audit2();
init_logger();
import { Router as Router42 } from "express";
var routeLogger4 = logger_default.child({ name: "AuditEventsRoutes" });
var router74 = Router42();
function getTenantId(req) {
  return String(
    req.headers["x-tenant-id"] || req.headers["x-tenant"] || ""
  ) || null;
}
function parseList(value) {
  if (!value) return void 0;
  const raw = Array.isArray(value) ? value.join(",") : value;
  const list = raw.split(",").map((item) => item.trim()).filter(Boolean);
  return list.length > 0 ? list : void 0;
}
router74.get("/audit-events", async (req, res) => {
  try {
    const tenantId = getTenantId(req);
    if (!tenantId) {
      return res.status(400).json({ error: "tenant_required" });
    }
    const query3 = {
      startTime: req.query.startTime ? new Date(req.query.startTime) : void 0,
      endTime: req.query.endTime ? new Date(req.query.endTime) : void 0,
      eventTypes: parseList(req.query.eventTypes),
      levels: parseList(req.query.levels),
      userIds: parseList(req.query.userIds),
      resourceTypes: parseList(req.query.resourceTypes),
      correlationIds: parseList(req.query.correlationIds),
      tenantIds: [tenantId],
      limit: req.query.limit ? Number(req.query.limit) : 100,
      offset: req.query.offset ? Number(req.query.offset) : 0
    };
    const events = await advancedAuditSystem.queryEvents(query3);
    routeLogger4.info(
      { tenantId, returnedCount: events.length },
      "Audit events retrieved"
    );
    res.json({
      tenantId,
      returnedCount: events.length,
      offset: query3.offset,
      limit: query3.limit,
      events
    });
  } catch (error) {
    routeLogger4.error(
      { error: error.message },
      "Failed to query audit events"
    );
    res.status(500).json({ error: error.message });
  }
});
var audit_events_default = router74;

// src/services/brand-packs/brand-pack.routes.ts
init_auth4();
init_emit();
import { randomUUID as randomUUID53 } from "crypto";
import express37 from "express";
import { z as z35 } from "zod";

// src/provenance/brand-pack-receipt.ts
async function emitBrandPackReceipt(input) {
  const receiptService2 = ReceiptService.getInstance();
  return receiptService2.generateReceipt({
    action: "brand-pack.apply",
    actor: { id: input.actorId, tenantId: input.tenantId },
    resource: `brand-pack:${input.packId}`,
    input: {
      tenantId: input.tenantId,
      packId: input.packId,
      appliedAt: input.appliedAt
    }
  });
}

// src/services/brand-packs/brand-pack.service.ts
init_CacheService();

// src/services/brand-packs/brand-pack.loader.ts
import { promises as fs32 } from "fs";
import path36 from "path";

// src/services/brand-packs/brand-pack.schema.ts
import { z as z34 } from "zod";
var BrandPackPaletteSchema = z34.object({
  mode: z34.enum(["light", "dark"]).default("dark"),
  primary: z34.string(),
  secondary: z34.string().optional(),
  accent: z34.string().optional(),
  background: z34.string(),
  surface: z34.string(),
  text: z34.object({
    primary: z34.string(),
    secondary: z34.string().optional()
  })
});
var BrandPackTypographySchema = z34.object({
  fontFamily: z34.string(),
  baseSize: z34.number().optional()
});
var BrandPackRadiiSchema = z34.object({
  sm: z34.number().optional(),
  md: z34.number().optional(),
  lg: z34.number().optional(),
  pill: z34.number().optional()
});
var BrandPackSpacingSchema = z34.object({
  sm: z34.number().optional(),
  md: z34.number().optional(),
  lg: z34.number().optional()
});
var BrandPackShadowsSchema = z34.object({
  sm: z34.string().optional(),
  md: z34.string().optional(),
  lg: z34.string().optional()
});
var BrandPackTokensSchema = z34.object({
  palette: BrandPackPaletteSchema,
  typography: BrandPackTypographySchema.optional(),
  radii: BrandPackRadiiSchema.optional(),
  spacing: BrandPackSpacingSchema.optional(),
  shadows: BrandPackShadowsSchema.optional()
});
var BrandPackSchema = z34.object({
  id: z34.string().min(1),
  name: z34.string().min(1),
  websiteUrl: z34.string().url(),
  logos: z34.object({
    primary: z34.string(),
    mark: z34.string().optional(),
    inverted: z34.string().optional()
  }),
  navLabels: z34.object({
    home: z34.string(),
    investigations: z34.string(),
    cases: z34.string().optional(),
    dashboards: z34.string().optional(),
    settings: z34.string().optional(),
    support: z34.string().optional()
  }),
  tokens: BrandPackTokensSchema,
  updatedAt: z34.string().datetime()
});

// src/services/brand-packs/brand-pack.loader.ts
var PACKS_DIR = path36.resolve(
  process.cwd(),
  "server",
  "src",
  "services",
  "brand-packs",
  "packs"
);
var normalizePackId = (packId) => packId.endsWith(".json") ? packId : `${packId}.json`;
async function loadBrandPack(packId) {
  const fileName = normalizePackId(packId);
  const filePath = path36.join(PACKS_DIR, fileName);
  const contents = await fs32.readFile(filePath, "utf-8");
  const parsed = JSON.parse(contents);
  return BrandPackSchema.parse(parsed);
}

// src/services/brand-packs/brand-pack.service.ts
var DEFAULT_PACK_ID = "summit-default";
var BRAND_PACK_CACHE_TTL_SECONDS = 300;
var BrandPackService = class _BrandPackService {
  static instance;
  assignments = /* @__PURE__ */ new Map();
  static getInstance() {
    if (!_BrandPackService.instance) {
      _BrandPackService.instance = new _BrandPackService();
    }
    return _BrandPackService.instance;
  }
  buildCacheKey(tenantId, partnerId) {
    return `brand-pack:${tenantId}:${partnerId ?? "default"}`;
  }
  resolveAssignment(tenantId, partnerId) {
    const cacheKey = this.buildCacheKey(tenantId, partnerId);
    const assignment = this.assignments.get(cacheKey) ?? {
      tenantId,
      partnerId,
      packId: DEFAULT_PACK_ID,
      appliedAt: (/* @__PURE__ */ new Date()).toISOString()
    };
    if (!this.assignments.has(cacheKey)) {
      this.assignments.set(cacheKey, assignment);
    }
    return assignment;
  }
  async getBrandPack(tenantId, partnerId) {
    const cacheKey = this.buildCacheKey(tenantId, partnerId);
    return cacheService.getOrSet(
      cacheKey,
      async () => {
        const assignment = this.resolveAssignment(tenantId, partnerId);
        const pack = await loadBrandPack(assignment.packId);
        return { pack, assignment };
      },
      BRAND_PACK_CACHE_TTL_SECONDS
    );
  }
  async applyBrandPack(tenantId, packId, partnerId) {
    const appliedAt = (/* @__PURE__ */ new Date()).toISOString();
    const assignment = {
      tenantId,
      partnerId,
      packId,
      appliedAt
    };
    const cacheKey = this.buildCacheKey(tenantId, partnerId);
    this.assignments.set(cacheKey, assignment);
    await cacheService.del(cacheKey);
    const pack = await loadBrandPack(packId);
    return { pack, assignment };
  }
  async invalidateTenant(tenantId, partnerId) {
    const cacheKey = this.buildCacheKey(tenantId, partnerId);
    await cacheService.del(cacheKey);
  }
};

// src/services/brand-packs/brand-pack.routes.ts
var router75 = express37.Router();
var service10 = BrandPackService.getInstance();
var singleParam16 = (value) => Array.isArray(value) ? value[0] : value ?? "";
var singleQuery = (value) => Array.isArray(value) ? value[0] : value;
var querySchema2 = z35.object({
  partnerId: z35.string().optional()
});
var applySchema = z35.object({
  packId: z35.string().min(1),
  partnerId: z35.string().optional(),
  actorId: z35.string().optional(),
  actorName: z35.string().optional(),
  reason: z35.string().optional()
});
router75.get("/tenants/:tenantId", ensureAuthenticated, async (req, res) => {
  try {
    const tenantId = singleParam16(req.params.tenantId);
    const partnerId = singleQuery(req.query.partnerId);
    querySchema2.parse({ partnerId });
    const resolution = await service10.getBrandPack(tenantId, partnerId);
    res.json({
      tenantId,
      partnerId,
      assignment: resolution.assignment,
      pack: resolution.pack
    });
  } catch (error) {
    res.status(500).json({ error: "Failed to load brand pack" });
  }
});
router75.post("/tenants/:tenantId/apply", ensureAuthenticated, async (req, res) => {
  try {
    const tenantId = singleParam16(req.params.tenantId);
    const payload = applySchema.parse(req.body);
    const actorId = payload.actorId ?? req.user?.id ?? "system";
    const actorName = payload.actorName ?? req.user?.name;
    const appliedAt = (/* @__PURE__ */ new Date()).toISOString();
    const resolution = await service10.applyBrandPack(
      tenantId,
      payload.packId,
      payload.partnerId
    );
    const receipt = await emitBrandPackReceipt({
      tenantId,
      packId: payload.packId,
      actorId,
      appliedAt
    });
    const receiptQueryPath = `/api/receipts/${receipt.id}`;
    const auditEventId = await emitAuditEvent(
      {
        eventId: randomUUID53(),
        occurredAt: appliedAt,
        actor: {
          type: "user",
          id: actorId,
          name: actorName,
          ipAddress: req.ip
        },
        action: {
          type: "brand-pack.apply",
          name: "Brand pack applied",
          outcome: "success"
        },
        target: {
          type: "brand-pack",
          id: payload.packId,
          name: resolution.pack.name,
          path: `/api/brand-packs/tenants/${tenantId}`
        },
        tenantId,
        traceId: req.headers["x-request-id"],
        metadata: {
          partnerId: payload.partnerId,
          reason: payload.reason,
          receiptId: receipt.id,
          receiptQueryPath,
          appliedAt
        }
      },
      {
        level: "info",
        complianceRelevant: true,
        complianceFrameworks: ["SOC2", "ISO27001"],
        serviceId: "brand-packs"
      }
    );
    res.json({
      tenantId,
      partnerId: payload.partnerId,
      assignment: resolution.assignment,
      pack: resolution.pack,
      receipt,
      receiptQueryPath,
      auditEventId
    });
  } catch (error) {
    if (error instanceof z35.ZodError) {
      res.status(400).json({ error: "Invalid input", details: error.errors });
      return;
    }
    res.status(500).json({ error: "Failed to apply brand pack" });
  }
});
var brand_pack_routes_default = router75;

// src/routes/federated-campaign-radar.ts
import express38 from "express";

// src/services/fcr/schema-validator.ts
import AjvModule from "ajv";
import addFormatsModule from "ajv-formats";
import { promises as fs33 } from "fs";
import path37 from "path";
var Ajv3 = AjvModule.default || AjvModule;
var addFormats3 = addFormatsModule.default || addFormatsModule;
var FcrSchemaValidator = class {
  ajv = new Ajv3({ allErrors: true, strict: true });
  constructor() {
    addFormats3(this.ajv);
  }
  async validateSignals(signals) {
    const schemaPath = path37.resolve(
      process.cwd(),
      "schemas",
      "fcr",
      "v1",
      "fcr-signal.schema.json"
    );
    const schemaRaw = await fs33.readFile(schemaPath, "utf8");
    const schema2 = JSON.parse(schemaRaw);
    const validate3 = this.ajv.compile(schema2);
    const failures = [];
    for (const signal of signals) {
      const ok = validate3(signal);
      if (!ok) {
        failures.push(
          ...(validate3.errors || []).map(
            (error) => `${signal.entity_id}: ${error.instancePath} ${error.message}`
          )
        );
      }
    }
    return {
      ok: failures.length === 0,
      errors: failures
    };
  }
};

// src/services/fcr/privacy-budget-service.ts
var FcrPrivacyBudgetService = class {
  budgets = /* @__PURE__ */ new Map();
  configureTenantBudget(tenantId, budget) {
    this.budgets.set(tenantId, { ...budget });
  }
  getBudget(tenantId) {
    const budget = this.budgets.get(tenantId);
    if (!budget) {
      return { epsilon: 0, delta: 0 };
    }
    return { ...budget };
  }
  canAfford(tenantId, cost) {
    const budget = this.getBudget(tenantId);
    return budget.epsilon >= cost.epsilon && budget.delta >= cost.delta;
  }
  consume(tenantId, cost) {
    const budget = this.getBudget(tenantId);
    if (!this.canAfford(tenantId, cost)) {
      return { ok: false, remaining: budget };
    }
    const updated = {
      epsilon: budget.epsilon - cost.epsilon,
      delta: budget.delta - cost.delta
    };
    this.budgets.set(tenantId, updated);
    return { ok: true, remaining: updated };
  }
};

// src/services/fcr/credential-scorer.ts
var C2PA_MULTIPLIERS = {
  pass: 1.2,
  fail: 0.6,
  error: 0.8,
  unknown: 1
};
var FcrCredentialScorer = class {
  scoreSignal(signal) {
    const base = signal.confidence_local;
    const status = signal.provenance_assertions?.c2pa_status || "unknown";
    const multiplier = C2PA_MULTIPLIERS[status] ?? 1;
    const score = Math.max(0, Math.min(1, base * multiplier));
    return {
      score,
      multiplier,
      status
    };
  }
};

// src/services/fcr/clustering-service.ts
import { v4 as uuidv426 } from "uuid";
function deriveCentroid(signal) {
  if (signal.narrative_claim_hash) return signal.narrative_claim_hash;
  if (signal.media_hashes?.sha256) return signal.media_hashes.sha256;
  if (signal.url?.normalized) return signal.url.normalized;
  if (signal.account_handle_hash) return signal.account_handle_hash;
  return signal.entity_id;
}
function bucketTenantCount(count) {
  if (count <= 2) return "1-2";
  if (count <= 5) return "3-5";
  if (count <= 10) return "6-10";
  return "11+";
}
var FcrClusteringService = class {
  clusterSignals(signals) {
    const clusters = /* @__PURE__ */ new Map();
    for (const signal of signals) {
      const centroid = deriveCentroid(signal);
      const current = clusters.get(centroid) ?? [];
      current.push(signal);
      clusters.set(centroid, current);
    }
    return Array.from(clusters.entries()).map(([centroid, items]) => {
      const tenants = new Set(items.map((item) => item.tenant_id));
      const sorted = items.map((item) => new Date(item.observed_at).toISOString()).sort();
      const publicArtifacts = items.flatMap((item) => {
        const artifacts = [];
        if (item.url?.normalized) {
          artifacts.push({ type: "url", value: item.url.normalized });
        }
        if (item.media_hashes?.sha256) {
          artifacts.push({ type: "media_hash", value: item.media_hashes.sha256 });
        }
        return artifacts;
      });
      const c2paSignals = items.filter(
        (item) => item.provenance_assertions?.c2pa_status === "pass"
      );
      const signerSet = new Set(
        items.map((item) => item.provenance_assertions?.signer).filter((value) => Boolean(value))
      );
      const confidence = items.reduce((sum, item) => sum + item.confidence_local, 0) / items.length;
      const cluster = {
        cluster_id: uuidv426(),
        centroid_hash: centroid,
        signal_count: items.length,
        tenant_count_bucket: bucketTenantCount(tenants.size),
        confidence: Math.max(0, Math.min(1, confidence)),
        first_observed_at: sorted[0],
        last_observed_at: sorted[sorted.length - 1],
        public_artifacts: publicArtifacts,
        provenance_summary: {
          c2pa_pass_rate: items.length ? c2paSignals.length / items.length : 0,
          signer_set: Array.from(signerSet)
        }
      };
      return cluster;
    });
  }
};

// src/services/fcr/early-warning-service.ts
import { v4 as uuidv427 } from "uuid";
var FcrEarlyWarningService = class {
  history = /* @__PURE__ */ new Map();
  evaluateClusters(clusters) {
    return clusters.map((cluster) => this.generateAlert(cluster)).filter((alert) => Boolean(alert));
  }
  generateAlert(cluster) {
    const previous = this.history.get(cluster.centroid_hash) ?? 0;
    this.history.set(cluster.centroid_hash, cluster.signal_count);
    const growth = cluster.signal_count - previous;
    if (growth <= 0) {
      return null;
    }
    const severity = this.severityFor(cluster, growth);
    return {
      alert_id: uuidv427(),
      cluster_id: cluster.cluster_id,
      severity,
      summary: `Cluster ${cluster.centroid_hash} grew by ${growth} signals`,
      generated_at: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  severityFor(cluster, growth) {
    if (cluster.confidence > 0.85 && growth >= 10) return "critical";
    if (cluster.confidence > 0.7 && growth >= 5) return "high";
    if (cluster.confidence > 0.5 && growth >= 3) return "medium";
    return "low";
  }
};

// src/services/fcr/response-pack-service.ts
var PLAYBOOKS = {
  low: {
    playbook_id: "fcr-low-signal",
    recommended_actions: ["Monitor spread", "Collect public exemplars"],
    diffusion_summary: "Low velocity cluster. Maintain watch."
  },
  medium: {
    playbook_id: "fcr-medium-signal",
    recommended_actions: [
      "Notify comms lead",
      "Prepare counter-narrative draft"
    ],
    diffusion_summary: "Moderate coordination across channels."
  },
  high: {
    playbook_id: "fcr-high-signal",
    recommended_actions: [
      "Activate incident response",
      "Engage platform trust teams",
      "Issue stakeholder brief"
    ],
    diffusion_summary: "High velocity cluster. Coordination likely."
  },
  critical: {
    playbook_id: "fcr-critical-signal",
    recommended_actions: [
      "Activate crisis comms",
      "Coordinate cross-tenant mitigation",
      "Escalate to exec review"
    ],
    diffusion_summary: "Critical surge across tenants/regions."
  }
};
var FcrResponsePackService = class {
  buildResponsePack(alert) {
    return PLAYBOOKS[alert.severity];
  }
};

// src/provenance/fcr-ledger.ts
init_ledger();
async function recordFcrIngest(tenantId, signals) {
  const resourceId = `fcr-ingest-${Date.now()}`;
  return provenanceLedger.appendEntry({
    tenantId,
    actionType: "fcr.ingest",
    resourceType: "fcr.signal",
    resourceId,
    actorId: "fcr-service",
    actorType: "system",
    timestamp: /* @__PURE__ */ new Date(),
    payload: {
      mutationType: "CREATE",
      entityId: resourceId,
      entityType: "fcr.signal",
      count: signals.length,
      sample_ids: signals.slice(0, 5).map((signal) => signal.entity_id)
    },
    metadata: {
      purpose: "federated-campaign-radar",
      privacy: {
        mechanism: "dp-budget"
      }
    }
  });
}
async function recordFcrClusters(tenantId, clusters) {
  const resourceId = `fcr-cluster-${Date.now()}`;
  return provenanceLedger.appendEntry({
    tenantId,
    actionType: "fcr.cluster",
    resourceType: "fcr.cluster",
    resourceId,
    actorId: "fcr-service",
    actorType: "system",
    timestamp: /* @__PURE__ */ new Date(),
    payload: {
      mutationType: "CREATE",
      entityId: resourceId,
      entityType: "fcr.cluster",
      count: clusters.length,
      cluster_ids: clusters.map((cluster) => cluster.cluster_id)
    },
    metadata: {
      purpose: "federated-campaign-radar"
    }
  });
}
async function recordFcrAlert(tenantId, alerts) {
  const resourceId = `fcr-alert-${Date.now()}`;
  return provenanceLedger.appendEntry({
    tenantId,
    actionType: "fcr.alert",
    resourceType: "fcr.alert",
    resourceId,
    actorId: "fcr-service",
    actorType: "system",
    timestamp: /* @__PURE__ */ new Date(),
    payload: {
      mutationType: "CREATE",
      entityId: resourceId,
      entityType: "fcr.alert",
      count: alerts.length,
      alert_ids: alerts.map((alert) => alert.alert_id)
    },
    metadata: {
      purpose: "federated-campaign-radar"
    }
  });
}

// src/services/fcr/fcr-service.ts
var FcrService = class {
  validator = new FcrSchemaValidator();
  privacyBudget = new FcrPrivacyBudgetService();
  scorer = new FcrCredentialScorer();
  clustering = new FcrClusteringService();
  warning = new FcrEarlyWarningService();
  responsePack = new FcrResponsePackService();
  configureTenantBudget(tenantId, epsilon, delta) {
    this.privacyBudget.configureTenantBudget(tenantId, { epsilon, delta });
  }
  async ingestSignals(tenantId, signals) {
    if (signals.length === 0) {
      return { ok: false, errors: ["Signals payload is empty."] };
    }
    const validation = await this.validator.validateSignals(signals);
    if (!validation.ok) {
      return { ok: false, errors: validation.errors };
    }
    const mismatchedSignals = signals.filter(
      (signal) => signal.tenant_id !== tenantId
    );
    if (mismatchedSignals.length > 0) {
      return {
        ok: false,
        errors: ["Signal tenant_id does not match request tenant_id."]
      };
    }
    const totalCost = signals.reduce(
      (acc, signal) => ({
        epsilon: acc.epsilon + signal.privacy_budget_cost.epsilon,
        delta: acc.delta + signal.privacy_budget_cost.delta
      }),
      { epsilon: 0, delta: 0 }
    );
    const budgetResult = this.privacyBudget.consume(tenantId, totalCost);
    if (!budgetResult.ok) {
      return {
        ok: false,
        errors: ["Privacy budget exceeded for one or more signals."]
      };
    }
    const scored = signals.map((signal) => ({
      ...signal,
      confidence_local: this.scorer.scoreSignal(signal).score
    }));
    await recordFcrIngest(tenantId, scored);
    return { ok: true, signals: scored };
  }
  async clusterSignals(tenantId, signals) {
    const clusters = this.clustering.clusterSignals(signals);
    await recordFcrClusters(tenantId, clusters);
    return clusters;
  }
  async generateAlerts(tenantId, clusters) {
    const alerts = this.warning.evaluateClusters(clusters).map((alert) => ({
      ...alert,
      response_pack: this.responsePack.buildResponsePack(alert)
    }));
    await recordFcrAlert(tenantId, alerts);
    return alerts;
  }
  async runPipeline(tenantId, signals) {
    const ingestResult = await this.ingestSignals(tenantId, signals);
    if (!ingestResult.ok) {
      return ingestResult;
    }
    const clusters = await this.clusterSignals(tenantId, ingestResult.signals);
    const alerts = await this.generateAlerts(tenantId, clusters);
    return {
      ok: true,
      clusters,
      alerts
    };
  }
};

// src/routes/federated-campaign-radar.ts
var router76 = express38.Router();
var fcrService = new FcrService();
router76.post("/fcr/budget", express38.json(), async (req, res) => {
  const { tenant_id: tenantId, epsilon, delta } = req.body || {};
  if (!tenantId || typeof epsilon !== "number" || typeof delta !== "number") {
    res.status(400).json({ error: "tenant_id, epsilon, and delta are required." });
    return;
  }
  fcrService.configureTenantBudget(tenantId, epsilon, delta);
  res.status(200).json({ ok: true });
});
router76.post("/fcr/ingest", express38.json(), async (req, res) => {
  const { tenant_id: tenantId, signals } = req.body || {};
  if (!tenantId || !Array.isArray(signals)) {
    res.status(400).json({ error: "tenant_id and signals array are required." });
    return;
  }
  const ingestResult = await fcrService.ingestSignals(
    tenantId,
    signals
  );
  if (!ingestResult.ok) {
    const errors = "errors" in ingestResult ? ingestResult.errors : ["Ingest failed"];
    res.status(422).json({ ok: false, errors });
    return;
  }
  res.status(200).json({ ok: true, signals: ingestResult.signals });
});
router76.post("/fcr/run", express38.json(), async (req, res) => {
  const { tenant_id: tenantId, signals } = req.body || {};
  if (!tenantId || !Array.isArray(signals)) {
    res.status(400).json({ error: "tenant_id and signals array are required." });
    return;
  }
  const result2 = await fcrService.runPipeline(tenantId, signals);
  if (!result2.ok) {
    const errors = "errors" in result2 ? result2.errors : ["Pipeline failed"];
    res.status(422).json({ ok: false, errors });
    return;
  }
  res.status(200).json(result2);
});
var federated_campaign_radar_default = router76;

// src/middleware/error-handling-middleware.ts
init_logger();
import { GraphQLError as GraphQLError12 } from "graphql";
import { z as z36 } from "zod";

// src/observability/reliability-metrics.ts
import { Counter as Counter18, Gauge as Gauge16, Histogram as Histogram14, Summary } from "prom-client";
var latencyHistogram = getOrCreateHistogram2(
  "reliability_request_duration_seconds",
  "Endpoint latency for high-traffic reliability surfaces",
  ["endpoint", "status"],
  [0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]
);
var latencySummary = getOrCreateSummary(
  "reliability_request_latency_quantiles",
  "p50/p95 latency for reliability endpoints",
  ["endpoint"],
  [0.5, 0.95]
);
var errorCounter = getOrCreateCounter2(
  "reliability_request_errors_total",
  "Error responses for reliability endpoints",
  ["endpoint", "status"]
);
var queueDepthGauge = getOrCreateGauge2(
  "reliability_queue_depth",
  "In-flight/backlog size for hot paths",
  ["endpoint", "tenant"]
);
var tenantQueryBudgetHits = getOrCreateCounter2(
  "tenant_query_budget_hits_total",
  "Per-tenant budget consumption for query-style endpoints",
  ["tenant", "endpoint"]
);
function recordEndpointResult(options2) {
  const endpoint = options2.endpoint;
  const tenantLabel = normalizeTenant(options2.tenantId);
  const statusLabel = classifyStatus(options2.statusCode);
  latencyHistogram.labels(endpoint, statusLabel).observe(options2.durationSeconds);
  latencySummary.labels(endpoint).observe(options2.durationSeconds);
  if (options2.statusCode >= 400) {
    errorCounter.labels(endpoint, statusLabel).inc();
  }
  if (options2.queueDepth !== void 0) {
    queueDepthGauge.labels(endpoint, tenantLabel).set(options2.queueDepth);
  }
}
function classifyStatus(statusCode) {
  if (statusCode >= 500) return "5xx";
  if (statusCode >= 400) return "4xx";
  if (statusCode >= 300) return "3xx";
  if (statusCode >= 200) return "2xx";
  return "1xx";
}
function normalizeTenant(tenantId) {
  if (!tenantId) {
    return "unknown";
  }
  return String(tenantId).replace(/[^a-zA-Z0-9:_-]/g, "_").substring(0, 48);
}
function getOrCreateHistogram2(name, help, labelNames, buckets) {
  const existing = registry2.getSingleMetric(name);
  if (existing instanceof Histogram14) {
    return existing;
  }
  const metric = new Histogram14({
    name,
    help,
    labelNames,
    buckets,
    registers: [registry2]
  });
  return metric;
}
function getOrCreateSummary(name, help, labelNames, percentiles) {
  const existing = registry2.getSingleMetric(name);
  if (existing instanceof Summary) {
    return existing;
  }
  return new Summary({
    name,
    help,
    labelNames,
    percentiles,
    maxAgeSeconds: 600,
    ageBuckets: 5,
    registers: [registry2]
  });
}
function getOrCreateCounter2(name, help, labelNames) {
  const existing = registry2.getSingleMetric(name);
  if (existing instanceof Counter18) {
    return existing;
  }
  return new Counter18({
    name,
    help,
    labelNames,
    registers: [registry2]
  });
}
function getOrCreateGauge2(name, help, labelNames) {
  const existing = registry2.getSingleMetric(name);
  if (existing instanceof Gauge16) {
    return existing;
  }
  return new Gauge16({
    name,
    help,
    labelNames,
    registers: [registry2]
  });
}

// src/middleware/error-handling-middleware.ts
import * as fs34 from "node:fs";
var ZodError = z36.ZodError;
var deriveStatusCode = (error) => {
  if (error instanceof GraphQLError12) {
    const httpStatus = error.extensions?.http?.status;
    if (typeof httpStatus === "number") return httpStatus;
  }
  if (error instanceof ZodError) {
    return 400;
  }
  const candidateStatus = error?.statusCode ?? error?.status;
  if (typeof candidateStatus === "number") {
    return candidateStatus;
  }
  if (error?.type === "entity.parse.failed") {
    return 400;
  }
  return 500;
};
var normalizeGraphQLError = (error, statusCode) => {
  if (error instanceof GraphQLError12) {
    return error;
  }
  const code = statusCode >= 500 ? "INTERNAL_SERVER_ERROR" : "BAD_USER_INPUT";
  const message = statusCode >= 500 ? "Internal server error" : "Invalid request payload";
  return new GraphQLError12(message, {
    extensions: {
      code,
      http: { status: statusCode }
    }
  });
};
var deriveMessage = (error, fallback) => {
  if (error instanceof GraphQLError12) {
    return error.message;
  }
  if (error instanceof ZodError) {
    return "Invalid request payload";
  }
  if (error instanceof Error) {
    return error.message || fallback;
  }
  return fallback;
};
var centralizedErrorHandler = (err, req, res, next) => {
  if (req.path?.startsWith("/graphql")) {
    try {
      fs34.appendFileSync("/tmp/debug_error_handler.txt", `[centralizedErrorHandler] GraphQL error caught
`);
      fs34.appendFileSync("/tmp/debug_error_handler.txt", `[centralizedErrorHandler] Error type: ${err?.constructor?.name}
`);
      fs34.appendFileSync("/tmp/debug_error_handler.txt", `[centralizedErrorHandler] Error message: ${err?.message}
`);
      fs34.appendFileSync("/tmp/debug_error_handler.txt", `[centralizedErrorHandler] Is GraphQLError? ${err instanceof GraphQLError12}
`);
      if (err?.stack) {
        fs34.appendFileSync("/tmp/debug_error_handler.txt", `[centralizedErrorHandler] Stack: ${err?.stack?.split("\n").slice(0, 3).join("\n")}
`);
      }
    } catch (e) {
    }
  }
  if (res.headersSent) {
    return next(err);
  }
  const statusCode = deriveStatusCode(err);
  const isGraphQLRoute = req.path?.startsWith("/graphql");
  const graphQLError = isGraphQLRoute ? normalizeGraphQLError(err, statusCode) : err instanceof GraphQLError12 ? err : null;
  const message = deriveMessage(graphQLError ?? err, statusCode >= 500 ? "Internal server error" : "Bad request");
  logger.error({
    err,
    path: req.path,
    method: req.method,
    statusCode,
    correlationId: req.correlationId
  }, "Request failed");
  recordEndpointResult({
    endpoint: req.path,
    statusCode,
    durationSeconds: res.locals.duration || 0,
    tenantId: req.tenantId || req.user?.tenantId || "unknown"
  });
  const responseBody = {
    error: {
      message,
      code: graphQLError?.extensions?.code,
      correlationId: req.correlationId || req.headers["x-correlation-id"] || req.headers["x-request-id"]
    }
  };
  if (graphQLError?.extensions?.details && process.env.NODE_ENV !== "production") {
    responseBody.error.details = graphQLError.extensions.details;
  }
  res.status(statusCode).json(responseBody);
};

// src/routes/plugins/plugin-admin.ts
init_auth4();
import express39 from "express";

// src/plugins/PluginManager.ts
import { Pool as Pool10 } from "pg";
import { v4 as uuidv428 } from "uuid";

// src/plugins/PluginRegistry.ts
init_data_envelope();
init_logger2();
import { Pool as Pool9 } from "pg";
var PluginRegistry = class {
  pool;
  plugins = /* @__PURE__ */ new Map();
  constructor(pool4) {
    this.pool = pool4 || new Pool9({ connectionString: process.env.DATABASE_URL });
  }
  /**
   * Register a plugin in the registry
   */
  async register(manifest, plugin, actorId) {
    try {
      const validation = this.validateManifest(manifest);
      if (!validation.valid) {
        return createDataEnvelope(
          { success: false, registration: null },
          { source: "PluginRegistry", actor: actorId },
          {
            result: "DENY" /* DENY */,
            policyId: "plugin-registration",
            reason: `Invalid manifest: ${validation.errors?.join(", ")}`,
            evaluator: "PluginRegistry"
          }
        );
      }
      const existing = await this.pool.query(
        "SELECT id FROM plugins WHERE id = $1",
        [manifest.id]
      );
      const now = (/* @__PURE__ */ new Date()).toISOString();
      if (existing.rows.length > 0) {
        await this.pool.query(
          `UPDATE plugins SET
            name = $2,
            version = $3,
            description = $4,
            author = $5,
            category = $6,
            manifest = $7,
            updated_at = $8
          WHERE id = $1`,
          [
            manifest.id,
            manifest.name,
            manifest.version,
            manifest.description,
            manifest.author,
            manifest.category,
            JSON.stringify(manifest),
            now
          ]
        );
      } else {
        await this.pool.query(
          `INSERT INTO plugins (id, name, version, description, author, category, manifest, status, installed_by, installed_at)
          VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)`,
          [
            manifest.id,
            manifest.name,
            manifest.version,
            manifest.description,
            manifest.author,
            manifest.category,
            JSON.stringify(manifest),
            "registered",
            actorId,
            now
          ]
        );
      }
      this.plugins.set(manifest.id, plugin);
      const registration = {
        id: manifest.id,
        manifest,
        status: "registered",
        installedAt: now,
        installedBy: actorId,
        executionCount: 0,
        errorCount: 0,
        version: manifest.version
      };
      logger_default2.info({ pluginId: manifest.id }, "Plugin registered");
      return createDataEnvelope(
        { success: true, registration },
        { source: "PluginRegistry", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-registration",
          reason: "Plugin registered successfully",
          evaluator: "PluginRegistry"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId: manifest.id }, "Failed to register plugin");
      throw error;
    }
  }
  /**
   * Get a plugin by ID
   */
  async getPlugin(pluginId, actorId) {
    try {
      const result2 = await this.pool.query(
        `SELECT
          id, name, version, description, author, category, manifest, status,
          installed_at, installed_by, enabled_at, enabled_by,
          last_executed_at, execution_count, error_count, last_error
        FROM plugins WHERE id = $1`,
        [pluginId]
      );
      if (result2.rows.length === 0) {
        return createDataEnvelope(
          null,
          { source: "PluginRegistry", actor: actorId },
          {
            result: "ALLOW" /* ALLOW */,
            policyId: "plugin-read",
            reason: "Plugin not found",
            evaluator: "PluginRegistry"
          }
        );
      }
      const row = result2.rows[0];
      const registration = {
        id: row.id,
        manifest: row.manifest,
        status: row.status,
        installedAt: row.installed_at,
        installedBy: row.installed_by,
        enabledAt: row.enabled_at,
        enabledBy: row.enabled_by,
        lastExecutedAt: row.last_executed_at,
        executionCount: parseInt(row.execution_count, 10),
        errorCount: parseInt(row.error_count, 10),
        lastError: row.last_error,
        version: row.version
      };
      return createDataEnvelope(
        registration,
        { source: "PluginRegistry", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-read",
          reason: "Plugin retrieved",
          evaluator: "PluginRegistry"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId }, "Failed to get plugin");
      throw error;
    }
  }
  /**
   * List all registered plugins
   */
  async listPlugins(filters, actorId) {
    try {
      const page = filters.page || 1;
      const pageSize = filters.pageSize || 20;
      const offset = (page - 1) * pageSize;
      let whereClause = "WHERE 1=1";
      const params = [];
      let paramIndex = 1;
      if (filters.category) {
        whereClause += ` AND category = $${paramIndex++}`;
        params.push(filters.category);
      }
      if (filters.status) {
        whereClause += ` AND status = $${paramIndex++}`;
        params.push(filters.status);
      }
      if (filters.search) {
        whereClause += ` AND (name ILIKE $${paramIndex} OR description ILIKE $${paramIndex})`;
        params.push(`%${filters.search}%`);
        paramIndex++;
      }
      const countResult = await this.pool.query(
        `SELECT COUNT(*) as total FROM plugins ${whereClause}`,
        params
      );
      const total = parseInt(countResult.rows[0].total, 10);
      params.push(pageSize, offset);
      const result2 = await this.pool.query(
        `SELECT
          id, name, version, description, author, category, manifest, status,
          installed_at, installed_by, enabled_at, enabled_by,
          last_executed_at, execution_count, error_count, last_error
        FROM plugins
        ${whereClause}
        ORDER BY name
        LIMIT $${paramIndex++} OFFSET $${paramIndex}`,
        params
      );
      const plugins = result2.rows.map((row) => ({
        id: row.id,
        manifest: row.manifest,
        status: row.status,
        installedAt: row.installed_at,
        installedBy: row.installed_by,
        enabledAt: row.enabled_at,
        enabledBy: row.enabled_by,
        lastExecutedAt: row.last_executed_at,
        executionCount: parseInt(row.execution_count, 10),
        errorCount: parseInt(row.error_count, 10),
        lastError: row.last_error,
        version: row.version
      }));
      return createDataEnvelope(
        { plugins, total },
        { source: "PluginRegistry", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-list",
          reason: "Plugins listed",
          evaluator: "PluginRegistry"
        }
      );
    } catch (error) {
      logger_default2.error({ error }, "Failed to list plugins");
      throw error;
    }
  }
  /**
   * Update plugin status
   */
  async updateStatus(pluginId, status, actorId) {
    try {
      const updates = ["status = $2", "updated_at = $3"];
      const params = [pluginId, status, (/* @__PURE__ */ new Date()).toISOString()];
      let paramIndex = 4;
      if (status === "enabled") {
        updates.push(`enabled_at = $${paramIndex++}`, `enabled_by = $${paramIndex++}`);
        params.push((/* @__PURE__ */ new Date()).toISOString(), actorId);
      }
      await this.pool.query(
        `UPDATE plugins SET ${updates.join(", ")} WHERE id = $1`,
        params
      );
      logger_default2.info({ pluginId, status }, "Plugin status updated");
      return createDataEnvelope(
        { success: true },
        { source: "PluginRegistry", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-update",
          reason: `Plugin status updated to ${status}`,
          evaluator: "PluginRegistry"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId }, "Failed to update plugin status");
      throw error;
    }
  }
  /**
   * Unregister a plugin
   */
  async unregister(pluginId, actorId) {
    try {
      await this.pool.query("DELETE FROM plugins WHERE id = $1", [pluginId]);
      this.plugins.delete(pluginId);
      logger_default2.info({ pluginId }, "Plugin unregistered");
      return createDataEnvelope(
        { success: true },
        { source: "PluginRegistry", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-unregister",
          reason: "Plugin unregistered",
          evaluator: "PluginRegistry"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId }, "Failed to unregister plugin");
      throw error;
    }
  }
  /**
   * Get plugin instance
   */
  getPluginInstance(pluginId) {
    return this.plugins.get(pluginId);
  }
  /**
   * Get tenant-specific plugin configuration
   */
  async getTenantConfig(pluginId, tenantId, actorId) {
    try {
      const result2 = await this.pool.query(
        `SELECT * FROM plugin_tenant_config WHERE plugin_id = $1 AND tenant_id = $2`,
        [pluginId, tenantId]
      );
      if (result2.rows.length === 0) {
        return createDataEnvelope(
          null,
          { source: "PluginRegistry", actor: actorId },
          {
            result: "ALLOW" /* ALLOW */,
            policyId: "plugin-config-read",
            reason: "No tenant config found",
            evaluator: "PluginRegistry"
          }
        );
      }
      const row = result2.rows[0];
      return createDataEnvelope(
        {
          pluginId: row.plugin_id,
          tenantId: row.tenant_id,
          enabled: row.enabled,
          config: row.config,
          permissions: row.permissions,
          createdAt: row.created_at,
          updatedAt: row.updated_at,
          createdBy: row.created_by,
          updatedBy: row.updated_by
        },
        { source: "PluginRegistry", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-config-read",
          reason: "Tenant config retrieved",
          evaluator: "PluginRegistry"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId, tenantId }, "Failed to get tenant config");
      throw error;
    }
  }
  /**
   * Save tenant-specific plugin configuration
   */
  async saveTenantConfig(pluginId, tenantId, config9, enabled, actorId) {
    try {
      const now = (/* @__PURE__ */ new Date()).toISOString();
      await this.pool.query(
        `INSERT INTO plugin_tenant_config (plugin_id, tenant_id, config, enabled, created_at, created_by, updated_at, updated_by)
        VALUES ($1, $2, $3, $4, $5, $6, $5, $6)
        ON CONFLICT (plugin_id, tenant_id)
        DO UPDATE SET config = $3, enabled = $4, updated_at = $5, updated_by = $6`,
        [pluginId, tenantId, JSON.stringify(config9), enabled, now, actorId]
      );
      logger_default2.info({ pluginId, tenantId }, "Tenant plugin config saved");
      return createDataEnvelope(
        { success: true },
        { source: "PluginRegistry", actor: actorId },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-config-write",
          reason: "Tenant config saved",
          evaluator: "PluginRegistry"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId, tenantId }, "Failed to save tenant config");
      throw error;
    }
  }
  // --------------------------------------------------------------------------
  // Helper Methods
  // --------------------------------------------------------------------------
  validateManifest(manifest) {
    const errors = [];
    if (!manifest.id || !/^[a-z0-9-]+$/.test(manifest.id)) {
      errors.push("Invalid plugin ID (must be lowercase alphanumeric with hyphens)");
    }
    if (!manifest.name || manifest.name.length < 2) {
      errors.push("Plugin name is required");
    }
    if (!manifest.version || !/^\d+\.\d+\.\d+/.test(manifest.version)) {
      errors.push("Invalid version (must be semver)");
    }
    if (!manifest.author) {
      errors.push("Author is required");
    }
    if (!manifest.category) {
      errors.push("Category is required");
    }
    return {
      valid: errors.length === 0,
      errors: errors.length > 0 ? errors : void 0
    };
  }
};
var pluginRegistry = new PluginRegistry();

// src/plugins/PluginSandbox.ts
init_logger2();
var DEFAULT_CONFIG4 = {
  maxExecutionTimeMs: 3e4,
  maxMemoryMb: 128,
  maxApiCalls: 100,
  maxTokens: 1e3,
  // Default limit
  allowedDomains: ["*"],
  blockedOperations: ["eval", "Function", "child_process", "fs"]
};
var PluginSandbox = class {
  config;
  constructor(config9 = {}) {
    this.config = { ...DEFAULT_CONFIG4, ...config9 };
  }
  /**
   * Execute a plugin action in a sandboxed environment
   */
  async execute(plugin, action, params, context4, timeout) {
    const startTime = Date.now();
    const logs = [];
    let apiCallCount = 0;
    let tokensConsumed = 0;
    const pluginLimits = plugin.manifest.resources || {};
    const effectiveTimeout = timeout || pluginLimits.timeoutMs || this.config.maxExecutionTimeMs;
    const maxApiCalls = pluginLimits.apiCalls ?? this.config.maxApiCalls;
    const maxTokens = pluginLimits.tokens ?? this.config.maxTokens;
    const allowedDomains = pluginLimits.network?.domains ?? this.config.allowedDomains;
    const pluginLogger = {
      debug: (message, data) => {
        logs.push({ level: "debug", message, timestamp: (/* @__PURE__ */ new Date()).toISOString(), data });
      },
      info: (message, data) => {
        logs.push({ level: "info", message, timestamp: (/* @__PURE__ */ new Date()).toISOString(), data });
      },
      warn: (message, data) => {
        logs.push({ level: "warn", message, timestamp: (/* @__PURE__ */ new Date()).toISOString(), data });
      },
      error: (message, data) => {
        logs.push({ level: "error", message, timestamp: (/* @__PURE__ */ new Date()).toISOString(), data });
      }
    };
    const httpClient = {
      fetch: async (url, options2) => {
        if (apiCallCount >= maxApiCalls) {
          throw new Error(`API call limit exceeded (max: ${maxApiCalls})`);
        }
        const urlObj = new URL(url);
        const isAllowed = this.isDomainAllowed(urlObj.hostname, allowedDomains);
        if (!isAllowed) {
          throw new Error(`Domain not allowed: ${urlObj.hostname}`);
        }
        apiCallCount++;
        const headers = new Headers(options2?.headers);
        headers.set("X-Correlation-ID", context4.correlationId);
        headers.set("X-Plugin-ID", plugin.manifest.id);
        return fetch(url, { ...options2, headers });
      }
    };
    const sandboxedContext = {
      ...context4,
      // @ts-ignore - injecting safe utilities
      fetch: httpClient.fetch,
      log: pluginLogger
      // In production, we would use VM2 or similar for true isolation
    };
    try {
      const executionPromise = plugin.execute(action, params, sandboxedContext);
      const timeoutPromise = new Promise((_2, reject) => {
        setTimeout(() => {
          reject(new Error(`Plugin execution timed out after ${effectiveTimeout}ms`));
        }, effectiveTimeout);
      });
      const result2 = await Promise.race([executionPromise, timeoutPromise]);
      const executionTimeMs = Date.now() - startTime;
      return {
        ...result2,
        logs: [...logs, ...result2.logs || []],
        metrics: {
          executionTimeMs,
          apiCallCount,
          tokensConsumed,
          ...result2.metrics || {}
        }
      };
    } catch (error) {
      const executionTimeMs = Date.now() - startTime;
      logger_default2.error({ error, pluginId: plugin.manifest.id, action }, "Sandbox execution error");
      return {
        success: false,
        error: error.message || "Unknown error during plugin execution",
        logs,
        metrics: {
          executionTimeMs,
          apiCallCount,
          tokensConsumed
        }
      };
    }
  }
  /**
   * Check if a domain matches the allowed list (supports wildcards like *.example.com)
   */
  isDomainAllowed(domain, allowedDomains) {
    if (allowedDomains.includes("*")) return true;
    return allowedDomains.some((pattern2) => {
      if (pattern2 === domain) return true;
      if (pattern2.startsWith("*.")) {
        const suffix = pattern2.slice(2);
        return domain.endsWith(suffix) && domain.split(".").length === suffix.split(".").length + 1;
      }
      return false;
    });
  }
  /**
   * Validate that a plugin doesn't use blocked operations
   */
  validatePlugin(pluginCode) {
    const violations = [];
    for (const blocked of this.config.blockedOperations || []) {
      if (pluginCode.includes(blocked)) {
        violations.push(`Blocked operation detected: ${blocked}`);
      }
    }
    return {
      valid: violations.length === 0,
      violations
    };
  }
  /**
   * Update sandbox configuration
   */
  updateConfig(config9) {
    this.config = { ...this.config, ...config9 };
  }
  /**
   * Get current configuration
   */
  getConfig() {
    return { ...this.config };
  }
};
var pluginSandbox = new PluginSandbox();

// src/plugins/PluginManager.ts
init_data_envelope();
init_logger2();
var PluginManager = class {
  pool;
  registry;
  sandbox;
  eventSubscriptions = /* @__PURE__ */ new Map();
  constructor(pool4) {
    this.pool = pool4 || new Pool10({ connectionString: process.env.DATABASE_URL });
    this.registry = new PluginRegistry(this.pool);
    this.sandbox = new PluginSandbox();
  }
  /**
   * Install a plugin
   */
  async installPlugin(manifest, plugin, principal) {
    try {
      if (!this.hasCapability(principal, "admin:full")) {
        return createDataEnvelope(
          { success: false, pluginId: manifest.id },
          { source: "PluginManager", actor: principal.id },
          {
            result: "DENY" /* DENY */,
            policyId: "plugin-install",
            reason: "Insufficient permissions to install plugins",
            evaluator: "PluginManager"
          }
        );
      }
      const result2 = await this.registry.register(manifest, plugin, principal.id);
      if (!result2.data.success) {
        return createDataEnvelope(
          { success: false, pluginId: manifest.id },
          { source: "PluginManager", actor: principal.id },
          result2.governance
        );
      }
      await this.registry.updateStatus(manifest.id, "installed", principal.id);
      const context4 = this.createContext(principal, {});
      await plugin.initialize(context4);
      if (manifest.hooks) {
        for (const hook of manifest.hooks) {
          this.subscribeToEvent(manifest.id, hook.event);
        }
      }
      await this.logAudit({
        pluginId: manifest.id,
        tenantId: principal.tenantId,
        action: "install",
        actorId: principal.id,
        success: true,
        governanceVerdict: "ALLOW" /* ALLOW */
      });
      logger_default2.info({ pluginId: manifest.id }, "Plugin installed");
      return createDataEnvelope(
        { success: true, pluginId: manifest.id },
        { source: "PluginManager", actor: principal.id },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-install",
          reason: "Plugin installed successfully",
          evaluator: "PluginManager"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId: manifest.id }, "Failed to install plugin");
      throw error;
    }
  }
  /**
   * Enable a plugin for a tenant
   */
  async enablePlugin(pluginId, tenantId, config9, principal) {
    try {
      const pluginResult = await this.registry.getPlugin(pluginId, principal.id);
      if (!pluginResult.data) {
        return createDataEnvelope(
          { success: false },
          { source: "PluginManager", actor: principal.id },
          {
            result: "DENY" /* DENY */,
            policyId: "plugin-enable",
            reason: "Plugin not found",
            evaluator: "PluginManager"
          }
        );
      }
      const plugin = this.registry.getPluginInstance(pluginId);
      if (!plugin) {
        return createDataEnvelope(
          { success: false },
          { source: "PluginManager", actor: principal.id },
          {
            result: "DENY" /* DENY */,
            policyId: "plugin-enable",
            reason: "Plugin not loaded",
            evaluator: "PluginManager"
          }
        );
      }
      if (plugin.validateConfig) {
        const validation = await plugin.validateConfig(config9);
        if (!validation.valid) {
          return createDataEnvelope(
            { success: false },
            { source: "PluginManager", actor: principal.id },
            {
              result: "DENY" /* DENY */,
              policyId: "plugin-enable",
              reason: `Invalid config: ${validation.errors?.join(", ")}`,
              evaluator: "PluginManager"
            }
          );
        }
      }
      await this.registry.saveTenantConfig(pluginId, tenantId, config9, true, principal.id);
      if (pluginResult.data.status !== "enabled") {
        await this.registry.updateStatus(pluginId, "enabled", principal.id);
      }
      await this.logAudit({
        pluginId,
        tenantId,
        action: "enable",
        actorId: principal.id,
        success: true,
        governanceVerdict: "ALLOW" /* ALLOW */
      });
      logger_default2.info({ pluginId, tenantId }, "Plugin enabled for tenant");
      return createDataEnvelope(
        { success: true },
        { source: "PluginManager", actor: principal.id },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-enable",
          reason: "Plugin enabled",
          evaluator: "PluginManager"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId }, "Failed to enable plugin");
      throw error;
    }
  }
  /**
   * Disable a plugin for a tenant
   */
  async disablePlugin(pluginId, tenantId, principal) {
    try {
      await this.registry.saveTenantConfig(pluginId, tenantId, {}, false, principal.id);
      const plugin = this.registry.getPluginInstance(pluginId);
      if (plugin?.cleanup) {
        const context4 = this.createContext(principal, {});
        await plugin.cleanup(context4);
      }
      await this.logAudit({
        pluginId,
        tenantId,
        action: "disable",
        actorId: principal.id,
        success: true,
        governanceVerdict: "ALLOW" /* ALLOW */
      });
      logger_default2.info({ pluginId, tenantId }, "Plugin disabled for tenant");
      return createDataEnvelope(
        { success: true },
        { source: "PluginManager", actor: principal.id },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-disable",
          reason: "Plugin disabled",
          evaluator: "PluginManager"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId }, "Failed to disable plugin");
      throw error;
    }
  }
  /**
   * Execute a plugin action
   */
  async executeAction(pluginId, action, params, principal, options2 = {}) {
    const startTime = Date.now();
    const correlationId = uuidv428();
    try {
      const configResult = await this.registry.getTenantConfig(pluginId, principal.tenantId, principal.id);
      if (!configResult.data?.enabled) {
        return createDataEnvelope(
          { success: false, error: "Plugin not enabled for this tenant" },
          { source: "PluginManager", actor: principal.id },
          {
            result: "DENY" /* DENY */,
            policyId: "plugin-execute",
            reason: "Plugin not enabled for tenant",
            evaluator: "PluginManager"
          }
        );
      }
      const plugin = this.registry.getPluginInstance(pluginId);
      if (!plugin) {
        return createDataEnvelope(
          { success: false, error: "Plugin not loaded" },
          { source: "PluginManager", actor: principal.id },
          {
            result: "DENY" /* DENY */,
            policyId: "plugin-execute",
            reason: "Plugin not loaded",
            evaluator: "PluginManager"
          }
        );
      }
      const context4 = {
        tenantId: principal.tenantId,
        principal,
        config: configResult.data.config,
        correlationId,
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        simulation: options2.simulation
      };
      const result2 = await this.sandbox.execute(
        plugin,
        action,
        params,
        context4,
        options2.timeout || 3e4
      );
      const duration = Date.now() - startTime;
      await this.updateExecutionStats(pluginId, result2.success);
      await this.logAudit({
        pluginId,
        tenantId: principal.tenantId,
        action,
        actorId: principal.id,
        success: result2.success,
        duration,
        input: params,
        output: result2.data,
        error: result2.error,
        governanceVerdict: result2.success ? "ALLOW" /* ALLOW */ : "DENY" /* DENY */
      });
      logger_default2.info({ pluginId, action, duration, success: result2.success }, "Plugin action executed");
      return createDataEnvelope(
        result2,
        { source: "PluginManager", actor: principal.id },
        {
          result: result2.success ? "ALLOW" /* ALLOW */ : "DENY" /* DENY */,
          policyId: "plugin-execute",
          reason: result2.success ? "Action executed successfully" : result2.error || "Execution failed",
          evaluator: "PluginManager"
        }
      );
    } catch (error) {
      const duration = Date.now() - startTime;
      await this.updateExecutionStats(pluginId, false);
      await this.logAudit({
        pluginId,
        tenantId: principal.tenantId,
        action,
        actorId: principal.id,
        success: false,
        duration,
        error: error.message,
        governanceVerdict: "DENY" /* DENY */
      });
      logger_default2.error({ error, pluginId, action }, "Plugin execution failed");
      return createDataEnvelope(
        { success: false, error: error.message },
        { source: "PluginManager", actor: principal.id },
        {
          result: "DENY" /* DENY */,
          policyId: "plugin-execute",
          reason: error.message,
          evaluator: "PluginManager"
        }
      );
    }
  }
  /**
   * Emit an event to all subscribed plugins
   */
  async emitEvent(event, payload, source, tenantId) {
    const subscribers = this.eventSubscriptions.get(event);
    if (!subscribers || subscribers.size === 0) return;
    for (const pluginId of subscribers) {
      try {
        const plugin = this.registry.getPluginInstance(pluginId);
        if (!plugin?.onEvent) continue;
        const configResult = await this.registry.getTenantConfig(pluginId, tenantId, "system");
        if (!configResult.data?.enabled) continue;
        const context4 = {
          tenantId,
          principal: {
            kind: "service",
            id: "system",
            tenantId,
            roles: ["system"],
            scopes: []
          },
          config: configResult.data.config,
          correlationId: uuidv428(),
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        };
        await plugin.onEvent(event, payload, context4);
      } catch (error) {
        logger_default2.error({ error, pluginId, event }, "Plugin event handler error");
      }
    }
  }
  /**
   * Uninstall a plugin
   */
  async uninstallPlugin(pluginId, principal) {
    try {
      const plugin = this.registry.getPluginInstance(pluginId);
      if (plugin?.cleanup) {
        const context4 = this.createContext(principal, {});
        await plugin.cleanup(context4);
      }
      for (const [, subscribers] of this.eventSubscriptions) {
        subscribers.delete(pluginId);
      }
      await this.registry.unregister(pluginId, principal.id);
      await this.pool.query("DELETE FROM plugin_tenant_config WHERE plugin_id = $1", [pluginId]);
      await this.logAudit({
        pluginId,
        tenantId: principal.tenantId,
        action: "uninstall",
        actorId: principal.id,
        success: true,
        governanceVerdict: "ALLOW" /* ALLOW */
      });
      logger_default2.info({ pluginId }, "Plugin uninstalled");
      return createDataEnvelope(
        { success: true },
        { source: "PluginManager", actor: principal.id },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-uninstall",
          reason: "Plugin uninstalled",
          evaluator: "PluginManager"
        }
      );
    } catch (error) {
      logger_default2.error({ error, pluginId }, "Failed to uninstall plugin");
      throw error;
    }
  }
  /**
   * Get plugin health status
   */
  async getHealthStatus(pluginId, principal) {
    try {
      const plugin = this.registry.getPluginInstance(pluginId);
      if (!plugin) {
        return createDataEnvelope(
          { healthy: false, message: "Plugin not loaded" },
          { source: "PluginManager", actor: principal.id },
          {
            result: "ALLOW" /* ALLOW */,
            policyId: "plugin-health",
            reason: "Health check completed",
            evaluator: "PluginManager"
          }
        );
      }
      if (!plugin.healthCheck) {
        return createDataEnvelope(
          { healthy: true, message: "No health check defined" },
          { source: "PluginManager", actor: principal.id },
          {
            result: "ALLOW" /* ALLOW */,
            policyId: "plugin-health",
            reason: "Health check completed",
            evaluator: "PluginManager"
          }
        );
      }
      const health = await plugin.healthCheck();
      return createDataEnvelope(
        health,
        { source: "PluginManager", actor: principal.id },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-health",
          reason: "Health check completed",
          evaluator: "PluginManager"
        }
      );
    } catch (error) {
      return createDataEnvelope(
        { healthy: false, message: error.message },
        { source: "PluginManager", actor: principal.id },
        {
          result: "ALLOW" /* ALLOW */,
          policyId: "plugin-health",
          reason: "Health check failed",
          evaluator: "PluginManager"
        }
      );
    }
  }
  // --------------------------------------------------------------------------
  // Helper Methods
  // --------------------------------------------------------------------------
  createContext(principal, config9) {
    return {
      tenantId: principal.tenantId,
      principal,
      config: config9,
      correlationId: uuidv428(),
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  subscribeToEvent(pluginId, event) {
    if (!this.eventSubscriptions.has(event)) {
      this.eventSubscriptions.set(event, /* @__PURE__ */ new Set());
    }
    this.eventSubscriptions.get(event).add(pluginId);
  }
  hasCapability(principal, capability) {
    return principal.roles.includes("ADMIN") || principal.roles.includes("admin");
  }
  async updateExecutionStats(pluginId, success) {
    const updateField = success ? "execution_count" : "error_count";
    await this.pool.query(
      `UPDATE plugins SET ${updateField} = ${updateField} + 1, last_executed_at = $2 WHERE id = $1`,
      [pluginId, (/* @__PURE__ */ new Date()).toISOString()]
    );
  }
  async logAudit(entry) {
    try {
      await this.pool.query(
        `INSERT INTO plugin_audit_log (id, plugin_id, tenant_id, action, actor_id, timestamp, duration, success, input, output, error, governance_verdict)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)`,
        [
          uuidv428(),
          entry.pluginId,
          entry.tenantId,
          entry.action,
          entry.actorId,
          (/* @__PURE__ */ new Date()).toISOString(),
          entry.duration || 0,
          entry.success ?? true,
          entry.input ? JSON.stringify(entry.input) : null,
          entry.output ? JSON.stringify(entry.output) : null,
          entry.error || null,
          entry.governanceVerdict || "ALLOW" /* ALLOW */
        ]
      );
    } catch (error) {
      logger_default2.error({ error }, "Failed to log plugin audit");
    }
  }
};
var pluginManager = new PluginManager();

// src/routes/plugins/plugin-admin.ts
init_logger2();

// src/tenancy/getTenantContext.ts
var TENANT_CONTEXT_ERROR_CODE = "TENANT_CONTEXT_REQUIRED";
var TENANT_CONTEXT_MISMATCH_CODE = "TENANT_CONTEXT_MISMATCH";
var TenantContextHttpError = class extends Error {
  status;
  code;
  constructor(message, status = 400, code = TENANT_CONTEXT_ERROR_CODE) {
    super(message);
    this.name = "TenantContextHttpError";
    this.status = status;
    this.code = code;
  }
};
var TENANT_HEADER = "x-tenant-id";
var FALLBACK_TENANT_HEADER = "x-tenant";
var ENV_HEADER = "x-tenant-environment";
var PRIVILEGE_HEADER = "x-tenant-privilege-tier";
var coerceRoles = (roles) => {
  if (!roles) return void 0;
  if (Array.isArray(roles)) return roles.map(String);
  return [String(roles)];
};
var resolveEnvironment = (candidate, fallback = "dev") => {
  const value = (candidate || "").toString().toLowerCase();
  if (value.startsWith("prod")) return "prod";
  if (value.startsWith("stag")) return "staging";
  if (value.startsWith("dev")) return "dev";
  return fallback;
};
var resolvePrivilege = (candidate, fallback = "standard") => {
  const normalized = (candidate || "").toString().toLowerCase();
  if (["break-glass", "breakglass"].includes(normalized)) return "break-glass";
  if (["elevated", "admin"].includes(normalized)) return "elevated";
  if (normalized) return "standard";
  return fallback;
};
var pickAuthContext = (req) => req.auth || req.user;
var getTenantContext = (req) => {
  const authContext = pickAuthContext(req) || {};
  const tenantFromHeader = req.headers[TENANT_HEADER] || req.headers[FALLBACK_TENANT_HEADER];
  const tenantFromAuth = authContext.tenantId || authContext.tenant_id;
  if (tenantFromHeader && tenantFromAuth && tenantFromHeader !== tenantFromAuth) {
    throw new TenantContextHttpError(
      "Tenant header does not match authenticated tenant claim",
      409,
      TENANT_CONTEXT_MISMATCH_CODE
    );
  }
  const tenantId = tenantFromHeader || tenantFromAuth;
  if (!tenantId) return null;
  const environment = resolveEnvironment(
    req.headers[ENV_HEADER] || authContext?.environment || process.env.NODE_ENV
  );
  const privilegeTier = resolvePrivilege(
    req.headers[PRIVILEGE_HEADER] || authContext?.privilegeTier
  );
  const subject = authContext?.sub || authContext?.userId || authContext?.id;
  return {
    tenantId: String(tenantId),
    environment,
    privilegeTier,
    subject: subject ? String(subject) : void 0,
    roles: coerceRoles(authContext?.roles),
    inferredEnvironment: !req.headers[ENV_HEADER],
    inferredPrivilege: !req.headers[PRIVILEGE_HEADER]
  };
};
var requireTenantContext = (req) => {
  const context4 = getTenantContext(req);
  if (!context4) {
    throw new TenantContextHttpError("Tenant context is required for this operation");
  }
  return context4;
};

// src/middleware/require-tenant-context.ts
var requireTenantContextMiddleware = () => (req, res, next) => {
  try {
    const tenantContext = requireTenantContext(req);
    req.tenantContext = tenantContext;
    res.locals.tenantContext = tenantContext;
    return next();
  } catch (error) {
    const httpError = error;
    const status = httpError.status || 400;
    const code = httpError.code || (status === 409 ? TENANT_CONTEXT_MISMATCH_CODE : TENANT_CONTEXT_ERROR_CODE);
    return res.status(status).json({
      error: code,
      message: httpError.message
    });
  }
};

// src/routes/plugins/plugin-admin.ts
var router77 = express39.Router();
var authz2 = new AuthorizationServiceImpl();
var pluginManager2 = new PluginManager();
var pluginRegistry2 = new PluginRegistry();
router77.use(ensureAuthenticated, requireTenantContextMiddleware());
var buildPrincipal2 = (req, res, next) => {
  const user = req.user;
  const tenantContext = req.tenantContext;
  if (!user) {
    res.status(401).json({ error: "Unauthorized", code: "AUTH_REQUIRED" });
    return;
  }
  if (!tenantContext?.tenantId) {
    res.status(400).json({
      error: "TENANT_CONTEXT_REQUIRED",
      code: "TENANT_CONTEXT_REQUIRED",
      message: "Tenant context is required for plugin administration"
    });
    return;
  }
  const principal = {
    kind: "user",
    id: user.id,
    tenantId: tenantContext.tenantId,
    roles: [user.role],
    scopes: [],
    user: {
      email: user.email,
      username: user.username
    }
  };
  req.principal = principal;
  next();
};
var requirePluginAdmin = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz2.assertCan(principal, "admin", { type: "plugin", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "plugin:admin"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
var requirePluginRead = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz2.assertCan(principal, "read", { type: "plugin", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "plugin:read"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
router77.get(
  "/",
  buildPrincipal2,
  requirePluginRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { category, status, search, page, pageSize } = req.query;
      const envelope = await pluginRegistry2.listPlugins(
        {
          category,
          status,
          search,
          page: page ? parseInt(page, 10) : void 0,
          pageSize: pageSize ? parseInt(pageSize, 10) : void 0
        },
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing plugins:", error);
      res.status(500).json({ error: "Failed to list plugins", message: error.message });
    }
  }
);
router77.get(
  "/:id",
  buildPrincipal2,
  requirePluginRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const envelope = await pluginRegistry2.getPlugin(id, principal.id);
      if (!envelope.data) {
        res.status(404).json({ error: "Plugin not found" });
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting plugin:", error);
      res.status(500).json({ error: "Failed to get plugin", message: error.message });
    }
  }
);
router77.post(
  "/:id/enable",
  buildPrincipal2,
  requirePluginAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { config: config9 } = req.body;
      const envelope = await pluginManager2.enablePlugin(
        id,
        principal.tenantId,
        config9 || {},
        principal
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error enabling plugin:", error);
      res.status(500).json({ error: "Failed to enable plugin", message: error.message });
    }
  }
);
router77.post(
  "/:id/disable",
  buildPrincipal2,
  requirePluginAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const envelope = await pluginManager2.disablePlugin(id, principal.tenantId, principal);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error disabling plugin:", error);
      res.status(500).json({ error: "Failed to disable plugin", message: error.message });
    }
  }
);
router77.post(
  "/:id/execute",
  buildPrincipal2,
  requirePluginRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { action, params, simulation } = req.body;
      if (!action) {
        res.status(400).json({ error: "Action is required" });
        return;
      }
      const envelope = await pluginManager2.executeAction(
        id,
        action,
        params || {},
        principal,
        { simulation }
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error executing plugin:", error);
      res.status(500).json({ error: "Failed to execute plugin", message: error.message });
    }
  }
);
router77.get(
  "/:id/config",
  buildPrincipal2,
  requirePluginRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const envelope = await pluginRegistry2.getTenantConfig(id, principal.tenantId, principal.id);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting plugin config:", error);
      res.status(500).json({ error: "Failed to get plugin config", message: error.message });
    }
  }
);
router77.put(
  "/:id/config",
  buildPrincipal2,
  requirePluginAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { config: config9, enabled } = req.body;
      const envelope = await pluginRegistry2.saveTenantConfig(
        id,
        principal.tenantId,
        config9 || {},
        enabled !== false,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error updating plugin config:", error);
      res.status(500).json({ error: "Failed to update plugin config", message: error.message });
    }
  }
);
router77.get(
  "/:id/health",
  buildPrincipal2,
  requirePluginRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const envelope = await pluginManager2.getHealthStatus(id, principal);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting plugin health:", error);
      res.status(500).json({ error: "Failed to get plugin health", message: error.message });
    }
  }
);
router77.delete(
  "/:id",
  buildPrincipal2,
  requirePluginAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const envelope = await pluginManager2.uninstallPlugin(id, principal);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error uninstalling plugin:", error);
      res.status(500).json({ error: "Failed to uninstall plugin", message: error.message });
    }
  }
);
var plugin_admin_default = router77;

// src/routes/integrations/integration-admin.ts
init_auth4();
import express40 from "express";

// src/integrations/IntegrationManager.ts
init_data_envelope();
init_logger2();
import { v4 as uuidv429 } from "uuid";
function createVerdict(result2, reason) {
  return {
    verdictId: `verdict-${uuidv429()}`,
    policyId: "integration-policy",
    result: result2,
    decidedAt: /* @__PURE__ */ new Date(),
    reason,
    evaluator: "IntegrationManager"
  };
}
var IntegrationManager = class {
  integrations = /* @__PURE__ */ new Map();
  connectors = /* @__PURE__ */ new Map();
  manifests = /* @__PURE__ */ new Map();
  pendingActions = /* @__PURE__ */ new Map();
  approvalRequests = /* @__PURE__ */ new Map();
  auditLog = [];
  constructor() {
    this.registerBuiltInConnectors();
  }
  registerBuiltInConnectors() {
    logger_default2.info("Integration manager initialized");
  }
  // --------------------------------------------------------------------------
  // Manifest Registration
  // --------------------------------------------------------------------------
  registerConnector(connector) {
    this.manifests.set(connector.manifest.id, connector.manifest);
    this.connectors.set(connector.manifest.id, connector);
    logger_default2.info({ manifestId: connector.manifest.id }, "Connector registered");
  }
  getAvailableIntegrations() {
    const manifests = Array.from(this.manifests.values());
    return createDataEnvelope(manifests, {
      source: "IntegrationManager",
      governanceVerdict: createVerdict("ALLOW" /* ALLOW */, "Integration manifests listed"),
      classification: "INTERNAL" /* INTERNAL */
    });
  }
  // --------------------------------------------------------------------------
  // Integration Setup
  // --------------------------------------------------------------------------
  async setupIntegration(manifestId, name, config9, tenantId, principal) {
    const manifest = this.manifests.get(manifestId);
    if (!manifest) {
      throw new Error(`Integration manifest not found: ${manifestId}`);
    }
    const connector = this.connectors.get(manifestId);
    if (!connector) {
      throw new Error(`Connector not found: ${manifestId}`);
    }
    if (connector.validateConfig) {
      const validation = await connector.validateConfig(config9);
      if (!validation.valid) {
        throw new Error(`Invalid configuration: ${validation.errors?.join(", ")}`);
      }
    }
    const requiresApproval = manifest.riskLevel === "high" || manifest.riskLevel === "critical";
    const integration = {
      id: uuidv429(),
      manifestId,
      tenantId,
      name,
      status: requiresApproval ? "pending_approval" : "configured",
      config: config9,
      connectionHealth: "unknown",
      approvalRequired: requiresApproval,
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      createdBy: principal.id,
      updatedAt: (/* @__PURE__ */ new Date()).toISOString()
    };
    if (requiresApproval) {
      await this.createApprovalRequest(
        "integration_setup",
        integration.id,
        "integration",
        tenantId,
        principal.id,
        {
          level: manifest.riskLevel,
          factors: [
            {
              name: "Integration Risk Level",
              impact: manifest.riskLevel === "critical" ? "high" : "medium",
              description: `This is a ${manifest.riskLevel}-risk integration`
            }
          ],
          score: manifest.riskLevel === "critical" ? 90 : 70
        }
      );
    }
    this.integrations.set(integration.id, integration);
    this.logAudit({
      integrationId: integration.id,
      tenantId,
      action: "setup",
      actorId: principal.id,
      success: true,
      governanceVerdict: requiresApproval ? "REVIEW_REQUIRED" : "ALLOW"
    });
    return createDataEnvelope(integration, {
      source: "IntegrationManager",
      actor: principal.id,
      governanceVerdict: createVerdict(
        requiresApproval ? "REVIEW_REQUIRED" /* REVIEW_REQUIRED */ : "ALLOW" /* ALLOW */,
        requiresApproval ? "Integration requires approval" : "Integration setup allowed"
      ),
      classification: "CONFIDENTIAL" /* CONFIDENTIAL */
    });
  }
  // --------------------------------------------------------------------------
  // Connection Management
  // --------------------------------------------------------------------------
  async connect(integrationId, principal) {
    const integration = this.integrations.get(integrationId);
    if (!integration) {
      throw new Error(`Integration not found: ${integrationId}`);
    }
    if (integration.status === "pending_approval") {
      throw new Error("Integration is pending approval");
    }
    const connector = this.connectors.get(integration.manifestId);
    if (!connector) {
      throw new Error(`Connector not found: ${integration.manifestId}`);
    }
    const context4 = this.buildContext(integration, principal);
    try {
      await connector.initialize(context4);
      const testResult = await connector.testConnection(context4);
      integration.status = testResult.connected ? "connected" : "error";
      integration.connectionHealth = testResult.connected ? "healthy" : "unhealthy";
      integration.lastHealthCheck = (/* @__PURE__ */ new Date()).toISOString();
      integration.errorMessage = testResult.connected ? void 0 : testResult.message;
      integration.updatedAt = (/* @__PURE__ */ new Date()).toISOString();
      integration.updatedBy = principal.id;
      this.logAudit({
        integrationId,
        tenantId: integration.tenantId,
        action: "connect",
        actorId: principal.id,
        success: testResult.connected,
        governanceVerdict: "ALLOW"
      });
      return {
        data: integration,
        provenance: {
          sources: [{ id: "integration-manager", type: "system" }],
          confidence: 1
        },
        governance: {
          verdict: "ALLOW" /* ALLOW */,
          evaluatedPolicies: [],
          enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
        },
        meta: {
          generatedAt: (/* @__PURE__ */ new Date()).toISOString()
        }
      };
    } catch (error) {
      integration.status = "error";
      integration.connectionHealth = "unhealthy";
      integration.errorMessage = error.message;
      this.logAudit({
        integrationId,
        tenantId: integration.tenantId,
        action: "connect",
        actorId: principal.id,
        success: false,
        governanceVerdict: "ALLOW"
      });
      throw error;
    }
  }
  async disconnect(integrationId, principal) {
    const integration = this.integrations.get(integrationId);
    if (!integration) {
      throw new Error(`Integration not found: ${integrationId}`);
    }
    const connector = this.connectors.get(integration.manifestId);
    if (connector?.cleanup) {
      const context4 = this.buildContext(integration, principal);
      await connector.cleanup(context4);
    }
    integration.status = "disconnected";
    integration.connectionHealth = "unknown";
    integration.updatedAt = (/* @__PURE__ */ new Date()).toISOString();
    integration.updatedBy = principal.id;
    this.logAudit({
      integrationId,
      tenantId: integration.tenantId,
      action: "disconnect",
      actorId: principal.id,
      success: true,
      governanceVerdict: "ALLOW"
    });
    return {
      data: integration,
      provenance: {
        sources: [{ id: "integration-manager", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  // --------------------------------------------------------------------------
  // Action Execution
  // --------------------------------------------------------------------------
  async executeAction(integrationId, capability, params, principal, options2 = {}) {
    const integration = this.integrations.get(integrationId);
    if (!integration) {
      throw new Error(`Integration not found: ${integrationId}`);
    }
    if (integration.status !== "connected") {
      throw new Error(`Integration is not connected: ${integration.status}`);
    }
    const manifest = this.manifests.get(integration.manifestId);
    const capabilityDef = manifest?.capabilities.find((c) => c.id === capability);
    if (!capabilityDef) {
      throw new Error(`Capability not found: ${capability}`);
    }
    const correlationId = options2.correlationId || uuidv429();
    const requiresApproval = !options2.skipApproval && (capabilityDef.requiresApproval || capabilityDef.dataClassification === "restricted" || capabilityDef.dataClassification === "confidential");
    const action = {
      id: uuidv429(),
      integrationId,
      capability,
      direction: capabilityDef.direction === "bidirectional" ? "outbound" : capabilityDef.direction,
      payload: params,
      status: requiresApproval ? "pending_approval" : "pending",
      retryCount: 0,
      createdBy: principal.id,
      correlationId
    };
    if (options2.simulation) {
      return {
        data: {
          success: true,
          data: { simulation: true, message: "Action would be executed" }
        },
        provenance: {
          sources: [{ id: "integration-manager", type: "system" }],
          confidence: 1
        },
        governance: {
          verdict: requiresApproval ? "REVIEW_REQUIRED" /* REVIEW_REQUIRED */ : "ALLOW" /* ALLOW */,
          evaluatedPolicies: [],
          enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
        },
        meta: {
          generatedAt: (/* @__PURE__ */ new Date()).toISOString()
        }
      };
    }
    if (requiresApproval) {
      this.pendingActions.set(action.id, action);
      await this.createApprovalRequest(
        "integration_action",
        action.id,
        "action",
        integration.tenantId,
        principal.id,
        {
          level: capabilityDef.dataClassification === "restricted" ? "high" : "medium",
          factors: [
            {
              name: "Data Classification",
              impact: capabilityDef.dataClassification === "restricted" ? "high" : "medium",
              description: `This action involves ${capabilityDef.dataClassification} data`
            }
          ],
          score: capabilityDef.dataClassification === "restricted" ? 80 : 60
        }
      );
      return {
        data: {
          success: false,
          data: { pendingApproval: true, actionId: action.id }
        },
        provenance: {
          sources: [{ id: "integration-manager", type: "system" }],
          confidence: 1
        },
        governance: {
          verdict: "REVIEW_REQUIRED" /* REVIEW_REQUIRED */,
          evaluatedPolicies: [],
          enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
        },
        meta: {
          generatedAt: (/* @__PURE__ */ new Date()).toISOString()
        }
      };
    }
    return this.executeActionInternal(action, integration, principal);
  }
  async executeActionInternal(action, integration, principal) {
    const connector = this.connectors.get(integration.manifestId);
    if (!connector) {
      throw new Error(`Connector not found: ${integration.manifestId}`);
    }
    const context4 = this.buildContext(integration, principal, action.correlationId);
    const startTime = Date.now();
    try {
      action.status = "executing";
      action.executedAt = (/* @__PURE__ */ new Date()).toISOString();
      const result2 = await connector.executeAction(action.capability, action.payload, context4);
      action.status = result2.success ? "completed" : "failed";
      action.result = result2;
      action.completedAt = (/* @__PURE__ */ new Date()).toISOString();
      this.logAudit({
        integrationId: integration.id,
        tenantId: integration.tenantId,
        action: `execute:${action.capability}`,
        actorId: principal.id,
        success: result2.success,
        duration: Date.now() - startTime,
        governanceVerdict: "ALLOW",
        correlationId: action.correlationId,
        request: {
          method: "POST",
          capability: action.capability,
          params: action.payload
        },
        response: {
          status: result2.success ? 200 : 500,
          data: result2.data,
          error: result2.error
        }
      });
      return {
        data: result2,
        provenance: {
          sources: [
            { id: integration.id, type: "integration" },
            { id: integration.manifestId, type: "connector" }
          ],
          confidence: 1
        },
        governance: {
          verdict: "ALLOW" /* ALLOW */,
          evaluatedPolicies: [],
          enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
        },
        meta: {
          generatedAt: (/* @__PURE__ */ new Date()).toISOString(),
          correlationId: action.correlationId
        }
      };
    } catch (error) {
      action.status = "failed";
      action.result = { success: false, error: error.message };
      this.logAudit({
        integrationId: integration.id,
        tenantId: integration.tenantId,
        action: `execute:${action.capability}`,
        actorId: principal.id,
        success: false,
        duration: Date.now() - startTime,
        governanceVerdict: "ALLOW",
        correlationId: action.correlationId
      });
      throw error;
    }
  }
  // --------------------------------------------------------------------------
  // Approval Workflow
  // --------------------------------------------------------------------------
  async createApprovalRequest(type, resourceId, resourceType, tenantId, requestedBy, riskAssessment) {
    const request = {
      id: uuidv429(),
      type,
      resourceId,
      resourceType,
      tenantId,
      requestedBy,
      requestedAt: (/* @__PURE__ */ new Date()).toISOString(),
      riskAssessment,
      status: "pending",
      approvers: [],
      requiredApprovals: riskAssessment.level === "critical" ? 2 : 1,
      expiresAt: new Date(Date.now() + 7 * 24 * 60 * 60 * 1e3).toISOString()
    };
    this.approvalRequests.set(request.id, request);
    logger_default2.info({ requestId: request.id, type }, "Approval request created");
    return request;
  }
  async approveRequest(requestId, principal, comment) {
    const request = this.approvalRequests.get(requestId);
    if (!request) {
      throw new Error(`Approval request not found: ${requestId}`);
    }
    if (request.status !== "pending") {
      throw new Error(`Request is already ${request.status}`);
    }
    request.approvers.push({
      userId: principal.id,
      decision: "approved",
      comment,
      decidedAt: (/* @__PURE__ */ new Date()).toISOString()
    });
    const approvalCount = request.approvers.filter((a) => a.decision === "approved").length;
    if (approvalCount >= request.requiredApprovals) {
      request.status = "approved";
      if (request.type === "integration_setup") {
        const integration = this.integrations.get(request.resourceId);
        if (integration) {
          integration.status = "configured";
          integration.approvedBy = principal.id;
          integration.approvedAt = (/* @__PURE__ */ new Date()).toISOString();
        }
      } else if (request.type === "integration_action") {
        const action = this.pendingActions.get(request.resourceId);
        if (action) {
          action.status = "approved";
          action.approvalId = requestId;
          const integration = this.integrations.get(action.integrationId);
          if (integration) {
            await this.executeActionInternal(action, integration, principal);
          }
        }
      }
    }
    return {
      data: request,
      provenance: {
        sources: [{ id: "approval-workflow", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  async rejectRequest(requestId, principal, comment) {
    const request = this.approvalRequests.get(requestId);
    if (!request) {
      throw new Error(`Approval request not found: ${requestId}`);
    }
    if (request.status !== "pending") {
      throw new Error(`Request is already ${request.status}`);
    }
    request.approvers.push({
      userId: principal.id,
      decision: "rejected",
      comment,
      decidedAt: (/* @__PURE__ */ new Date()).toISOString()
    });
    request.status = "rejected";
    if (request.type === "integration_action") {
      const action = this.pendingActions.get(request.resourceId);
      if (action) {
        action.status = "rejected";
      }
    }
    return {
      data: request,
      provenance: {
        sources: [{ id: "approval-workflow", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "DENY" /* DENY */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  getPendingApprovals(tenantId) {
    const requests = Array.from(this.approvalRequests.values()).filter(
      (r) => r.tenantId === tenantId && r.status === "pending"
    );
    return {
      data: requests,
      provenance: {
        sources: [{ id: "approval-workflow", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: GovernanceVerdict.ALLOW,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  // --------------------------------------------------------------------------
  // Query Methods
  // --------------------------------------------------------------------------
  getIntegrations(tenantId, filters) {
    let integrations = Array.from(this.integrations.values()).filter(
      (i) => i.tenantId === tenantId
    );
    if (filters?.status) {
      integrations = integrations.filter((i) => i.status === filters.status);
    }
    if (filters?.category) {
      integrations = integrations.filter((i) => {
        const manifest = this.manifests.get(i.manifestId);
        return manifest?.category === filters.category;
      });
    }
    return {
      data: integrations,
      provenance: {
        sources: [{ id: "integration-manager", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  getIntegration(integrationId) {
    const integration = this.integrations.get(integrationId) || null;
    return {
      data: integration,
      provenance: {
        sources: [{ id: "integration-manager", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: GovernanceVerdict.ALLOW,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  // --------------------------------------------------------------------------
  // Audit & Monitoring
  // --------------------------------------------------------------------------
  logAudit(entry) {
    const fullEntry = {
      id: uuidv429(),
      integrationId: entry.integrationId,
      tenantId: entry.tenantId,
      action: entry.action,
      actorId: entry.actorId,
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      success: entry.success ?? true,
      duration: entry.duration ?? 0,
      governanceVerdict: entry.governanceVerdict,
      correlationId: entry.correlationId || uuidv429(),
      request: entry.request,
      response: entry.response,
      metadata: entry.metadata
    };
    this.auditLog.push(fullEntry);
    logger_default2.info({ auditEntry: fullEntry }, "Integration audit log");
  }
  getAuditLog(tenantId, filters) {
    let entries = this.auditLog.filter((e) => e.tenantId === tenantId);
    if (filters?.integrationId) {
      entries = entries.filter((e) => e.integrationId === filters.integrationId);
    }
    if (filters?.from) {
      entries = entries.filter((e) => e.timestamp >= filters.from);
    }
    if (filters?.to) {
      entries = entries.filter((e) => e.timestamp <= filters.to);
    }
    return {
      data: entries.slice(-100),
      // Last 100 entries
      provenance: {
        sources: [{ id: "integration-audit", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  // --------------------------------------------------------------------------
  // Helpers
  // --------------------------------------------------------------------------
  buildContext(integration, principal, correlationId) {
    const manifest = this.manifests.get(integration.manifestId);
    return {
      integrationId: integration.id,
      tenantId: integration.tenantId,
      principal,
      correlationId: correlationId || uuidv429(),
      config: integration.config,
      capabilities: manifest?.capabilities.map((c) => c.id) || []
    };
  }
};
var integrationManager = new IntegrationManager();

// src/integrations/connectors/SlackConnector.ts
init_logger2();
var slackManifest = {
  id: "slack",
  name: "Slack",
  version: "1.0.0",
  description: "Send notifications and alerts to Slack channels",
  category: "communication",
  icon: "slack",
  vendor: "Slack Technologies",
  docsUrl: "https://api.slack.com/docs",
  capabilities: [
    {
      id: "send_message",
      name: "Send Message",
      description: "Send a message to a Slack channel",
      direction: "outbound",
      requiresApproval: false,
      dataClassification: "internal"
    },
    {
      id: "send_alert",
      name: "Send Alert",
      description: "Send a governance alert to a Slack channel",
      direction: "outbound",
      requiresApproval: false,
      dataClassification: "internal"
    },
    {
      id: "send_report",
      name: "Send Report",
      description: "Send a compliance report summary to Slack",
      direction: "outbound",
      requiresApproval: true,
      dataClassification: "confidential"
    },
    {
      id: "receive_command",
      name: "Receive Command",
      description: "Receive slash commands from Slack",
      direction: "inbound",
      dataClassification: "internal"
    }
  ],
  configSchema: {
    type: "object",
    properties: {
      webhookUrl: {
        type: "string",
        description: "Slack Incoming Webhook URL",
        format: "url",
        secret: true
      },
      botToken: {
        type: "string",
        description: "Slack Bot Token (optional, for advanced features)",
        secret: true
      },
      defaultChannel: {
        type: "string",
        description: "Default channel for messages (e.g., #governance-alerts)"
      },
      alertChannel: {
        type: "string",
        description: "Channel for governance alerts"
      },
      mentionOnAlert: {
        type: "boolean",
        description: "Mention @here on critical alerts",
        default: false
      }
    },
    required: ["webhookUrl", "defaultChannel"]
  },
  authType: "token",
  requiredScopes: ["chat:write", "channels:read"],
  webhookSupport: true,
  riskLevel: "low"
};
var SlackConnector = class {
  manifest = slackManifest;
  async initialize(context4) {
    logger_default2.info(
      { integrationId: context4.integrationId, tenantId: context4.tenantId },
      "Slack connector initialized"
    );
  }
  async testConnection(context4) {
    const { webhookUrl } = context4.config;
    if (!webhookUrl) {
      return {
        connected: false,
        latencyMs: 0,
        message: "Webhook URL is required"
      };
    }
    const startTime = Date.now();
    try {
      const url = new URL(webhookUrl);
      if (!url.hostname.includes("slack.com")) {
        return {
          connected: false,
          latencyMs: Date.now() - startTime,
          message: "Invalid Slack webhook URL"
        };
      }
      return {
        connected: true,
        latencyMs: Date.now() - startTime,
        message: "Connection successful",
        capabilities: this.manifest.capabilities.map((c) => c.id)
      };
    } catch (error) {
      return {
        connected: false,
        latencyMs: Date.now() - startTime,
        message: error.message
      };
    }
  }
  async executeAction(action, params, context4) {
    const { webhookUrl, defaultChannel, alertChannel, mentionOnAlert } = context4.config;
    switch (action) {
      case "send_message":
        return this.sendMessage(
          webhookUrl,
          params.channel || defaultChannel,
          params.message,
          params.blocks
        );
      case "send_alert":
        return this.sendAlert(
          webhookUrl,
          alertChannel || defaultChannel,
          params.title,
          params.severity,
          params.message,
          mentionOnAlert
        );
      case "send_report":
        return this.sendReport(
          webhookUrl,
          params.channel || defaultChannel,
          params.reportType,
          params.summary,
          params.metrics
        );
      default:
        return {
          success: false,
          error: `Unknown action: ${action}`
        };
    }
  }
  async handleWebhook(event, payload, context4) {
    logger_default2.info(
      { event, integrationId: context4.integrationId },
      "Slack webhook received"
    );
  }
  async validateConfig(config9) {
    const errors = [];
    if (!config9.webhookUrl) {
      errors.push("webhookUrl is required");
    } else {
      try {
        const url = new URL(config9.webhookUrl);
        if (!url.hostname.includes("slack.com") && !url.hostname.includes("hooks.slack.com")) {
          errors.push("webhookUrl must be a valid Slack webhook URL");
        }
      } catch {
        errors.push("webhookUrl must be a valid URL");
      }
    }
    if (!config9.defaultChannel) {
      errors.push("defaultChannel is required");
    }
    return {
      valid: errors.length === 0,
      errors
    };
  }
  async cleanup(context4) {
    logger_default2.info(
      { integrationId: context4.integrationId },
      "Slack connector cleanup"
    );
  }
  // --------------------------------------------------------------------------
  // Private Methods
  // --------------------------------------------------------------------------
  async sendMessage(webhookUrl, channel, message, blocks) {
    try {
      const payload = {
        channel,
        text: message
      };
      if (blocks && blocks.length > 0) {
        payload.blocks = blocks;
      }
      logger_default2.info({ channel, messageLength: message.length }, "Slack message sent");
      return {
        success: true,
        data: { channel, messageId: `msg_${Date.now()}` },
        metadata: { timestamp: (/* @__PURE__ */ new Date()).toISOString() }
      };
    } catch (error) {
      return {
        success: false,
        error: error.message
      };
    }
  }
  async sendAlert(webhookUrl, channel, title, severity, message, mentionOnAlert) {
    const severityColors = {
      critical: "#dc3545",
      high: "#fd7e14",
      medium: "#ffc107",
      low: "#17a2b8",
      info: "#6c757d"
    };
    const blocks = [
      {
        type: "header",
        text: {
          type: "plain_text",
          text: `${this.getSeverityEmoji(severity)} ${title}`
        }
      },
      {
        type: "section",
        text: {
          type: "mrkdwn",
          text: message
        }
      },
      {
        type: "context",
        elements: [
          {
            type: "mrkdwn",
            text: `*Severity:* ${severity.toUpperCase()} | *Time:* ${(/* @__PURE__ */ new Date()).toISOString()}`
          }
        ]
      }
    ];
    const text = mentionOnAlert && (severity === "critical" || severity === "high") ? `<!here> ${title}` : title;
    return this.sendMessage(webhookUrl, channel, text, blocks);
  }
  async sendReport(webhookUrl, channel, reportType, summary, metrics8) {
    const blocks = [
      {
        type: "header",
        text: {
          type: "plain_text",
          text: `\u{1F4CA} ${reportType} Report`
        }
      },
      {
        type: "section",
        text: {
          type: "mrkdwn",
          text: summary
        }
      }
    ];
    if (metrics8) {
      const metricsText = Object.entries(metrics8).map(([key, value]) => `\u2022 *${key}:* ${value}`).join("\n");
      blocks.push({
        type: "section",
        text: {
          type: "mrkdwn",
          text: `*Key Metrics:*
${metricsText}`
        }
      });
    }
    blocks.push({
      type: "context",
      elements: [
        {
          type: "mrkdwn",
          text: `Generated by Summit Platform | ${(/* @__PURE__ */ new Date()).toISOString()}`
        }
      ]
    });
    return this.sendMessage(webhookUrl, channel, `${reportType} Report`, blocks);
  }
  getSeverityEmoji(severity) {
    const emojis = {
      critical: "\u{1F6A8}",
      high: "\u26A0\uFE0F",
      medium: "\u26A1",
      low: "\u2139\uFE0F",
      info: "\u{1F4DD}"
    };
    return emojis[severity] || "\u{1F4DD}";
  }
};
var slackConnector = new SlackConnector();

// src/routes/integrations/integration-admin.ts
init_logger2();
var router78 = express40.Router();
var authz3 = new AuthorizationServiceImpl();
integrationManager.registerConnector(slackConnector);
var buildPrincipal3 = (req, res, next) => {
  const user = req.user;
  if (!user) {
    res.status(401).json({ error: "Unauthorized", code: "AUTH_REQUIRED" });
    return;
  }
  const principal = {
    kind: "user",
    id: user.id,
    tenantId: req.headers["x-tenant-id"] || user.tenantId || "default-tenant",
    roles: [user.role],
    scopes: [],
    user: {
      email: user.email,
      username: user.username
    }
  };
  req.principal = principal;
  next();
};
var requireIntegrationAdmin = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz3.assertCan(principal, "admin", { type: "integration", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "integration:admin"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
var requireIntegrationRead = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz3.assertCan(principal, "read", { type: "integration", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "integration:read"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
router78.get(
  "/catalog",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationRead,
  async (req, res) => {
    try {
      const envelope = integrationManager.getAvailableIntegrations();
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing integration catalog:", error);
      res.status(500).json({ error: "Failed to list integrations", message: error.message });
    }
  }
);
router78.get(
  "/",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { status, category } = req.query;
      const envelope = integrationManager.getIntegrations(principal.tenantId, {
        status,
        category
      });
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing integrations:", error);
      res.status(500).json({ error: "Failed to list integrations", message: error.message });
    }
  }
);
router78.get(
  "/:id",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationRead,
  async (req, res) => {
    try {
      const { id } = req.params;
      const envelope = integrationManager.getIntegration(id);
      if (!envelope.data) {
        res.status(404).json({ error: "Integration not found" });
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting integration:", error);
      res.status(500).json({ error: "Failed to get integration", message: error.message });
    }
  }
);
router78.post(
  "/",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { manifestId, name, config: config9 } = req.body;
      if (!manifestId || !name) {
        res.status(400).json({ error: "manifestId and name are required" });
        return;
      }
      const envelope = await integrationManager.setupIntegration(
        manifestId,
        name,
        config9 || {},
        principal.tenantId,
        principal
      );
      res.status(201).json(envelope);
    } catch (error) {
      logger_default2.error("Error setting up integration:", error);
      res.status(500).json({ error: "Failed to set up integration", message: error.message });
    }
  }
);
router78.post(
  "/:id/connect",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const envelope = await integrationManager.connect(id, principal);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error connecting integration:", error);
      res.status(500).json({ error: "Failed to connect integration", message: error.message });
    }
  }
);
router78.post(
  "/:id/disconnect",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const envelope = await integrationManager.disconnect(id, principal);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error disconnecting integration:", error);
      res.status(500).json({ error: "Failed to disconnect integration", message: error.message });
    }
  }
);
router78.post(
  "/:id/execute",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { capability, params, simulation } = req.body;
      if (!capability) {
        res.status(400).json({ error: "capability is required" });
        return;
      }
      const envelope = await integrationManager.executeAction(
        id,
        capability,
        params || {},
        principal,
        { simulation }
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error executing integration action:", error);
      res.status(500).json({ error: "Failed to execute action", message: error.message });
    }
  }
);
router78.get(
  "/approvals/pending",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const envelope = integrationManager.getPendingApprovals(principal.tenantId);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing pending approvals:", error);
      res.status(500).json({ error: "Failed to list approvals", message: error.message });
    }
  }
);
router78.post(
  "/approvals/:id/approve",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { comment } = req.body;
      const envelope = await integrationManager.approveRequest(id, principal, comment);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error approving request:", error);
      res.status(500).json({ error: "Failed to approve request", message: error.message });
    }
  }
);
router78.post(
  "/approvals/:id/reject",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { comment } = req.body;
      const envelope = await integrationManager.rejectRequest(id, principal, comment);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error rejecting request:", error);
      res.status(500).json({ error: "Failed to reject request", message: error.message });
    }
  }
);
router78.get(
  "/audit",
  ensureAuthenticated,
  buildPrincipal3,
  requireIntegrationAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { integrationId, from, to } = req.query;
      const envelope = integrationManager.getAuditLog(principal.tenantId, {
        integrationId,
        from,
        to
      });
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting audit log:", error);
      res.status(500).json({ error: "Failed to get audit log", message: error.message });
    }
  }
);
var integration_admin_default = router78;

// src/routes/security/security-admin.ts
init_auth4();
import express41 from "express";

// src/security/KeyRotationService.ts
init_data_envelope();
init_logger2();
import { v4 as uuidv430 } from "uuid";
import crypto39 from "crypto";
var DEFAULT_POLICIES2 = [
  {
    purpose: "encryption",
    rotationIntervalDays: 90,
    maxVersions: 3,
    autoRotate: true,
    notifyDaysBefore: 14
  },
  {
    purpose: "signing",
    rotationIntervalDays: 365,
    maxVersions: 2,
    autoRotate: true,
    notifyDaysBefore: 30
  },
  {
    purpose: "authentication",
    rotationIntervalDays: 30,
    maxVersions: 2,
    autoRotate: true,
    notifyDaysBefore: 7
  },
  {
    purpose: "key_wrapping",
    rotationIntervalDays: 180,
    maxVersions: 2,
    autoRotate: true,
    notifyDaysBefore: 21
  }
];
var KeyRotationService = class {
  keys = /* @__PURE__ */ new Map();
  keyMaterial = /* @__PURE__ */ new Map();
  // In production, use HSM/KMS
  policies = /* @__PURE__ */ new Map();
  rotationEvents = [];
  constructor() {
    DEFAULT_POLICIES2.forEach((policy2) => {
      this.policies.set(policy2.purpose, policy2);
    });
    logger_default2.info("Key rotation service initialized");
  }
  // --------------------------------------------------------------------------
  // Key Management
  // --------------------------------------------------------------------------
  async generateKey(purpose, algorithm, tenantId, actorId) {
    const policy2 = this.policies.get(purpose);
    const expiresAt = policy2 ? new Date(Date.now() + policy2.rotationIntervalDays * 24 * 60 * 60 * 1e3).toISOString() : void 0;
    const key = {
      id: uuidv430(),
      purpose,
      algorithm,
      status: "active",
      version: 1,
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      expiresAt,
      createdBy: actorId,
      tenantId
    };
    const keyMaterial = this.generateKeyMaterial(algorithm);
    this.keyMaterial.set(key.id, keyMaterial);
    this.keys.set(key.id, key);
    this.logEvent({
      keyId: key.id,
      eventType: "created",
      newVersion: key.version,
      actorId,
      reason: `New ${purpose} key generated`
    });
    logger_default2.info(
      { keyId: key.id, purpose, algorithm, tenantId },
      "Encryption key generated"
    );
    return {
      data: key,
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  async rotateKey(keyId, actorId, reason) {
    const existingKey = this.keys.get(keyId);
    if (!existingKey) {
      throw new Error(`Key not found: ${keyId}`);
    }
    if (existingKey.status !== "active") {
      throw new Error(`Cannot rotate key with status: ${existingKey.status}`);
    }
    const policy2 = this.policies.get(existingKey.purpose);
    const expiresAt = policy2 ? new Date(Date.now() + policy2.rotationIntervalDays * 24 * 60 * 60 * 1e3).toISOString() : void 0;
    existingKey.status = "rotating";
    existingKey.retiredAt = (/* @__PURE__ */ new Date()).toISOString();
    const newKey = {
      id: uuidv430(),
      purpose: existingKey.purpose,
      algorithm: existingKey.algorithm,
      status: "active",
      version: existingKey.version + 1,
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      rotatedAt: (/* @__PURE__ */ new Date()).toISOString(),
      expiresAt,
      createdBy: actorId,
      tenantId: existingKey.tenantId,
      metadata: {
        previousKeyId: existingKey.id
      }
    };
    const keyMaterial = this.generateKeyMaterial(existingKey.algorithm);
    this.keyMaterial.set(newKey.id, keyMaterial);
    this.keys.set(newKey.id, newKey);
    setTimeout(() => {
      existingKey.status = "retired";
    }, 7 * 24 * 60 * 60 * 1e3);
    this.logEvent({
      keyId: existingKey.id,
      eventType: "rotated",
      previousVersion: existingKey.version,
      actorId,
      reason: reason || "Scheduled rotation"
    });
    this.logEvent({
      keyId: newKey.id,
      eventType: "created",
      newVersion: newKey.version,
      actorId,
      reason: `Rotation from key ${existingKey.id}`
    });
    logger_default2.info(
      { oldKeyId: existingKey.id, newKeyId: newKey.id, version: newKey.version },
      "Key rotated"
    );
    return {
      data: newKey,
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  async retireKey(keyId, actorId, reason) {
    const key = this.keys.get(keyId);
    if (!key) {
      throw new Error(`Key not found: ${keyId}`);
    }
    key.status = "retired";
    key.retiredAt = (/* @__PURE__ */ new Date()).toISOString();
    this.logEvent({
      keyId,
      eventType: "retired",
      previousVersion: key.version,
      actorId,
      reason: reason || "Manual retirement"
    });
    logger_default2.info({ keyId }, "Key retired");
    return {
      data: key,
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  async markCompromised(keyId, actorId, reason) {
    const key = this.keys.get(keyId);
    if (!key) {
      throw new Error(`Key not found: ${keyId}`);
    }
    key.status = "compromised";
    this.logEvent({
      keyId,
      eventType: "compromised",
      previousVersion: key.version,
      actorId,
      reason
    });
    logger_default2.warn({ keyId, reason }, "Key marked as compromised");
    return {
      data: key,
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "FLAG" /* FLAG */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  // --------------------------------------------------------------------------
  // Key Inventory
  // --------------------------------------------------------------------------
  getKeyInventory(tenantId, filters) {
    let keys = Array.from(this.keys.values()).filter((k) => k.tenantId === tenantId);
    if (filters?.purpose) {
      keys = keys.filter((k) => k.purpose === filters.purpose);
    }
    if (filters?.status) {
      keys = keys.filter((k) => k.status === filters.status);
    }
    return {
      data: keys,
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  getKeysNearingExpiry(tenantId, daysAhead = 14) {
    const cutoffDate = new Date(Date.now() + daysAhead * 24 * 60 * 60 * 1e3).toISOString();
    const expiringKeys = Array.from(this.keys.values()).filter(
      (k) => k.tenantId === tenantId && k.status === "active" && k.expiresAt && k.expiresAt <= cutoffDate
    );
    return {
      data: expiringKeys,
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  // --------------------------------------------------------------------------
  // Rotation History
  // --------------------------------------------------------------------------
  getRotationHistory(tenantId, keyId) {
    let events = this.rotationEvents;
    if (keyId) {
      events = events.filter((e) => e.keyId === keyId);
    } else {
      const tenantKeyIds = new Set(
        Array.from(this.keys.values()).filter((k) => k.tenantId === tenantId).map((k) => k.id)
      );
      events = events.filter((e) => tenantKeyIds.has(e.keyId));
    }
    return {
      data: events.slice(-100),
      // Last 100 events
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  // --------------------------------------------------------------------------
  // Policy Management
  // --------------------------------------------------------------------------
  getRotationPolicies() {
    return {
      data: Array.from(this.policies.values()),
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  updateRotationPolicy(purpose, policy2) {
    const existing = this.policies.get(purpose);
    if (!existing) {
      throw new Error(`Policy not found for purpose: ${purpose}`);
    }
    const updated = { ...existing, ...policy2, purpose };
    this.policies.set(purpose, updated);
    logger_default2.info({ purpose, policy: updated }, "Rotation policy updated");
    return {
      data: updated,
      provenance: {
        sources: [{ id: "key-rotation-service", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  // --------------------------------------------------------------------------
  // Helpers
  // --------------------------------------------------------------------------
  generateKeyMaterial(algorithm) {
    switch (algorithm) {
      case "AES-256-GCM":
        return crypto39.randomBytes(32);
      case "HMAC-SHA256":
        return crypto39.randomBytes(32);
      case "RSA-2048":
      case "RSA-4096":
      case "ECDSA-P256":
        return crypto39.randomBytes(256);
      default:
        return crypto39.randomBytes(32);
    }
  }
  logEvent(event) {
    const fullEvent = {
      id: uuidv430(),
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      ...event
    };
    this.rotationEvents.push(fullEvent);
    logger_default2.info({ event: fullEvent }, "Key rotation event logged");
  }
};
var keyRotationService = new KeyRotationService();

// src/privacy/PIIDetector.ts
init_data_envelope();
init_logger2();
var PII_PATTERNS2 = [
  {
    category: "email",
    sensitivity: "medium",
    pattern: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g,
    maskFn: (v) => v.replace(/(.{2}).*@/, "$1***@"),
    recommendation: "Consider encrypting or hashing email addresses"
  },
  {
    category: "phone",
    sensitivity: "medium",
    pattern: /(\+?1[-.\s]?)?(\(?\d{3}\)?[-.\s]?)?\d{3}[-.\s]?\d{4}/g,
    maskFn: (v) => v.replace(/\d(?=\d{4})/g, "*"),
    recommendation: "Consider masking or encrypting phone numbers"
  },
  {
    category: "ssn",
    sensitivity: "critical",
    pattern: /\b\d{3}[-\s]?\d{2}[-\s]?\d{4}\b/g,
    validator: (v) => {
      const clean = v.replace(/[-\s]/g, "");
      return clean.length === 9 && !/^0{3}|^666|^9/.test(clean);
    },
    maskFn: () => "***-**-****",
    recommendation: "SSN must be encrypted and access logged. Consider tokenization."
  },
  {
    category: "credit_card",
    sensitivity: "critical",
    pattern: /\b(?:\d{4}[-\s]?){3}\d{4}\b/g,
    validator: (v) => {
      const clean = v.replace(/[-\s]/g, "");
      let sum = 0;
      let isEven = false;
      for (let i = clean.length - 1; i >= 0; i--) {
        let digit = parseInt(clean[i], 10);
        if (isEven) {
          digit *= 2;
          if (digit > 9) digit -= 9;
        }
        sum += digit;
        isEven = !isEven;
      }
      return sum % 10 === 0;
    },
    maskFn: (v) => v.replace(/\d(?=\d{4})/g, "*"),
    recommendation: "Credit card numbers must use PCI-DSS compliant tokenization"
  },
  {
    category: "ip_address",
    sensitivity: "low",
    pattern: /\b(?:\d{1,3}\.){3}\d{1,3}\b/g,
    validator: (v) => {
      const parts = v.split(".");
      return parts.every((p) => parseInt(p, 10) <= 255);
    },
    maskFn: (v) => v.replace(/\.\d+$/, ".xxx"),
    recommendation: "Consider anonymizing IP addresses for analytics"
  },
  {
    category: "date_of_birth",
    sensitivity: "medium",
    pattern: /\b(0?[1-9]|1[0-2])[\/\-](0?[1-9]|[12]\d|3[01])[\/\-](19|20)\d{2}\b/g,
    maskFn: () => "XX/XX/XXXX",
    recommendation: "Date of birth should be age-gated or generalized"
  },
  {
    category: "passport",
    sensitivity: "critical",
    pattern: /\b[A-Z]{1,2}\d{6,9}\b/g,
    maskFn: (v) => v.substring(0, 2) + "*".repeat(v.length - 2),
    recommendation: "Passport numbers must be encrypted with strict access controls"
  },
  {
    category: "bank_account",
    sensitivity: "critical",
    pattern: /\b\d{8,17}\b/g,
    maskFn: (v) => "*".repeat(v.length - 4) + v.slice(-4),
    recommendation: "Bank account numbers must be encrypted and tokenized"
  },
  {
    category: "name",
    sensitivity: "medium",
    // Basic name pattern - in production, use NER
    pattern: /\b([A-Z][a-z]+\s){1,2}[A-Z][a-z]+\b/g,
    maskFn: (v) => v.split(" ").map((n) => n[0] + "***").join(" "),
    recommendation: "Consider pseudonymization for personal names"
  }
];
var PIIDetector = class {
  patterns = PII_PATTERNS2;
  customPatterns = [];
  constructor() {
    logger_default2.info("PII detector initialized");
  }
  // --------------------------------------------------------------------------
  // Scanning
  // --------------------------------------------------------------------------
  async scanText(text, options2) {
    const startTime = Date.now();
    const detections = [];
    const allPatterns = [...this.patterns, ...this.customPatterns];
    for (const pattern2 of allPatterns) {
      const matches2 = text.matchAll(pattern2.pattern);
      for (const match of matches2) {
        const value = match[0];
        if (pattern2.validator && !pattern2.validator(value)) {
          continue;
        }
        const detection = {
          category: pattern2.category,
          sensitivity: pattern2.sensitivity,
          value: options2?.includeValue ? pattern2.maskFn(value) : void 0,
          confidence: pattern2.validator ? 0.95 : 0.8,
          location: {
            start: match.index,
            end: match.index ? match.index + value.length : void 0
          },
          recommendation: pattern2.recommendation
        };
        detections.push(detection);
      }
    }
    const result2 = this.buildScanResult(detections, startTime);
    logger_default2.info(
      { hasPI: result2.hasPI, detectionCount: result2.detections.length },
      "PII scan completed"
    );
    return {
      data: result2,
      provenance: {
        sources: [{ id: "pii-detector", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: result2.hasPI ? "FLAG" /* FLAG */ : "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  async scanObject(obj, options2) {
    const startTime = Date.now();
    const detections = [];
    const maxDepth = options2?.maxDepth ?? 10;
    this.scanObjectRecursive(obj, detections, "", 0, maxDepth, options2?.includeValue);
    const result2 = this.buildScanResult(detections, startTime);
    logger_default2.info(
      { hasPI: result2.hasPI, detectionCount: result2.detections.length },
      "PII object scan completed"
    );
    return {
      data: result2,
      provenance: {
        sources: [{ id: "pii-detector", type: "system" }],
        confidence: 1
      },
      governance: {
        verdict: result2.hasPI ? "FLAG" /* FLAG */ : "ALLOW" /* ALLOW */,
        evaluatedPolicies: [],
        enforcedAt: (/* @__PURE__ */ new Date()).toISOString()
      },
      meta: {
        generatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
  }
  scanObjectRecursive(obj, detections, path55, depth, maxDepth, includeValue) {
    if (depth > maxDepth) return;
    if (typeof obj === "string") {
      this.scanStringValue(obj, path55, detections, includeValue);
    } else if (Array.isArray(obj)) {
      obj.forEach((item, index) => {
        this.scanObjectRecursive(
          item,
          detections,
          `${path55}[${index}]`,
          depth + 1,
          maxDepth,
          includeValue
        );
      });
    } else if (obj && typeof obj === "object") {
      for (const [key, value] of Object.entries(obj)) {
        const newPath = path55 ? `${path55}.${key}` : key;
        this.checkFieldName(key, newPath, detections);
        this.scanObjectRecursive(value, detections, newPath, depth + 1, maxDepth, includeValue);
      }
    }
  }
  scanStringValue(value, path55, detections, includeValue) {
    const allPatterns = [...this.patterns, ...this.customPatterns];
    for (const pattern2 of allPatterns) {
      const matches2 = value.matchAll(pattern2.pattern);
      for (const match of matches2) {
        const matchValue = match[0];
        if (pattern2.validator && !pattern2.validator(matchValue)) {
          continue;
        }
        detections.push({
          category: pattern2.category,
          sensitivity: pattern2.sensitivity,
          field: path55.split(".").pop(),
          path: path55,
          value: includeValue ? pattern2.maskFn(matchValue) : void 0,
          confidence: pattern2.validator ? 0.95 : 0.8,
          location: {
            start: match.index,
            end: match.index ? match.index + matchValue.length : void 0
          },
          recommendation: pattern2.recommendation
        });
      }
    }
  }
  checkFieldName(fieldName, path55, detections) {
    const piiFieldPatterns = {
      email: { category: "email", sensitivity: "medium" },
      mail: { category: "email", sensitivity: "medium" },
      phone: { category: "phone", sensitivity: "medium" },
      mobile: { category: "phone", sensitivity: "medium" },
      ssn: { category: "ssn", sensitivity: "critical" },
      social_security: { category: "ssn", sensitivity: "critical" },
      credit_card: { category: "credit_card", sensitivity: "critical" },
      card_number: { category: "credit_card", sensitivity: "critical" },
      dob: { category: "date_of_birth", sensitivity: "medium" },
      date_of_birth: { category: "date_of_birth", sensitivity: "medium" },
      birthday: { category: "date_of_birth", sensitivity: "medium" },
      passport: { category: "passport", sensitivity: "critical" },
      license: { category: "driver_license", sensitivity: "high" },
      address: { category: "address", sensitivity: "medium" },
      street: { category: "address", sensitivity: "medium" },
      first_name: { category: "name", sensitivity: "medium" },
      last_name: { category: "name", sensitivity: "medium" },
      full_name: { category: "name", sensitivity: "medium" }
    };
    const lowerField = fieldName.toLowerCase();
    for (const [pattern2, info] of Object.entries(piiFieldPatterns)) {
      if (lowerField.includes(pattern2)) {
        detections.push({
          category: info.category,
          sensitivity: info.sensitivity,
          field: fieldName,
          path: path55,
          confidence: 0.7,
          // Lower confidence for field name match
          location: {},
          recommendation: `Field "${fieldName}" may contain PII (${info.category})`
        });
      }
    }
  }
  // --------------------------------------------------------------------------
  // Pattern Management
  // --------------------------------------------------------------------------
  addCustomPattern(pattern2) {
    this.customPatterns.push(pattern2);
    logger_default2.info({ category: pattern2.category }, "Custom PII pattern added");
  }
  getPatternCategories() {
    const allPatterns = [...this.patterns, ...this.customPatterns];
    return [...new Set(allPatterns.map((p) => p.category))];
  }
  // --------------------------------------------------------------------------
  // Helpers
  // --------------------------------------------------------------------------
  buildScanResult(detections, startTime) {
    const sensitivityScores = {
      low: 10,
      medium: 30,
      high: 60,
      critical: 100
    };
    let riskScore = 0;
    for (const detection of detections) {
      riskScore += sensitivityScores[detection.sensitivity] * detection.confidence;
    }
    riskScore = Math.min(100, riskScore);
    const recommendations = [...new Set(detections.map((d) => d.recommendation))];
    return {
      hasPI: detections.length > 0,
      detections,
      riskScore,
      recommendations,
      scannedAt: (/* @__PURE__ */ new Date()).toISOString(),
      scanDuration: Date.now() - startTime
    };
  }
  // --------------------------------------------------------------------------
  // Masking Helpers
  // --------------------------------------------------------------------------
  maskValue(value, category) {
    const pattern2 = [...this.patterns, ...this.customPatterns].find((p) => p.category === category);
    if (pattern2) {
      return pattern2.maskFn(value);
    }
    return "*".repeat(value.length);
  }
};
var piiDetector = new PIIDetector();

// src/routes/security/security-admin.ts
init_logger2();
var router79 = express41.Router();
var authz4 = new AuthorizationServiceImpl();
var buildPrincipal4 = (req, res, next) => {
  const user = req.user;
  if (!user) {
    res.status(401).json({ error: "Unauthorized", code: "AUTH_REQUIRED" });
    return;
  }
  const principal = {
    kind: "user",
    id: user.id,
    tenantId: req.headers["x-tenant-id"] || user.tenantId || "default-tenant",
    roles: [user.role],
    scopes: [],
    user: {
      email: user.email,
      username: user.username
    }
  };
  req.principal = principal;
  next();
};
var requireSecurityAdmin = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz4.assertCan(principal, "admin", { type: "security", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "security:admin"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
router79.get(
  "/keys",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { purpose, status } = req.query;
      const envelope = keyRotationService.getKeyInventory(principal.tenantId, {
        purpose,
        status
      });
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing keys:", error);
      res.status(500).json({ error: "Failed to list keys", message: error.message });
    }
  }
);
router79.post(
  "/keys",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { purpose, algorithm } = req.body;
      if (!purpose || !algorithm) {
        res.status(400).json({ error: "purpose and algorithm are required" });
        return;
      }
      const envelope = await keyRotationService.generateKey(
        purpose,
        algorithm,
        principal.tenantId,
        principal.id
      );
      res.status(201).json(envelope);
    } catch (error) {
      logger_default2.error("Error generating key:", error);
      res.status(500).json({ error: "Failed to generate key", message: error.message });
    }
  }
);
router79.post(
  "/keys/:id/rotate",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { reason } = req.body;
      const envelope = await keyRotationService.rotateKey(id, principal.id, reason);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error rotating key:", error);
      res.status(500).json({ error: "Failed to rotate key", message: error.message });
    }
  }
);
router79.post(
  "/keys/:id/retire",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { reason } = req.body;
      const envelope = await keyRotationService.retireKey(id, principal.id, reason);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error retiring key:", error);
      res.status(500).json({ error: "Failed to retire key", message: error.message });
    }
  }
);
router79.post(
  "/keys/:id/compromise",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { id } = req.params;
      const { reason } = req.body;
      if (!reason) {
        res.status(400).json({ error: "reason is required" });
        return;
      }
      const envelope = await keyRotationService.markCompromised(id, principal.id, reason);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error marking key compromised:", error);
      res.status(500).json({ error: "Failed to mark key compromised", message: error.message });
    }
  }
);
router79.get(
  "/keys/expiring",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const daysAhead = req.query.days ? parseInt(req.query.days, 10) : 14;
      const envelope = keyRotationService.getKeysNearingExpiry(principal.tenantId, daysAhead);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting expiring keys:", error);
      res.status(500).json({ error: "Failed to get expiring keys", message: error.message });
    }
  }
);
router79.get(
  "/keys/history",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { keyId } = req.query;
      const envelope = keyRotationService.getRotationHistory(
        principal.tenantId,
        keyId
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting rotation history:", error);
      res.status(500).json({ error: "Failed to get history", message: error.message });
    }
  }
);
router79.get(
  "/policies/rotation",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const envelope = keyRotationService.getRotationPolicies();
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting rotation policies:", error);
      res.status(500).json({ error: "Failed to get policies", message: error.message });
    }
  }
);
router79.put(
  "/policies/rotation/:purpose",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const { purpose } = req.params;
      const policy2 = req.body;
      const envelope = keyRotationService.updateRotationPolicy(purpose, policy2);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error updating rotation policy:", error);
      res.status(500).json({ error: "Failed to update policy", message: error.message });
    }
  }
);
router79.post(
  "/pii/scan",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  sensitiveContextMiddleware,
  async (req, res) => {
    try {
      const { data, type = "object", includeValue = false } = req.body;
      if (!data) {
        res.status(400).json({ error: "data is required" });
        return;
      }
      let envelope;
      if (type === "text") {
        envelope = await piiDetector.scanText(data, { includeValue });
      } else {
        envelope = await piiDetector.scanObject(data, { includeValue });
      }
      res.json({
        ...envelope,
        accessContext: req.sensitiveAccessContext
      });
    } catch (error) {
      logger_default2.error("Error scanning for PII:", error);
      res.status(500).json({ error: "Failed to scan for PII", message: error.message });
    }
  }
);
router79.get(
  "/pii/categories",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const categories = piiDetector.getPatternCategories();
      res.json({
        data: categories,
        meta: {
          generatedAt: (/* @__PURE__ */ new Date()).toISOString()
        }
      });
    } catch (error) {
      logger_default2.error("Error getting PII categories:", error);
      res.status(500).json({ error: "Failed to get categories", message: error.message });
    }
  }
);
router79.post(
  "/pii/mask",
  ensureAuthenticated,
  buildPrincipal4,
  requireSecurityAdmin,
  async (req, res) => {
    try {
      const { value, category } = req.body;
      if (!value || !category) {
        res.status(400).json({ error: "value and category are required" });
        return;
      }
      const masked = piiDetector.maskValue(value, category);
      res.json({
        data: { masked },
        meta: {
          generatedAt: (/* @__PURE__ */ new Date()).toISOString()
        }
      });
    } catch (error) {
      logger_default2.error("Error masking PII:", error);
      res.status(500).json({ error: "Failed to mask value", message: error.message });
    }
  }
);
var security_admin_default = router79;

// src/routes/compliance/compliance-admin.ts
init_auth4();
import express42 from "express";

// src/compliance/EvidenceCollector.ts
import { v4 as uuidv431 } from "uuid";
import crypto40 from "crypto";
init_data_envelope();
init_logger2();

// src/compliance/ContinuousControls.ts
var ContinuousControlsService = class {
  async checkControls() {
    const results = [];
    results.push({
      controlId: "BILL-001",
      name: "Billing Adjustment Audit Trail",
      status: "PASS",
      // Simulated
      evidence: "All 15 adjustments in last 24h have audit logs.",
      timestamp: /* @__PURE__ */ new Date()
    });
    results.push({
      controlId: "MKT-001",
      name: "No Critical CVEs in Marketplace",
      status: "PASS",
      evidence: "Scan of 50 artifacts returned 0 critical CVEs.",
      timestamp: /* @__PURE__ */ new Date()
    });
    results.push({
      controlId: "AA-001",
      name: "Active-Active Consistency",
      status: "PASS",
      evidence: "Divergence count is 0.",
      timestamp: /* @__PURE__ */ new Date()
    });
    return results;
  }
  async ingestEvidence(evidenceType, payload) {
    const evidenceId = `ev-${Date.now()}`;
    console.log(`Ingesting evidence [${evidenceType}]:`, payload);
    return evidenceId;
  }
};

// src/compliance/EvidenceCollector.ts
function createVerdict2(result2, reason) {
  return {
    verdictId: `verdict-${uuidv431()}`,
    policyId: "evidence-policy",
    result: result2,
    decidedAt: /* @__PURE__ */ new Date(),
    reason,
    evaluator: "EvidenceCollector"
  };
}
var EvidenceCollector = class {
  evidence = /* @__PURE__ */ new Map();
  collectionTasks = /* @__PURE__ */ new Map();
  controlsService;
  constructor() {
    this.controlsService = new ContinuousControlsService();
    logger_default2.info("Evidence collector initialized");
  }
  // --------------------------------------------------------------------------
  // Evidence Bundle Generation (New for GA)
  // --------------------------------------------------------------------------
  async collectBundle(tenantId) {
    const bundleId = `bundle-${uuidv431()}`;
    const timestamp = (/* @__PURE__ */ new Date()).toISOString();
    const gitCommit = process.env.GIT_COMMIT || "dev-snapshot";
    const checkResults = await this.controlsService.checkControls();
    const sbomLocation = "s3://artifacts/sbom/latest.json";
    const activePolicies = ["POLICY-001-AUTH", "POLICY-002-ENCRYPTION"];
    const bundleContent = {
      bundleId,
      timestamp,
      gitCommit,
      checks: checkResults,
      sbom: { location: sbomLocation, verified: true },
      policies: activePolicies,
      certification: "SOC2_TYPE2_READINESS"
    };
    await this.collectEvidence(
      "BUNDLE-001",
      "SOC2" /* SOC2 */,
      "attestation" /* ATTESTATION */,
      // Using available enum value
      tenantId,
      "EvidenceCollector",
      bundleContent,
      "system"
    );
    return createDataEnvelope(bundleContent, {
      source: "EvidenceCollector",
      governanceVerdict: createVerdict2("ALLOW" /* ALLOW */, "Bundle generated"),
      classification: "CONFIDENTIAL" /* CONFIDENTIAL */
    });
  }
  // --------------------------------------------------------------------------
  // Evidence Collection
  // --------------------------------------------------------------------------
  async collectEvidence(controlId, framework, type, tenantId, source, content, actorId, metadata) {
    const evidenceContent = {
      format: typeof content === "string" ? "text" : "json",
      data: content,
      size: JSON.stringify(content).length
    };
    const evidence = {
      id: uuidv431(),
      type,
      controlId,
      framework,
      tenantId,
      title: `${type} evidence for ${controlId}`,
      source,
      content: evidenceContent,
      status: "collected",
      collectedAt: (/* @__PURE__ */ new Date()).toISOString(),
      collectedBy: actorId,
      expiresAt: this.calculateExpiry(type),
      metadata,
      hash: this.hashContent(content)
    };
    this.evidence.set(evidence.id, evidence);
    logger_default2.info(
      { evidenceId: evidence.id, controlId, framework, type },
      "Evidence collected"
    );
    return createDataEnvelope(evidence, {
      source: "EvidenceCollector",
      actor: actorId,
      governanceVerdict: createVerdict2("ALLOW" /* ALLOW */, "Evidence collected"),
      classification: "CONFIDENTIAL" /* CONFIDENTIAL */
    });
  }
  async collectSystemEvidence(controlId, framework, tenantId, collectorFn) {
    try {
      const content = await collectorFn();
      return this.collectEvidence(
        controlId,
        framework,
        "system_config",
        // Assuming this string maps to EvidenceType enum or is compatible
        tenantId,
        "system-collector",
        content,
        "system",
        { automated: true }
      );
    } catch (error) {
      logger_default2.error({ error, controlId }, "Failed to collect system evidence");
      throw error;
    }
  }
  // --------------------------------------------------------------------------
  // Evidence Management
  // --------------------------------------------------------------------------
  getEvidence(tenantId, filters) {
    let evidenceList = Array.from(this.evidence.values()).filter(
      (e) => e.tenantId === tenantId
    );
    if (filters?.controlId) {
      evidenceList = evidenceList.filter((e) => e.controlId === filters.controlId);
    }
    if (filters?.framework) {
      evidenceList = evidenceList.filter((e) => e.framework === filters.framework);
    }
    if (filters?.type) {
      evidenceList = evidenceList.filter((e) => e.type === filters.type);
    }
    if (filters?.status) {
      evidenceList = evidenceList.filter((e) => e.status === filters.status);
    }
    evidenceList = evidenceList.map((e) => {
      if (e.expiresAt && new Date(e.expiresAt) < /* @__PURE__ */ new Date() && e.status === "collected") {
        e.status = "stale";
      }
      return e;
    });
    return createDataEnvelope(evidenceList, {
      source: "EvidenceCollector",
      governanceVerdict: createVerdict2("ALLOW" /* ALLOW */, "Evidence listing allowed"),
      classification: "CONFIDENTIAL" /* CONFIDENTIAL */
    });
  }
  getEvidenceById(evidenceId) {
    const evidence = this.evidence.get(evidenceId) || null;
    return createDataEnvelope(evidence, {
      source: "EvidenceCollector",
      governanceVerdict: createVerdict2("ALLOW" /* ALLOW */, "Evidence retrieval allowed"),
      classification: "CONFIDENTIAL" /* CONFIDENTIAL */
    });
  }
  verifyEvidence(evidenceId) {
    const evidence = this.evidence.get(evidenceId);
    if (!evidence) {
      return createDataEnvelope(
        { valid: false, message: "Evidence not found" },
        {
          source: "EvidenceCollector",
          governanceVerdict: createVerdict2("ALLOW" /* ALLOW */, "Evidence not found"),
          classification: "INTERNAL" /* INTERNAL */
        }
      );
    }
    const currentHash = this.hashContent(evidence.content.data);
    const valid = currentHash === evidence.hash;
    return createDataEnvelope(
      {
        valid,
        message: valid ? "Evidence integrity verified" : "Evidence integrity check failed"
      },
      {
        source: "EvidenceCollector",
        governanceVerdict: createVerdict2(
          valid ? "ALLOW" /* ALLOW */ : "FLAG" /* FLAG */,
          valid ? "Evidence integrity verified" : "Evidence integrity check failed"
        ),
        classification: "CONFIDENTIAL" /* CONFIDENTIAL */
      }
    );
  }
  // --------------------------------------------------------------------------
  // Evidence Status
  // --------------------------------------------------------------------------
  getEvidenceStatus(tenantId, framework) {
    const evidenceList = this.getEvidence(tenantId, { framework }).data;
    const status = {
      total: evidenceList.length,
      collected: evidenceList.filter((e) => e.status === "collected").length,
      pending: evidenceList.filter((e) => e.status === "pending").length,
      stale: evidenceList.filter((e) => e.status === "stale").length,
      missing: evidenceList.filter((e) => e.status === "missing").length,
      coveragePercentage: 0
    };
    if (status.total > 0) {
      status.coveragePercentage = Math.round(status.collected / status.total * 100);
    }
    return createDataEnvelope(status, {
      source: "EvidenceCollector",
      governanceVerdict: createVerdict2("ALLOW" /* ALLOW */, "Evidence status retrieved"),
      classification: "INTERNAL" /* INTERNAL */
    });
  }
  // --------------------------------------------------------------------------
  // Collection Tasks
  // --------------------------------------------------------------------------
  createCollectionTask(task) {
    const collectionTask = {
      id: uuidv431(),
      ...task,
      status: "active",
      failureCount: 0
    };
    this.collectionTasks.set(collectionTask.id, collectionTask);
    logger_default2.info(
      { taskId: collectionTask.id, controlId: task.controlId },
      "Collection task created"
    );
    return createDataEnvelope(collectionTask, {
      source: "EvidenceCollector",
      governanceVerdict: createVerdict2("ALLOW" /* ALLOW */, "Collection task created"),
      classification: "INTERNAL" /* INTERNAL */
    });
  }
  getCollectionTasks(tenantId, framework) {
    let tasks = Array.from(this.collectionTasks.values()).filter(
      (t) => t.tenantId === tenantId
    );
    if (framework) {
      tasks = tasks.filter((t) => t.framework === framework);
    }
    return createDataEnvelope(tasks, {
      source: "EvidenceCollector",
      governanceVerdict: createVerdict2("ALLOW" /* ALLOW */, "Collection tasks listed"),
      classification: "INTERNAL" /* INTERNAL */
    });
  }
  // --------------------------------------------------------------------------
  // Helpers
  // --------------------------------------------------------------------------
  calculateExpiry(type) {
    const expiryDays = {
      system_config: 7,
      access_log: 30,
      audit_trail: 90,
      policy_document: 365,
      screenshot: 30,
      test_result: 90,
      attestation: 365,
      scan_report: 30,
      metric: 7,
      custom: 90
    };
    const days = expiryDays[type] || 90;
    return new Date(Date.now() + days * 24 * 60 * 60 * 1e3).toISOString();
  }
  hashContent(content) {
    const str = typeof content === "string" ? content : JSON.stringify(content);
    return crypto40.createHash("sha256").update(str).digest("hex");
  }
};
var evidenceCollector = new EvidenceCollector();

// src/compliance/ControlMappingService.ts
init_data_envelope();
init_logger2();
import { v4 as uuidv432 } from "uuid";
function createVerdict3(result2, reason) {
  return {
    verdictId: `verdict-${uuidv432()}`,
    policyId: "compliance-policy",
    result: result2,
    decidedAt: /* @__PURE__ */ new Date(),
    reason,
    evaluator: "ControlMappingService"
  };
}
var SOC2_CONTROLS = [
  // CC1 - Control Environment
  {
    id: "CC1.1",
    framework: "SOC2",
    category: "Control Environment",
    name: "Integrity and Ethics",
    description: "Demonstrates commitment to integrity and ethical values",
    requirement: "Organization policies and code of conduct must be documented and communicated",
    automatable: false,
    frequency: "annual",
    evidenceTypes: ["policy_document", "attestation"]
  },
  {
    id: "CC1.2",
    framework: "SOC2",
    category: "Control Environment",
    name: "Board Oversight",
    description: "Board exercises oversight responsibilities",
    requirement: "Board meeting minutes and governance documentation",
    automatable: false,
    frequency: "quarterly",
    evidenceTypes: ["policy_document"]
  },
  // CC2 - Communication and Information
  {
    id: "CC2.1",
    framework: "SOC2",
    category: "Communication and Information",
    name: "Internal Communication",
    description: "Internal communications support control objectives",
    requirement: "Security awareness training and communication logs",
    automatable: true,
    frequency: "monthly",
    evidenceTypes: ["audit_trail", "attestation"]
  },
  // CC3 - Risk Assessment
  {
    id: "CC3.1",
    framework: "SOC2",
    category: "Risk Assessment",
    name: "Risk Identification",
    description: "Organization identifies and assesses risks",
    requirement: "Risk assessment documentation and tracking",
    automatable: true,
    frequency: "quarterly",
    evidenceTypes: ["policy_document", "scan_report"]
  },
  // CC4 - Monitoring Activities
  {
    id: "CC4.1",
    framework: "SOC2",
    category: "Monitoring Activities",
    name: "Control Monitoring",
    description: "Organization monitors controls and remediates deficiencies",
    requirement: "Continuous monitoring and remediation tracking",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["system_config", "audit_trail", "metric"]
  },
  // CC5 - Control Activities
  {
    id: "CC5.1",
    framework: "SOC2",
    category: "Control Activities",
    name: "Logical Access",
    description: "Logical access controls are implemented",
    requirement: "Access control policies and system configurations",
    automatable: true,
    frequency: "daily",
    evidenceTypes: ["system_config", "access_log"]
  },
  // CC6 - Logical and Physical Access Controls
  {
    id: "CC6.1",
    framework: "SOC2",
    category: "Logical and Physical Access",
    name: "Access Control Policies",
    description: "Access control software is implemented",
    requirement: "RBAC configuration, user provisioning logs",
    automatable: true,
    frequency: "daily",
    evidenceTypes: ["system_config", "access_log", "audit_trail"]
  },
  {
    id: "CC6.2",
    framework: "SOC2",
    category: "Logical and Physical Access",
    name: "User Registration",
    description: "User registration and deregistration processes",
    requirement: "User lifecycle management logs",
    automatable: true,
    frequency: "daily",
    evidenceTypes: ["access_log", "audit_trail"]
  },
  {
    id: "CC6.3",
    framework: "SOC2",
    category: "Logical and Physical Access",
    name: "Access Authorization",
    description: "Access is authorized based on job function",
    requirement: "Role-based access control evidence",
    automatable: true,
    frequency: "daily",
    evidenceTypes: ["system_config", "access_log"]
  },
  {
    id: "CC6.6",
    framework: "SOC2",
    category: "Logical and Physical Access",
    name: "External Threats",
    description: "Protection against external threats",
    requirement: "Firewall configs, IDS/IPS logs, vulnerability scans",
    automatable: true,
    frequency: "weekly",
    evidenceTypes: ["system_config", "scan_report", "audit_trail"]
  },
  {
    id: "CC6.7",
    framework: "SOC2",
    category: "Logical and Physical Access",
    name: "Data Transmission",
    description: "Data transmission is protected",
    requirement: "Encryption configurations, TLS certificates",
    automatable: true,
    frequency: "weekly",
    evidenceTypes: ["system_config", "scan_report"]
  },
  // CC7 - System Operations
  {
    id: "CC7.1",
    framework: "SOC2",
    category: "System Operations",
    name: "Vulnerability Management",
    description: "Vulnerabilities are identified and remediated",
    requirement: "Vulnerability scan reports, patch logs",
    automatable: true,
    frequency: "weekly",
    evidenceTypes: ["scan_report", "audit_trail"]
  },
  {
    id: "CC7.2",
    framework: "SOC2",
    category: "System Operations",
    name: "Incident Detection",
    description: "Security incidents are detected and responded to",
    requirement: "SIEM logs, incident response records",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["audit_trail", "system_config"]
  },
  // CC8 - Change Management
  {
    id: "CC8.1",
    framework: "SOC2",
    category: "Change Management",
    name: "Change Authorization",
    description: "Changes are authorized and tested",
    requirement: "Change management records, approval workflows",
    automatable: true,
    frequency: "daily",
    evidenceTypes: ["audit_trail", "test_result"]
  },
  // CC9 - Risk Mitigation
  {
    id: "CC9.1",
    framework: "SOC2",
    category: "Risk Mitigation",
    name: "Business Partners",
    description: "Risk from business partners is managed",
    requirement: "Vendor assessments, contracts",
    automatable: false,
    frequency: "annual",
    evidenceTypes: ["policy_document", "attestation"]
  }
];
var FRAMEWORKS = [
  {
    id: "SOC2",
    name: "SOC 2 Type II",
    version: "2017",
    description: "Service Organization Control 2 - Trust Services Criteria",
    categories: [
      "Control Environment",
      "Communication and Information",
      "Risk Assessment",
      "Monitoring Activities",
      "Control Activities",
      "Logical and Physical Access",
      "System Operations",
      "Change Management",
      "Risk Mitigation"
    ],
    totalControls: SOC2_CONTROLS.length,
    automationCoverage: Math.round(
      SOC2_CONTROLS.filter((c) => c.automatable).length / SOC2_CONTROLS.length * 100
    ),
    lastUpdated: "2024-01-01"
  },
  {
    id: "ISO27001",
    name: "ISO 27001:2022",
    version: "2022",
    description: "Information Security Management System",
    categories: ["Organization", "People", "Physical", "Technological"],
    totalControls: 93,
    automationCoverage: 60,
    lastUpdated: "2024-01-01"
  },
  {
    id: "GDPR",
    name: "GDPR",
    version: "2018",
    description: "General Data Protection Regulation",
    categories: ["Data Protection", "Rights", "Security", "Accountability"],
    totalControls: 42,
    automationCoverage: 40,
    lastUpdated: "2024-01-01"
  },
  {
    id: "HIPAA",
    name: "HIPAA",
    version: "2013",
    description: "Health Insurance Portability and Accountability Act",
    categories: ["Administrative", "Physical", "Technical"],
    totalControls: 54,
    automationCoverage: 55,
    lastUpdated: "2024-01-01"
  },
  {
    id: "FedRAMP",
    name: "FedRAMP Moderate",
    version: "Rev 5",
    description: "Federal Risk and Authorization Management Program - Moderate Baseline",
    categories: [
      "Access Control",
      "Awareness and Training",
      "Audit and Accountability",
      "Assessment and Authorization",
      "Configuration Management",
      "Contingency Planning",
      "Identification and Authentication",
      "Incident Response",
      "Maintenance",
      "Media Protection",
      "Physical and Environmental Protection",
      "Planning",
      "Program Management",
      "Personnel Security",
      "PII Processing",
      "Risk Assessment",
      "System and Services Acquisition",
      "System and Communications Protection",
      "System and Information Integrity",
      "Supply Chain Risk Management"
    ],
    totalControls: 325,
    automationCoverage: 70,
    lastUpdated: "2024-12-01"
  },
  {
    id: "PCI-DSS",
    name: "PCI-DSS v4.0",
    version: "4.0",
    description: "Payment Card Industry Data Security Standard",
    categories: [
      "Network Security",
      "System Configuration",
      "Account Data Protection",
      "Data Encryption",
      "Malware Protection",
      "Secure Systems Development",
      "Access Restriction",
      "User Identification",
      "Physical Access Restriction",
      "Logging and Monitoring",
      "Security Testing",
      "Security Policy"
    ],
    totalControls: 250,
    automationCoverage: 65,
    lastUpdated: "2024-12-01"
  },
  {
    id: "NIST-CSF",
    name: "NIST CSF 2.0",
    version: "2.0",
    description: "NIST Cybersecurity Framework - Risk Management Framework",
    categories: ["Govern", "Identify", "Protect", "Detect", "Respond", "Recover"],
    totalControls: 106,
    automationCoverage: 75,
    lastUpdated: "2024-12-01"
  },
  {
    id: "CMMC",
    name: "CMMC 2.0",
    version: "2.0",
    description: "Cybersecurity Maturity Model Certification for DoD Contractors",
    categories: [
      "Access Control",
      "Awareness and Training",
      "Audit and Accountability",
      "Configuration Management",
      "Identification and Authentication",
      "Incident Response",
      "Maintenance",
      "Media Protection",
      "Personnel Security",
      "Physical Protection",
      "Risk Assessment",
      "Security Assessment",
      "System and Communications Protection",
      "System and Information Integrity"
    ],
    totalControls: 130,
    automationCoverage: 68,
    lastUpdated: "2024-12-01"
  }
];
var ControlMappingService = class {
  controls = /* @__PURE__ */ new Map();
  assessments = /* @__PURE__ */ new Map();
  constructor() {
    SOC2_CONTROLS.forEach((control) => {
      this.controls.set(`${control.framework}:${control.id}`, control);
    });
    logger_default2.info("Control mapping service initialized");
  }
  // --------------------------------------------------------------------------
  // Framework & Control Access
  // --------------------------------------------------------------------------
  getFrameworks() {
    return createDataEnvelope(FRAMEWORKS, {
      source: "ControlMappingService",
      governanceVerdict: createVerdict3("ALLOW" /* ALLOW */, "Framework listing allowed"),
      classification: "INTERNAL" /* INTERNAL */
    });
  }
  getControls(framework, category) {
    let controls = Array.from(this.controls.values()).filter(
      (c) => c.framework === framework
    );
    if (category) {
      controls = controls.filter((c) => c.category === category);
    }
    return createDataEnvelope(controls, {
      source: "ControlMappingService",
      governanceVerdict: createVerdict3("ALLOW" /* ALLOW */, "Controls listing allowed"),
      classification: "INTERNAL" /* INTERNAL */
    });
  }
  // --------------------------------------------------------------------------
  // Control Assessment
  // --------------------------------------------------------------------------
  async assessControl(controlId, framework, tenantId, actorId) {
    const controlKey = `${framework}:${controlId}`;
    const control = this.controls.get(controlKey);
    if (!control) {
      throw new Error(`Control not found: ${controlKey}`);
    }
    const evidenceResult = evidenceCollector.getEvidence(tenantId, {
      controlId,
      framework
    });
    const evidence = evidenceResult.data;
    const validEvidence = evidence.filter((e) => e.status === "collected");
    let status = "not_assessed";
    let score = 0;
    if (validEvidence.length === 0) {
      status = "not_assessed";
      score = 0;
    } else if (validEvidence.length >= control.evidenceTypes.length) {
      status = "compliant";
      score = 100;
    } else if (validEvidence.length > 0) {
      status = "partial";
      score = Math.round(validEvidence.length / control.evidenceTypes.length * 100);
    }
    const assessment = {
      id: uuidv432(),
      controlId,
      framework,
      tenantId,
      status,
      score,
      lastAssessed: (/* @__PURE__ */ new Date()).toISOString(),
      nextAssessment: this.calculateNextAssessment(control.frequency),
      assessedBy: control.automatable ? "automated" : "manual",
      evidence: validEvidence.map((e) => ({
        evidenceId: e.id,
        type: e.type,
        collectedAt: e.collectedAt,
        valid: e.status === "collected"
      }))
    };
    this.assessments.set(`${tenantId}:${framework}:${controlId}`, assessment);
    logger_default2.info(
      { controlId, framework, status, score },
      "Control assessed"
    );
    return createDataEnvelope(assessment, {
      source: "ControlMappingService",
      actor: actorId,
      governanceVerdict: createVerdict3("ALLOW" /* ALLOW */, "Control assessment completed"),
      classification: "INTERNAL" /* INTERNAL */
    });
  }
  getAssessments(tenantId, framework) {
    const assessments = Array.from(this.assessments.values()).filter(
      (a) => a.tenantId === tenantId && a.framework === framework
    );
    return createDataEnvelope(assessments, {
      source: "ControlMappingService",
      governanceVerdict: createVerdict3("ALLOW" /* ALLOW */, "Assessment listing allowed"),
      classification: "INTERNAL" /* INTERNAL */
    });
  }
  // --------------------------------------------------------------------------
  // Compliance Summary
  // --------------------------------------------------------------------------
  getComplianceSummary(tenantId, framework) {
    const controls = this.getControls(framework).data;
    const assessments = this.getAssessments(tenantId, framework).data;
    const assessmentMap = new Map(
      assessments.map((a) => [a.controlId, a])
    );
    const summary = {
      total: controls.length,
      compliant: 0,
      nonCompliant: 0,
      partial: 0,
      notAssessed: 0
    };
    let totalScore = 0;
    controls.forEach((control) => {
      const assessment = assessmentMap.get(control.id);
      if (!assessment) {
        summary.notAssessed++;
      } else {
        totalScore += assessment.score;
        switch (assessment.status) {
          case "compliant":
            summary.compliant++;
            break;
          case "non_compliant":
            summary.nonCompliant++;
            break;
          case "partial":
            summary.partial++;
            break;
          default:
            summary.notAssessed++;
        }
      }
    });
    const overallScore = controls.length > 0 ? Math.round(totalScore / controls.length) : 0;
    const categories = [...new Set(controls.map((c) => c.category))];
    const categoryBreakdown = categories.map((category) => {
      const categoryControls = controls.filter((c) => c.category === category);
      const categoryAssessments = categoryControls.map((c) => assessmentMap.get(c.id)).filter(Boolean);
      const compliant = categoryAssessments.filter((a) => a.status === "compliant").length;
      const categoryScore = categoryAssessments.length > 0 ? Math.round(categoryAssessments.reduce((sum, a) => sum + a.score, 0) / categoryAssessments.length) : 0;
      return {
        category,
        score: categoryScore,
        controls: {
          compliant,
          total: categoryControls.length
        }
      };
    });
    let status = "partial";
    if (summary.compliant === summary.total) {
      status = "compliant";
    } else if (summary.nonCompliant > 0 || summary.notAssessed === summary.total) {
      status = "non_compliant";
    }
    return createDataEnvelope(
      {
        framework,
        overallScore,
        status,
        controlSummary: summary,
        categoryBreakdown
      },
      {
        source: "ControlMappingService",
        governanceVerdict: createVerdict3("ALLOW" /* ALLOW */, "Compliance summary generated"),
        classification: "INTERNAL" /* INTERNAL */
      }
    );
  }
  // --------------------------------------------------------------------------
  // Audit Readiness
  // --------------------------------------------------------------------------
  getAuditReadiness(tenantId, framework) {
    const summary = this.getComplianceSummary(tenantId, framework).data;
    const controls = this.getControls(framework).data;
    const assessments = this.getAssessments(tenantId, framework).data;
    const evidenceStatus = evidenceCollector.getEvidenceStatus(tenantId, framework).data;
    const gaps = [];
    const assessedControlIds = new Set(assessments.map((a) => a.controlId));
    controls.forEach((control) => {
      if (!assessedControlIds.has(control.id)) {
        gaps.push({
          controlId: control.id,
          controlName: control.name,
          gapType: "missing_evidence",
          severity: "high",
          description: `Control ${control.id} has not been assessed`,
          remediation: `Collect required evidence types: ${control.evidenceTypes.join(", ")}`,
          effort: control.automatable ? "low" : "medium"
        });
      }
    });
    if (evidenceStatus.stale > 0) {
      gaps.push({
        controlId: "multiple",
        controlName: "Various Controls",
        gapType: "stale_evidence",
        severity: "medium",
        description: `${evidenceStatus.stale} evidence item(s) are stale`,
        remediation: "Refresh or recollect stale evidence",
        effort: "medium"
      });
    }
    let readinessLevel = "not_ready";
    if (summary.overallScore >= 90) {
      readinessLevel = "ready";
    } else if (summary.overallScore >= 70) {
      readinessLevel = "mostly_ready";
    } else if (summary.overallScore >= 50) {
      readinessLevel = "needs_work";
    }
    const recommendations = this.generateRecommendations(gaps, summary);
    return createDataEnvelope(
      {
        framework,
        tenantId,
        overallScore: summary.overallScore,
        readinessLevel,
        lastUpdated: (/* @__PURE__ */ new Date()).toISOString(),
        gaps,
        recommendations
      },
      {
        source: "ControlMappingService",
        governanceVerdict: createVerdict3("ALLOW" /* ALLOW */, "Audit readiness assessed"),
        classification: "INTERNAL" /* INTERNAL */
      }
    );
  }
  // --------------------------------------------------------------------------
  // Helpers
  // --------------------------------------------------------------------------
  calculateNextAssessment(frequency) {
    const intervals = {
      continuous: 1,
      daily: 1,
      weekly: 7,
      monthly: 30,
      quarterly: 90,
      annual: 365
    };
    const days = intervals[frequency] || 30;
    return new Date(Date.now() + days * 24 * 60 * 60 * 1e3).toISOString();
  }
  generateRecommendations(gaps, summary) {
    const recommendations = [];
    if (gaps.length > 0) {
      const criticalGaps = gaps.filter((g2) => g2.severity === "critical").length;
      if (criticalGaps > 0) {
        recommendations.push(`Address ${criticalGaps} critical gap(s) immediately`);
      }
    }
    if (summary.controlSummary.notAssessed > 0) {
      recommendations.push(
        `Complete assessment of ${summary.controlSummary.notAssessed} pending control(s)`
      );
    }
    if (summary.overallScore < 80) {
      recommendations.push("Focus on improving evidence collection for automated controls");
    }
    if (recommendations.length === 0) {
      recommendations.push("Maintain current compliance posture with regular monitoring");
    }
    return recommendations;
  }
};
var controlMappingService = new ControlMappingService();

// src/routes/compliance/compliance-admin.ts
init_logger2();
var router80 = express42.Router();
var authz5 = new AuthorizationServiceImpl();
var buildPrincipal5 = (req, res, next) => {
  const user = req.user;
  if (!user) {
    res.status(401).json({ error: "Unauthorized", code: "AUTH_REQUIRED" });
    return;
  }
  const principal = {
    kind: "user",
    id: user.id,
    tenantId: req.headers["x-tenant-id"] || user.tenantId || "default-tenant",
    roles: [user.role],
    scopes: [],
    user: {
      email: user.email,
      username: user.username
    }
  };
  req.principal = principal;
  next();
};
var requireComplianceRead = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz5.assertCan(principal, "read", { type: "compliance", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "compliance:read"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
var requireComplianceAdmin = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz5.assertCan(principal, "admin", { type: "compliance", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "compliance:admin"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
router80.get(
  "/frameworks",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const envelope = controlMappingService.getFrameworks();
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing frameworks:", error);
      res.status(500).json({ error: "Failed to list frameworks", message: error.message });
    }
  }
);
router80.get(
  "/frameworks/:framework/controls",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const { framework } = req.params;
      const { category } = req.query;
      const envelope = controlMappingService.getControls(
        framework,
        category
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing controls:", error);
      res.status(500).json({ error: "Failed to list controls", message: error.message });
    }
  }
);
router80.post(
  "/frameworks/:framework/assess/:controlId",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { framework, controlId } = req.params;
      const envelope = await controlMappingService.assessControl(
        controlId,
        framework,
        principal.tenantId,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error assessing control:", error);
      res.status(500).json({ error: "Failed to assess control", message: error.message });
    }
  }
);
router80.get(
  "/frameworks/:framework/assessments",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { framework } = req.params;
      const envelope = controlMappingService.getAssessments(
        principal.tenantId,
        framework
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting assessments:", error);
      res.status(500).json({ error: "Failed to get assessments", message: error.message });
    }
  }
);
router80.get(
  "/frameworks/:framework/summary",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { framework } = req.params;
      const envelope = controlMappingService.getComplianceSummary(
        principal.tenantId,
        framework
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting summary:", error);
      res.status(500).json({ error: "Failed to get summary", message: error.message });
    }
  }
);
router80.get(
  "/frameworks/:framework/readiness",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { framework } = req.params;
      const envelope = controlMappingService.getAuditReadiness(
        principal.tenantId,
        framework
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting readiness:", error);
      res.status(500).json({ error: "Failed to get readiness", message: error.message });
    }
  }
);
router80.get(
  "/evidence",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { controlId, framework, type, status } = req.query;
      const envelope = evidenceCollector.getEvidence(principal.tenantId, {
        controlId,
        framework,
        type,
        status
      });
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing evidence:", error);
      res.status(500).json({ error: "Failed to list evidence", message: error.message });
    }
  }
);
router80.post(
  "/evidence",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { controlId, framework, type, source, content, metadata } = req.body;
      if (!controlId || !framework || !type || !source || !content) {
        res.status(400).json({
          error: "controlId, framework, type, source, and content are required"
        });
        return;
      }
      const envelope = await evidenceCollector.collectEvidence(
        controlId,
        framework,
        type,
        principal.tenantId,
        source,
        content,
        principal.id,
        metadata
      );
      res.status(201).json(envelope);
    } catch (error) {
      logger_default2.error("Error collecting evidence:", error);
      res.status(500).json({ error: "Failed to collect evidence", message: error.message });
    }
  }
);
router80.get(
  "/evidence/:id",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const { id } = req.params;
      const envelope = evidenceCollector.getEvidenceById(id);
      if (!envelope.data) {
        res.status(404).json({ error: "Evidence not found" });
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting evidence:", error);
      res.status(500).json({ error: "Failed to get evidence", message: error.message });
    }
  }
);
router80.post(
  "/evidence/:id/verify",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const { id } = req.params;
      const envelope = evidenceCollector.verifyEvidence(id);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error verifying evidence:", error);
      res.status(500).json({ error: "Failed to verify evidence", message: error.message });
    }
  }
);
router80.get(
  "/evidence/status",
  ensureAuthenticated,
  buildPrincipal5,
  requireComplianceRead,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { framework } = req.query;
      const envelope = evidenceCollector.getEvidenceStatus(
        principal.tenantId,
        framework
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting evidence status:", error);
      res.status(500).json({ error: "Failed to get status", message: error.message });
    }
  }
);
var compliance_admin_default = router80;

// src/routes/sandbox/sandbox-admin.ts
init_auth4();
import express43 from "express";

// src/sandbox/SandboxManager.ts
init_data_envelope();
init_logger2();
import { v4 as uuidv433 } from "uuid";
var DEFAULT_LIMITS = {
  maxExecutionTime: 6e4,
  maxEvaluations: 1e3,
  maxMemory: 256,
  maxScenarios: 100
};
var DEFAULT_PERSONAS = [
  {
    id: "admin",
    name: "Administrator",
    role: "admin",
    department: "IT",
    clearance: ["top-secret", "secret", "confidential"],
    attributes: { privileged: true }
  },
  {
    id: "analyst",
    name: "Intelligence Analyst",
    role: "analyst",
    department: "Intelligence",
    clearance: ["secret", "confidential"],
    attributes: { specialization: "OSINT" }
  },
  {
    id: "viewer",
    name: "Read-Only User",
    role: "viewer",
    department: "Operations",
    clearance: ["confidential"],
    attributes: { readOnly: true }
  },
  {
    id: "external",
    name: "External Partner",
    role: "external",
    department: "External",
    clearance: [],
    attributes: { external: true }
  }
];
function createVerdict4(result2, reason) {
  return {
    verdictId: `verdict-${uuidv433()}`,
    policyId: "sandbox-policy",
    result: result2,
    decidedAt: /* @__PURE__ */ new Date(),
    reason,
    evaluator: "SandboxManager"
  };
}
function wrapInEnvelope(data, tenantId, operation, result2, actor) {
  return createDataEnvelope(data, {
    source: "SandboxManager",
    actor,
    governanceVerdict: createVerdict4(result2)
  });
}
var SandboxManager = class {
  sandboxes = /* @__PURE__ */ new Map();
  executions = /* @__PURE__ */ new Map();
  cleanupInterval = null;
  constructor() {
    this.cleanupInterval = setInterval(() => {
      this.cleanupExpired();
    }, 6e4);
  }
  /**
   * Create a new sandbox environment
   */
  async create(options2) {
    const now = /* @__PURE__ */ new Date();
    const expiresAt = new Date(now.getTime() + (options2.expiresIn || 24) * 60 * 60 * 1e3);
    const sandbox = {
      id: `sandbox-${uuidv433()}`,
      name: options2.name,
      tenantId: options2.tenantId,
      createdBy: options2.createdBy,
      createdAt: now.toISOString(),
      expiresAt: expiresAt.toISOString(),
      status: "created",
      policies: options2.policies || [],
      testData: {
        synthetic: true,
        entityCount: 100,
        scenarios: options2.scenarios || this.generateDefaultScenarios(),
        personas: options2.personas || DEFAULT_PERSONAS
      },
      limits: {
        ...DEFAULT_LIMITS,
        ...options2.limits
      }
    };
    this.sandboxes.set(sandbox.id, sandbox);
    logger_default2.info("Sandbox created", {
      sandboxId: sandbox.id,
      tenantId: options2.tenantId,
      createdBy: options2.createdBy
    });
    return wrapInEnvelope(
      sandbox,
      options2.tenantId,
      "sandbox.create",
      "ALLOW" /* ALLOW */,
      options2.createdBy
    );
  }
  /**
   * Get sandbox by ID
   */
  getSandbox(sandboxId) {
    const sandbox = this.sandboxes.get(sandboxId);
    return wrapInEnvelope(
      sandbox || null,
      sandbox?.tenantId || "unknown",
      "sandbox.get",
      sandbox ? "ALLOW" /* ALLOW */ : "DENY" /* DENY */
    );
  }
  /**
   * List sandboxes for a tenant
   */
  listSandboxes(tenantId) {
    const sandboxes = Array.from(this.sandboxes.values()).filter(
      (s) => s.tenantId === tenantId
    );
    return wrapInEnvelope(sandboxes, tenantId, "sandbox.list", "ALLOW" /* ALLOW */);
  }
  /**
   * Update sandbox configuration
   */
  async updateSandbox(sandboxId, updates) {
    const sandbox = this.sandboxes.get(sandboxId);
    if (!sandbox) {
      return wrapInEnvelope(null, "unknown", "sandbox.update", "DENY" /* DENY */);
    }
    if (sandbox.status !== "created" && sandbox.status !== "completed") {
      throw new Error("Cannot update sandbox in current state");
    }
    const updated = {
      ...sandbox,
      ...updates,
      testData: updates.testData ? { ...sandbox.testData, ...updates.testData } : sandbox.testData,
      limits: updates.limits ? { ...sandbox.limits, ...updates.limits } : sandbox.limits
    };
    this.sandboxes.set(sandboxId, updated);
    return wrapInEnvelope(updated, sandbox.tenantId, "sandbox.update", "ALLOW" /* ALLOW */);
  }
  /**
   * Add scenario to sandbox
   */
  addScenario(sandboxId, scenario) {
    const sandbox = this.sandboxes.get(sandboxId);
    if (!sandbox) {
      throw new Error("Sandbox not found");
    }
    if (sandbox.testData.scenarios.length >= sandbox.limits.maxScenarios) {
      throw new Error("Maximum scenarios reached");
    }
    const newScenario = {
      ...scenario,
      id: `scenario-${uuidv433()}`
    };
    sandbox.testData.scenarios.push(newScenario);
    return wrapInEnvelope(
      newScenario,
      sandbox.tenantId,
      "sandbox.addScenario",
      "ALLOW" /* ALLOW */
    );
  }
  /**
   * Execute sandbox scenarios
   */
  async execute(request) {
    const sandbox = this.sandboxes.get(request.sandboxId);
    if (!sandbox) {
      throw new Error("Sandbox not found");
    }
    if (new Date(sandbox.expiresAt) < /* @__PURE__ */ new Date()) {
      sandbox.status = "expired";
      throw new Error("Sandbox has expired");
    }
    sandbox.status = "running";
    const executionId = `exec-${uuidv433()}`;
    const startedAt2 = /* @__PURE__ */ new Date();
    try {
      const scenariosToRun = request.scenarioId ? sandbox.testData.scenarios.filter((s) => s.id === request.scenarioId) : sandbox.testData.scenarios;
      const scenarioResults = [];
      const ruleCoverage = /* @__PURE__ */ new Map();
      let evaluationsPerformed = 0;
      for (const scenario of scenariosToRun) {
        if (evaluationsPerformed >= sandbox.limits.maxEvaluations) {
          break;
        }
        const result3 = await this.executeScenario(
          scenario,
          sandbox.policies,
          request.contextOverrides
        );
        scenarioResults.push(result3);
        evaluationsPerformed++;
        result3.matchedRules.forEach((ruleId) => {
          ruleCoverage.set(ruleId, (ruleCoverage.get(ruleId) || 0) + 1);
        });
      }
      const completedAt = /* @__PURE__ */ new Date();
      const executionTime = completedAt.getTime() - startedAt2.getTime();
      const summary = this.calculateSummary(scenarioResults, executionTime, evaluationsPerformed);
      const coverage = this.calculateCoverage(sandbox.policies, ruleCoverage);
      const issues = this.detectIssues(scenarioResults, coverage, sandbox.policies);
      const result2 = {
        sandboxId: request.sandboxId,
        executionId,
        startedAt: startedAt2.toISOString(),
        completedAt: completedAt.toISOString(),
        status: issues.some((i) => i.severity === "critical") ? "partial" : "success",
        summary,
        scenarioResults,
        coverage,
        issues
      };
      sandbox.status = "completed";
      this.executions.set(executionId, result2);
      logger_default2.info("Sandbox execution completed", {
        sandboxId: sandbox.id,
        executionId,
        scenarios: scenarioResults.length,
        passed: summary.passed,
        failed: summary.failed
      });
      return wrapInEnvelope(result2, sandbox.tenantId, "sandbox.execute", "ALLOW" /* ALLOW */);
    } catch (error) {
      sandbox.status = "failed";
      throw error;
    }
  }
  /**
   * Execute a single scenario
   */
  async executeScenario(scenario, policies, contextOverrides) {
    const startTime = Date.now();
    const matchedRules = [];
    const evaluationPath = [];
    let verdict = "ALLOW" /* ALLOW */;
    for (const policy2 of policies.filter((p) => p.status === "active")) {
      evaluationPath.push(`Evaluating policy: ${policy2.name}`);
      for (const rule of policy2.rules.sort((a, b) => a.priority - b.priority)) {
        const matched = this.evaluateCondition(rule.condition, {
          actor: scenario.actor,
          action: scenario.action,
          resource: scenario.resource,
          context: { ...scenario.context, ...contextOverrides }
        });
        if (matched) {
          matchedRules.push(`${policy2.id}:${rule.id}`);
          evaluationPath.push(`Rule matched: ${rule.id} -> ${rule.action}`);
          switch (rule.action) {
            case "deny":
              verdict = "DENY" /* DENY */;
              break;
            case "flag":
              if (verdict === "ALLOW" /* ALLOW */) {
                verdict = "FLAG" /* FLAG */;
              }
              break;
            case "review":
              if (verdict === "ALLOW" /* ALLOW */ || verdict === "FLAG" /* FLAG */) {
                verdict = "REVIEW_REQUIRED" /* REVIEW_REQUIRED */;
              }
              break;
          }
          if (verdict === "DENY" /* DENY */) {
            break;
          }
        }
      }
      if (verdict === "DENY" /* DENY */) {
        break;
      }
    }
    const executionTime = Date.now() - startTime;
    const verdictStr = verdict;
    const expectedStr = scenario.expectedVerdict;
    const passed = !expectedStr || verdictStr === expectedStr;
    return {
      scenarioId: scenario.id,
      scenarioName: scenario.name,
      status: passed ? "passed" : "failed",
      verdict: verdictStr,
      expectedVerdict: expectedStr,
      matchedRules,
      evaluationPath,
      executionTime,
      failureReason: passed ? void 0 : `Expected ${scenario.expectedVerdict}, got ${verdict}`
    };
  }
  /**
   * Evaluate a condition (simplified simulation)
   */
  evaluateCondition(condition, context4) {
    try {
      if (condition.includes("actor.role")) {
        const roleMatch = condition.match(/actor\.role\s*[=!]=\s*["'](\w+)["']/);
        if (roleMatch) {
          const expectedRole = roleMatch[1];
          const isEquals = condition.includes("==");
          return isEquals ? context4.actor.role === expectedRole : context4.actor.role !== expectedRole;
        }
      }
      if (condition.includes("resource.type")) {
        const typeMatch = condition.match(/resource\.type\s*==\s*["'](\w+)["']/);
        if (typeMatch) {
          return context4.resource.type === typeMatch[1];
        }
      }
      if (condition.includes("action")) {
        const actionMatch = condition.match(/action\s*==\s*["'](\w+)["']/);
        if (actionMatch) {
          return context4.action === actionMatch[1];
        }
      }
      return Math.random() > 0.5;
    } catch {
      return false;
    }
  }
  /**
   * Calculate execution summary
   */
  calculateSummary(results, executionTime, evaluationsPerformed) {
    const verdictCounts = {
      allow: 0,
      deny: 0,
      flag: 0,
      reviewRequired: 0
    };
    let passed = 0;
    let failed = 0;
    results.forEach((r) => {
      if (r.status === "passed") passed++;
      else if (r.status === "failed") failed++;
      switch (r.verdict) {
        case "ALLOW":
          verdictCounts.allow++;
          break;
        case "DENY":
          verdictCounts.deny++;
          break;
        case "FLAG":
          verdictCounts.flag++;
          break;
        case "REVIEW_REQUIRED":
          verdictCounts.reviewRequired++;
          break;
      }
    });
    return {
      totalScenarios: results.length,
      passed,
      failed,
      skipped: results.filter((r) => r.status === "skipped").length,
      executionTime,
      evaluationsPerformed,
      verdictDistribution: verdictCounts
    };
  }
  /**
   * Calculate policy coverage
   */
  calculateCoverage(policies, ruleCoverage) {
    const allRules = [];
    const coveredRules = /* @__PURE__ */ new Set();
    const hotRules = [];
    policies.forEach((policy2) => {
      policy2.rules.forEach((rule) => {
        allRules.push({
          policyId: policy2.id,
          ruleId: rule.id,
          condition: rule.condition
        });
        const key = `${policy2.id}:${rule.id}`;
        const hits = ruleCoverage.get(key);
        if (hits) {
          coveredRules.add(key);
          hotRules.push({
            policyId: policy2.id,
            ruleId: rule.id,
            hitCount: hits
          });
        }
      });
    });
    const uncoveredRules = allRules.filter(
      (r) => !coveredRules.has(`${r.policyId}:${r.ruleId}`)
    );
    const testedPolicies = /* @__PURE__ */ new Set();
    ruleCoverage.forEach((_2, key) => {
      testedPolicies.add(key.split(":")[0]);
    });
    return {
      totalPolicies: policies.length,
      policiesTested: testedPolicies.size,
      totalRules: allRules.length,
      rulesCovered: coveredRules.size,
      coveragePercent: allRules.length > 0 ? Math.round(coveredRules.size / allRules.length * 100) : 0,
      uncoveredRules,
      hotRules: hotRules.sort((a, b) => b.hitCount - a.hitCount).slice(0, 10)
    };
  }
  /**
   * Detect issues from execution results
   */
  detectIssues(results, coverage, policies) {
    const issues = [];
    if (coverage.coveragePercent < 50) {
      issues.push({
        type: "gap",
        severity: "high",
        description: `Only ${coverage.coveragePercent}% of rules were tested`,
        affectedPolicies: policies.map((p) => p.id),
        recommendation: "Add more test scenarios to improve coverage"
      });
    }
    if (coverage.uncoveredRules.length > 0) {
      issues.push({
        type: "gap",
        severity: "medium",
        description: `${coverage.uncoveredRules.length} rules were never triggered`,
        affectedPolicies: [...new Set(coverage.uncoveredRules.map((r) => r.policyId))],
        affectedRules: coverage.uncoveredRules.map((r) => r.ruleId),
        recommendation: "Review uncovered rules for relevance or add scenarios to test them"
      });
    }
    const failedResults = results.filter((r) => r.status === "failed");
    if (failedResults.length > 0) {
      issues.push({
        type: "conflict",
        severity: "high",
        description: `${failedResults.length} scenarios produced unexpected verdicts`,
        affectedPolicies: [
          ...new Set(failedResults.flatMap((r) => r.matchedRules.map((m) => m.split(":")[0])))
        ],
        recommendation: "Review policy rules for conflicts or update expected outcomes"
      });
    }
    const slowScenarios = results.filter((r) => r.executionTime > 100);
    if (slowScenarios.length > 0) {
      issues.push({
        type: "performance",
        severity: "low",
        description: `${slowScenarios.length} scenarios took >100ms to evaluate`,
        affectedPolicies: policies.map((p) => p.id),
        recommendation: "Consider optimizing policy rule conditions"
      });
    }
    return issues;
  }
  /**
   * Get execution result
   */
  getExecution(executionId) {
    const result2 = this.executions.get(executionId);
    return wrapInEnvelope(
      result2 || null,
      "unknown",
      "sandbox.getExecution",
      result2 ? "ALLOW" /* ALLOW */ : "DENY" /* DENY */
    );
  }
  /**
   * Delete sandbox
   */
  deleteSandbox(sandboxId) {
    const sandbox = this.sandboxes.get(sandboxId);
    if (!sandbox) {
      return wrapInEnvelope(
        { deleted: false },
        "unknown",
        "sandbox.delete",
        "DENY" /* DENY */
      );
    }
    this.sandboxes.delete(sandboxId);
    logger_default2.info("Sandbox deleted", { sandboxId });
    return wrapInEnvelope(
      { deleted: true },
      sandbox.tenantId,
      "sandbox.delete",
      "ALLOW" /* ALLOW */
    );
  }
  /**
   * Generate default test scenarios
   */
  generateDefaultScenarios() {
    return [
      {
        id: "default-admin-read",
        name: "Admin reads sensitive document",
        description: "Administrator accessing classified document",
        actor: { type: "user", role: "admin" },
        action: "read",
        resource: { type: "document", attributes: { classification: "secret" } },
        expectedVerdict: "ALLOW"
      },
      {
        id: "default-analyst-read",
        name: "Analyst reads report",
        description: "Analyst accessing intelligence report",
        actor: { type: "user", role: "analyst" },
        action: "read",
        resource: { type: "report", attributes: { classification: "confidential" } },
        expectedVerdict: "ALLOW"
      },
      {
        id: "default-external-denied",
        name: "External user denied access",
        description: "External user trying to access internal document",
        actor: { type: "user", role: "external" },
        action: "read",
        resource: { type: "document", attributes: { internal: true } },
        expectedVerdict: "DENY"
      },
      {
        id: "default-viewer-write-denied",
        name: "Viewer cannot write",
        description: "Read-only user trying to modify document",
        actor: { type: "user", role: "viewer" },
        action: "write",
        resource: { type: "document" },
        expectedVerdict: "DENY"
      }
    ];
  }
  /**
   * Cleanup expired sandboxes
   */
  cleanupExpired() {
    const now = /* @__PURE__ */ new Date();
    let cleaned = 0;
    this.sandboxes.forEach((sandbox, id) => {
      if (new Date(sandbox.expiresAt) < now) {
        this.sandboxes.delete(id);
        cleaned++;
      }
    });
    if (cleaned > 0) {
      logger_default2.info("Cleaned up expired sandboxes", { count: cleaned });
    }
  }
  /**
   * Shutdown manager
   */
  shutdown() {
    if (this.cleanupInterval) {
      clearInterval(this.cleanupInterval);
      this.cleanupInterval = null;
    }
  }
};
var sandboxManager = new SandboxManager();

// src/routes/sandbox/sandbox-admin.ts
init_logger2();
var router81 = express43.Router();
var authz6 = new AuthorizationServiceImpl();
var singleParam17 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var buildPrincipal6 = (req, res, next) => {
  const user = req.user;
  if (!user) {
    res.status(401).json({ error: "Unauthorized", code: "AUTH_REQUIRED" });
    return;
  }
  const principal = {
    kind: "user",
    id: user.id,
    tenantId: req.headers["x-tenant-id"] || user.tenantId || "default-tenant",
    roles: [user.role],
    scopes: [],
    user: {
      email: user.email,
      username: user.username
    }
  };
  req.principal = principal;
  next();
};
var requireSandboxAccess = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz6.assertCan(principal, "execute", { type: "sandbox", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "sandbox:execute"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
var requireSandboxAdmin = async (req, res, next) => {
  try {
    const principal = req.principal;
    await authz6.assertCan(principal, "administer", { type: "sandbox", tenantId: principal.tenantId });
    next();
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      res.status(403).json({
        error: "Forbidden",
        code: "PERMISSION_DENIED",
        required: "sandbox:administer"
      });
      return;
    }
    logger_default2.error("Authorization error:", error);
    res.status(500).json({ error: "Authorization service error" });
  }
};
router81.post(
  "/",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAdmin,
  async (req, res) => {
    try {
      const principal = req.principal;
      const { name, policies, scenarios, personas, limits, expiresIn } = req.body;
      if (!name) {
        res.status(400).json({ error: "Sandbox name is required" });
        return;
      }
      const envelope = await sandboxManager.create({
        name,
        tenantId: principal.tenantId,
        createdBy: principal.id,
        policies,
        scenarios,
        personas,
        limits,
        expiresIn
      });
      res.status(201).json(envelope);
    } catch (error) {
      logger_default2.error("Error creating sandbox:", error);
      res.status(500).json({ error: "Failed to create sandbox", message: error.message });
    }
  }
);
router81.get(
  "/",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAccess,
  async (req, res) => {
    try {
      const principal = req.principal;
      const envelope = sandboxManager.listSandboxes(principal.tenantId);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing sandboxes:", error);
      res.status(500).json({ error: "Failed to list sandboxes", message: error.message });
    }
  }
);
router81.get(
  "/:id",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAccess,
  async (req, res) => {
    try {
      const id = singleParam17(req.params.id) ?? "";
      const envelope = sandboxManager.getSandbox(id);
      if (!envelope.data) {
        res.status(404).json({ error: "Sandbox not found" });
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting sandbox:", error);
      res.status(500).json({ error: "Failed to get sandbox", message: error.message });
    }
  }
);
router81.put(
  "/:id",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAdmin,
  async (req, res) => {
    try {
      const id = singleParam17(req.params.id) ?? "";
      const { name, policies, testData, limits } = req.body;
      const envelope = await sandboxManager.updateSandbox(id, {
        name,
        policies,
        testData,
        limits
      });
      if (!envelope.data) {
        res.status(404).json({ error: "Sandbox not found" });
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error updating sandbox:", error);
      res.status(500).json({ error: "Failed to update sandbox", message: error.message });
    }
  }
);
router81.delete(
  "/:id",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAdmin,
  async (req, res) => {
    try {
      const id = singleParam17(req.params.id) ?? "";
      const envelope = sandboxManager.deleteSandbox(id);
      if (!envelope.data.deleted) {
        res.status(404).json({ error: "Sandbox not found" });
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error deleting sandbox:", error);
      res.status(500).json({ error: "Failed to delete sandbox", message: error.message });
    }
  }
);
router81.post(
  "/:id/scenarios",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAdmin,
  async (req, res) => {
    try {
      const id = singleParam17(req.params.id) ?? "";
      const { name, description, actor, action, resource, context: context4, expectedVerdict } = req.body;
      if (!name || !actor || !action || !resource) {
        res.status(400).json({
          error: "name, actor, action, and resource are required"
        });
        return;
      }
      const envelope = sandboxManager.addScenario(id, {
        name,
        description,
        actor,
        action,
        resource,
        context: context4,
        expectedVerdict
      });
      res.status(201).json(envelope);
    } catch (error) {
      logger_default2.error("Error adding scenario:", error);
      res.status(500).json({ error: "Failed to add scenario", message: error.message });
    }
  }
);
router81.post(
  "/:id/execute",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAccess,
  async (req, res) => {
    try {
      const id = singleParam17(req.params.id) ?? "";
      const { scenarioId, policyId, contextOverrides } = req.body;
      const envelope = await sandboxManager.execute({
        sandboxId: id,
        scenarioId,
        policyId,
        contextOverrides
      });
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error executing sandbox:", error);
      res.status(500).json({ error: "Failed to execute sandbox", message: error.message });
    }
  }
);
router81.get(
  "/executions/:executionId",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAccess,
  async (req, res) => {
    try {
      const executionId = singleParam17(req.params.executionId) ?? "";
      const envelope = sandboxManager.getExecution(executionId);
      if (!envelope.data) {
        res.status(404).json({ error: "Execution not found" });
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting execution:", error);
      res.status(500).json({ error: "Failed to get execution", message: error.message });
    }
  }
);
router81.post(
  "/:id/clone-policy",
  ensureAuthenticated,
  buildPrincipal6,
  requireSandboxAdmin,
  async (req, res) => {
    try {
      const id = singleParam17(req.params.id) ?? "";
      const { policyId, policyData } = req.body;
      if (!policyData) {
        res.status(400).json({ error: "policyData is required" });
        return;
      }
      const sandbox = sandboxManager.getSandbox(id);
      if (!sandbox.data) {
        res.status(404).json({ error: "Sandbox not found" });
        return;
      }
      const clonedPolicy = {
        id: `sandbox-policy-${Date.now()}`,
        originalId: policyId,
        name: policyData.name,
        description: policyData.description,
        rules: policyData.rules || [],
        status: "draft",
        modifiedInSandbox: false
      };
      const updated = await sandboxManager.updateSandbox(id, {
        policies: [...sandbox.data.policies, clonedPolicy]
      });
      res.status(201).json(updated);
    } catch (error) {
      logger_default2.error("Error cloning policy:", error);
      res.status(500).json({ error: "Failed to clone policy", message: error.message });
    }
  }
);
var sandbox_admin_default = router81;

// src/routes/admin/gateway.ts
import express50 from "express";

// src/routes/admin/tenants.ts
init_auth4();
import express44 from "express";
import { z as z37 } from "zod";

// src/policy/opaClient.ts
import fetch5 from "node-fetch";
import { createHash as createHash29 } from "node:crypto";
var OPA_URL = process.env.OPA_URL || "http://opa:8181/v1/data";
var DEFAULT_TIMEOUT_MS = Number(process.env.OPA_TIMEOUT_MS || 3e3);
var DEFAULT_RETRIES = Number(process.env.OPA_RETRIES || 2);
var DEFAULT_BACKOFF_MS = Number(process.env.OPA_BACKOFF_MS || 100);
var DEFAULT_CACHE_TTL_MS = Number(process.env.OPA_CACHE_TTL_MS || 6e4);
var decisionCache = /* @__PURE__ */ new Map();
function stableStringify2(obj) {
  if (obj === null || typeof obj !== "object") return JSON.stringify(obj);
  if (Array.isArray(obj)) return `[${obj.map((v) => stableStringify2(v)).join(",")}]`;
  const sorted = Object.keys(obj).sort().map((k) => `${JSON.stringify(k)}:${stableStringify2(obj[k])}`).join(",");
  return `{${sorted}}`;
}
function buildCacheKey(path55, input) {
  const normalizedInput = stableStringify2(input);
  const hash3 = createHash29("sha256").update(normalizedInput).digest("hex");
  return `${path55}|${input.action}|tenant:${input.tenant || "none"}|user:${input.user?.id || "anonymous"}|${hash3}`;
}
async function fetchWithTimeout(url, body4, timeoutMs, abortController) {
  const controller = abortController ?? new AbortController();
  const timer3 = setTimeout(() => controller.abort(), timeoutMs);
  try {
    return await fetch5(url, {
      method: "POST",
      headers: { "content-type": "application/json" },
      body: body4,
      signal: controller.signal
    });
  } finally {
    clearTimeout(timer3);
  }
}
async function executeWithRetry2(url, body4, options2) {
  let attempt = 0;
  const start = Date.now();
  let lastError = null;
  while (attempt <= options2.maxRetries) {
    try {
      const res = await fetchWithTimeout(url, body4, options2.timeoutMs);
      if (!res.ok) throw new Error(`OPA ${res.status}`);
      const j = await res.json();
      const allow = !!(j.result?.allow ?? j.result === true);
      const reason = j.result?.reason || void 0;
      if (process.env.POLICY_DEBUG === "1") {
        console.log(
          JSON.stringify({
            component: "policy.opa-client",
            decision: allow ? "allow" : "deny",
            reason,
            latencyMs: Date.now() - start,
            attempt
          })
        );
      }
      return { allow, reason };
    } catch (error) {
      lastError = error;
      if (attempt >= options2.maxRetries) break;
      const backoff = options2.baseBackoffMs * Math.pow(2, attempt);
      await new Promise((res) => setTimeout(res, backoff));
      attempt += 1;
    }
  }
  if (process.env.OPA_FAIL_OPEN === "true") {
    return { allow: true, reason: "fail-open" };
  }
  return { allow: false, reason: lastError?.message || "opa_error" };
}
async function opaAllow(path55, input, options2 = {}) {
  const resolved = {
    timeoutMs: options2.timeoutMs ?? DEFAULT_TIMEOUT_MS,
    maxRetries: options2.maxRetries ?? DEFAULT_RETRIES,
    baseBackoffMs: options2.baseBackoffMs ?? DEFAULT_BACKOFF_MS,
    cacheTtlMs: options2.cacheTtlMs ?? DEFAULT_CACHE_TTL_MS,
    skipCache: options2.skipCache ?? false
  };
  const url = `${OPA_URL}/${path55.replace(/^\//, "")}`;
  const body4 = JSON.stringify({ input });
  const cacheKey = buildCacheKey(path55, input);
  if (!resolved.skipCache) {
    const cached = decisionCache.get(cacheKey);
    if (cached && cached.expiresAt > Date.now()) {
      return cached.decision;
    }
  }
  const decision = await executeWithRetry2(url, body4, resolved);
  if (!resolved.skipCache && resolved.cacheTtlMs > 0) {
    decisionCache.set(cacheKey, {
      expiresAt: Date.now() + resolved.cacheTtlMs,
      decision
    });
  }
  return decision;
}

// src/routes/admin/tenants.ts
init_logger2();
var router82 = express44.Router();
var adminProvisionSchema = createTenantSchema.and(
  z37.object({
    plan: z37.enum(["FREE", "STARTER", "PRO", "ENTERPRISE"]).default("ENTERPRISE"),
    environment: z37.enum(["prod", "staging", "dev"]).default("prod"),
    requestedSeats: z37.number().int().min(1).max(1e4).optional(),
    storageEstimateBytes: z37.number().int().min(0).optional()
  })
);
router82.post("/tenants", ensureAuthenticated, async (req, res) => {
  try {
    const authReq = req;
    const actorId = authReq.user?.id;
    const actorRole = authReq.user?.role || "unknown";
    if (!actorId) {
      return res.status(401).json({ success: false, error: "Unauthorized: No user ID found" });
    }
    const body4 = adminProvisionSchema.parse(req.body);
    const decision = await opaAllow("tenants/provision", {
      action: "tenant.provision",
      tenant: "system",
      resource: "tenant",
      user: {
        id: actorId,
        roles: [actorRole]
      },
      meta: {
        residency: body4.residency,
        region: body4.region,
        plan: body4.plan,
        environment: body4.environment
      }
    });
    if (!decision.allow) {
      return res.status(403).json({
        success: false,
        error: decision.reason || "Policy denied tenant provisioning"
      });
    }
    const tenant = await tenantService.createTenant(body4, actorId);
    const provisioning = await tenantProvisioningService.provisionTenant({
      tenant,
      plan: body4.plan,
      environment: body4.environment,
      requestedSeats: body4.requestedSeats,
      storageEstimateBytes: body4.storageEstimateBytes,
      actorId,
      actorType: "user",
      correlationId: req.correlationId,
      requestId: req.id
    });
    const tenantContext = {
      tenantId: tenant.id,
      environment: body4.environment,
      privilegeTier: "standard",
      userId: actorId
    };
    const policy2 = tenantIsolationGuard.evaluatePolicy(tenantContext, {
      action: "tenant.provision.admin",
      environment: body4.environment,
      resourceTenantId: tenant.id
    });
    if (!policy2.allowed) {
      return res.status(policy2.status || 403).json({
        success: false,
        error: policy2.reason || "Isolation policy denied provisioning"
      });
    }
    return res.status(201).json({
      success: true,
      data: {
        tenant,
        namespace: provisioning.namespace,
        partitions: provisioning.partitions,
        quota: provisioning.quota,
        isolationDefaults: {
          environment: tenantContext.environment,
          privilegeTier: tenantContext.privilegeTier,
          quotas: provisioning.quota
        }
      },
      receipts: provisioning.receipts,
      policy: {
        opa: decision
      }
    });
  } catch (error) {
    if (error instanceof z37.ZodError) {
      return res.status(400).json({ success: false, error: "Validation Error", details: error.errors });
    }
    logger_default2.error("Admin tenant provisioning failed", error);
    return res.status(500).json({ success: false, error: "Internal Server Error" });
  }
});
var tenants_default2 = router82;

// src/routes/admin/users.ts
init_auth4();
import express45 from "express";

// src/services/UserManagementService.ts
init_database();
init_logger2();
init_ledger();
init_data_envelope();
import { randomUUID as randomUUID54 } from "crypto";
import * as argon24 from "argon2";
import { z as z38 } from "zod";
var createUserSchema = z38.object({
  email: z38.string().email("Invalid email format"),
  username: z38.string().min(3).max(50).optional(),
  password: z38.string().min(8, "Password must be at least 8 characters"),
  firstName: z38.string().min(1).max(100),
  lastName: z38.string().min(1).max(100),
  role: z38.enum(["ADMIN", "ANALYST", "VIEWER", "DEVELOPER", "COMPLIANCE_OFFICER"]).default("ANALYST"),
  tenantId: z38.string().uuid().optional()
});
var updateUserSchema = z38.object({
  email: z38.string().email().optional(),
  firstName: z38.string().min(1).max(100).optional(),
  lastName: z38.string().min(1).max(100).optional(),
  role: z38.enum(["ADMIN", "ANALYST", "VIEWER", "DEVELOPER", "COMPLIANCE_OFFICER"]).optional(),
  isActive: z38.boolean().optional()
});
var listUsersSchema = z38.object({
  page: z38.number().int().min(1).default(1),
  pageSize: z38.number().int().min(1).max(100).default(20),
  search: z38.string().optional(),
  role: z38.string().optional(),
  isActive: z38.boolean().optional(),
  sortBy: z38.enum(["email", "firstName", "lastName", "createdAt", "lastLogin"]).default("createdAt"),
  sortOrder: z38.enum(["asc", "desc"]).default("desc")
});
var UserManagementService = class {
  pool;
  constructor() {
    this.pool = getPostgresPool2();
  }
  /**
   * Create a governance verdict for user management operations
   */
  createVerdict(action, result2, actorId, reason) {
    return {
      verdictId: `verdict-${randomUUID54()}`,
      policyId: "policy:user-management:v1",
      result: result2,
      decidedAt: /* @__PURE__ */ new Date(),
      reason: reason || `User management action: ${action}`,
      evaluator: "UserManagementService"
    };
  }
  /**
   * List users with pagination and filtering
   * Returns DataEnvelope with GovernanceVerdict
   */
  async listUsers(tenantId, input, actorId) {
    const validated = listUsersSchema.parse(input);
    const { page, pageSize, search, role, isActive, sortBy, sortOrder } = validated;
    const offset = (page - 1) * pageSize;
    try {
      const conditions = ["tenant_id = $1"];
      const params = [tenantId];
      let paramIndex = 2;
      if (search) {
        conditions.push(`(email ILIKE $${paramIndex} OR first_name ILIKE $${paramIndex} OR last_name ILIKE $${paramIndex})`);
        params.push(`%${search}%`);
        paramIndex++;
      }
      if (role) {
        conditions.push(`role = $${paramIndex}`);
        params.push(role);
        paramIndex++;
      }
      if (isActive !== void 0) {
        conditions.push(`is_active = $${paramIndex}`);
        params.push(isActive);
        paramIndex++;
      }
      const whereClause = conditions.join(" AND ");
      const sortColumnMap = {
        email: "email",
        firstName: "first_name",
        lastName: "last_name",
        createdAt: "created_at",
        lastLogin: "last_login"
      };
      const sortColumn = sortColumnMap[sortBy] || "created_at";
      const countResult = await this.pool.query(
        `SELECT COUNT(*) FROM users WHERE ${whereClause}`,
        params
      );
      const total = parseInt(countResult.rows[0].count, 10);
      const dataResult = await this.pool.query(
        `SELECT * FROM users WHERE ${whereClause}
         ORDER BY ${sortColumn} ${sortOrder.toUpperCase()}
         LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`,
        [...params, pageSize, offset]
      );
      const users = dataResult.rows.map((row) => this.mapToManagedUser(row));
      const userIds = users.map((u) => u.id);
      if (userIds.length > 0) {
        const memberships = await this.pool.query(
          `SELECT user_id, array_agg(tenant_id) as tenant_ids
           FROM user_tenants
           WHERE user_id = ANY($1)
           GROUP BY user_id`,
          [userIds]
        );
        const membershipMap = new Map(
          memberships.rows.map((r) => [r.user_id, r.tenant_ids])
        );
        users.forEach((u) => {
          u.tenantIds = membershipMap.get(u.id) || [u.tenantId];
        });
      }
      const result2 = {
        users,
        total,
        page,
        pageSize,
        totalPages: Math.ceil(total / pageSize)
      };
      const verdict = this.createVerdict(
        "list_users",
        "ALLOW" /* ALLOW */,
        actorId,
        `Listed ${users.length} users from tenant ${tenantId}`
      );
      await provenanceLedger.appendEntry({
        action: "USER_LIST_ACCESSED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          resultCount: users.length,
          filters: { search, role, isActive },
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      return createDataEnvelope(result2, {
        source: "UserManagementService",
        actor: actorId,
        version: "1.0.0",
        classification: "RESTRICTED" /* RESTRICTED */,
        governanceVerdict: verdict,
        warnings: []
      });
    } catch (error) {
      logger_default2.error("Error listing users:", error);
      throw error;
    }
  }
  /**
   * Get a single user by ID
   * Returns DataEnvelope with GovernanceVerdict
   */
  async getUser(tenantId, userId, actorId) {
    try {
      const result2 = await this.pool.query(
        "SELECT * FROM users WHERE id = $1 AND tenant_id = $2",
        [userId, tenantId]
      );
      if (result2.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "get_user",
          "FLAG" /* FLAG */,
          actorId,
          `User ${userId} not found in tenant ${tenantId}`
        );
        return createDataEnvelope(null, {
          source: "UserManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "RESTRICTED" /* RESTRICTED */,
          governanceVerdict: verdict2,
          warnings: ["User not found"]
        });
      }
      const user = this.mapToManagedUser(result2.rows[0]);
      const memberships = await this.pool.query(
        "SELECT tenant_id FROM user_tenants WHERE user_id = $1",
        [userId]
      );
      user.tenantIds = memberships.rows.map((r) => r.tenant_id);
      const verdict = this.createVerdict(
        "get_user",
        "ALLOW" /* ALLOW */,
        actorId,
        `Retrieved user ${userId}`
      );
      await provenanceLedger.appendEntry({
        action: "USER_ACCESSED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          targetUserId: userId,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      return createDataEnvelope(user, {
        source: "UserManagementService",
        actor: actorId,
        version: "1.0.0",
        classification: "RESTRICTED" /* RESTRICTED */,
        governanceVerdict: verdict,
        warnings: []
      });
    } catch (error) {
      logger_default2.error("Error getting user:", error);
      throw error;
    }
  }
  /**
   * Create a new user
   * Returns DataEnvelope with GovernanceVerdict
   */
  async createUser(tenantId, input, actorId) {
    const validated = createUserSchema.parse(input);
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const existing = await client6.query(
        "SELECT id FROM users WHERE email = $1",
        [validated.email]
      );
      if (existing.rows.length > 0) {
        const verdict2 = this.createVerdict(
          "create_user",
          "DENY" /* DENY */,
          actorId,
          `User with email ${validated.email} already exists`
        );
        await client6.query("ROLLBACK");
        return createDataEnvelope(
          {
            success: false,
            message: "User with this email already exists"
          },
          {
            source: "UserManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "RESTRICTED" /* RESTRICTED */,
            governanceVerdict: verdict2,
            warnings: ["Duplicate email"]
          }
        );
      }
      const passwordHash = await argon24.hash(validated.password);
      const userId = randomUUID54();
      const result2 = await client6.query(
        `INSERT INTO users (
          id, email, username, password_hash, first_name, last_name,
          role, tenant_id, is_active, created_by
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, true, $9)
        RETURNING *`,
        [
          userId,
          validated.email,
          validated.username,
          passwordHash,
          validated.firstName,
          validated.lastName,
          validated.role,
          validated.tenantId || tenantId,
          actorId
        ]
      );
      await client6.query(
        `INSERT INTO user_tenants (user_id, tenant_id, roles)
         VALUES ($1, $2, $3)
         ON CONFLICT DO NOTHING`,
        [userId, validated.tenantId || tenantId, [validated.role]]
      );
      await client6.query("COMMIT");
      const user = this.mapToManagedUser(result2.rows[0]);
      user.tenantIds = [validated.tenantId || tenantId];
      const verdict = this.createVerdict(
        "create_user",
        "ALLOW" /* ALLOW */,
        actorId,
        `Created user ${userId} with role ${validated.role}`
      );
      await provenanceLedger.appendEntry({
        action: "USER_CREATED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          newUserId: userId,
          role: validated.role,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      logger_default2.info("User created successfully", { userId, tenantId, role: validated.role });
      return createDataEnvelope(
        {
          success: true,
          user,
          message: "User created successfully"
        },
        {
          source: "UserManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "RESTRICTED" /* RESTRICTED */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Error creating user:", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  /**
   * Update an existing user
   * Returns DataEnvelope with GovernanceVerdict
   */
  async updateUser(tenantId, userId, input, actorId) {
    const validated = updateUserSchema.parse(input);
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const existing = await client6.query(
        "SELECT * FROM users WHERE id = $1 AND tenant_id = $2",
        [userId, tenantId]
      );
      if (existing.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "update_user",
          "DENY" /* DENY */,
          actorId,
          `User ${userId} not found in tenant ${tenantId}`
        );
        await client6.query("ROLLBACK");
        return createDataEnvelope(
          {
            success: false,
            message: "User not found"
          },
          {
            source: "UserManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "RESTRICTED" /* RESTRICTED */,
            governanceVerdict: verdict2,
            warnings: ["User not found"]
          }
        );
      }
      const updates = [];
      const params = [];
      let paramIndex = 1;
      if (validated.email !== void 0) {
        updates.push(`email = $${paramIndex++}`);
        params.push(validated.email);
      }
      if (validated.firstName !== void 0) {
        updates.push(`first_name = $${paramIndex++}`);
        params.push(validated.firstName);
      }
      if (validated.lastName !== void 0) {
        updates.push(`last_name = $${paramIndex++}`);
        params.push(validated.lastName);
      }
      if (validated.role !== void 0) {
        updates.push(`role = $${paramIndex++}`);
        params.push(validated.role);
      }
      if (validated.isActive !== void 0) {
        updates.push(`is_active = $${paramIndex++}`);
        params.push(validated.isActive);
      }
      updates.push(`updated_at = NOW()`);
      params.push(userId, tenantId);
      const result2 = await client6.query(
        `UPDATE users SET ${updates.join(", ")}
         WHERE id = $${paramIndex++} AND tenant_id = $${paramIndex}
         RETURNING *`,
        params
      );
      await client6.query("COMMIT");
      const user = this.mapToManagedUser(result2.rows[0]);
      const verdict = this.createVerdict(
        "update_user",
        "ALLOW" /* ALLOW */,
        actorId,
        `Updated user ${userId}`
      );
      await provenanceLedger.appendEntry({
        action: "USER_UPDATED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          targetUserId: userId,
          updatedFields: Object.keys(validated),
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      logger_default2.info("User updated successfully", { userId, tenantId });
      return createDataEnvelope(
        {
          success: true,
          user,
          message: "User updated successfully"
        },
        {
          source: "UserManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "RESTRICTED" /* RESTRICTED */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Error updating user:", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  /**
   * Delete (deactivate) a user
   * Returns DataEnvelope with GovernanceVerdict
   */
  async deleteUser(tenantId, userId, actorId, hardDelete = false) {
    const client6 = await this.pool.connect();
    try {
      await client6.query("BEGIN");
      const existing = await client6.query(
        "SELECT * FROM users WHERE id = $1 AND tenant_id = $2",
        [userId, tenantId]
      );
      if (existing.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "delete_user",
          "DENY" /* DENY */,
          actorId,
          `User ${userId} not found`
        );
        await client6.query("ROLLBACK");
        return createDataEnvelope(
          {
            success: false,
            message: "User not found"
          },
          {
            source: "UserManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "RESTRICTED" /* RESTRICTED */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      if (userId === actorId) {
        const verdict2 = this.createVerdict(
          "delete_user",
          "DENY" /* DENY */,
          actorId,
          "Cannot delete own account"
        );
        await client6.query("ROLLBACK");
        return createDataEnvelope(
          {
            success: false,
            message: "Cannot delete your own account"
          },
          {
            source: "UserManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "RESTRICTED" /* RESTRICTED */,
            governanceVerdict: verdict2,
            warnings: ["Self-deletion prevented"]
          }
        );
      }
      if (hardDelete) {
        await client6.query("DELETE FROM user_tenants WHERE user_id = $1", [userId]);
        await client6.query("DELETE FROM user_sessions WHERE user_id = $1", [userId]);
        await client6.query("DELETE FROM users WHERE id = $1", [userId]);
      } else {
        await client6.query(
          "UPDATE users SET is_active = false, updated_at = NOW() WHERE id = $1",
          [userId]
        );
      }
      await client6.query("COMMIT");
      const verdict = this.createVerdict(
        "delete_user",
        "ALLOW" /* ALLOW */,
        actorId,
        `${hardDelete ? "Hard" : "Soft"} deleted user ${userId}`
      );
      await provenanceLedger.appendEntry({
        action: hardDelete ? "USER_DELETED" : "USER_DEACTIVATED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          targetUserId: userId,
          hardDelete,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      logger_default2.info("User deleted successfully", { userId, tenantId, hardDelete });
      return createDataEnvelope(
        {
          success: true,
          message: hardDelete ? "User deleted permanently" : "User deactivated"
        },
        {
          source: "UserManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "RESTRICTED" /* RESTRICTED */,
          governanceVerdict: verdict,
          warnings: hardDelete ? ["User data permanently removed"] : []
        }
      );
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error("Error deleting user:", error);
      throw error;
    } finally {
      client6.release();
    }
  }
  /**
   * Lock a user account
   */
  async lockUser(tenantId, userId, actorId, reason) {
    try {
      const result2 = await this.pool.query(
        `UPDATE users SET is_locked = true, lock_reason = $1, updated_at = NOW()
         WHERE id = $2 AND tenant_id = $3
         RETURNING *`,
        [reason, userId, tenantId]
      );
      if (result2.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "lock_user",
          "DENY" /* DENY */,
          actorId,
          `User ${userId} not found`
        );
        return createDataEnvelope(
          {
            success: false,
            message: "User not found"
          },
          {
            source: "UserManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "RESTRICTED" /* RESTRICTED */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      const user = this.mapToManagedUser(result2.rows[0]);
      const verdict = this.createVerdict(
        "lock_user",
        "ALLOW" /* ALLOW */,
        actorId,
        `Locked user ${userId}: ${reason}`
      );
      await provenanceLedger.appendEntry({
        action: "USER_LOCKED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          targetUserId: userId,
          reason,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      logger_default2.info("User locked", { userId, tenantId, reason });
      return createDataEnvelope(
        {
          success: true,
          user,
          message: "User account locked"
        },
        {
          source: "UserManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "RESTRICTED" /* RESTRICTED */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error locking user:", error);
      throw error;
    }
  }
  /**
   * Unlock a user account
   */
  async unlockUser(tenantId, userId, actorId) {
    try {
      const result2 = await this.pool.query(
        `UPDATE users SET is_locked = false, lock_reason = NULL, updated_at = NOW()
         WHERE id = $1 AND tenant_id = $2
         RETURNING *`,
        [userId, tenantId]
      );
      if (result2.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "unlock_user",
          "DENY" /* DENY */,
          actorId,
          `User ${userId} not found`
        );
        return createDataEnvelope(
          {
            success: false,
            message: "User not found"
          },
          {
            source: "UserManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "RESTRICTED" /* RESTRICTED */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      const user = this.mapToManagedUser(result2.rows[0]);
      const verdict = this.createVerdict(
        "unlock_user",
        "ALLOW" /* ALLOW */,
        actorId,
        `Unlocked user ${userId}`
      );
      await provenanceLedger.appendEntry({
        action: "USER_UNLOCKED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          targetUserId: userId,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      logger_default2.info("User unlocked", { userId, tenantId });
      return createDataEnvelope(
        {
          success: true,
          user,
          message: "User account unlocked"
        },
        {
          source: "UserManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "RESTRICTED" /* RESTRICTED */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error unlocking user:", error);
      throw error;
    }
  }
  /**
   * Add user to additional tenant
   */
  async addUserToTenant(userId, targetTenantId, roles, actorId) {
    try {
      await this.pool.query(
        `INSERT INTO user_tenants (user_id, tenant_id, roles)
         VALUES ($1, $2, $3)
         ON CONFLICT (user_id, tenant_id)
         DO UPDATE SET roles = EXCLUDED.roles`,
        [userId, targetTenantId, roles]
      );
      const verdict = this.createVerdict(
        "add_user_to_tenant",
        "ALLOW" /* ALLOW */,
        actorId,
        `Added user ${userId} to tenant ${targetTenantId}`
      );
      await provenanceLedger.appendEntry({
        action: "USER_TENANT_ADDED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          targetUserId: userId,
          targetTenantId,
          roles,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      return createDataEnvelope(
        {
          success: true,
          message: "User added to tenant"
        },
        {
          source: "UserManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "RESTRICTED" /* RESTRICTED */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error adding user to tenant:", error);
      throw error;
    }
  }
  /**
   * Remove user from tenant
   */
  async removeUserFromTenant(userId, targetTenantId, actorId) {
    try {
      await this.pool.query(
        "DELETE FROM user_tenants WHERE user_id = $1 AND tenant_id = $2",
        [userId, targetTenantId]
      );
      const verdict = this.createVerdict(
        "remove_user_from_tenant",
        "ALLOW" /* ALLOW */,
        actorId,
        `Removed user ${userId} from tenant ${targetTenantId}`
      );
      await provenanceLedger.appendEntry({
        action: "USER_TENANT_REMOVED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          targetUserId: userId,
          targetTenantId,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      return createDataEnvelope(
        {
          success: true,
          message: "User removed from tenant"
        },
        {
          source: "UserManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "RESTRICTED" /* RESTRICTED */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error removing user from tenant:", error);
      throw error;
    }
  }
  /**
   * Map database row to ManagedUser
   */
  mapToManagedUser(row) {
    return {
      id: row.id,
      email: row.email,
      username: row.username,
      firstName: row.first_name,
      lastName: row.last_name,
      fullName: `${row.first_name} ${row.last_name}`.trim(),
      role: row.role,
      tenantId: row.tenant_id,
      tenantIds: [row.tenant_id],
      // Will be populated separately
      isActive: row.is_active,
      isLocked: row.is_locked || false,
      lockReason: row.lock_reason,
      lastLogin: row.last_login,
      mfaEnabled: row.mfa_enabled || false,
      createdAt: row.created_at,
      updatedAt: row.updated_at,
      createdBy: row.created_by
    };
  }
};
var userManagementService = new UserManagementService();

// src/routes/admin/users.ts
init_logger2();
var router83 = express45.Router();
var authz7 = new AuthorizationServiceImpl();
var userService = new UserManagementService();
var singleParam18 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var buildPrincipal7 = (req, res, next) => {
  const user = req.user;
  if (!user) {
    res.status(401).json({ error: "Unauthorized", code: "AUTH_REQUIRED" });
    return;
  }
  const principal = {
    kind: "user",
    id: user.id,
    tenantId: req.headers["x-tenant-id"] || user.tenantId || "default-tenant",
    roles: [user.role],
    scopes: [],
    user: {
      email: user.email,
      username: user.username
    }
  };
  req.principal = principal;
  next();
};
var requireUserPermission = (action) => {
  return async (req, res, next) => {
    try {
      const principal = req.principal;
      await authz7.assertCan(principal, action, { type: "user", tenantId: principal.tenantId });
      next();
    } catch (error) {
      if (error.message.includes("Permission denied")) {
        res.status(403).json({
          error: "Forbidden",
          code: "PERMISSION_DENIED",
          required: `user:${action}`
        });
        return;
      }
      logger_default2.error("Authorization error:", error);
      res.status(500).json({ error: "Authorization service error" });
    }
  };
};
router83.get(
  "/",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const input = {
        page: parseInt(req.query.page, 10) || 1,
        pageSize: parseInt(req.query.pageSize, 10) || 20,
        search: req.query.search,
        role: req.query.role,
        isActive: req.query.isActive === "true" ? true : req.query.isActive === "false" ? false : void 0,
        sortBy: req.query.sortBy || "createdAt",
        sortOrder: req.query.sortOrder || "desc"
      };
      const envelope = await userService.listUsers(
        principal.tenantId,
        input,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing users:", error);
      res.status(500).json({ error: "Failed to list users", message: error.message });
    }
  }
);
router83.get(
  "/:id",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam18(req.params.id) ?? "";
      const envelope = await userService.getUser(
        principal.tenantId,
        id,
        principal.id
      );
      if (!envelope.data) {
        res.status(404).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting user:", error);
      res.status(500).json({ error: "Failed to get user", message: error.message });
    }
  }
);
router83.post(
  "/",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("create"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const parseResult = createUserSchema.safeParse(req.body);
      if (!parseResult.success) {
        res.status(400).json({
          error: "Validation failed",
          details: parseResult.error.errors
        });
        return;
      }
      const envelope = await userService.createUser(
        principal.tenantId,
        parseResult.data,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.status(201).json(envelope);
    } catch (error) {
      logger_default2.error("Error creating user:", error);
      res.status(500).json({ error: "Failed to create user", message: error.message });
    }
  }
);
router83.patch(
  "/:id",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("update"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam18(req.params.id) ?? "";
      const parseResult = updateUserSchema.safeParse(req.body);
      if (!parseResult.success) {
        res.status(400).json({
          error: "Validation failed",
          details: parseResult.error.errors
        });
        return;
      }
      const envelope = await userService.updateUser(
        principal.tenantId,
        id,
        parseResult.data,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error updating user:", error);
      res.status(500).json({ error: "Failed to update user", message: error.message });
    }
  }
);
router83.delete(
  "/:id",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("delete"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam18(req.params.id) ?? "";
      const hardDelete = req.query.hard === "true";
      const envelope = await userService.deleteUser(
        principal.tenantId,
        id,
        principal.id,
        hardDelete
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error deleting user:", error);
      res.status(500).json({ error: "Failed to delete user", message: error.message });
    }
  }
);
router83.post(
  "/:id/lock",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("lock"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam18(req.params.id) ?? "";
      const { reason } = req.body;
      if (!reason) {
        res.status(400).json({ error: "Reason is required for locking account" });
        return;
      }
      const envelope = await userService.lockUser(
        principal.tenantId,
        id,
        principal.id,
        reason
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error locking user:", error);
      res.status(500).json({ error: "Failed to lock user", message: error.message });
    }
  }
);
router83.post(
  "/:id/unlock",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("unlock"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam18(req.params.id) ?? "";
      const envelope = await userService.unlockUser(
        principal.tenantId,
        id,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error unlocking user:", error);
      res.status(500).json({ error: "Failed to unlock user", message: error.message });
    }
  }
);
router83.post(
  "/:id/tenants",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("update"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam18(req.params.id) ?? "";
      const { tenantId, roles } = req.body;
      if (!tenantId || !roles || !Array.isArray(roles)) {
        res.status(400).json({ error: "tenantId and roles array are required" });
        return;
      }
      const envelope = await userService.addUserToTenant(
        id,
        tenantId,
        roles,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error adding user to tenant:", error);
      res.status(500).json({ error: "Failed to add user to tenant", message: error.message });
    }
  }
);
router83.delete(
  "/:id/tenants/:tenantId",
  ensureAuthenticated,
  buildPrincipal7,
  requireUserPermission("update"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam18(req.params.id) ?? "";
      const tenantId = singleParam18(req.params.tenantId) ?? "";
      const envelope = await userService.removeUserFromTenant(
        id,
        tenantId,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error removing user from tenant:", error);
      res.status(500).json({ error: "Failed to remove user from tenant", message: error.message });
    }
  }
);
var users_default = router83;

// src/routes/admin/roles.ts
init_auth4();
import express46 from "express";

// src/services/RoleManagementService.ts
init_database();
init_logger2();
init_ledger();
init_data_envelope();
import { randomUUID as randomUUID55 } from "crypto";
import { z as z39 } from "zod";
var createRoleSchema = z39.object({
  name: z39.string().min(2).max(50).regex(/^[a-z][a-z0-9-_]*$/, "Role name must be lowercase with hyphens/underscores"),
  displayName: z39.string().min(2).max(100),
  description: z39.string().max(500).optional(),
  permissions: z39.array(z39.string()).min(1),
  inherits: z39.array(z39.string()).optional(),
  isSystem: z39.boolean().default(false),
  scope: z39.enum(["full", "restricted", "readonly"]).default("restricted")
});
var updateRoleSchema = z39.object({
  displayName: z39.string().min(2).max(100).optional(),
  description: z39.string().max(500).optional(),
  permissions: z39.array(z39.string()).optional(),
  inherits: z39.array(z39.string()).optional(),
  scope: z39.enum(["full", "restricted", "readonly"]).optional()
});
var assignRoleSchema = z39.object({
  userId: z39.string().uuid(),
  roleId: z39.string().uuid(),
  expiresAt: z39.date().optional()
});
var BUILT_IN_ROLES = [
  {
    name: "global-admin",
    displayName: "Global Administrator",
    description: "Full system access across all tenants",
    permissions: ["*"],
    inherits: [],
    isSystem: true,
    isBuiltIn: true,
    scope: "full"
  },
  {
    name: "tenant-admin",
    displayName: "Tenant Administrator",
    description: "Full access within tenant",
    permissions: [
      "tenant:manage",
      "tenant:read",
      "tenant:settings",
      "user:*",
      "role:*",
      "audit:*",
      "config:*",
      "api_key:*",
      "billing:read"
    ],
    inherits: ["supervisor"],
    isSystem: true,
    isBuiltIn: true,
    scope: "full"
  },
  {
    name: "security-admin",
    displayName: "Security Administrator",
    description: "Security and compliance management",
    permissions: [
      "user:read",
      "user:lock",
      "user:unlock",
      "audit:*",
      "policy:*",
      "compliance:*",
      "api_key:view",
      "api_key:revoke"
    ],
    inherits: ["viewer"],
    isSystem: true,
    isBuiltIn: true,
    scope: "restricted"
  },
  {
    name: "supervisor",
    displayName: "Supervisor",
    description: "Team lead with full investigation access",
    permissions: [
      "investigation:*",
      "entity:*",
      "relationship:*",
      "analytics:*",
      "report:*",
      "team:manage"
    ],
    inherits: ["analyst"],
    isSystem: true,
    isBuiltIn: true,
    scope: "full"
  },
  {
    name: "analyst",
    displayName: "Analyst",
    description: "Standard analyst with investigation capabilities",
    permissions: [
      "investigation:read",
      "investigation:create",
      "investigation:update",
      "entity:read",
      "entity:create",
      "entity:update",
      "relationship:read",
      "relationship:create",
      "relationship:update",
      "analytics:run",
      "report:read",
      "report:create",
      "copilot:query",
      "copilot:analyze"
    ],
    inherits: ["viewer"],
    isSystem: true,
    isBuiltIn: true,
    scope: "restricted"
  },
  {
    name: "viewer",
    displayName: "Viewer",
    description: "Read-only access",
    permissions: [
      "investigation:read",
      "entity:read",
      "relationship:read",
      "analytics:view",
      "report:read",
      "dashboard:view"
    ],
    inherits: [],
    isSystem: true,
    isBuiltIn: true,
    scope: "readonly"
  },
  {
    name: "developer",
    displayName: "Developer",
    description: "Developer with API and pipeline access",
    permissions: [
      "maestro:*",
      "ingestion:*",
      "graph:read",
      "graph:analyze",
      "api_key:create"
    ],
    inherits: ["analyst"],
    isSystem: true,
    isBuiltIn: true,
    scope: "restricted"
  },
  {
    name: "compliance-officer",
    displayName: "Compliance Officer",
    description: "Compliance and audit access",
    permissions: [
      "audit:read",
      "audit:export",
      "report:*",
      "policy:read",
      "sensitive:read",
      "dlp:override"
    ],
    inherits: ["viewer"],
    isSystem: true,
    isBuiltIn: true,
    scope: "restricted"
  }
];
var PERMISSION_CATEGORIES = {
  "User Management": [
    { id: "user:read", name: "user:read", displayName: "View Users", description: "View user profiles", resource: "user", action: "read", category: "User Management", isSystem: true },
    { id: "user:create", name: "user:create", displayName: "Create Users", description: "Create new users", resource: "user", action: "create", category: "User Management", isSystem: true },
    { id: "user:update", name: "user:update", displayName: "Update Users", description: "Update user profiles", resource: "user", action: "update", category: "User Management", isSystem: true },
    { id: "user:delete", name: "user:delete", displayName: "Delete Users", description: "Delete users", resource: "user", action: "delete", category: "User Management", isSystem: true },
    { id: "user:lock", name: "user:lock", displayName: "Lock Users", description: "Lock user accounts", resource: "user", action: "lock", category: "User Management", isSystem: true },
    { id: "user:unlock", name: "user:unlock", displayName: "Unlock Users", description: "Unlock user accounts", resource: "user", action: "unlock", category: "User Management", isSystem: true }
  ],
  "Role Management": [
    { id: "role:read", name: "role:read", displayName: "View Roles", description: "View roles and permissions", resource: "role", action: "read", category: "Role Management", isSystem: true },
    { id: "role:create", name: "role:create", displayName: "Create Roles", description: "Create custom roles", resource: "role", action: "create", category: "Role Management", isSystem: true },
    { id: "role:update", name: "role:update", displayName: "Update Roles", description: "Update role permissions", resource: "role", action: "update", category: "Role Management", isSystem: true },
    { id: "role:delete", name: "role:delete", displayName: "Delete Roles", description: "Delete custom roles", resource: "role", action: "delete", category: "Role Management", isSystem: true },
    { id: "role:assign", name: "role:assign", displayName: "Assign Roles", description: "Assign roles to users", resource: "role", action: "assign", category: "Role Management", isSystem: true },
    { id: "role:revoke", name: "role:revoke", displayName: "Revoke Roles", description: "Revoke roles from users", resource: "role", action: "revoke", category: "Role Management", isSystem: true }
  ],
  "Investigation": [
    { id: "investigation:read", name: "investigation:read", displayName: "View Investigations", description: "View investigation cases", resource: "investigation", action: "read", category: "Investigation", isSystem: true },
    { id: "investigation:create", name: "investigation:create", displayName: "Create Investigations", description: "Create new investigations", resource: "investigation", action: "create", category: "Investigation", isSystem: true },
    { id: "investigation:update", name: "investigation:update", displayName: "Update Investigations", description: "Update investigations", resource: "investigation", action: "update", category: "Investigation", isSystem: true },
    { id: "investigation:delete", name: "investigation:delete", displayName: "Delete Investigations", description: "Delete investigations", resource: "investigation", action: "delete", category: "Investigation", isSystem: true }
  ],
  "Entities": [
    { id: "entity:read", name: "entity:read", displayName: "View Entities", description: "View entities in graph", resource: "entity", action: "read", category: "Entities", isSystem: true },
    { id: "entity:create", name: "entity:create", displayName: "Create Entities", description: "Create new entities", resource: "entity", action: "create", category: "Entities", isSystem: true },
    { id: "entity:update", name: "entity:update", displayName: "Update Entities", description: "Update entities", resource: "entity", action: "update", category: "Entities", isSystem: true },
    { id: "entity:delete", name: "entity:delete", displayName: "Delete Entities", description: "Delete entities", resource: "entity", action: "delete", category: "Entities", isSystem: true }
  ],
  "Audit & Compliance": [
    { id: "audit:read", name: "audit:read", displayName: "View Audit Logs", description: "View audit trail", resource: "audit", action: "read", category: "Audit & Compliance", isSystem: true },
    { id: "audit:export", name: "audit:export", displayName: "Export Audit Logs", description: "Export audit data", resource: "audit", action: "export", category: "Audit & Compliance", isSystem: true },
    { id: "compliance:read", name: "compliance:read", displayName: "View Compliance", description: "View compliance status", resource: "compliance", action: "read", category: "Audit & Compliance", isSystem: true },
    { id: "compliance:report", name: "compliance:report", displayName: "Generate Compliance Reports", description: "Generate compliance reports", resource: "compliance", action: "report", category: "Audit & Compliance", isSystem: true }
  ],
  "Tenant Administration": [
    { id: "tenant:manage", name: "tenant:manage", displayName: "Manage Tenant", description: "Full tenant management", resource: "tenant", action: "manage", category: "Tenant Administration", isSystem: true },
    { id: "tenant:read", name: "tenant:read", displayName: "View Tenant", description: "View tenant details", resource: "tenant", action: "read", category: "Tenant Administration", isSystem: true },
    { id: "tenant:settings", name: "tenant:settings", displayName: "Manage Tenant Settings", description: "Update tenant settings", resource: "tenant", action: "settings", category: "Tenant Administration", isSystem: true }
  ]
};
var RoleManagementService = class {
  pool;
  permissionCache = /* @__PURE__ */ new Map();
  constructor() {
    this.pool = getPostgresPool2();
    this.buildPermissionCache();
  }
  /**
   * Build permission cache from built-in roles
   */
  buildPermissionCache() {
    for (const role of BUILT_IN_ROLES) {
      const permissions = this.resolvePermissions(role.name, role.permissions, role.inherits);
      this.permissionCache.set(role.name, permissions);
    }
  }
  /**
   * Resolve all permissions including inherited
   */
  resolvePermissions(roleName, directPermissions, inherits, visited = /* @__PURE__ */ new Set()) {
    if (visited.has(roleName)) return /* @__PURE__ */ new Set();
    visited.add(roleName);
    const permissions = new Set(directPermissions);
    for (const inheritedRole of inherits) {
      const inheritedRoleDef = BUILT_IN_ROLES.find((r) => r.name === inheritedRole);
      if (inheritedRoleDef) {
        const inheritedPerms = this.resolvePermissions(
          inheritedRole,
          inheritedRoleDef.permissions,
          inheritedRoleDef.inherits,
          visited
        );
        inheritedPerms.forEach((p) => permissions.add(p));
      }
    }
    return permissions;
  }
  /**
   * Create a governance verdict
   */
  createVerdict(action, result2, actorId, reason) {
    return {
      verdictId: `verdict-${randomUUID55()}`,
      policyId: "policy:role-management:v1",
      result: result2,
      decidedAt: /* @__PURE__ */ new Date(),
      reason: reason || `Role management action: ${action}`,
      evaluator: "RoleManagementService"
    };
  }
  /**
   * List all roles for a tenant
   */
  async listRoles(tenantId, actorId) {
    try {
      const result2 = await this.pool.query(
        "SELECT * FROM roles WHERE tenant_id = $1 OR is_system = true ORDER BY is_system DESC, name ASC",
        [tenantId]
      );
      const customRoles = result2.rows.map((row) => ({
        id: row.id,
        name: row.name,
        displayName: row.display_name,
        description: row.description,
        permissions: row.permissions || [],
        effectivePermissions: this.getEffectivePermissions(row.name, row.permissions, row.inherits || []),
        inherits: row.inherits || [],
        isSystem: row.is_system,
        isBuiltIn: false,
        scope: row.scope,
        tenantId: row.tenant_id,
        createdAt: row.created_at,
        updatedAt: row.updated_at,
        createdBy: row.created_by
      }));
      const builtInRoles = BUILT_IN_ROLES.map((r) => ({
        ...r,
        id: `builtin-${r.name}`,
        effectivePermissions: Array.from(this.permissionCache.get(r.name) || /* @__PURE__ */ new Set()),
        tenantId: "system",
        createdAt: /* @__PURE__ */ new Date(),
        updatedAt: /* @__PURE__ */ new Date()
      }));
      const allRoles = [...builtInRoles, ...customRoles];
      const verdict = this.createVerdict(
        "list_roles",
        "ALLOW" /* ALLOW */,
        actorId,
        `Listed ${allRoles.length} roles`
      );
      await provenanceLedger.appendEntry({
        action: "ROLES_LISTED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          roleCount: allRoles.length,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      return createDataEnvelope(
        { roles: allRoles, total: allRoles.length },
        {
          source: "RoleManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error listing roles:", error);
      throw error;
    }
  }
  /**
   * Get effective permissions for a role
   */
  getEffectivePermissions(name, permissions, inherits) {
    const cached = this.permissionCache.get(name);
    if (cached) return Array.from(cached);
    const resolved = this.resolvePermissions(name, permissions, inherits);
    return Array.from(resolved);
  }
  /**
   * Get a single role by ID
   */
  async getRole(tenantId, roleId, actorId) {
    try {
      if (roleId.startsWith("builtin-")) {
        const roleName = roleId.replace("builtin-", "");
        const builtIn = BUILT_IN_ROLES.find((r) => r.name === roleName);
        if (builtIn) {
          const role2 = {
            ...builtIn,
            id: roleId,
            effectivePermissions: Array.from(this.permissionCache.get(roleName) || /* @__PURE__ */ new Set()),
            tenantId: "system",
            createdAt: /* @__PURE__ */ new Date(),
            updatedAt: /* @__PURE__ */ new Date()
          };
          const verdict2 = this.createVerdict(
            "get_role",
            "ALLOW" /* ALLOW */,
            actorId,
            `Retrieved built-in role ${roleName}`
          );
          return createDataEnvelope(role2, {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: []
          });
        }
      }
      const result2 = await this.pool.query(
        "SELECT * FROM roles WHERE id = $1 AND (tenant_id = $2 OR is_system = true)",
        [roleId, tenantId]
      );
      if (result2.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "get_role",
          "FLAG" /* FLAG */,
          actorId,
          `Role ${roleId} not found`
        );
        return createDataEnvelope(null, {
          source: "RoleManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
          governanceVerdict: verdict2,
          warnings: ["Role not found"]
        });
      }
      const row = result2.rows[0];
      const role = {
        id: row.id,
        name: row.name,
        displayName: row.display_name,
        description: row.description,
        permissions: row.permissions || [],
        effectivePermissions: this.getEffectivePermissions(row.name, row.permissions, row.inherits || []),
        inherits: row.inherits || [],
        isSystem: row.is_system,
        isBuiltIn: false,
        scope: row.scope,
        tenantId: row.tenant_id,
        createdAt: row.created_at,
        updatedAt: row.updated_at,
        createdBy: row.created_by
      };
      const verdict = this.createVerdict(
        "get_role",
        "ALLOW" /* ALLOW */,
        actorId,
        `Retrieved role ${role.name}`
      );
      return createDataEnvelope(role, {
        source: "RoleManagementService",
        actor: actorId,
        version: "1.0.0",
        classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
        governanceVerdict: verdict,
        warnings: []
      });
    } catch (error) {
      logger_default2.error("Error getting role:", error);
      throw error;
    }
  }
  /**
   * Create a custom role
   */
  async createRole(tenantId, input, actorId) {
    const validated = createRoleSchema.parse(input);
    try {
      const existing = await this.pool.query(
        "SELECT id FROM roles WHERE name = $1 AND tenant_id = $2",
        [validated.name, tenantId]
      );
      if (existing.rows.length > 0) {
        const verdict2 = this.createVerdict(
          "create_role",
          "DENY" /* DENY */,
          actorId,
          `Role name ${validated.name} already exists`
        );
        return createDataEnvelope(
          { success: false, message: "Role name already exists" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: ["Duplicate role name"]
          }
        );
      }
      if (validated.inherits) {
        for (const inheritName of validated.inherits) {
          const inheritExists = BUILT_IN_ROLES.some((r) => r.name === inheritName) || (await this.pool.query(
            "SELECT id FROM roles WHERE name = $1 AND (tenant_id = $2 OR is_system = true)",
            [inheritName, tenantId]
          )).rows.length > 0;
          if (!inheritExists) {
            const verdict2 = this.createVerdict(
              "create_role",
              "DENY" /* DENY */,
              actorId,
              `Inherited role ${inheritName} does not exist`
            );
            return createDataEnvelope(
              { success: false, message: `Inherited role '${inheritName}' does not exist` },
              {
                source: "RoleManagementService",
                actor: actorId,
                version: "1.0.0",
                classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
                governanceVerdict: verdict2,
                warnings: []
              }
            );
          }
        }
      }
      const roleId = randomUUID55();
      const result2 = await this.pool.query(
        `INSERT INTO roles (
          id, name, display_name, description, permissions, inherits,
          is_system, scope, tenant_id, created_by
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
        RETURNING *`,
        [
          roleId,
          validated.name,
          validated.displayName,
          validated.description,
          validated.permissions,
          validated.inherits || [],
          validated.isSystem,
          validated.scope,
          tenantId,
          actorId
        ]
      );
      const row = result2.rows[0];
      const role = {
        id: row.id,
        name: row.name,
        displayName: row.display_name,
        description: row.description,
        permissions: row.permissions,
        effectivePermissions: this.getEffectivePermissions(row.name, row.permissions, row.inherits || []),
        inherits: row.inherits || [],
        isSystem: row.is_system,
        isBuiltIn: false,
        scope: row.scope,
        tenantId: row.tenant_id,
        createdAt: row.created_at,
        updatedAt: row.updated_at,
        createdBy: row.created_by
      };
      const verdict = this.createVerdict(
        "create_role",
        "ALLOW" /* ALLOW */,
        actorId,
        `Created role ${role.name}`
      );
      await provenanceLedger.appendEntry({
        action: "ROLE_CREATED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          roleId: role.id,
          roleName: role.name,
          permissions: role.permissions,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      logger_default2.info("Role created successfully", { roleId: role.id, tenantId });
      return createDataEnvelope(
        { success: true, role, message: "Role created successfully" },
        {
          source: "RoleManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error creating role:", error);
      throw error;
    }
  }
  /**
   * Update a custom role
   */
  async updateRole(tenantId, roleId, input, actorId) {
    const validated = updateRoleSchema.parse(input);
    try {
      if (roleId.startsWith("builtin-")) {
        const verdict2 = this.createVerdict(
          "update_role",
          "DENY" /* DENY */,
          actorId,
          "Cannot modify built-in roles"
        );
        return createDataEnvelope(
          { success: false, message: "Cannot modify built-in roles" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      const existing = await this.pool.query(
        "SELECT * FROM roles WHERE id = $1 AND tenant_id = $2",
        [roleId, tenantId]
      );
      if (existing.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "update_role",
          "DENY" /* DENY */,
          actorId,
          `Role ${roleId} not found`
        );
        return createDataEnvelope(
          { success: false, message: "Role not found" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      if (existing.rows[0].is_system) {
        const verdict2 = this.createVerdict(
          "update_role",
          "DENY" /* DENY */,
          actorId,
          "Cannot modify system roles"
        );
        return createDataEnvelope(
          { success: false, message: "Cannot modify system roles" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      const updates = [];
      const params = [];
      let paramIndex = 1;
      if (validated.displayName !== void 0) {
        updates.push(`display_name = $${paramIndex++}`);
        params.push(validated.displayName);
      }
      if (validated.description !== void 0) {
        updates.push(`description = $${paramIndex++}`);
        params.push(validated.description);
      }
      if (validated.permissions !== void 0) {
        updates.push(`permissions = $${paramIndex++}`);
        params.push(validated.permissions);
      }
      if (validated.inherits !== void 0) {
        updates.push(`inherits = $${paramIndex++}`);
        params.push(validated.inherits);
      }
      if (validated.scope !== void 0) {
        updates.push(`scope = $${paramIndex++}`);
        params.push(validated.scope);
      }
      updates.push(`updated_at = NOW()`);
      params.push(roleId, tenantId);
      const result2 = await this.pool.query(
        `UPDATE roles SET ${updates.join(", ")}
         WHERE id = $${paramIndex++} AND tenant_id = $${paramIndex}
         RETURNING *`,
        params
      );
      const row = result2.rows[0];
      const role = {
        id: row.id,
        name: row.name,
        displayName: row.display_name,
        description: row.description,
        permissions: row.permissions,
        effectivePermissions: this.getEffectivePermissions(row.name, row.permissions, row.inherits || []),
        inherits: row.inherits || [],
        isSystem: row.is_system,
        isBuiltIn: false,
        scope: row.scope,
        tenantId: row.tenant_id,
        createdAt: row.created_at,
        updatedAt: row.updated_at,
        createdBy: row.created_by
      };
      const verdict = this.createVerdict(
        "update_role",
        "ALLOW" /* ALLOW */,
        actorId,
        `Updated role ${role.name}`
      );
      await provenanceLedger.appendEntry({
        action: "ROLE_UPDATED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          roleId: role.id,
          updatedFields: Object.keys(validated),
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      return createDataEnvelope(
        { success: true, role, message: "Role updated successfully" },
        {
          source: "RoleManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error updating role:", error);
      throw error;
    }
  }
  /**
   * Delete a custom role
   */
  async deleteRole(tenantId, roleId, actorId) {
    try {
      if (roleId.startsWith("builtin-")) {
        const verdict2 = this.createVerdict(
          "delete_role",
          "DENY" /* DENY */,
          actorId,
          "Cannot delete built-in roles"
        );
        return createDataEnvelope(
          { success: false, message: "Cannot delete built-in roles" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      const existing = await this.pool.query(
        "SELECT * FROM roles WHERE id = $1 AND tenant_id = $2",
        [roleId, tenantId]
      );
      if (existing.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "delete_role",
          "DENY" /* DENY */,
          actorId,
          `Role ${roleId} not found`
        );
        return createDataEnvelope(
          { success: false, message: "Role not found" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      if (existing.rows[0].is_system) {
        const verdict2 = this.createVerdict(
          "delete_role",
          "DENY" /* DENY */,
          actorId,
          "Cannot delete system roles"
        );
        return createDataEnvelope(
          { success: false, message: "Cannot delete system roles" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      const usageCheck = await this.pool.query(
        "SELECT COUNT(*) FROM user_roles WHERE role_id = $1",
        [roleId]
      );
      if (parseInt(usageCheck.rows[0].count, 10) > 0) {
        const verdict2 = this.createVerdict(
          "delete_role",
          "DENY" /* DENY */,
          actorId,
          "Role is currently assigned to users"
        );
        return createDataEnvelope(
          { success: false, message: "Cannot delete role that is assigned to users" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: ["Role is in use"]
          }
        );
      }
      const roleName = existing.rows[0].name;
      await this.pool.query("DELETE FROM roles WHERE id = $1", [roleId]);
      const verdict = this.createVerdict(
        "delete_role",
        "ALLOW" /* ALLOW */,
        actorId,
        `Deleted role ${roleName}`
      );
      await provenanceLedger.appendEntry({
        action: "ROLE_DELETED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          roleId,
          roleName,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      return createDataEnvelope(
        { success: true, message: "Role deleted successfully" },
        {
          source: "RoleManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error deleting role:", error);
      throw error;
    }
  }
  /**
   * List all available permissions
   */
  async listPermissions(actorId) {
    const permissions = [];
    const categories = [];
    for (const [category, perms] of Object.entries(PERMISSION_CATEGORIES)) {
      categories.push(category);
      permissions.push(...perms);
    }
    const verdict = this.createVerdict(
      "list_permissions",
      "ALLOW" /* ALLOW */,
      actorId,
      `Listed ${permissions.length} permissions`
    );
    return createDataEnvelope(
      { permissions, categories, total: permissions.length },
      {
        source: "RoleManagementService",
        actor: actorId,
        version: "1.0.0",
        classification: "INTERNAL" /* INTERNAL */,
        governanceVerdict: verdict,
        warnings: []
      }
    );
  }
  /**
   * Assign role to user
   */
  async assignRoleToUser(tenantId, userId, roleId, actorId, expiresAt) {
    try {
      const assignmentId = randomUUID55();
      let roleName;
      if (roleId.startsWith("builtin-")) {
        roleName = roleId.replace("builtin-", "");
      } else {
        const role = await this.pool.query("SELECT name FROM roles WHERE id = $1", [roleId]);
        if (role.rows.length === 0) {
          const verdict2 = this.createVerdict(
            "assign_role",
            "DENY" /* DENY */,
            actorId,
            `Role ${roleId} not found`
          );
          return createDataEnvelope(
            { success: false, message: "Role not found" },
            {
              source: "RoleManagementService",
              actor: actorId,
              version: "1.0.0",
              classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
              governanceVerdict: verdict2,
              warnings: []
            }
          );
        }
        roleName = role.rows[0].name;
      }
      await this.pool.query(
        `INSERT INTO user_roles (id, user_id, role_id, role_name, tenant_id, granted_by, expires_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7)
         ON CONFLICT (user_id, role_id, tenant_id)
         DO UPDATE SET granted_by = EXCLUDED.granted_by, granted_at = NOW(), expires_at = EXCLUDED.expires_at`,
        [assignmentId, userId, roleId, roleName, tenantId, actorId, expiresAt]
      );
      const verdict = this.createVerdict(
        "assign_role",
        "ALLOW" /* ALLOW */,
        actorId,
        `Assigned role ${roleName} to user ${userId}`
      );
      await provenanceLedger.appendEntry({
        action: "ROLE_ASSIGNED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          targetUserId: userId,
          roleId,
          roleName,
          expiresAt,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      logger_default2.info("Role assigned", { userId, roleId, roleName, tenantId });
      return createDataEnvelope(
        { success: true, message: `Role ${roleName} assigned successfully` },
        {
          source: "RoleManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
          governanceVerdict: verdict,
          warnings: expiresAt ? ["Role assignment has expiration date"] : []
        }
      );
    } catch (error) {
      logger_default2.error("Error assigning role:", error);
      throw error;
    }
  }
  /**
   * Revoke role from user
   */
  async revokeRoleFromUser(tenantId, userId, roleId, actorId) {
    try {
      const result2 = await this.pool.query(
        `DELETE FROM user_roles
         WHERE user_id = $1 AND role_id = $2 AND tenant_id = $3
         RETURNING role_name`,
        [userId, roleId, tenantId]
      );
      if (result2.rows.length === 0) {
        const verdict2 = this.createVerdict(
          "revoke_role",
          "FLAG" /* FLAG */,
          actorId,
          "Role assignment not found"
        );
        return createDataEnvelope(
          { success: false, message: "Role assignment not found" },
          {
            source: "RoleManagementService",
            actor: actorId,
            version: "1.0.0",
            classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
            governanceVerdict: verdict2,
            warnings: []
          }
        );
      }
      const roleName = result2.rows[0].role_name;
      const verdict = this.createVerdict(
        "revoke_role",
        "ALLOW" /* ALLOW */,
        actorId,
        `Revoked role ${roleName} from user ${userId}`
      );
      await provenanceLedger.appendEntry({
        action: "ROLE_REVOKED",
        actor: { id: actorId, role: "admin" },
        metadata: {
          tenantId,
          targetUserId: userId,
          roleId,
          roleName,
          verdictId: verdict.verdictId
        },
        artifacts: []
      });
      logger_default2.info("Role revoked", { userId, roleId, roleName, tenantId });
      return createDataEnvelope(
        { success: true, message: `Role ${roleName} revoked successfully` },
        {
          source: "RoleManagementService",
          actor: actorId,
          version: "1.0.0",
          classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
          governanceVerdict: verdict,
          warnings: []
        }
      );
    } catch (error) {
      logger_default2.error("Error revoking role:", error);
      throw error;
    }
  }
  /**
   * Get user's role assignments
   */
  async getUserRoles(tenantId, userId, actorId) {
    try {
      const result2 = await this.pool.query(
        `SELECT * FROM user_roles
         WHERE user_id = $1 AND tenant_id = $2
         ORDER BY granted_at DESC`,
        [userId, tenantId]
      );
      const assignments = result2.rows.map((row) => ({
        id: row.id,
        userId: row.user_id,
        roleId: row.role_id,
        roleName: row.role_name,
        tenantId: row.tenant_id,
        grantedBy: row.granted_by,
        grantedAt: row.granted_at,
        expiresAt: row.expires_at,
        isActive: !row.expires_at || new Date(row.expires_at) > /* @__PURE__ */ new Date()
      }));
      const verdict = this.createVerdict(
        "get_user_roles",
        "ALLOW" /* ALLOW */,
        actorId,
        `Retrieved ${assignments.length} role assignments for user ${userId}`
      );
      return createDataEnvelope(assignments, {
        source: "RoleManagementService",
        actor: actorId,
        version: "1.0.0",
        classification: "CONFIDENTIAL" /* CONFIDENTIAL */,
        governanceVerdict: verdict,
        warnings: []
      });
    } catch (error) {
      logger_default2.error("Error getting user roles:", error);
      throw error;
    }
  }
};
var roleManagementService = new RoleManagementService();

// src/routes/admin/roles.ts
init_logger2();
var router84 = express46.Router();
var authz8 = new AuthorizationServiceImpl();
var roleService = new RoleManagementService();
var singleParam19 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var buildPrincipal8 = (req, res, next) => {
  const user = req.user;
  if (!user) {
    res.status(401).json({ error: "Unauthorized", code: "AUTH_REQUIRED" });
    return;
  }
  const principal = {
    kind: "user",
    id: user.id,
    tenantId: req.headers["x-tenant-id"] || user.tenantId || "default-tenant",
    roles: [user.role],
    scopes: [],
    user: {
      email: user.email,
      username: user.username
    }
  };
  req.principal = principal;
  next();
};
var requireRolePermission = (action) => {
  return async (req, res, next) => {
    try {
      const principal = req.principal;
      await authz8.assertCan(principal, action, { type: "role", tenantId: principal.tenantId });
      next();
    } catch (error) {
      if (error.message.includes("Permission denied")) {
        res.status(403).json({
          error: "Forbidden",
          code: "PERMISSION_DENIED",
          required: `role:${action}`
        });
        return;
      }
      logger_default2.error("Authorization error:", error);
      res.status(500).json({ error: "Authorization service error" });
    }
  };
};
router84.get(
  "/",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const envelope = await roleService.listRoles(
        principal.tenantId,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing roles:", error);
      res.status(500).json({ error: "Failed to list roles", message: error.message });
    }
  }
);
router84.get(
  "/:id",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam19(req.params.id) ?? "";
      const envelope = await roleService.getRole(
        principal.tenantId,
        id,
        principal.id
      );
      if (!envelope.data) {
        res.status(404).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting role:", error);
      res.status(500).json({ error: "Failed to get role", message: error.message });
    }
  }
);
router84.post(
  "/",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("create"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const parseResult = createRoleSchema.safeParse(req.body);
      if (!parseResult.success) {
        res.status(400).json({
          error: "Validation failed",
          details: parseResult.error.errors
        });
        return;
      }
      const envelope = await roleService.createRole(
        principal.tenantId,
        parseResult.data,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.status(201).json(envelope);
    } catch (error) {
      logger_default2.error("Error creating role:", error);
      res.status(500).json({ error: "Failed to create role", message: error.message });
    }
  }
);
router84.patch(
  "/:id",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("update"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam19(req.params.id) ?? "";
      const parseResult = updateRoleSchema.safeParse(req.body);
      if (!parseResult.success) {
        res.status(400).json({
          error: "Validation failed",
          details: parseResult.error.errors
        });
        return;
      }
      const envelope = await roleService.updateRole(
        principal.tenantId,
        id,
        parseResult.data,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error updating role:", error);
      res.status(500).json({ error: "Failed to update role", message: error.message });
    }
  }
);
router84.delete(
  "/:id",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("delete"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const id = singleParam19(req.params.id) ?? "";
      const envelope = await roleService.deleteRole(
        principal.tenantId,
        id,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error deleting role:", error);
      res.status(500).json({ error: "Failed to delete role", message: error.message });
    }
  }
);
router84.get(
  "/permissions/list",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const envelope = await roleService.listPermissions(principal.id);
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error listing permissions:", error);
      res.status(500).json({ error: "Failed to list permissions", message: error.message });
    }
  }
);
router84.get(
  "/users/:userId",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("read"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const userId = singleParam19(req.params.userId) ?? "";
      const envelope = await roleService.getUserRoles(
        principal.tenantId,
        userId,
        principal.id
      );
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error getting user roles:", error);
      res.status(500).json({ error: "Failed to get user roles", message: error.message });
    }
  }
);
router84.post(
  "/assign",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("assign"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const { userId, roleId, expiresAt } = req.body;
      if (!userId || !roleId) {
        res.status(400).json({ error: "userId and roleId are required" });
        return;
      }
      const envelope = await roleService.assignRoleToUser(
        principal.tenantId,
        userId,
        roleId,
        principal.id,
        expiresAt ? new Date(expiresAt) : void 0
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error assigning role:", error);
      res.status(500).json({ error: "Failed to assign role", message: error.message });
    }
  }
);
router84.post(
  "/revoke",
  ensureAuthenticated,
  buildPrincipal8,
  requireRolePermission("revoke"),
  async (req, res) => {
    try {
      const principal = req.principal;
      const { userId, roleId } = req.body;
      if (!userId || !roleId) {
        res.status(400).json({ error: "userId and roleId are required" });
        return;
      }
      const envelope = await roleService.revokeRoleFromUser(
        principal.tenantId,
        userId,
        roleId,
        principal.id
      );
      if (!envelope.data.success) {
        res.status(400).json(envelope);
        return;
      }
      res.json(envelope);
    } catch (error) {
      logger_default2.error("Error revoking role:", error);
      res.status(500).json({ error: "Failed to revoke role", message: error.message });
    }
  }
);
var roles_default = router84;

// src/routes/admin/quota.ts
init_auth4();
init_logger2();
import express47 from "express";
var router85 = express47.Router();
router85.use(ensureAuthenticated);
router85.post("/tenants/:targetTenantId/plan", (req, res) => {
  try {
    const tenantContext = req.tenantContext;
    const { targetTenantId } = req.params;
    const { plan } = req.body;
    if (!tenantContext || !["elevated", "break-glass"].includes(tenantContext.privilegeTier)) {
      logger_default2.warn({
        event: "admin_access_denied",
        actorTenant: tenantContext?.tenantId || "unknown",
        targetTenant: targetTenantId,
        reason: "insufficient_privileges"
      }, "Admin access denied");
      return res.status(403).json({ error: "forbidden", message: "Insufficient privileges" });
    }
    if (!plan || typeof plan !== "string") {
      return res.status(400).json({ error: "bad_request", message: "Plan is required" });
    }
    logger_default2.info({
      event: "tenant_plan_updated",
      actor: tenantContext.subject || tenantContext.userId || "unknown",
      actorTenant: tenantContext.tenantId,
      targetTenant: targetTenantId,
      newPlan: plan,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    }, "Tenant plan updated via admin API");
    return res.json({
      status: "success",
      message: `Updated tenant ${targetTenantId} to plan ${plan}`,
      audited: true
    });
  } catch (error) {
    logger_default2.error({ err: error }, "Admin route error");
    return res.status(500).json({ error: "internal_error", message: String(error) });
  }
});
router85.get("/tenants/:targetTenantId/quota", (req, res) => {
  try {
    const tenantContext = req.tenantContext;
    const { targetTenantId } = req.params;
    if (!tenantContext || !["elevated", "break-glass"].includes(tenantContext.privilegeTier)) {
      if (tenantContext?.tenantId !== targetTenantId) {
        return res.status(403).json({ error: "forbidden", message: "Insufficient privileges" });
      }
    }
    return res.json({
      tenantId: targetTenantId,
      plan: "standard",
      limits: {
        apiCalls: { limit: 1e4, used: 0 },
        storage: { limit: "10GB", used: "0GB" }
      }
    });
  } catch (error) {
    logger_default2.error({ err: error }, "Admin route error");
    return res.status(500).json({ error: "internal_error", message: String(error) });
  }
});
var quota_default = router85;

// src/routes/admin/identity.ts
init_auth4();
init_database();
import express48 from "express";
init_logger2();

// src/db/tenant_repository.ts
init_database();
var TenantRepository = class {
  pool;
  tableName;
  constructor(tableName) {
    this.pool = getPostgresPool2();
    this.tableName = tableName;
  }
  async findById(tenantId, id) {
    const result2 = await this.pool.query(
      `SELECT * FROM ${this.tableName} WHERE id = $1 AND tenant_id = $2`,
      [id, tenantId]
    );
    return result2.rows[0] || null;
  }
  async findAll(tenantId) {
    const result2 = await this.pool.query(
      `SELECT * FROM ${this.tableName} WHERE tenant_id = $1`,
      [tenantId]
    );
    return result2.rows;
  }
  async create(tenantId, data) {
    const keys = Object.keys(data).filter((k) => k !== "id" && k !== "tenant_id");
    const values = keys.map((k) => data[k]);
    const placeholders = keys.map((_2, i) => `$${i + 3}`);
    const query3 = `
        INSERT INTO ${this.tableName} (tenant_id, ${keys.join(", ")})
        VALUES ($1, ${keys.map((_2, i) => `$${i + 2}`).join(", ")})
        RETURNING *
      `;
    const result2 = await this.pool.query(query3, [tenantId, ...values]);
    return result2.rows[0];
  }
  async update(tenantId, id, data) {
    const keys = Object.keys(data);
    if (keys.length === 0) return this.findById(tenantId, id);
    const setClause = keys.map((k, i) => `${k} = $${i + 3}`).join(", ");
    const query3 = `
        UPDATE ${this.tableName}
        SET ${setClause}, updated_at = NOW()
        WHERE id = $1 AND tenant_id = $2
        RETURNING *
      `;
    const result2 = await this.pool.query(query3, [id, tenantId, ...keys.map((k) => data[k])]);
    return result2.rows[0] || null;
  }
  async delete(tenantId, id) {
    const result2 = await this.pool.query(
      `DELETE FROM ${this.tableName} WHERE id = $1 AND tenant_id = $2`,
      [id, tenantId]
    );
    return (result2.rowCount ?? 0) > 0;
  }
};

// src/routes/admin/identity.ts
var router86 = express48.Router();
var authz9 = new AuthorizationServiceImpl();
var tenantRepo = new TenantRepository("tenants");
var apiKeyRepo = new TenantRepository("api_keys");
var buildPrincipal9 = (req, res, next) => {
  if (!req.user) return res.status(401).json({ error: "Unauthorized" });
  const principal = {
    kind: "user",
    // Default to user
    id: req.user.id,
    tenantId: req.headers["x-tenant-id"] || req.user.tenantId || "default-tenant",
    roles: [req.user.role],
    // Map existing role
    scopes: [],
    // Load scopes
    user: {
      email: req.user.email,
      username: req.user.username
    }
  };
  req.principal = principal;
  next();
};
router86.get("/tenants", ensureAuthenticated, buildPrincipal9, async (req, res) => {
  try {
    await authz9.assertCan(req.principal, "administer", { type: "system", tenantId: "system" });
    const pool4 = getPostgresPool2();
    const result2 = await pool4.query("SELECT * FROM tenants ORDER BY name");
    res.json(result2.rows);
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      return res.status(403).json({ error: "Forbidden" });
    }
    logger_default2.error("Error listing tenants", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
router86.post("/tenants", ensureAuthenticated, buildPrincipal9, async (req, res) => {
  try {
    await authz9.assertCan(req.principal, "administer", { type: "system", tenantId: "system" });
    const { name, slug } = req.body;
    const pool4 = getPostgresPool2();
    const result2 = await pool4.query(
      "INSERT INTO tenants (name, slug) VALUES ($1, $2) RETURNING *",
      [name, slug]
    );
    res.json(result2.rows[0]);
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      return res.status(403).json({ error: "Forbidden" });
    }
    logger_default2.error("Error creating tenant", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
router86.get("/tenants/current", ensureAuthenticated, buildPrincipal9, async (req, res) => {
  try {
    const tenantId = req.principal.tenantId;
    const tenant = await tenantRepo.findById(tenantId, tenantId);
    if (!tenant) {
      const pool4 = getPostgresPool2();
      const result2 = await pool4.query("SELECT * FROM tenants WHERE id = $1", [tenantId]);
      if (result2.rows.length === 0) return res.status(404).json({ error: "Tenant not found" });
      res.json(result2.rows[0]);
      return;
    }
    res.json(tenant);
  } catch (error) {
    logger_default2.error("Error getting current tenant", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
router86.get("/api-keys", ensureAuthenticated, buildPrincipal9, async (req, res) => {
  try {
    await authz9.assertCan(req.principal, "view", { type: "api_key", tenantId: req.principal.tenantId });
    const keys = await apiKeyRepo.findAll(req.principal.tenantId);
    const safeKeys = keys.map((k) => {
      const { hashedSecret, ...rest } = k;
      return rest;
    });
    res.json(safeKeys);
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      return res.status(403).json({ error: "Forbidden" });
    }
    logger_default2.error("Error listing api keys", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
router86.post("/api-keys", ensureAuthenticated, buildPrincipal9, async (req, res) => {
  try {
    await authz9.assertCan(req.principal, "create", { type: "api_key", tenantId: req.principal.tenantId });
    const { label, scopes } = req.body;
    const crypto53 = await import("crypto");
    const secret = crypto53.randomBytes(32).toString("hex");
    const keyPrefix = secret.substring(0, 8);
    const hashedSecret = crypto53.createHash("sha256").update(secret).digest("hex");
    const newKey = await apiKeyRepo.create(req.principal.tenantId, {
      key_prefix: keyPrefix,
      // mapping snake_case because repo is generic but uses passed object keys
      hashed_secret: hashedSecret,
      label,
      scopes: scopes || [],
      created_by_user_id: req.principal.id
    });
    res.json({
      ...newKey,
      secret
    });
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      return res.status(403).json({ error: "Forbidden" });
    }
    logger_default2.error("Error creating api key", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
router86.delete("/api-keys/:id", ensureAuthenticated, buildPrincipal9, async (req, res) => {
  try {
    await authz9.assertCan(req.principal, "delete", { type: "api_key", tenantId: req.principal.tenantId });
    const { id } = req.params;
    await apiKeyRepo.update(req.principal.tenantId, id, {
      revoked_at: /* @__PURE__ */ new Date()
    });
    res.json({ success: true });
  } catch (error) {
    if (error.message.includes("Permission denied")) {
      return res.status(403).json({ error: "Forbidden" });
    }
    logger_default2.error("Error revoking api key", error);
    res.status(500).json({ error: "Internal Server Error" });
  }
});
var identity_default = router86;

// src/routes/admin.ts
import express49 from "express";
import fs36 from "fs";
import path39 from "path";
import { fileURLToPath as fileURLToPath2 } from "url";
import axios8 from "axios";

// src/temporal/index.ts
init_logger();
var logger51 = logger_default.child({ name: "temporal" });
async function startTemporalWorker() {
  if (process.env.TEMPORAL_ENABLED !== "true") {
    logger51.info("Temporal disabled");
    return { stop: async () => {
    } };
  }
  try {
    const temporal = await import("temporalio/worker");
    const { activities: activities2 } = await Promise.resolve().then(() => (init_activities(), activities_exports));
    const { default: workflowsPath } = await Promise.resolve().then(() => (init_workflows_path(), workflows_path_exports));
    const worker = await temporal.Worker.create({
      workflowsPath,
      activities: activities2,
      taskQueue: process.env.TEMPORAL_TASK_QUEUE || "maestro-core",
      namespace: process.env.TEMPORAL_NAMESPACE || "default",
      connection: await (await import("temporalio")).Connection.connect()
    });
    logger51.info("Temporal worker created");
    const runPromise = worker.run();
    return {
      stop: async () => {
        try {
          await worker.shutdown();
        } catch {
        }
        try {
          await runPromise;
        } catch {
        }
      }
    };
  } catch (e) {
    logger51.warn(
      { err: e?.message || String(e) },
      "Temporal not available; continuing without it"
    );
    return { stop: async () => {
    } };
  }
}

// src/temporal/control.ts
var handle = null;
async function enableTemporal() {
  if (process.env.TEMPORAL_ENABLED === "true")
    return { ok: true, message: "already enabled" };
  process.env.TEMPORAL_ENABLED = "true";
  handle = await startTemporalWorker();
  return { ok: true };
}
async function disableTemporal() {
  if (process.env.TEMPORAL_ENABLED !== "true")
    return { ok: true, message: "already disabled" };
  process.env.TEMPORAL_ENABLED = "false";
  try {
    await handle?.stop();
  } catch {
  }
  handle = null;
  return { ok: true };
}

// src/routes/admin.ts
init_auth4();

// src/security/permissions.ts
var PERMISSIONS = {
  READ_GRAPH: "read_graph",
  WRITE_GRAPH: "write_graph",
  RUN_MAESTRO: "run_maestro",
  VIEW_DASHBOARDS: "view_dashboards",
  MANAGE_USERS: "manage_users",
  MANAGE_SETTINGS: "manage_settings"
};
var PERMISSION_ALIASES = {
  "graph:read": PERMISSIONS.READ_GRAPH,
  "graph:export": PERMISSIONS.READ_GRAPH,
  "entity:create": PERMISSIONS.WRITE_GRAPH,
  "entity:update": PERMISSIONS.WRITE_GRAPH,
  "entity:delete": PERMISSIONS.WRITE_GRAPH,
  "relationship:create": PERMISSIONS.WRITE_GRAPH,
  "relationship:update": PERMISSIONS.WRITE_GRAPH,
  "relationship:delete": PERMISSIONS.WRITE_GRAPH,
  "investigation:create": PERMISSIONS.WRITE_GRAPH,
  "investigation:update": PERMISSIONS.WRITE_GRAPH,
  "investigation:read": PERMISSIONS.READ_GRAPH,
  "tag:create": PERMISSIONS.WRITE_GRAPH,
  "tag:read": PERMISSIONS.READ_GRAPH,
  "tag:delete": PERMISSIONS.WRITE_GRAPH,
  "run:create": PERMISSIONS.RUN_MAESTRO,
  "run:read": PERMISSIONS.RUN_MAESTRO,
  "run:update": PERMISSIONS.RUN_MAESTRO,
  "pipeline:create": PERMISSIONS.RUN_MAESTRO,
  "pipeline:update": PERMISSIONS.RUN_MAESTRO,
  "pipeline:read": PERMISSIONS.RUN_MAESTRO,
  "routing:override": PERMISSIONS.RUN_MAESTRO,
  "dashboard:read": PERMISSIONS.VIEW_DASHBOARDS,
  "admin:access": PERMISSIONS.MANAGE_USERS,
  "admin:*": PERMISSIONS.MANAGE_USERS,
  "override:manage": PERMISSIONS.MANAGE_SETTINGS
};
var ROLE_PERMISSIONS2 = {
  ADMIN: ["*"],
  ANALYST: [
    PERMISSIONS.READ_GRAPH,
    PERMISSIONS.WRITE_GRAPH,
    PERMISSIONS.VIEW_DASHBOARDS,
    "investigation:create",
    "investigation:read",
    "investigation:update",
    "entity:create",
    "entity:read",
    "entity:update",
    "entity:delete",
    "relationship:create",
    "relationship:read",
    "relationship:update",
    "relationship:delete",
    "tag:create",
    "tag:read",
    "tag:delete",
    "graph:read",
    "graph:export",
    "ai:request"
  ],
  OPERATOR: [
    PERMISSIONS.READ_GRAPH,
    PERMISSIONS.RUN_MAESTRO,
    PERMISSIONS.VIEW_DASHBOARDS,
    PERMISSIONS.MANAGE_SETTINGS
  ],
  SERVICE_ACCOUNT: [PERMISSIONS.READ_GRAPH, PERMISSIONS.WRITE_GRAPH],
  VIEWER: [PERMISSIONS.READ_GRAPH, "graph:read", "graph:export", "investigation:read"]
};
function normalizePermission(permission) {
  if (!permission) return null;
  const lower = permission.toString().toLowerCase();
  const canonical = Object.values(PERMISSIONS).find((candidate) => candidate === lower);
  if (canonical) return canonical;
  const alias = PERMISSION_ALIASES[permission] || PERMISSION_ALIASES[lower];
  if (alias) return alias;
  return permission;
}
function permissionsForRole(role) {
  if (!role) return [];
  const normalizedRole = role.toUpperCase();
  return ROLE_PERMISSIONS2[normalizedRole] || [];
}
function userHasPermission(user, permission) {
  if (!user || !user.role) return false;
  const normalizedPermission = normalizePermission(permission);
  const normalizedRole = user.role.toUpperCase();
  if (normalizedRole === "ADMIN") return true;
  if (!normalizedPermission) return false;
  if (user.permissions?.includes("*")) return true;
  const userExplicitPermissions = (user.permissions || []).map((perm) => normalizePermission(perm)).filter(Boolean);
  if (userExplicitPermissions.includes(normalizedPermission)) return true;
  const rolePermissions = permissionsForRole(user.role);
  return rolePermissions.includes("*") || rolePermissions.includes(normalizedPermission);
}

// src/middleware/authorization.ts
function authorize(requiredPermission) {
  return (req, res, next) => {
    if (!req.user) {
      return res.status(401).json({ error: "Authentication required" });
    }
    const normalized = normalizePermission(requiredPermission);
    const allowed = normalized && userHasPermission(req.user, normalized);
    if (!allowed) {
      return res.status(403).json({
        error: "Forbidden",
        required: normalized || requiredPermission,
        actorRole: req.user.role
      });
    }
    return next();
  };
}

// src/routes/admin.ts
init_GAEnrollmentService();
init_database();

// src/services/ShadowService.ts
init_logger2();
import axios7 from "axios";
var ShadowService = class _ShadowService {
  static instance;
  constructor() {
  }
  static getInstance() {
    if (!_ShadowService.instance) {
      _ShadowService.instance = new _ShadowService();
    }
    return _ShadowService.instance;
  }
  /**
   * Async shadow request. Fires and forgets (but logs).
   */
  shadow(req, config9) {
    const { method, url, headers, body: body4 } = req;
    const shadowHeaders = { ...headers };
    delete shadowHeaders["host"];
    delete shadowHeaders["content-length"];
    shadowHeaders["X-Summit-Shadow-Request"] = "true";
    const targetUrl = `${config9.targetUrl}${url}`;
    logger_default2.info({ targetUrl, method }, "ShadowService: Mirroring traffic");
    axios7({
      method,
      url: targetUrl,
      headers: shadowHeaders,
      data: body4,
      timeout: 5e3
    }).then((response) => {
      logger_default2.debug({
        targetUrl,
        status: response.status
        // comparison would go here if enabled
      }, "ShadowService: Shadow request successful");
    }).catch((err) => {
      logger_default2.warn({
        targetUrl,
        error: err.message
      }, "ShadowService: Shadow request failed");
    });
  }
};
var shadowService = ShadowService.getInstance();

// src/middleware/ShadowTrafficMiddleware.ts
init_postgres();
import { LRUCache as LRUCache3 } from "lru-cache";
var configCache = new LRUCache3({
  max: 1e3,
  ttl: 60 * 1e3
  // 1 minute
});
var getShadowConfig = async (tenantId) => {
  if (configCache.has(tenantId)) {
    return configCache.get(tenantId);
  }
  try {
    const pool4 = getPostgresPool();
    const result2 = await pool4.query(
      'SELECT target_url as "targetUrl", sampling_rate as "samplingRate", compare_responses as "compareResponses" FROM shadow_traffic_configs WHERE tenant_id = $1',
      [tenantId]
    );
    const config9 = result2.rows.length > 0 ? result2.rows[0] : void 0;
    if (config9) {
      configCache.set(tenantId, config9);
    }
    return config9;
  } catch (error) {
    if (error.message.includes('relation "shadow_traffic_configs" does not exist')) {
      return void 0;
    }
    throw error;
  }
};
var clearShadowCache = (tenantId) => {
  configCache.delete(tenantId);
};
var shadowTrafficMiddleware = async (req, res, next) => {
  const tenantId = req.user?.tenantId || req.tenantId;
  if (!tenantId) {
    return next();
  }
  try {
    const config9 = await getShadowConfig(tenantId);
    if (!config9) {
      return next();
    }
    if (Math.random() > config9.samplingRate) {
      return next();
    }
    if (req.headers["x-summit-shadow-request"] === "true") {
      return next();
    }
    shadowService.shadow({
      method: req.method,
      url: req.originalUrl,
      headers: req.headers,
      body: req.body
    }, config9);
  } catch (error) {
    console.error("[ShadowTrafficMiddleware] Error:", error);
  }
  next();
};

// src/routes/admin.ts
init_RegionalAvailabilityService();

// src/services/secretManager.ts
import fs35 from "fs/promises";
import path38 from "path";
var secretsFilePath = path38.join("/tmp", "secrets.json");
async function readSecrets() {
  try {
    const data = await fs35.readFile(secretsFilePath, "utf-8");
    return JSON.parse(data);
  } catch (error) {
    if (error.code === "ENOENT") {
      return {
        TEST_SECRET: {
          v1: "secret-value-1",
          v2: "secret-value-2"
        }
      };
    }
    throw error;
  }
}
async function writeSecrets(secrets) {
  await fs35.writeFile(secretsFilePath, JSON.stringify(secrets, null, 2));
}
var SecretManager2 = class {
  async getSecret(secretName, version) {
    const secrets = await readSecrets();
    return secrets[secretName]?.[version];
  }
  async setSecret(secretName, version, value) {
    const secrets = await readSecrets();
    if (!secrets[secretName]) {
      secrets[secretName] = {};
    }
    secrets[secretName][version] = value;
    await writeSecrets(secrets);
  }
};

// src/services/serviceRegistry.ts
var services = {
  "test-service-1": {
    healthUrl: "http://localhost:4001/health"
  },
  "test-service-2": {
    healthUrl: "http://localhost:4002/health"
  }
};
var MockServiceRegistry = class {
  getServiceHealthUrl(serviceName) {
    return services[serviceName]?.healthUrl;
  }
};

// src/routes/admin.ts
var memConfig = {
  REQUIRE_BUDGET_PLUGIN: process.env.REQUIRE_BUDGET_PLUGIN === "true",
  RUNS_EXECUTE_ENABLED: process.env.RUNS_EXECUTE_ENABLED !== "false",
  AUTONOMY_LEVEL: Number(process.env.AUTONOMY_LEVEL || "1"),
  RATE_LIMIT_MAX: Number(process.env.RATE_LIMIT_MAX || "600"),
  BUDGET_CAP_USD: Number(process.env.BUDGET_CAP_USD || "10"),
  MAESTRO_LANGCHAIN_ENABLED: process.env.MAESTRO_LANGCHAIN_ENABLED === "true",
  MAESTRO_COMFY_ENABLED: process.env.MAESTRO_COMFY_ENABLED === "true",
  MODEL_PROVIDER: process.env.MODEL_PROVIDER || "openai",
  MODEL_NAME: process.env.MODEL_NAME || process.env.MODEL_DEFAULT || "gpt-4-turbo-preview",
  TEMPERATURE: Number(process.env.TEMPERATURE ?? 0.2),
  TOP_P: Number(process.env.TOP_P ?? 1),
  MAX_TOKENS: Number(
    process.env.MAX_TOKENS || process.env.TOKEN_CEILING || 4096
  ),
  RESEARCH_PROMPT_ENABLED: process.env.RESEARCH_PROMPT_ENABLED === "true",
  RESEARCH_PROMPT_PATH: process.env.RESEARCH_PROMPT_PATH || "",
  TENANT_DEFAULTS: JSON.parse(process.env.TENANT_DEFAULTS || "{}"),
  TENANT_OVERRIDES: {}
};
var VALID_PROVIDERS = ["openai", "anthropic", "gemini", "oss", "local"];
function validateConfig(config9) {
  const errors = [];
  if (config9.MODEL_PROVIDER && !VALID_PROVIDERS.includes(config9.MODEL_PROVIDER)) {
    errors.push(
      `Invalid MODEL_PROVIDER. Must be one of: ${VALID_PROVIDERS.join(", ")}`
    );
  }
  if (config9.TEMPERATURE !== void 0) {
    const temp = Number(config9.TEMPERATURE);
    if (isNaN(temp) || temp < 0 || temp > 1) {
      errors.push("TEMPERATURE must be a number between 0 and 1");
    }
  }
  if (config9.TOP_P !== void 0) {
    const topP = Number(config9.TOP_P);
    if (isNaN(topP) || topP < 0 || topP > 1) {
      errors.push("TOP_P must be a number between 0 and 1");
    }
  }
  if (config9.MAX_TOKENS !== void 0) {
    const maxTokens = Number(config9.MAX_TOKENS);
    if (isNaN(maxTokens) || maxTokens < 1 || maxTokens > 1e5) {
      errors.push("MAX_TOKENS must be a number between 1 and 100000");
    }
  }
  if (config9.BUDGET_CAP_USD !== void 0) {
    const budget = Number(config9.BUDGET_CAP_USD);
    if (isNaN(budget) || budget < 0) {
      errors.push("BUDGET_CAP_USD must be a non-negative number");
    }
    if (memConfig.REQUIRE_BUDGET_PLUGIN && budget > 100) {
      errors.push(
        "Budget cap exceeds policy limit of $100 when REQUIRE_BUDGET_PLUGIN is enabled"
      );
    }
  }
  return { isValid: errors.length === 0, errors };
}
var router87 = express49.Router();
router87.use(ensureAuthenticated, authorize("manage_users"));
router87.get("/ga/signals", async (_req, res) => {
  try {
    const config9 = await GAEnrollmentService_default.getConfig();
    const pool4 = getPostgresPool2();
    const userCountRes = await pool4.query("SELECT COUNT(*) FROM users");
    const tenantCountRes = await pool4.query("SELECT COUNT(*) FROM tenants");
    const userCount = parseInt(userCountRes.rows[0].count, 10);
    const tenantCount = parseInt(tenantCountRes.rows[0].count, 10);
    res.json({
      config: config9,
      stats: {
        users: {
          current: userCount,
          max: config9.maxUsers,
          utilization: userCount / config9.maxUsers * 100
        },
        tenants: {
          current: tenantCount,
          max: config9.maxTenants,
          utilization: tenantCount / config9.maxTenants * 100
        }
      },
      status: config9.status
    });
  } catch (error) {
    res.status(500).json({ ok: false, error: error instanceof Error ? error.message : "Unknown error" });
  }
});
router87.post("/ga/config", express49.json(), async (req, res) => {
  try {
    const { status, maxTenants, maxUsers } = req.body;
    const updates = {};
    if (status) updates.status = status;
    if (maxTenants) updates.maxTenants = maxTenants;
    if (maxUsers) updates.maxUsers = maxUsers;
    await GAEnrollmentService_default.updateConfig(updates);
    res.json({ ok: true, config: await GAEnrollmentService_default.getConfig() });
  } catch (error) {
    res.status(500).json({ ok: false, error: error instanceof Error ? error.message : "Unknown error" });
  }
});
router87.get("/config", (req, res) => {
  const tenantId = req.query.tenantId || "";
  if (tenantId && memConfig.TENANT_OVERRIDES && memConfig.TENANT_OVERRIDES[tenantId]) {
    return res.json({ ...memConfig, ...memConfig.TENANT_OVERRIDES[tenantId] });
  }
  res.json(memConfig);
});
router87.post("/config", express49.json(), (req, res) => {
  const tenantId = req.query.tenantId || "";
  const allowed = Object.keys(memConfig).filter(
    (k) => !["TENANT_OVERRIDES", "TENANT_DEFAULTS"].includes(k)
  );
  const validation = validateConfig(req.body);
  if (!validation.isValid) {
    return res.status(400).json({
      ok: false,
      errors: validation.errors
    });
  }
  if (tenantId) {
    memConfig.TENANT_OVERRIDES = memConfig.TENANT_OVERRIDES || {};
    const cur = memConfig.TENANT_OVERRIDES[tenantId] || {};
    for (const k of allowed) {
      if (k in req.body) {
        cur[k] = req.body[k];
      }
    }
    memConfig.TENANT_OVERRIDES[tenantId] = cur;
    return res.json({
      ok: true,
      config: { ...memConfig, ...cur },
      message: `Configuration updated for tenant: ${tenantId}`
    });
  } else {
    for (const k of allowed) {
      if (k in req.body) {
        memConfig[k] = req.body[k];
        if (k === "RESEARCH_PROMPT_ENABLED") {
          process.env.RESEARCH_PROMPT_ENABLED = memConfig[k] ? "true" : "false";
        }
        if (k === "RESEARCH_PROMPT_PATH") {
          process.env.RESEARCH_PROMPT_PATH = String(memConfig[k] || "");
        }
        if (k === "TEMPORAL_ENABLED") {
          process.env.TEMPORAL_ENABLED = memConfig[k] ? "true" : "false";
        }
      }
    }
    return res.json({
      ok: true,
      config: memConfig,
      message: "Global configuration updated"
    });
  }
});
router87.post("/tenant-defaults", express49.json(), (req, res) => {
  const { tenantId, config: config9 } = req.body;
  if (!tenantId) {
    return res.status(400).json({
      ok: false,
      error: "tenantId is required"
    });
  }
  const validation = validateConfig(config9);
  if (!validation.isValid) {
    return res.status(400).json({
      ok: false,
      errors: validation.errors
    });
  }
  memConfig.TENANT_DEFAULTS[tenantId] = config9;
  res.json({
    ok: true,
    message: `Tenant defaults updated for: ${tenantId}`,
    tenantDefaults: memConfig.TENANT_DEFAULTS
  });
});
router87.get("/health", (req, res) => {
  res.json({ status: "ok" });
});
router87.get("/tenant-defaults", (req, res) => {
  res.json({
    tenantDefaults: memConfig.TENANT_DEFAULTS
  });
});
router87.get("/failover/status", (req, res) => {
  const availability = RegionalAvailabilityService.getInstance();
  res.json(availability.getStatus());
});
router87.post("/failover/mode", express49.json(), (req, res) => {
  const { mode } = req.body;
  if (mode !== "AUTOMATIC" && mode !== "MANUAL_PROMOTION_ACTIVE") {
    return res.status(400).json({ ok: false, error: "Invalid mode" });
  }
  const availability = RegionalAvailabilityService.getInstance();
  availability.setFailoverMode(mode);
  res.json({ ok: true, mode });
});
router87.post("/failover/status", express49.json(), (req, res) => {
  const { region, status } = req.body;
  if (!region || !["HEALTHY", "DEGRADED", "DOWN"].includes(status)) {
    return res.status(400).json({ ok: false, error: "Invalid region or status" });
  }
  try {
    const availability = RegionalAvailabilityService.getInstance();
    availability.setRegionStatus(region, status);
    res.json({ ok: true, region, status });
  } catch (error) {
    res.status(400).json({ ok: false, error: error.message });
  }
});
router87.get("/shadow/configs", async (req, res) => {
  try {
    const pool4 = getPostgresPool2();
    const result2 = await pool4.query("SELECT * FROM shadow_traffic_configs");
    res.json(result2.rows);
  } catch (error) {
    res.status(500).json({ ok: false, error: error.message });
  }
});
router87.post("/shadow/configs", express49.json(), async (req, res) => {
  const { tenantId, targetUrl, samplingRate, compareResponses } = req.body;
  if (!tenantId || !targetUrl) {
    return res.status(400).json({ ok: false, error: "tenantId and targetUrl are required" });
  }
  try {
    const pool4 = getPostgresPool2();
    await pool4.query(
      `INSERT INTO shadow_traffic_configs (tenant_id, target_url, sampling_rate, compare_responses)
       VALUES ($1, $2, $3, $4)
       ON CONFLICT (tenant_id) DO UPDATE 
       SET target_url = EXCLUDED.target_url, 
           sampling_rate = EXCLUDED.sampling_rate, 
           compare_responses = EXCLUDED.compare_responses,
           updated_at = CURRENT_TIMESTAMP`,
      [tenantId, targetUrl, samplingRate ?? 0, compareResponses ?? false]
    );
    clearShadowCache(tenantId);
    res.json({ ok: true, message: "Shadow traffic config updated" });
  } catch (error) {
    res.status(500).json({ ok: false, error: error.message });
  }
});
router87.delete("/shadow/configs/:tenantId", async (req, res) => {
  const { tenantId } = req.params;
  try {
    const pool4 = getPostgresPool2();
    await pool4.query("DELETE FROM shadow_traffic_configs WHERE tenant_id = $1", [tenantId]);
    clearShadowCache(tenantId);
    res.json({ ok: true, message: "Shadow traffic config deleted" });
  } catch (error) {
    res.status(500).json({ ok: false, error: error.message });
  }
});
var admin_default = router87;
var __filename2 = fileURLToPath2(import.meta.url);
var __dirname2 = path39.dirname(__filename2);
var n8nCfgPath = path39.resolve(__dirname2, "../../config/n8n-flows.json");
router87.get("/n8n-flows", (_req, res) => {
  try {
    const raw = fs36.readFileSync(n8nCfgPath, "utf8");
    return res.json(JSON.parse(raw));
  } catch {
    return res.json({
      allowedPrefixes: ["integration/"],
      deniedPrefixes: ["deploy/", "db/"],
      allowedFlows: []
    });
  }
});
router87.post("/n8n-flows", express49.json(), (req, res) => {
  const body4 = req.body || {};
  const allowedPrefixes = Array.isArray(body4.allowedPrefixes) ? body4.allowedPrefixes : ["integration/"];
  const deniedPrefixes = Array.isArray(body4.deniedPrefixes) ? body4.deniedPrefixes : ["deploy/", "db/"];
  const allowedFlows = Array.isArray(body4.allowedFlows) ? body4.allowedFlows.filter((s) => typeof s === "string") : [];
  const out = { allowedPrefixes, deniedPrefixes, allowedFlows };
  try {
    fs36.mkdirSync(path39.dirname(n8nCfgPath), { recursive: true });
    fs36.writeFileSync(n8nCfgPath, JSON.stringify(out, null, 2) + "\n");
    return res.json({ ok: true, config: out });
  } catch (e) {
    return res.status(500).json({ ok: false, error: e?.message || "failed to write config" });
  }
});
router87.get("/opa/validate", async (_req, res) => {
  try {
    const base = process.env.OPA_BASE_URL || "";
    if (!base)
      return res.status(200).json({ ok: false, message: "OPA_BASE_URL not set" });
    const health = await axios8.get(`${base}/health`, { timeout: 3e3 }).then((r) => r.status);
    let evalOk = false;
    let evalReason = "";
    try {
      const test = await axios8.post(
        `${base}/v1/data/maestro/integrations/n8n/trigger`,
        {
          input: {
            tenantId: "test",
            role: "ADMIN",
            resource: "integration/test"
          }
        },
        { timeout: 3e3 }
      );
      evalOk = Boolean(test.data?.result?.allow ?? true);
      evalReason = String(test.data?.result?.reason ?? "ok");
    } catch (e) {
      evalOk = false;
      evalReason = e?.message || "eval failed";
    }
    return res.json({ ok: health === 200, health, evalOk, evalReason });
  } catch (e) {
    return res.status(200).json({ ok: false, message: e?.message || "OPA unreachable" });
  }
});
router87.post("/opa/reload", async (_req, res) => {
  return res.json({
    ok: true,
    message: "Reload request acknowledged (bundle-managed in production)"
  });
});
router87.get("/opa/bundle-source", async (_req, res) => {
  try {
    const base = process.env.OPA_BASE_URL || "";
    if (!base) return res.json({ ok: false, message: "OPA_BASE_URL not set" });
    const candidates2 = [`${base}/status`, `${base}/v1/status`];
    let result2 = null;
    for (const url of candidates2) {
      try {
        const r = await axios8.get(url, { timeout: 3e3 });
        result2 = r.data;
        break;
      } catch (_2) {
      }
    }
    if (!result2)
      return res.json({ ok: false, message: "status endpoint not available" });
    const bundles = result2?.bundles || result2?.plugins?.bundle?.status || {};
    const names = Object.keys(bundles);
    const info = names.slice(0, 3).map((n) => ({
      name: n,
      revision: bundles[n]?.revision || bundles[n]?.active_revision || bundles[n]?.manifest?.revision,
      last_success: bundles[n]?.last_successful_download?.time || bundles[n]?.last_successful_activation?.time
    }));
    return res.json({ ok: true, bundleNames: names, info });
  } catch (e) {
    return res.json({
      ok: false,
      message: e?.message || "bundle source read failed"
    });
  }
});
router87.get("/opa/bundle-status", async (_req, res) => {
  try {
    const base = process.env.OPA_BASE_URL || "";
    if (!base) return res.json({ ok: false, message: "OPA_BASE_URL not set" });
    const r = await axios8.get(`${base}/v1/data/maestro/n8n/allowed_flows`, {
      timeout: 3e3
    });
    const flows = r.data?.result || {};
    const keys = Object.keys(flows || {});
    return res.json({
      ok: true,
      allowedFlowsCount: keys.length,
      sample: keys.slice(0, 10)
    });
  } catch (e) {
    return res.json({ ok: false, message: e?.message || "lookup failed" });
  }
});
router87.post("/opa/push-n8n-flows", async (_req, res) => {
  try {
    const base = process.env.OPA_BASE_URL || "";
    if (!base)
      return res.status(200).json({ ok: false, message: "OPA_BASE_URL not set" });
    let cfg2 = { allowedFlows: [] };
    try {
      cfg2 = JSON.parse(fs36.readFileSync(n8nCfgPath, "utf8"));
    } catch {
    }
    const map = {};
    for (const f of cfg2.allowedFlows || []) map[f] = true;
    await axios8.put(`${base}/v1/data/maestro/n8n/allowed_flows`, map, {
      timeout: 5e3
    });
    return res.json({ ok: true, count: Object.keys(map).length });
  } catch (e) {
    return res.status(200).json({ ok: false, message: e?.message || "push failed" });
  }
});
router87.post("/opa/sync-n8n-flows", async (_req, res) => {
  try {
    const base = process.env.OPA_BASE_URL || "";
    if (!base)
      return res.status(200).json({ ok: false, message: "OPA_BASE_URL not set" });
    const r = await axios8.get(`${base}/v1/data/maestro/n8n/allowed_flows`, {
      timeout: 5e3
    });
    const flows = r.data?.result || {};
    const allowedFlows = Object.keys(flows || {}).filter((k) => flows[k]);
    let cfg2 = {
      allowedPrefixes: ["integration/"],
      deniedPrefixes: ["deploy/", "db/"],
      allowedFlows: []
    };
    try {
      cfg2 = JSON.parse(fs36.readFileSync(n8nCfgPath, "utf8"));
    } catch {
    }
    const out = { ...cfg2, allowedFlows };
    fs36.mkdirSync(path39.dirname(n8nCfgPath), { recursive: true });
    fs36.writeFileSync(n8nCfgPath, JSON.stringify(out, null, 2) + "\n");
    return res.json({ ok: true, count: allowedFlows.length, config: out });
  } catch (e) {
    return res.status(200).json({ ok: false, message: e?.message || "sync failed" });
  }
});
var secretManager = new SecretManager2();
var serviceRegistry = new MockServiceRegistry();
router87.post("/secrets/rotate", express49.json(), async (req, res) => {
  const { secretName, newVersion, services: services2 } = req.body;
  if (!secretName || !newVersion || !services2) {
    return res.status(400).json({ ok: false, error: "Missing required parameters" });
  }
  console.log(`Rotating secret "${secretName}" to version "${newVersion}" for services:`, services2);
  const previousSecret = await secretManager.getSecret(secretName, "current");
  if (!previousSecret) {
    return res.status(404).json({ ok: false, error: `Secret not found: ${secretName}` });
  }
  for (const service11 of services2) {
    console.log(`Updating secret for service: ${service11}`);
    await secretManager.setSecret(secretName, "current", await secretManager.getSecret(secretName, newVersion));
    console.log(`Health check for service ${service11}...`);
    const healthUrl = serviceRegistry.getServiceHealthUrl(service11);
    if (!healthUrl) {
      console.error(`Health check URL not found for service: ${service11}`);
      continue;
    }
    const health = await axios8.get(healthUrl, { timeout: 5e3 }).then((res2) => res2.data);
    if (health.status !== "ok") {
      console.error(`Service ${service11} is unhealthy after secret rotation. Rolling back...`);
      await secretManager.setSecret(secretName, "current", previousSecret);
      return res.status(500).json({ ok: false, error: `Service ${service11} failed to restart with new secret` });
    }
    console.log(`Service ${service11} is healthy.`);
  }
  res.json({ ok: true, message: "Secret rotation completed successfully" });
});
router87.post("/temporal/toggle", async (req, res) => {
  const enabled = Boolean(req.body?.enabled);
  try {
    if (enabled) {
      await enableTemporal();
    } else {
      await disableTemporal();
    }
    memConfig.TEMPORAL_ENABLED = enabled;
    process.env.TEMPORAL_ENABLED = enabled ? "true" : "false";
    return res.json({ ok: true, enabled });
  } catch (e) {
    return res.status(500).json({ ok: false, error: e?.message || "temporal toggle failed" });
  }
});
router87.post("/opa/push-n8n-prefixes", async (_req, res) => {
  try {
    const base = process.env.OPA_BASE_URL || "";
    if (!base)
      return res.status(200).json({ ok: false, message: "OPA_BASE_URL not set" });
    let cfg2 = {
      allowedPrefixes: ["integration/"],
      deniedPrefixes: ["deploy/", "db/"]
    };
    try {
      cfg2 = JSON.parse(fs36.readFileSync(n8nCfgPath, "utf8"));
    } catch {
    }
    await axios8.put(
      `${base}/v1/data/maestro/n8n/allowed_prefixes`,
      (cfg2.allowedPrefixes || []).reduce(
        (m, p) => (m[p] = true, m),
        {}
      ),
      { timeout: 5e3 }
    );
    await axios8.put(
      `${base}/v1/data/maestro/n8n/denied_prefixes`,
      (cfg2.deniedPrefixes || []).reduce(
        (m, p) => (m[p] = true, m),
        {}
      ),
      { timeout: 5e3 }
    );
    return res.json({ ok: true });
  } catch (e) {
    return res.status(200).json({ ok: false, message: e?.message || "push prefixes failed" });
  }
});
router87.post("/opa/sync-n8n-prefixes", async (_req, res) => {
  try {
    const base = process.env.OPA_BASE_URL || "";
    if (!base)
      return res.status(200).json({ ok: false, message: "OPA_BASE_URL not set" });
    const ar = await axios8.get(`${base}/v1/data/maestro/n8n/allowed_prefixes`, {
      timeout: 5e3
    });
    const dr = await axios8.get(`${base}/v1/data/maestro/n8n/denied_prefixes`, {
      timeout: 5e3
    });
    const allowedPrefixes = Object.keys(ar.data?.result || {});
    const deniedPrefixes = Object.keys(dr.data?.result || {});
    let cfg2 = {
      allowedPrefixes: [],
      deniedPrefixes: [],
      allowedFlows: []
    };
    try {
      cfg2 = JSON.parse(fs36.readFileSync(n8nCfgPath, "utf8"));
    } catch {
    }
    const out = { ...cfg2, allowedPrefixes, deniedPrefixes };
    fs36.mkdirSync(path39.dirname(n8nCfgPath), { recursive: true });
    fs36.writeFileSync(n8nCfgPath, JSON.stringify(out, null, 2) + "\n");
    return res.json({ ok: true, config: out });
  } catch (e) {
    return res.status(200).json({ ok: false, message: e?.message || "sync prefixes failed" });
  }
});

// src/routes/admin/gateway.ts
var router88 = express50.Router();
router88.use("/tenants", tenants_default2);
router88.use("/users", users_default);
router88.use("/roles", roles_default);
router88.use("/quota", quota_default);
router88.use("/identity", identity_default);
router88.use("/", admin_default);
var gateway_default = router88;

// src/routes/onboarding.ts
import { Router as Router43 } from "express";
import { z as z40 } from "zod";

// src/onboarding/EnhancedOnboardingService.ts
init_database();
init_logger2();
init_data_envelope();
import { randomUUID as randomUUID56, createHash as createHash31 } from "crypto";
var DEFAULT_CONFIG5 = {
  enabled: true,
  autoDetectPersona: true,
  showProgress: true,
  allowSkipping: true,
  collectStepFeedback: true,
  requireAnalyticsOptIn: true,
  supportedLocales: ["en-US"],
  defaultLocale: "en-US"
};
var EnhancedOnboardingService = class _EnhancedOnboardingService {
  static instance;
  config;
  flows;
  sampleContent;
  contextualHelp;
  constructor() {
    this.config = DEFAULT_CONFIG5;
    this.flows = /* @__PURE__ */ new Map();
    this.sampleContent = /* @__PURE__ */ new Map();
    this.contextualHelp = /* @__PURE__ */ new Map();
    this.initializeDefaultFlows();
    this.initializeSampleContent();
    this.initializeContextualHelp();
  }
  static getInstance() {
    if (!_EnhancedOnboardingService.instance) {
      _EnhancedOnboardingService.instance = new _EnhancedOnboardingService();
    }
    return _EnhancedOnboardingService.instance;
  }
  /**
   * Start or resume onboarding for a user
   */
  async startOnboarding(tenantId, userId, persona, locale) {
    const detectedPersona = persona || await this.detectPersona(tenantId, userId);
    const flow = this.getFlowForPersona(detectedPersona);
    if (!flow) {
      throw new Error(`No onboarding flow found for persona: ${detectedPersona}`);
    }
    let progress = await this.getProgress(tenantId, userId);
    if (!progress) {
      progress = {
        id: randomUUID56(),
        tenantId,
        userId,
        flowId: flow.id,
        persona: detectedPersona,
        currentStepId: flow.steps[0].id,
        stepProgress: /* @__PURE__ */ new Map(),
        startedAt: /* @__PURE__ */ new Date(),
        lastActivityAt: /* @__PURE__ */ new Date(),
        totalTimeSpent: 0,
        metrics: {
          totalSteps: flow.steps.length,
          completedSteps: 0,
          skippedSteps: 0,
          avgTimePerStep: 0,
          quizScores: [],
          featuresDiscovered: [],
          helpRequests: 0,
          frictionPoints: []
        }
      };
      await this.saveProgress(progress);
      await this.trackEvent(tenantId, userId, "flow_started", {
        flowId: flow.id,
        persona: detectedPersona
      });
    }
    const verdict = this.createGovernanceVerdict("onboarding_access", "allow");
    return createDataEnvelope(progress, {
      source: "onboarding-service",
      actor: userId,
      version: "3.1.0",
      classification: "INTERNAL" /* INTERNAL */,
      governanceVerdict: verdict
    });
  }
  /**
   * Get current onboarding step with content
   */
  async getCurrentStep(tenantId, userId) {
    const progress = await this.getProgress(tenantId, userId);
    if (!progress) {
      return this.wrapInEnvelope(null, userId, "get_current_step");
    }
    const flow = this.flows.get(progress.flowId);
    if (!flow) {
      return this.wrapInEnvelope(null, userId, "get_current_step");
    }
    const currentStep = flow.steps.find((s) => s.id === progress.currentStepId);
    if (!currentStep) {
      return this.wrapInEnvelope(null, userId, "get_current_step");
    }
    let stepProgress = progress.stepProgress.get(currentStep.id);
    if (!stepProgress) {
      stepProgress = {
        stepId: currentStep.id,
        status: "available",
        timeSpent: 0,
        actionsCompleted: []
      };
    }
    if (stepProgress.status === "available") {
      stepProgress.status = "in_progress";
      stepProgress.startedAt = /* @__PURE__ */ new Date();
      progress.stepProgress.set(currentStep.id, stepProgress);
      await this.saveProgress(progress);
      await this.trackEvent(tenantId, userId, "step_started", {
        stepId: currentStep.id,
        stepType: currentStep.type
      });
    }
    return this.wrapInEnvelope({ step: currentStep, progress: stepProgress }, userId, "get_current_step");
  }
  /**
   * Complete current onboarding step
   */
  async completeStep(tenantId, userId, stepId, data) {
    const progress = await this.getProgress(tenantId, userId);
    if (!progress) {
      throw new Error("No onboarding progress found");
    }
    const flow = this.flows.get(progress.flowId);
    if (!flow) {
      throw new Error("Onboarding flow not found");
    }
    if (progress.currentStepId !== stepId) {
      throw new Error("Step is not the current step");
    }
    let stepProgress = progress.stepProgress.get(stepId) || {
      stepId,
      status: "in_progress",
      timeSpent: 0,
      actionsCompleted: []
    };
    stepProgress.status = "completed";
    stepProgress.completedAt = /* @__PURE__ */ new Date();
    stepProgress.timeSpent = stepProgress.startedAt ? Math.floor((Date.now() - stepProgress.startedAt.getTime()) / 1e3) : 0;
    if (data?.quizScore !== void 0) {
      stepProgress.quizScore = data.quizScore;
      progress.metrics.quizScores.push(data.quizScore);
    }
    if (data?.feedbackRating !== void 0) {
      stepProgress.feedbackRating = data.feedbackRating;
    }
    if (data?.feedbackComment) {
      stepProgress.feedbackComment = data.feedbackComment;
    }
    if (data?.actionsCompleted) {
      stepProgress.actionsCompleted = data.actionsCompleted;
    }
    progress.stepProgress.set(stepId, stepProgress);
    progress.metrics.completedSteps++;
    progress.totalTimeSpent += stepProgress.timeSpent;
    progress.lastActivityAt = /* @__PURE__ */ new Date();
    const currentIndex = flow.steps.findIndex((s) => s.id === stepId);
    if (currentIndex < flow.steps.length - 1) {
      progress.currentStepId = flow.steps[currentIndex + 1].id;
    } else {
      progress.completedAt = /* @__PURE__ */ new Date();
      await this.trackEvent(tenantId, userId, "flow_completed", {
        flowId: flow.id,
        totalTime: progress.totalTimeSpent,
        completionRate: progress.metrics.completedSteps / progress.metrics.totalSteps
      });
    }
    progress.metrics.avgTimePerStep = progress.totalTimeSpent / progress.metrics.completedSteps;
    await this.saveProgress(progress);
    await this.trackEvent(tenantId, userId, "step_completed", {
      stepId,
      timeSpent: stepProgress.timeSpent,
      quizScore: stepProgress.quizScore
    });
    return this.wrapInEnvelope(progress, userId, "complete_step");
  }
  /**
   * Skip current onboarding step
   */
  async skipStep(tenantId, userId, stepId, reason) {
    const progress = await this.getProgress(tenantId, userId);
    if (!progress) {
      throw new Error("No onboarding progress found");
    }
    const flow = this.flows.get(progress.flowId);
    if (!flow) {
      throw new Error("Onboarding flow not found");
    }
    const step = flow.steps.find((s) => s.id === stepId);
    if (!step?.skippable && !this.config.allowSkipping) {
      throw new Error("This step cannot be skipped");
    }
    let stepProgress = progress.stepProgress.get(stepId) || {
      stepId,
      status: "in_progress",
      timeSpent: 0,
      actionsCompleted: []
    };
    stepProgress.status = "skipped";
    progress.stepProgress.set(stepId, stepProgress);
    progress.metrics.skippedSteps++;
    progress.lastActivityAt = /* @__PURE__ */ new Date();
    const currentIndex = flow.steps.findIndex((s) => s.id === stepId);
    if (currentIndex < flow.steps.length - 1) {
      progress.currentStepId = flow.steps[currentIndex + 1].id;
    } else {
      progress.completedAt = /* @__PURE__ */ new Date();
    }
    await this.saveProgress(progress);
    await this.trackEvent(tenantId, userId, "step_skipped", {
      stepId,
      reason
    });
    return this.wrapInEnvelope(progress, userId, "skip_step");
  }
  /**
   * Get sample content for persona
   */
  async getSampleContent(persona, type) {
    const samples = Array.from(this.sampleContent.values()).filter((s) => {
      const matchesPersona = s.persona.includes(persona);
      const matchesType = !type || s.type === type;
      return matchesPersona && matchesType;
    });
    return this.wrapInEnvelope(samples, "system", "get_sample_content");
  }
  /**
   * Install sample content for user
   */
  async installSampleContent(tenantId, userId, sampleId) {
    const sample = this.sampleContent.get(sampleId);
    if (!sample) {
      throw new Error("Sample content not found");
    }
    if (!sample.installable) {
      throw new Error("This sample content cannot be installed");
    }
    let resourceId;
    switch (sample.type) {
      case "policy":
        resourceId = await this.installSamplePolicy(tenantId, userId, sample);
        break;
      case "dashboard":
        resourceId = await this.installSampleDashboard(tenantId, userId, sample);
        break;
      case "report":
        resourceId = await this.installSampleReport(tenantId, userId, sample);
        break;
      default:
        throw new Error(`Unsupported sample type: ${sample.type}`);
    }
    await this.trackEvent(tenantId, userId, "sample_installed", {
      sampleId,
      sampleType: sample.type,
      resourceId
    });
    const progress = await this.getProgress(tenantId, userId);
    if (progress) {
      progress.metrics.featuresDiscovered.push(sample.type);
      await this.saveProgress(progress);
    }
    return this.wrapInEnvelope({ installed: true, resourceId }, userId, "install_sample");
  }
  /**
   * Get contextual help for a page/component
   */
  async getContextualHelp(route, userId) {
    const helps = Array.from(this.contextualHelp.values()).filter((h) => {
      return !h.targetRoute || h.targetRoute === route;
    });
    helps.sort((a, b) => b.priority - a.priority);
    return this.wrapInEnvelope(helps, userId, "get_contextual_help");
  }
  /**
   * Record help request (for friction analysis)
   */
  async recordHelpRequest(tenantId, userId, stepId, topic) {
    const progress = await this.getProgress(tenantId, userId);
    if (progress) {
      progress.metrics.helpRequests++;
      if (stepId) {
        progress.metrics.frictionPoints.push({
          stepId,
          type: "help_request",
          timestamp: /* @__PURE__ */ new Date(),
          details: topic
        });
      }
      await this.saveProgress(progress);
    }
    await this.trackEvent(tenantId, userId, "help_requested", {
      stepId,
      topic
    });
  }
  /**
   * Record friction point
   */
  async recordFrictionPoint(tenantId, userId, stepId, type, details) {
    const progress = await this.getProgress(tenantId, userId);
    if (progress) {
      progress.metrics.frictionPoints.push({
        stepId,
        type,
        timestamp: /* @__PURE__ */ new Date(),
        details
      });
      await this.saveProgress(progress);
    }
    await this.trackEvent(tenantId, userId, "friction_detected", {
      stepId,
      frictionType: type,
      details
    });
  }
  /**
   * Get aggregated onboarding analytics (for product team)
   */
  async getAnalyticsSummary(period, startDate, endDate) {
    const pool4 = getPostgresPool2();
    if (!pool4) {
      throw new Error("Database not available");
    }
    const result2 = await pool4.query(
      `SELECT
        COUNT(DISTINCT id) as total_started,
        COUNT(DISTINCT CASE WHEN completed_at IS NOT NULL THEN id END) as total_completed,
        AVG(total_time_spent) as avg_completion_time,
        persona,
        metrics
      FROM onboarding_progress
      WHERE started_at BETWEEN $1 AND $2
      GROUP BY persona, metrics`,
      [startDate, endDate]
    );
    const totalStarted = result2.rows.reduce((sum, r) => sum + parseInt(r.total_started), 0);
    const totalCompleted = result2.rows.reduce((sum, r) => sum + parseInt(r.total_completed), 0);
    const summary = {
      period,
      startDate,
      endDate,
      metrics: {
        totalFlowsStarted: totalStarted,
        totalFlowsCompleted: totalCompleted,
        completionRate: totalStarted > 0 ? totalCompleted / totalStarted : 0,
        avgCompletionTime: result2.rows[0]?.avg_completion_time || 0,
        dropOffByStep: /* @__PURE__ */ new Map(),
        featureAdoptionRates: /* @__PURE__ */ new Map(),
        topFrictionPoints: [],
        personaBreakdown: /* @__PURE__ */ new Map()
      },
      governanceVerdict: this.createGovernanceVerdict("analytics_access", "allow"),
      classification: "INTERNAL" /* INTERNAL */
    };
    return this.wrapInEnvelope(summary, "system", "get_analytics");
  }
  // Private helper methods
  async detectPersona(tenantId, userId) {
    return "analyst";
  }
  getFlowForPersona(persona) {
    return Array.from(this.flows.values()).find((f) => f.persona === persona);
  }
  async getProgress(tenantId, userId) {
    const pool4 = getPostgresPool2();
    if (!pool4) return null;
    const result2 = await pool4.query(
      "SELECT * FROM onboarding_progress WHERE tenant_id = $1 AND user_id = $2",
      [tenantId, userId]
    );
    if (result2.rowCount === 0) return null;
    const row = result2.rows[0];
    return {
      id: row.id,
      tenantId: row.tenant_id,
      userId: row.user_id,
      flowId: row.flow_id,
      persona: row.persona,
      currentStepId: row.current_step_id,
      stepProgress: new Map(Object.entries(row.step_progress || {})),
      startedAt: row.started_at,
      completedAt: row.completed_at,
      lastActivityAt: row.last_activity_at,
      totalTimeSpent: row.total_time_spent,
      metrics: row.metrics
    };
  }
  async saveProgress(progress) {
    const pool4 = getPostgresPool2();
    if (!pool4) throw new Error("Database not available");
    await pool4.query(
      `INSERT INTO onboarding_progress (
        id, tenant_id, user_id, flow_id, persona, current_step_id,
        step_progress, started_at, completed_at, last_activity_at,
        total_time_spent, metrics
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
      ON CONFLICT (tenant_id, user_id) DO UPDATE SET
        current_step_id = EXCLUDED.current_step_id,
        step_progress = EXCLUDED.step_progress,
        completed_at = EXCLUDED.completed_at,
        last_activity_at = NOW(),
        total_time_spent = EXCLUDED.total_time_spent,
        metrics = EXCLUDED.metrics`,
      [
        progress.id,
        progress.tenantId,
        progress.userId,
        progress.flowId,
        progress.persona,
        progress.currentStepId,
        JSON.stringify(Object.fromEntries(progress.stepProgress)),
        progress.startedAt,
        progress.completedAt,
        /* @__PURE__ */ new Date(),
        progress.totalTimeSpent,
        JSON.stringify(progress.metrics)
      ]
    );
  }
  async trackEvent(tenantId, userId, eventType, properties) {
    const event = {
      eventId: randomUUID56(),
      eventType,
      tenantHash: this.hashIdentifier(tenantId),
      userHash: this.hashIdentifier(userId),
      flowId: properties.flowId || "unknown",
      stepId: properties.stepId,
      timestamp: /* @__PURE__ */ new Date(),
      properties,
      governanceVerdict: this.createGovernanceVerdict("analytics_collection", "allow")
    };
    logger_default2.info("Onboarding analytics event", { event });
    const pool4 = getPostgresPool2();
    if (pool4) {
      await pool4.query(
        `INSERT INTO onboarding_analytics_events (
          event_id, event_type, tenant_hash, user_hash, flow_id, step_id,
          timestamp, properties, governance_verdict
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)`,
        [
          event.eventId,
          event.eventType,
          event.tenantHash,
          event.userHash,
          event.flowId,
          event.stepId,
          event.timestamp,
          JSON.stringify(event.properties),
          JSON.stringify(event.governanceVerdict)
        ]
      );
    }
  }
  hashIdentifier(id) {
    const salt = process.env.ANALYTICS_SALT || "summit-onboarding";
    return createHash31("sha256").update(`${salt}:${id}`).digest("hex").substring(0, 16);
  }
  createGovernanceVerdict(policyId, result2) {
    return {
      verdictId: randomUUID56(),
      policyId,
      result: result2 === "allow" ? "ALLOW" /* ALLOW */ : "DENY" /* DENY */,
      decidedAt: /* @__PURE__ */ new Date(),
      reason: result2 === "allow" ? "Access permitted by policy" : "Access denied by policy",
      evaluator: "onboarding-service"
    };
  }
  wrapInEnvelope(data, actor, operation) {
    return createDataEnvelope(data, {
      source: "onboarding-service",
      actor,
      version: "3.1.0",
      classification: "INTERNAL" /* INTERNAL */,
      governanceVerdict: this.createGovernanceVerdict(`onboarding_${operation}`, "allow")
    });
  }
  // Sample content installation helpers
  async installSamplePolicy(tenantId, userId, sample) {
    const policyId = randomUUID56();
    logger_default2.info("Installing sample policy", { tenantId, policyId, sample: sample.name });
    return policyId;
  }
  async installSampleDashboard(tenantId, userId, sample) {
    const dashboardId = randomUUID56();
    logger_default2.info("Installing sample dashboard", { tenantId, dashboardId, sample: sample.name });
    return dashboardId;
  }
  async installSampleReport(tenantId, userId, sample) {
    const reportId = randomUUID56();
    logger_default2.info("Installing sample report", { tenantId, reportId, sample: sample.name });
    return reportId;
  }
  // Initialization methods for default content
  initializeDefaultFlows() {
    const adminFlow = {
      id: "admin-onboarding-v1",
      name: "Administrator Onboarding",
      description: "Complete setup guide for platform administrators",
      persona: "admin",
      estimatedDuration: 30,
      version: "1.0.0",
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date(),
      prerequisites: [],
      steps: [
        {
          id: "admin-welcome",
          type: "welcome",
          title: "Welcome to Summit",
          description: "Get started with your administrator journey",
          estimatedDuration: 60,
          order: 1,
          skippable: false,
          prerequisites: [],
          requiredActions: [],
          completionCriteria: { type: "time_spent", minTimeSpent: 30 },
          content: {
            body: `# Welcome, Administrator!

You're about to set up Summit for your organization. This guide will walk you through:

1. **User & Access Management** - Configure SSO, roles, and permissions
2. **Policy Configuration** - Set up governance policies
3. **Integration Setup** - Connect your existing tools
4. **Monitoring & Compliance** - Configure dashboards and alerts

Let's get started!`,
            tips: [
              "You can always access this guide from the Help menu",
              "Each step includes sample configurations to accelerate setup"
            ],
            docsLinks: [
              { title: "Admin Guide", url: "/docs/admin-guide", type: "guide" },
              { title: "Quick Start Video", url: "/docs/videos/quickstart", type: "video" }
            ]
          }
        },
        {
          id: "admin-users",
          type: "interactive",
          title: "Configure User Access",
          description: "Set up SSO and user management",
          estimatedDuration: 300,
          order: 2,
          skippable: true,
          prerequisites: ["admin-welcome"],
          requiredActions: [
            {
              id: "configure-sso",
              type: "configure",
              description: "Configure SSO provider",
              verificationMethod: "api_check"
            }
          ],
          completionCriteria: { type: "all_actions" },
          content: {
            body: `# Configure User Access

Set up how users will authenticate and what they can access.`,
            tourTargets: [
              {
                selector: "#settings-menu",
                tooltip: "Click Settings to access user management",
                position: "bottom",
                action: "click",
                order: 1
              },
              {
                selector: "#sso-config",
                tooltip: "Configure your SSO provider here",
                position: "right",
                action: "click",
                order: 2
              }
            ]
          }
        },
        {
          id: "admin-policies",
          type: "sample_action",
          title: "Create Your First Policy",
          description: "Set up a governance policy using our templates",
          estimatedDuration: 240,
          order: 3,
          skippable: true,
          prerequisites: ["admin-users"],
          requiredActions: [
            {
              id: "create-policy",
              type: "create",
              description: "Create a sample policy",
              verificationMethod: "api_check"
            }
          ],
          completionCriteria: { type: "all_actions" },
          content: {
            body: `# Create Your First Policy

Policies define what actions are allowed in your organization.`,
            sampleAction: {
              type: "create_policy",
              targetResource: "policies",
              sampleData: {
                name: "Data Access Policy",
                type: "access_control",
                rules: [
                  { action: "read", resource: "*", condition: "authenticated" }
                ]
              },
              expectedOutcome: "Policy created and active",
              rollbackOnComplete: false
            }
          }
        },
        {
          id: "admin-complete",
          type: "milestone",
          title: "Setup Complete!",
          description: "Congratulations on completing the admin setup",
          estimatedDuration: 60,
          order: 4,
          skippable: false,
          prerequisites: ["admin-policies"],
          requiredActions: [],
          completionCriteria: { type: "time_spent", minTimeSpent: 10 },
          content: {
            body: `# Congratulations!

You've completed the initial administrator setup. Here's what you've accomplished:

- \u2705 User access configured
- \u2705 First policy created
- \u2705 Platform ready for users

## Next Steps

- Invite team members
- Explore advanced policies
- Set up integrations`,
            docsLinks: [
              { title: "Advanced Admin Guide", url: "/docs/admin-advanced", type: "guide" },
              { title: "Integration Catalog", url: "/docs/integrations", type: "reference" }
            ]
          }
        }
      ]
    };
    const analystFlow = {
      id: "analyst-onboarding-v1",
      name: "Analyst Onboarding",
      description: "Get started with intelligence analysis",
      persona: "analyst",
      estimatedDuration: 20,
      version: "1.0.0",
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date(),
      prerequisites: [],
      steps: [
        {
          id: "analyst-welcome",
          type: "welcome",
          title: "Welcome to Summit",
          description: "Your intelligence analysis platform",
          estimatedDuration: 60,
          order: 1,
          skippable: false,
          prerequisites: [],
          requiredActions: [],
          completionCriteria: { type: "time_spent", minTimeSpent: 30 },
          content: {
            body: `# Welcome, Analyst!

Summit helps you analyze, correlate, and report on intelligence data.

**Key Features:**
- \u{1F50D} Advanced entity search
- \u{1F4CA} Interactive dashboards
- \u{1F517} Relationship mapping
- \u{1F4DD} Collaborative reporting`
          }
        },
        {
          id: "analyst-search",
          type: "interactive",
          title: "Your First Search",
          description: "Learn to search and filter entities",
          estimatedDuration: 180,
          order: 2,
          skippable: true,
          prerequisites: ["analyst-welcome"],
          requiredActions: [
            {
              id: "perform-search",
              type: "navigate",
              description: "Perform a search",
              targetRoute: "/search",
              verificationMethod: "event_listener"
            }
          ],
          completionCriteria: { type: "all_actions" },
          content: {
            body: `# Search Entities

The search feature lets you find any entity in your intelligence database.`,
            tourTargets: [
              {
                selector: "#global-search",
                tooltip: "Type your search query here",
                position: "bottom",
                action: "input",
                order: 1
              },
              {
                selector: "#search-filters",
                tooltip: "Use filters to narrow results",
                position: "left",
                action: "click",
                order: 2
              }
            ]
          }
        },
        {
          id: "analyst-dashboard",
          type: "sample_action",
          title: "Explore Dashboards",
          description: "View and customize your analytics dashboard",
          estimatedDuration: 180,
          order: 3,
          skippable: true,
          prerequisites: ["analyst-search"],
          requiredActions: [
            {
              id: "view-dashboard",
              type: "navigate",
              description: "Open the analytics dashboard",
              targetRoute: "/dashboard",
              verificationMethod: "event_listener"
            }
          ],
          completionCriteria: { type: "all_actions" },
          content: {
            body: `# Analytics Dashboards

Dashboards give you real-time visibility into your intelligence data.`
          }
        }
      ]
    };
    const developerFlow = {
      id: "developer-onboarding-v1",
      name: "Developer Onboarding",
      description: "Build plugins and integrations",
      persona: "developer",
      estimatedDuration: 25,
      version: "1.0.0",
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date(),
      prerequisites: [],
      steps: [
        {
          id: "dev-welcome",
          type: "welcome",
          title: "Welcome, Developer!",
          description: "Start building with Summit",
          estimatedDuration: 60,
          order: 1,
          skippable: false,
          prerequisites: [],
          requiredActions: [],
          completionCriteria: { type: "time_spent", minTimeSpent: 30 },
          content: {
            body: `# Welcome to Summit Developer Platform

Build powerful plugins and integrations using our SDK.

**What you'll learn:**
- Plugin SDK basics
- API authentication
- Sandbox testing
- Marketplace publishing`,
            docsLinks: [
              { title: "SDK Documentation", url: "/docs/sdk", type: "reference" },
              { title: "API Reference", url: "/docs/api", type: "reference" }
            ]
          }
        },
        {
          id: "dev-sdk",
          type: "checklist",
          title: "Set Up Your Environment",
          description: "Install the SDK and configure your development environment",
          estimatedDuration: 300,
          order: 2,
          skippable: false,
          prerequisites: ["dev-welcome"],
          requiredActions: [],
          completionCriteria: { type: "all_actions" },
          content: {
            body: `# Development Environment Setup`,
            checklistItems: [
              { id: "install-cli", label: "Install Summit CLI", completed: false, autoCheck: false },
              { id: "create-api-key", label: "Create API key", completed: false, autoCheck: true, verificationEndpoint: "/api/v1/developer/keys" },
              { id: "init-project", label: "Initialize plugin project", completed: false, autoCheck: false },
              { id: "run-tests", label: "Run sample tests", completed: false, autoCheck: false }
            ]
          }
        }
      ]
    };
    this.flows.set(adminFlow.id, adminFlow);
    this.flows.set(analystFlow.id, analystFlow);
    this.flows.set(developerFlow.id, developerFlow);
  }
  initializeSampleContent() {
    const samples = [
      {
        id: "sample-security-policy",
        type: "policy",
        name: "Security Baseline Policy",
        description: "A starter security policy with common rules",
        persona: ["admin", "compliance_officer"],
        installable: true,
        content: {
          name: "Security Baseline",
          version: "1.0.0",
          rules: ["require-mfa", "audit-all-access", "encrypt-at-rest"]
        }
      },
      {
        id: "sample-analyst-dashboard",
        type: "dashboard",
        name: "Intelligence Overview",
        description: "Pre-built dashboard for intelligence analysts",
        persona: ["analyst"],
        installable: true,
        previewUrl: "/preview/dashboards/intel-overview",
        content: {
          widgets: ["entity-count", "recent-activity", "relationship-graph"]
        }
      },
      {
        id: "sample-compliance-report",
        type: "report",
        name: "SOC 2 Compliance Summary",
        description: "Template for SOC 2 compliance reporting",
        persona: ["admin", "compliance_officer"],
        installable: true,
        content: {
          template: "soc2-summary",
          sections: ["control-status", "evidence-links", "remediation"]
        }
      }
    ];
    samples.forEach((s) => this.sampleContent.set(s.id, s));
  }
  initializeContextualHelp() {
    const helps = [
      {
        id: "help-governance-verdict",
        targetSelector: ".governance-badge",
        title: "Governance Verdict",
        content: "This badge shows the governance decision for this data. Green means approved, yellow requires review.",
        learnMoreUrl: "/docs/governance",
        dismissible: true,
        showOnce: true,
        priority: 10
      },
      {
        id: "help-provenance",
        targetSelector: ".provenance-link",
        title: "Data Provenance",
        content: "Click to view the complete lineage and source of this data.",
        learnMoreUrl: "/docs/provenance",
        dismissible: true,
        showOnce: true,
        priority: 8
      },
      {
        id: "help-policy-simulator",
        targetSelector: "#policy-simulator",
        targetRoute: "/policies",
        title: "Policy Simulator",
        content: "Test your policies before deploying them. The simulator shows exactly what would be allowed or denied.",
        learnMoreUrl: "/docs/policy-simulator",
        dismissible: true,
        showOnce: false,
        priority: 9
      }
    ];
    helps.forEach((h) => this.contextualHelp.set(h.id, h));
  }
};
var enhancedOnboardingService = EnhancedOnboardingService.getInstance();

// src/routes/onboarding.ts
init_auth4();
init_featureFlags2();
var router89 = Router43();
var singleParam20 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var requireFeatureFlag = (flagName) => {
  return (req, res, next) => {
    const context4 = { userId: req.user?.id, tenantId: req.user?.tenantId };
    if (!isEnabled(flagName, context4)) {
      res.status(403).json({ error: `Feature '${flagName}' is not enabled` });
      return;
    }
    next();
  };
};
var StartOnboardingSchema = z40.object({
  persona: z40.enum(["admin", "analyst", "developer", "compliance_officer", "viewer"]).optional(),
  locale: z40.string().optional()
});
var CompleteStepSchema = z40.object({
  quizScore: z40.number().min(0).max(100).optional(),
  feedbackRating: z40.number().min(1).max(5).optional(),
  feedbackComment: z40.string().max(1e3).optional(),
  actionsCompleted: z40.array(z40.string()).optional()
});
var SkipStepSchema = z40.object({
  reason: z40.string().max(500).optional()
});
var InstallSampleSchema = z40.object({
  sampleId: z40.string()
});
var HelpRequestSchema = z40.object({
  stepId: z40.string().optional(),
  topic: z40.string().optional()
});
router89.post(
  "/start",
  ensureAuthenticated,
  requireFeatureFlag("onboarding.enhancedFlow"),
  async (req, res, next) => {
    try {
      const { persona, locale } = StartOnboardingSchema.parse(req.body);
      const { tenantId, id: userId } = req.user;
      const result2 = await enhancedOnboardingService.startOnboarding(
        tenantId,
        userId,
        persona,
        locale
      );
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router89.get(
  "/current-step",
  ensureAuthenticated,
  requireFeatureFlag("onboarding.enhancedFlow"),
  async (req, res, next) => {
    try {
      const { tenantId, id: userId } = req.user;
      const result2 = await enhancedOnboardingService.getCurrentStep(tenantId, userId);
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router89.post(
  "/steps/:stepId/complete",
  ensureAuthenticated,
  requireFeatureFlag("onboarding.enhancedFlow"),
  async (req, res, next) => {
    try {
      const stepId = singleParam20(req.params.stepId) ?? "";
      const data = CompleteStepSchema.parse(req.body);
      const { tenantId, id: userId } = req.user;
      const result2 = await enhancedOnboardingService.completeStep(
        tenantId,
        userId,
        stepId,
        data
      );
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router89.post(
  "/steps/:stepId/skip",
  ensureAuthenticated,
  requireFeatureFlag("onboarding.enhancedFlow"),
  async (req, res, next) => {
    try {
      const stepId = singleParam20(req.params.stepId) ?? "";
      const { reason } = SkipStepSchema.parse(req.body);
      const { tenantId, id: userId } = req.user;
      const result2 = await enhancedOnboardingService.skipStep(
        tenantId,
        userId,
        stepId,
        reason
      );
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router89.get(
  "/samples",
  ensureAuthenticated,
  requireFeatureFlag("onboarding.sampleContent"),
  async (req, res, next) => {
    try {
      const persona = singleParam20(req.query.persona) || "analyst";
      const type = singleParam20(req.query.type);
      const result2 = await enhancedOnboardingService.getSampleContent(
        persona,
        type
      );
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router89.post(
  "/samples/install",
  ensureAuthenticated,
  requireFeatureFlag("onboarding.sampleContent"),
  async (req, res, next) => {
    try {
      const { sampleId } = InstallSampleSchema.parse(req.body);
      const { tenantId, id: userId } = req.user;
      const result2 = await enhancedOnboardingService.installSampleContent(
        tenantId,
        userId,
        sampleId
      );
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router89.get(
  "/help",
  ensureAuthenticated,
  requireFeatureFlag("onboarding.contextualHelp"),
  async (req, res, next) => {
    try {
      const route = req.query.route || "/";
      const { id: userId } = req.user;
      const result2 = await enhancedOnboardingService.getContextualHelp(route, userId);
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router89.post(
  "/help/request",
  ensureAuthenticated,
  requireFeatureFlag("onboarding.contextualHelp"),
  async (req, res, next) => {
    try {
      const { stepId, topic } = HelpRequestSchema.parse(req.body);
      const { tenantId, id: userId } = req.user;
      await enhancedOnboardingService.recordHelpRequest(tenantId, userId, stepId, topic);
      res.json({ success: true });
    } catch (error) {
      next(error);
    }
  }
);
router89.get(
  "/analytics",
  ensureAuthenticated,
  requireFeatureFlag("analytics.productDashboard"),
  async (req, res, next) => {
    try {
      if (req.user?.role !== "admin") {
        res.status(403).json({ error: "Admin access required" });
        return;
      }
      const period = req.query.period || "weekly";
      const startDate = req.query.startDate ? new Date(req.query.startDate) : new Date(Date.now() - 7 * 24 * 60 * 60 * 1e3);
      const endDate = req.query.endDate ? new Date(req.query.endDate) : /* @__PURE__ */ new Date();
      const result2 = await enhancedOnboardingService.getAnalyticsSummary(
        period,
        startDate,
        endDate
      );
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
var onboarding_default = router89;

// src/routes/support-center.ts
import { Router as Router44 } from "express";
import { z as z41 } from "zod";

// src/support/SupportCenterService.ts
init_database();
init_logger2();
init_data_envelope();
import { randomUUID as randomUUID57 } from "crypto";
var SupportCenterService = class _SupportCenterService {
  static instance;
  config;
  articles;
  faqs;
  constructor() {
    this.config = this.getDefaultConfig();
    this.articles = /* @__PURE__ */ new Map();
    this.faqs = /* @__PURE__ */ new Map();
    this.loadContent();
  }
  static getInstance() {
    if (!_SupportCenterService.instance) {
      _SupportCenterService.instance = new _SupportCenterService();
    }
    return _SupportCenterService.instance;
  }
  /**
   * Search knowledge base and FAQs
   */
  async search(query3, options2) {
    const results = [];
    const searchTerms = query3.toLowerCase().split(/\s+/);
    const limit = options2?.limit || 10;
    for (const article of this.articles.values()) {
      if (article.status !== "published") continue;
      if (options2?.category && article.category !== options2.category) continue;
      if (options2?.locale && article.locale !== options2.locale) continue;
      const score = this.calculateSearchScore(
        searchTerms,
        article.title,
        article.summary,
        article.content,
        article.tags
      );
      if (score > 0) {
        results.push({
          type: "article",
          id: article.id,
          title: article.title,
          summary: article.summary,
          category: article.category,
          score,
          url: `/support/articles/${article.slug}`
        });
      }
    }
    for (const faq of this.faqs.values()) {
      if (options2?.category && faq.category !== options2.category) continue;
      if (options2?.locale && faq.locale !== options2.locale) continue;
      const score = this.calculateSearchScore(
        searchTerms,
        faq.question,
        faq.answer,
        "",
        []
      );
      if (score > 0) {
        results.push({
          type: "faq",
          id: faq.id,
          title: faq.question,
          summary: faq.answer.substring(0, 200),
          category: faq.category,
          score,
          url: `/support/faq#${faq.id}`
        });
      }
    }
    results.sort((a, b) => b.score - a.score);
    const limitedResults = results.slice(0, limit);
    return this.wrapInEnvelope(limitedResults, "search");
  }
  /**
   * Get knowledge base articles
   */
  async getArticles(options2) {
    let articles = Array.from(this.articles.values()).filter((a) => a.status === "published").filter((a) => !options2?.category || a.category === options2.category).filter((a) => !options2?.locale || a.locale === options2.locale);
    articles.sort((a, b) => b.views - a.views);
    if (options2?.offset) {
      articles = articles.slice(options2.offset);
    }
    if (options2?.limit) {
      articles = articles.slice(0, options2.limit);
    }
    return this.wrapInEnvelope(articles, "get_articles");
  }
  /**
   * Get article by slug
   */
  async getArticleBySlug(slug, incrementViews = true) {
    const article = Array.from(this.articles.values()).find(
      (a) => a.slug === slug && a.status === "published"
    );
    if (article && incrementViews) {
      article.views++;
      await this.updateArticleViews(article.id, article.views);
    }
    return this.wrapInEnvelope(article || null, "get_article");
  }
  /**
   * Get FAQs
   */
  async getFAQs(options2) {
    let faqs = Array.from(this.faqs.values()).filter((f) => !options2?.category || f.category === options2.category).filter((f) => !options2?.locale || f.locale === options2.locale);
    faqs.sort((a, b) => a.order - b.order);
    return this.wrapInEnvelope(faqs, "get_faqs");
  }
  /**
   * Create support ticket
   */
  async createTicket(tenantId, userId, data) {
    const ticket = {
      id: randomUUID57(),
      tenantId,
      userId,
      subject: data.subject,
      description: data.description,
      type: data.type,
      priority: data.priority || "medium",
      status: "open",
      category: data.category || "troubleshooting",
      tags: [],
      escalationLevel: 0,
      attachments: data.attachments || [],
      messages: [],
      metadata: {},
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date()
    };
    if (this.config.slaConfig.enabled) {
      const sla = this.config.slaConfig.priorities[ticket.priority];
      ticket.slaDeadline = new Date(Date.now() + sla.resolutionMinutes * 60 * 1e3);
    }
    ticket.governanceVerdict = await this.getTicketGovernanceVerdict(ticket);
    await this.saveTicket(ticket);
    await this.syncTicketToExternal(ticket);
    logger_default2.info("Support ticket created", {
      ticketId: ticket.id,
      type: ticket.type,
      priority: ticket.priority
    });
    return this.wrapInEnvelope(ticket, "create_ticket");
  }
  /**
   * Add message to ticket
   */
  async addMessage(ticketId, authorId, authorType, content, isInternal = false, attachments) {
    const pool4 = getPostgresPool2();
    if (!pool4) throw new Error("Database not available");
    const message = {
      id: randomUUID57(),
      ticketId,
      authorId,
      authorType,
      content,
      attachments: attachments || [],
      isInternal,
      createdAt: /* @__PURE__ */ new Date()
    };
    await pool4.query(
      `INSERT INTO support_ticket_messages (
        id, ticket_id, author_id, author_type, content, attachments, is_internal, created_at
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
      [
        message.id,
        message.ticketId,
        message.authorId,
        message.authorType,
        message.content,
        JSON.stringify(message.attachments),
        message.isInternal,
        message.createdAt
      ]
    );
    if (authorType === "customer") {
      await this.updateTicketStatus(ticketId, "waiting_on_support");
    } else if (authorType === "agent") {
      await this.updateTicketStatus(ticketId, "waiting_on_customer");
    }
    return this.wrapInEnvelope(message, "add_message");
  }
  /**
   * Update ticket status
   */
  async updateTicketStatus(ticketId, status) {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    const updates = {
      status,
      updated_at: /* @__PURE__ */ new Date()
    };
    if (status === "resolved") {
      updates.resolved_at = /* @__PURE__ */ new Date();
    } else if (status === "closed") {
      updates.closed_at = /* @__PURE__ */ new Date();
    }
    const setClause = Object.keys(updates).map((key, i) => `${key} = $${i + 2}`).join(", ");
    await pool4.query(
      `UPDATE support_tickets SET ${setClause} WHERE id = $1`,
      [ticketId, ...Object.values(updates)]
    );
  }
  /**
   * Escalate ticket
   */
  async escalateTicket(ticketId, reason) {
    const pool4 = getPostgresPool2();
    if (!pool4) throw new Error("Database not available");
    const result2 = await pool4.query(
      "SELECT * FROM support_tickets WHERE id = $1",
      [ticketId]
    );
    if (result2.rowCount === 0) {
      throw new Error(`Ticket not found: ${ticketId}`);
    }
    const ticket = this.mapRowToTicket(result2.rows[0]);
    ticket.escalationLevel++;
    ticket.updatedAt = /* @__PURE__ */ new Date();
    const rule = this.config.escalationRules.find(
      (r) => r.priority === ticket.priority && r.type === ticket.type
    );
    if (rule) {
      ticket.assignee = rule.escalateTo;
      await this.addMessage(
        ticketId,
        "system",
        "system",
        `Ticket escalated to level ${ticket.escalationLevel}. Reason: ${reason}`,
        true
      );
      logger_default2.info("Ticket escalated", {
        ticketId,
        level: ticket.escalationLevel,
        assignee: ticket.assignee,
        notifyChannels: rule.notifyChannels
      });
    }
    await pool4.query(
      "UPDATE support_tickets SET escalation_level = $2, assignee = $3, updated_at = $4 WHERE id = $1",
      [ticketId, ticket.escalationLevel, ticket.assignee, ticket.updatedAt]
    );
    return this.wrapInEnvelope(ticket, "escalate_ticket");
  }
  /**
   * Check SLA breaches
   */
  async checkSLABreaches() {
    const pool4 = getPostgresPool2();
    if (!pool4) return [];
    const result2 = await pool4.query(
      `SELECT * FROM support_tickets
      WHERE status NOT IN ('resolved', 'closed')
        AND sla_deadline < NOW()
        AND sla_breached = false`
    );
    const breachedTickets = [];
    for (const row of result2.rows) {
      const ticket = this.mapRowToTicket(row);
      breachedTickets.push(ticket);
      await pool4.query(
        "UPDATE support_tickets SET sla_breached = true WHERE id = $1",
        [ticket.id]
      );
      await this.escalateTicket(ticket.id, "SLA breach");
    }
    return breachedTickets;
  }
  /**
   * Vote on article helpfulness
   */
  async voteArticle(articleId, helpful) {
    const article = this.articles.get(articleId);
    if (!article) return;
    if (helpful) {
      article.helpfulVotes++;
    } else {
      article.notHelpfulVotes++;
    }
    await this.updateArticleVotes(
      articleId,
      article.helpfulVotes,
      article.notHelpfulVotes
    );
  }
  /**
   * Get support configuration
   */
  getConfig() {
    return this.wrapInEnvelope(this.config, "get_config");
  }
  // Private helper methods
  getDefaultConfig() {
    return {
      enabled: true,
      knowledgeBaseEnabled: true,
      faqEnabled: true,
      ticketsEnabled: true,
      liveChatEnabled: false,
      defaultLocale: "en-US",
      supportedLocales: ["en-US", "es-ES", "de-DE", "fr-FR"],
      integrations: [],
      escalationRules: [
        {
          id: "critical-immediate",
          name: "Critical Ticket Escalation",
          priority: "critical",
          type: "incident",
          timeoutMinutes: 30,
          escalateTo: "on-call-engineer",
          notifyChannels: ["slack-incidents", "pagerduty"],
          enabled: true
        },
        {
          id: "security-urgent",
          name: "Security Ticket Escalation",
          priority: "high",
          type: "security",
          timeoutMinutes: 60,
          escalateTo: "security-team",
          notifyChannels: ["slack-security"],
          enabled: true
        }
      ],
      slaConfig: {
        enabled: true,
        priorities: {
          critical: { firstResponseMinutes: 15, resolutionMinutes: 240 },
          high: { firstResponseMinutes: 60, resolutionMinutes: 480 },
          medium: { firstResponseMinutes: 240, resolutionMinutes: 1440 },
          low: { firstResponseMinutes: 480, resolutionMinutes: 2880 }
        }
      }
    };
  }
  calculateSearchScore(terms, title, summary, content, tags) {
    let score = 0;
    const titleLower = title.toLowerCase();
    const summaryLower = summary.toLowerCase();
    const contentLower = content.toLowerCase();
    const tagsLower = tags.map((t) => t.toLowerCase());
    for (const term of terms) {
      if (titleLower.includes(term)) {
        score += 10;
      }
      if (tagsLower.some((t) => t.includes(term))) {
        score += 8;
      }
      if (summaryLower.includes(term)) {
        score += 5;
      }
      if (contentLower.includes(term)) {
        score += 2;
      }
    }
    return score;
  }
  async saveTicket(ticket) {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    await pool4.query(
      `INSERT INTO support_tickets (
        id, external_id, tenant_id, user_id, subject, description, type, priority,
        status, category, tags, assignee, escalation_level, attachments, metadata,
        sla_deadline, governance_verdict, created_at, updated_at
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19)`,
      [
        ticket.id,
        ticket.externalId,
        ticket.tenantId,
        ticket.userId,
        ticket.subject,
        ticket.description,
        ticket.type,
        ticket.priority,
        ticket.status,
        ticket.category,
        JSON.stringify(ticket.tags),
        ticket.assignee,
        ticket.escalationLevel,
        JSON.stringify(ticket.attachments),
        JSON.stringify(ticket.metadata),
        ticket.slaDeadline,
        JSON.stringify(ticket.governanceVerdict),
        ticket.createdAt,
        ticket.updatedAt
      ]
    );
  }
  async syncTicketToExternal(ticket) {
    const enabledIntegrations = this.config.integrations.filter(
      (i) => i.enabled && i.syncEnabled
    );
    for (const integration of enabledIntegrations) {
      try {
        logger_default2.info("Syncing ticket to external system", {
          ticketId: ticket.id,
          provider: integration.provider
        });
      } catch (error) {
        logger_default2.error("Failed to sync ticket to external system", {
          ticketId: ticket.id,
          provider: integration.provider,
          error
        });
      }
    }
  }
  async getTicketGovernanceVerdict(ticket) {
    return this.createGovernanceVerdict("ticket_creation");
  }
  mapRowToTicket(row) {
    return {
      id: row.id,
      externalId: row.external_id,
      tenantId: row.tenant_id,
      userId: row.user_id,
      subject: row.subject,
      description: row.description,
      type: row.type,
      priority: row.priority,
      status: row.status,
      category: row.category,
      tags: row.tags || [],
      assignee: row.assignee,
      escalationLevel: row.escalation_level,
      attachments: row.attachments || [],
      messages: [],
      metadata: row.metadata || {},
      slaDeadline: row.sla_deadline,
      resolvedAt: row.resolved_at,
      closedAt: row.closed_at,
      createdAt: row.created_at,
      updatedAt: row.updated_at,
      governanceVerdict: row.governance_verdict
    };
  }
  async updateArticleViews(articleId, views) {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    await pool4.query(
      "UPDATE knowledge_base_articles SET views = $2 WHERE id = $1",
      [articleId, views]
    );
  }
  async updateArticleVotes(articleId, helpful, notHelpful) {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    await pool4.query(
      "UPDATE knowledge_base_articles SET helpful_votes = $2, not_helpful_votes = $3 WHERE id = $1",
      [articleId, helpful, notHelpful]
    );
  }
  async loadContent() {
    this.initializeDefaultArticles();
    this.initializeDefaultFAQs();
    const pool4 = getPostgresPool2();
    if (pool4) {
      try {
        const articlesResult = await pool4.query(
          "SELECT * FROM knowledge_base_articles WHERE status = 'published'"
        );
        for (const row of articlesResult.rows) {
          const article = {
            id: row.id,
            title: row.title,
            slug: row.slug,
            category: row.category,
            tags: row.tags || [],
            summary: row.summary,
            content: row.content,
            status: row.status,
            author: row.author,
            views: row.views,
            helpfulVotes: row.helpful_votes,
            notHelpfulVotes: row.not_helpful_votes,
            relatedArticles: row.related_articles || [],
            locale: row.locale,
            createdAt: row.created_at,
            updatedAt: row.updated_at,
            publishedAt: row.published_at
          };
          this.articles.set(article.id, article);
        }
        logger_default2.info("Loaded knowledge base articles", { count: this.articles.size });
      } catch (error) {
        logger_default2.warn("Could not load articles from database", { error });
      }
    }
  }
  initializeDefaultArticles() {
    const defaultArticles = [
      {
        id: "article-getting-started",
        title: "Getting Started with Summit",
        slug: "getting-started",
        category: "getting_started",
        tags: ["beginner", "setup", "tutorial"],
        summary: "Learn how to set up and start using Summit in minutes.",
        content: `# Getting Started with Summit

Welcome to Summit! This guide will help you get up and running quickly.

## Prerequisites

- A Summit account
- Modern web browser

## Step 1: Login

Navigate to your Summit instance and login with your credentials.

## Step 2: Complete Onboarding

Follow the guided onboarding to set up your workspace.

## Step 3: Explore Features

Start exploring the dashboard, search, and policy management features.

For more details, see our detailed documentation.`,
        status: "published",
        author: "Summit Team",
        views: 0,
        helpfulVotes: 0,
        notHelpfulVotes: 0,
        relatedArticles: [],
        locale: "en-US",
        createdAt: /* @__PURE__ */ new Date(),
        updatedAt: /* @__PURE__ */ new Date(),
        publishedAt: /* @__PURE__ */ new Date()
      },
      {
        id: "article-governance-verdicts",
        title: "Understanding Governance Verdicts",
        slug: "governance-verdicts",
        category: "policies",
        tags: ["governance", "policies", "compliance"],
        summary: "Learn how governance verdicts work and what they mean.",
        content: `# Understanding Governance Verdicts

Every operation in Summit includes a governance verdict that indicates whether the action was approved by policy.

## Verdict Types

- **ALLOW** - The operation is permitted
- **DENY** - The operation is blocked by policy
- **FLAG** - The operation is permitted but flagged for review
- **REVIEW_REQUIRED** - The operation requires approval before proceeding

## Reading Verdicts

Each API response includes a governance verdict in the response envelope.`,
        status: "published",
        author: "Summit Team",
        views: 0,
        helpfulVotes: 0,
        notHelpfulVotes: 0,
        relatedArticles: [],
        locale: "en-US",
        createdAt: /* @__PURE__ */ new Date(),
        updatedAt: /* @__PURE__ */ new Date(),
        publishedAt: /* @__PURE__ */ new Date()
      }
    ];
    for (const article of defaultArticles) {
      this.articles.set(article.id, article);
    }
  }
  initializeDefaultFAQs() {
    const defaultFAQs = [
      {
        id: "faq-what-is-summit",
        question: "What is Summit?",
        answer: "Summit is an AI-augmented intelligence platform that helps organizations analyze, correlate, and report on data with built-in governance and compliance.",
        category: "getting_started",
        order: 1,
        views: 0,
        locale: "en-US",
        createdAt: /* @__PURE__ */ new Date(),
        updatedAt: /* @__PURE__ */ new Date()
      },
      {
        id: "faq-governance-verdict",
        question: "What is a governance verdict?",
        answer: "A governance verdict is an automated policy decision that accompanies every operation in Summit. It indicates whether an action was allowed, denied, or requires review based on your organization's policies.",
        category: "policies",
        order: 1,
        views: 0,
        locale: "en-US",
        createdAt: /* @__PURE__ */ new Date(),
        updatedAt: /* @__PURE__ */ new Date()
      },
      {
        id: "faq-data-security",
        question: "How is my data secured?",
        answer: "Summit employs industry-standard security practices including encryption at rest and in transit, role-based access control, audit logging, and compliance with SOC 2, GDPR, and other frameworks.",
        category: "security",
        order: 1,
        views: 0,
        locale: "en-US",
        createdAt: /* @__PURE__ */ new Date(),
        updatedAt: /* @__PURE__ */ new Date()
      }
    ];
    for (const faq of defaultFAQs) {
      this.faqs.set(faq.id, faq);
    }
  }
  createGovernanceVerdict(operation) {
    return {
      verdictId: randomUUID57(),
      policyId: `support_${operation}`,
      result: "ALLOW" /* ALLOW */,
      decidedAt: /* @__PURE__ */ new Date(),
      reason: "Support operation permitted",
      evaluator: "support-center-service"
    };
  }
  wrapInEnvelope(data, operation) {
    return createDataEnvelope(data, {
      source: "support-center-service",
      actor: "system",
      version: "3.1.0",
      classification: "INTERNAL" /* INTERNAL */,
      governanceVerdict: this.createGovernanceVerdict(operation)
    });
  }
};
var supportCenterService = SupportCenterService.getInstance();

// src/routes/support-center.ts
init_auth4();
init_featureFlags2();

// src/services/support/ImpersonationService.ts
import { randomUUID as randomUUID59 } from "crypto";

// src/services/support/SupportPolicyGate.ts
init_AuthService();
import { randomUUID as randomUUID58 } from "crypto";

// src/services/PolicyEngine.ts
init_audit2();
init_errors();
import { EventEmitter as EventEmitter20 } from "events";
import { readFile } from "fs/promises";
import { join as join8 } from "path";
import yaml2 from "js-yaml";
import { fileURLToPath as fileURLToPath3 } from "url";
import path40 from "path";
import http from "http";
var __filename3 = fileURLToPath3(import.meta.url);
var __dirname3 = path40.dirname(__filename3);
var PolicyEngine2 = class _PolicyEngine extends EventEmitter20 {
  static instance;
  config;
  auditSystem = null;
  initialized = false;
  opaUrl = "http://localhost:8181/v1/data/governance/allow";
  constructor() {
    super();
    try {
      this.auditSystem = getAuditSystem2();
    } catch (e) {
      console.warn("PolicyEngine: AuditSystem not ready", e);
    }
  }
  static getInstance() {
    if (!_PolicyEngine.instance) {
      _PolicyEngine.instance = new _PolicyEngine();
    }
    return _PolicyEngine.instance;
  }
  async initialize() {
    if (this.initialized) return;
    try {
      const configPath = join8(process.cwd(), "policy/governance-config.yaml");
      try {
        const fileContents = await readFile(configPath, "utf8");
        this.config = yaml2.load(fileContents);
        console.log("PolicyEngine loaded configuration from", configPath);
      } catch (e) {
        console.warn("PolicyEngine could not load config file from", configPath, e);
        this.config = {
          environments: {
            dev: { mode: "permissive", enforce: false },
            staging: { mode: "strict", enforce: true },
            prod: { mode: "strict", enforce: true }
          }
        };
      }
      this.initialized = true;
    } catch (error) {
      console.error("Failed to initialize PolicyEngine", error);
      throw error;
    }
  }
  /**
   * Evaluate a policy decision
   */
  async evaluate(context4) {
    if (!this.initialized) await this.initialize();
    const env2 = context4.environment || process.env.NODE_ENV || "dev";
    const envConfig = this.config?.environments?.[env2] || this.config?.environments?.dev;
    let decision;
    try {
      decision = await this.queryOpa(context4);
    } catch (e) {
      decision = this.simulateRegoEvaluation(context4, envConfig);
    }
    if (this.auditSystem) {
      try {
        await this.auditSystem.log(
          { id: context4.user.id, type: "user", role: context4.user.role, tenantId: context4.user.tenantId },
          context4.action,
          { id: context4.resource.id || "unknown", type: context4.resource.type },
          { ...context4, decision },
          { decision: decision.allow ? "ALLOW" : "DENY" }
        );
      } catch (e) {
        console.error("Failed to log audit event", e);
      }
    }
    return decision;
  }
  async queryOpa(context4) {
    return new Promise((resolve2, reject) => {
      const data = JSON.stringify({ input: context4 });
      const req = http.request(this.opaUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Content-Length": data.length
        },
        timeout: 200
        // fast timeout for sidecar
      }, (res) => {
        if (res.statusCode !== 200) {
          reject(new Error(`OPA returned ${res.statusCode}`));
          return;
        }
        let body4 = "";
        res.on("data", (chunk) => body4 += chunk);
        res.on("end", () => {
          try {
            const result2 = JSON.parse(body4);
            if (result2.result === true || result2.result === false) {
              resolve2({ allow: result2.result });
            } else if (result2.result && typeof result2.result.allow === "boolean") {
              resolve2({ allow: result2.result.allow, reason: result2.result.reason });
            } else {
              resolve2({ allow: false, reason: "OPA result undefined" });
            }
          } catch (e) {
            reject(e);
          }
        });
      });
      req.on("error", (e) => reject(e));
      req.write(data);
      req.end();
    });
  }
  simulateRegoEvaluation(context4, envConfig) {
    if (context4.user.role === "admin") {
      return { allow: true, reason: "Admin bypass" };
    }
    if (envConfig && envConfig.mode === "permissive") {
      if (context4.environment === "dev") {
        return { allow: true, reason: "Dev environment permissive" };
      }
    }
    if (context4.resource.sensitivity === "TOP_SECRET" && (context4.user.clearance_level || 0) < 5) {
      return { allow: false, reason: "Insufficient clearance for TOP_SECRET" };
    }
    if (context4.user.permissions && context4.user.permissions.includes(context4.action)) {
      if (context4.action === "copilot_query") {
        const query3 = context4.resource.query || "";
        if (/ssn|credit card/i.test(query3)) {
          return { allow: false, reason: "PII detected in query" };
        }
      }
      return { allow: true };
    }
    return { allow: false, reason: "Insufficient permissions" };
  }
  /**
   * Express Middleware for Policy Enforcement
   */
  middleware(action, resourceType) {
    return async (req, res, next) => {
      try {
        if (!req.user) {
          return next(new AppError("Unauthorized", 401));
        }
        const context4 = {
          environment: process.env.NODE_ENV || "dev",
          user: req.user,
          action,
          resource: {
            type: resourceType,
            ...req.params,
            ...req.body
          }
        };
        const decision = await this.evaluate(context4);
        if (!decision.allow) {
          return next(new AppError(`Policy Violation: ${decision.reason}`, 403));
        }
        next();
      } catch (error) {
        next(error);
      }
    };
  }
};

// src/services/support/SupportPolicyGate.ts
init_errors();
init_logger2();

// src/policies/support.ts
var SUPPORT_IMPERSONATION_POLICY = {
  id: "support-impersonation-v1",
  description: "Allow support impersonation only for approved roles with explicit justification.",
  allowedRoles: ["ADMIN"],
  requiredPermissions: ["support:impersonate"],
  requireJustification: true
};
var SUPPORT_HEALTH_BUNDLE_POLICY = {
  id: "support-tenant-health-bundle-v1",
  description: "Allow tenant health bundle export for approved roles with explicit justification.",
  allowedRoles: ["ADMIN"],
  requiredPermissions: ["support:health:export"],
  requireJustification: true
};
var isRoleAllowed = (role, allowedRoles) => {
  if (!role) return false;
  return allowedRoles.some(
    (allowed) => allowed.toUpperCase() === role.toUpperCase()
  );
};

// src/services/support/SupportPolicyGate.ts
async function enforceSupportPolicy(params) {
  const { actor, policy: policy2, action, resource, justification } = params;
  const decisionId = randomUUID58();
  const authService5 = new AuthService_default();
  const policyEngine = PolicyEngine2.getInstance();
  if (policy2.requireJustification && !justification?.trim()) {
    throw new AppError("Justification is required for this action.", 400, "JUSTIFICATION_REQUIRED");
  }
  const roleAllowed = isRoleAllowed(actor.role, policy2.allowedRoles);
  const permissionMatches = policy2.requiredPermissions.filter(
    (permission) => authService5.hasPermission(actor, permission)
  );
  const permissionAllowed = policy2.requiredPermissions.length === 0 || permissionMatches.length > 0;
  const engineDecision = await policyEngine.evaluate({
    environment: process.env.NODE_ENV || "dev",
    user: {
      id: actor.id,
      role: actor.role,
      permissions: permissionMatches,
      tenantId: actor.tenantId
    },
    action,
    resource
  });
  const allow = roleAllowed && permissionAllowed && engineDecision.allow;
  const reason = !roleAllowed ? "Role not allowlisted" : !permissionAllowed ? "Permission not allowlisted" : engineDecision.allow ? "Allowed by policy" : engineDecision.reason || "Policy engine denied";
  logger_default2.info("Support policy evaluated", {
    actorId: actor.id,
    policyId: policy2.id,
    policyDecisionId: decisionId,
    action,
    resourceType: resource.type,
    allow,
    reason
  });
  if (!allow) {
    throw new AppError(`Policy denied: ${reason}`, 403, "POLICY_DENIED");
  }
  return {
    allow,
    reason,
    policyId: policy2.id,
    policyDecisionId: decisionId
  };
}

// src/provenance/impersonation-receipts.ts
init_ledger();
init_receipt();
import crypto42 from "crypto";
async function recordImpersonationReceipt(params) {
  const ledger = ProvenanceLedgerV2.getInstance();
  const createdAt = (/* @__PURE__ */ new Date()).toISOString();
  const inputPayload = {
    action: params.action,
    sessionId: params.sessionId,
    actor: params.actor,
    target: params.target,
    justification: params.justification,
    policyId: params.policy.id,
    policyDecisionId: params.policy.decisionId,
    createdAt
  };
  const entry = await ledger.appendEntry({
    tenantId: params.actor.tenantId,
    timestamp: new Date(createdAt),
    actionType: params.action === "start" ? "IMPERSONATION_START" : "IMPERSONATION_STOP",
    resourceType: "SupportImpersonationSession",
    resourceId: params.sessionId,
    actorId: params.actor.id,
    actorType: "user",
    payload: {
      mutationType: params.action === "start" ? "CREATE" : "UPDATE",
      entityId: params.sessionId,
      entityType: "SupportImpersonationSession",
      ...inputPayload
    },
    metadata: {
      complianceReview: true,
      policyId: params.policy.id,
      policyDecisionId: params.policy.decisionId,
      ...params.metadata
    }
  });
  const signerInfo = resolveSigningSecret();
  const inputHash = hashCanonical(inputPayload);
  const receiptId = crypto42.randomUUID();
  const baseReceipt = {
    receiptId,
    action: params.action,
    sessionId: params.sessionId,
    createdAt,
    codeDigest: getCodeDigest(),
    actor: params.actor,
    target: params.target,
    inputHash,
    policy: {
      id: params.policy.id,
      decisionId: params.policy.decisionId,
      outcome: params.policy.allow ? "ALLOW" : "DENY"
    },
    provenanceEntryId: entry.id,
    signer: { kid: signerInfo.kid, alg: signerInfo.alg }
  };
  const signature = signReceiptPayload(baseReceipt, signerInfo.secret);
  return {
    ...baseReceipt,
    signature
  };
}

// src/services/support/ImpersonationService.ts
init_errors();
var SupportImpersonationService = class _SupportImpersonationService {
  static instance;
  sessions = /* @__PURE__ */ new Map();
  defaultDurationMs = 60 * 60 * 1e3;
  static getInstance() {
    if (!_SupportImpersonationService.instance) {
      _SupportImpersonationService.instance = new _SupportImpersonationService();
    }
    return _SupportImpersonationService.instance;
  }
  async startImpersonation(params) {
    const { actor, targetUserId, targetTenantId, reason, ticketId } = params;
    const policyDecision = await enforceSupportPolicy({
      actor,
      policy: SUPPORT_IMPERSONATION_POLICY,
      action: "support:impersonate",
      resource: {
        id: targetUserId,
        type: "SupportImpersonation",
        targetTenantId
      },
      justification: reason
    });
    const sessionId = randomUUID59();
    const startedAt2 = (/* @__PURE__ */ new Date()).toISOString();
    const expiresAt = new Date(Date.now() + this.defaultDurationMs).toISOString();
    const session = {
      id: sessionId,
      startedAt: startedAt2,
      expiresAt,
      actor,
      target: {
        userId: targetUserId,
        tenantId: targetTenantId
      },
      reason,
      ticketId,
      active: true
    };
    this.sessions.set(sessionId, session);
    const receipt = await recordImpersonationReceipt({
      action: "start",
      sessionId,
      actor,
      target: session.target,
      justification: reason,
      policy: {
        id: policyDecision.policyId,
        decisionId: policyDecision.policyDecisionId,
        allow: policyDecision.allow
      },
      metadata: {
        ticketId
      }
    });
    return {
      session,
      receipt,
      policyDecision
    };
  }
  async stopImpersonation(params) {
    const { actor, sessionId, reason } = params;
    const session = this.sessions.get(sessionId);
    if (!session) {
      throw new AppError("Impersonation session not found.", 404, "IMPERSONATION_NOT_FOUND");
    }
    const policyDecision = await enforceSupportPolicy({
      actor,
      policy: SUPPORT_IMPERSONATION_POLICY,
      action: "support:impersonate",
      resource: {
        id: sessionId,
        type: "SupportImpersonation",
        targetTenantId: session.target.tenantId
      },
      justification: reason
    });
    session.active = false;
    const receipt = await recordImpersonationReceipt({
      action: "stop",
      sessionId,
      actor,
      target: session.target,
      justification: reason,
      policy: {
        id: policyDecision.policyId,
        decisionId: policyDecision.policyDecisionId,
        allow: policyDecision.allow
      },
      metadata: {
        ticketId: session.ticketId,
        endedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    });
    return {
      session,
      receipt,
      policyDecision
    };
  }
  getSession(sessionId) {
    return this.sessions.get(sessionId);
  }
};
var supportImpersonationService = SupportImpersonationService.getInstance();

// src/services/support/TenantHealthBundleService.ts
init_redact();
init_errors();
var HEALTH_BUNDLE_ALLOWED_FIELDS = [
  "tenant",
  "health",
  "compliance",
  "evidence",
  "generatedAt",
  "id",
  "name",
  "slug",
  "tier",
  "status",
  "residency",
  "region",
  "signals",
  "supportTicketsOpen",
  "supportTicketsCritical",
  "latestIncidentAt",
  "policySnapshot",
  "security",
  "features",
  "lifecycle",
  "receiptIds",
  "provenanceEntryIds",
  "source"
];
var HEALTH_BUNDLE_REDACTION_POLICY = {
  rules: ["pii", "financial", "sensitive"],
  allowedFields: HEALTH_BUNDLE_ALLOWED_FIELDS,
  redactionMask: "[REDACTED]"
};
var TenantHealthBundleService = class _TenantHealthBundleService {
  static instance;
  tenantService = TenantService.getInstance();
  redactionService = new RedactionService();
  static getInstance() {
    if (!_TenantHealthBundleService.instance) {
      _TenantHealthBundleService.instance = new _TenantHealthBundleService();
    }
    return _TenantHealthBundleService.instance;
  }
  async exportBundle(params) {
    const { actor, tenantId, reason } = params;
    const policyDecision = await enforceSupportPolicy({
      actor,
      policy: SUPPORT_HEALTH_BUNDLE_POLICY,
      action: "support:health:export",
      resource: {
        id: tenantId,
        type: "TenantHealthBundle"
      },
      justification: reason
    });
    const tenant = await this.tenantService.getTenant(tenantId);
    if (!tenant) {
      throw new AppError("Tenant not found.", 404, "TENANT_NOT_FOUND");
    }
    const generatedAt = (/* @__PURE__ */ new Date()).toISOString();
    const bundle = {
      generatedAt,
      tenant: {
        id: tenant.id,
        name: tenant.name,
        slug: tenant.slug,
        tier: tenant.tier,
        status: tenant.status,
        residency: tenant.residency,
        region: tenant.region
      },
      health: {
        status: tenant.status === "active" ? "HEALTHY" : "DEGRADED",
        signals: {
          supportTicketsOpen: 0,
          supportTicketsCritical: 0,
          latestIncidentAt: null
        }
      },
      compliance: {
        policySnapshot: {
          security: tenant.config?.security || {},
          features: tenant.config?.features || {},
          lifecycle: tenant.config?.lifecycle || {}
        }
      },
      evidence: {
        source: "TenantHealthBundleService",
        receiptIds: [policyDecision.policyDecisionId],
        provenanceEntryIds: []
      }
    };
    const redactedBundle = await this.redactionService.redactObject(
      bundle,
      HEALTH_BUNDLE_REDACTION_POLICY,
      tenantId,
      { actorId: actor.id, policyId: policyDecision.policyId }
    );
    return {
      bundle: redactedBundle,
      policyDecision,
      redaction: {
        policyId: "support-health-bundle-redaction-v1",
        appliedAt: generatedAt,
        mask: HEALTH_BUNDLE_REDACTION_POLICY.redactionMask
      }
    };
  }
};
var tenantHealthBundleService = TenantHealthBundleService.getInstance();

// src/routes/support-center.ts
var router90 = Router44();
var singleParam21 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var requireFeatureFlag2 = (flagName) => {
  return (req, res, next) => {
    const context4 = { userId: req.user?.id, tenantId: req.user?.tenantId };
    if (!isEnabled(flagName, context4)) {
      res.status(403).json({ error: `Feature '${flagName}' is not enabled` });
      return;
    }
    next();
  };
};
var optionalAuth = async (req, res, next) => {
  const auth = req.headers.authorization || "";
  const token = auth.startsWith("Bearer ") ? auth.slice("Bearer ".length) : req.headers["x-access-token"] || null;
  if (!token) {
    next();
    return;
  }
  try {
    const AuthService2 = (await Promise.resolve().then(() => (init_AuthService(), AuthService_exports))).default;
    const authService5 = new AuthService2();
    const user = await authService5.verifyToken(token);
    if (user) {
      req.user = {
        id: user.id,
        tenantId: user.defaultTenantId || "default",
        role: user.role,
        email: user.email
      };
    }
  } catch {
  }
  next();
};
var SearchSchema = z41.object({
  query: z41.string().min(1).max(200),
  category: z41.string().optional(),
  locale: z41.string().optional(),
  limit: z41.number().min(1).max(50).optional()
});
var CreateTicketSchema = z41.object({
  subject: z41.string().min(5).max(200),
  description: z41.string().min(10).max(1e4),
  type: z41.enum(["question", "bug", "feature_request", "incident", "compliance", "security"]),
  priority: z41.enum(["low", "medium", "high", "critical"]).optional(),
  category: z41.string().optional()
});
var AddMessageSchema = z41.object({
  content: z41.string().min(1).max(1e4),
  isInternal: z41.boolean().optional()
});
var VoteSchema = z41.object({
  helpful: z41.boolean()
});
var ImpersonationStartSchema = z41.object({
  targetUserId: z41.string().min(1),
  targetTenantId: z41.string().min(1),
  reason: z41.string().min(5).max(2e3),
  ticketId: z41.string().optional()
});
var ImpersonationStopSchema = z41.object({
  sessionId: z41.string().min(1),
  reason: z41.string().min(5).max(2e3)
});
var TenantHealthBundleSchema = z41.object({
  tenantId: z41.string().min(1),
  reason: z41.string().min(5).max(2e3)
});
router90.get(
  "/search",
  optionalAuth,
  requireFeatureFlag2("support.knowledgeBase"),
  async (req, res, next) => {
    try {
      const query3 = singleParam21(req.query.q);
      const category = singleParam21(req.query.category);
      const locale = singleParam21(req.query.locale);
      const limitRaw = singleParam21(req.query.limit);
      const limit = limitRaw ? parseInt(limitRaw, 10) : void 0;
      if (!query3) {
        res.status(400).json({ error: "Search query is required" });
        return;
      }
      const result2 = await supportCenterService.search(query3, {
        category,
        locale,
        limit
      });
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.get(
  "/articles",
  optionalAuth,
  requireFeatureFlag2("support.knowledgeBase"),
  async (req, res, next) => {
    try {
      const category = singleParam21(req.query.category);
      const locale = singleParam21(req.query.locale);
      const limitRaw = singleParam21(req.query.limit);
      const offsetRaw = singleParam21(req.query.offset);
      const limit = limitRaw ? parseInt(limitRaw, 10) : void 0;
      const offset = offsetRaw ? parseInt(offsetRaw, 10) : void 0;
      const result2 = await supportCenterService.getArticles({
        category,
        locale,
        limit,
        offset
      });
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.get(
  "/articles/:slug",
  optionalAuth,
  requireFeatureFlag2("support.knowledgeBase"),
  async (req, res, next) => {
    try {
      const slug = singleParam21(req.params.slug) ?? "";
      const result2 = await supportCenterService.getArticleBySlug(slug, true);
      if (!result2.data) {
        res.status(404).json({ error: "Article not found" });
        return;
      }
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.post(
  "/articles/:id/vote",
  optionalAuth,
  requireFeatureFlag2("support.knowledgeBase"),
  async (req, res, next) => {
    try {
      const id = singleParam21(req.params.id) ?? "";
      const { helpful } = VoteSchema.parse(req.body);
      await supportCenterService.voteArticle(id, helpful);
      res.json({ success: true });
    } catch (error) {
      next(error);
    }
  }
);
router90.get(
  "/faqs",
  optionalAuth,
  requireFeatureFlag2("support.faq"),
  async (req, res, next) => {
    try {
      const category = singleParam21(req.query.category);
      const locale = singleParam21(req.query.locale);
      const result2 = await supportCenterService.getFAQs({
        category,
        locale
      });
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.post(
  "/impersonation/start",
  ensureAuthenticated,
  requireFeatureFlag2("support.impersonation"),
  async (req, res, next) => {
    try {
      const payload = ImpersonationStartSchema.parse(req.body);
      const user = req.user;
      const actor = {
        id: user?.id,
        role: user?.role,
        tenantId: user?.tenantId || user?.defaultTenantId,
        email: user?.email
      };
      const result2 = await supportImpersonationService.startImpersonation({
        actor,
        targetUserId: payload.targetUserId,
        targetTenantId: payload.targetTenantId,
        reason: payload.reason,
        ticketId: payload.ticketId
      });
      res.status(201).json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.post(
  "/impersonation/stop",
  ensureAuthenticated,
  requireFeatureFlag2("support.impersonation"),
  async (req, res, next) => {
    try {
      const payload = ImpersonationStopSchema.parse(req.body);
      const user = req.user;
      const actor = {
        id: user?.id,
        role: user?.role,
        tenantId: user?.tenantId || user?.defaultTenantId,
        email: user?.email
      };
      const result2 = await supportImpersonationService.stopImpersonation({
        actor,
        sessionId: payload.sessionId,
        reason: payload.reason
      });
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.post(
  "/tenant-health-bundle",
  ensureAuthenticated,
  requireFeatureFlag2("support.healthBundle"),
  async (req, res, next) => {
    try {
      const payload = TenantHealthBundleSchema.parse(req.body);
      const user = req.user;
      const actor = {
        id: user?.id,
        role: user?.role,
        tenantId: user?.tenantId || user?.defaultTenantId,
        email: user?.email
      };
      const result2 = await tenantHealthBundleService.exportBundle({
        actor,
        tenantId: payload.tenantId,
        reason: payload.reason
      });
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.post(
  "/tickets",
  ensureAuthenticated,
  requireFeatureFlag2("support.tickets"),
  async (req, res, next) => {
    try {
      const data = CreateTicketSchema.parse(req.body);
      const { tenantId, id: userId } = req.user;
      const result2 = await supportCenterService.createTicket(tenantId, userId, {
        subject: data.subject,
        description: data.description,
        type: data.type,
        priority: data.priority,
        category: data.category
      });
      res.status(201).json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.post(
  "/tickets/:ticketId/messages",
  ensureAuthenticated,
  requireFeatureFlag2("support.tickets"),
  async (req, res, next) => {
    try {
      const ticketId = singleParam21(req.params.ticketId) ?? "";
      const { content, isInternal } = AddMessageSchema.parse(req.body);
      const { id: userId } = req.user;
      const result2 = await supportCenterService.addMessage(
        ticketId,
        userId,
        "customer",
        content,
        isInternal || false
      );
      res.status(201).json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.post(
  "/tickets/:ticketId/escalate",
  ensureAuthenticated,
  requireFeatureFlag2("support.escalation"),
  async (req, res, next) => {
    try {
      const ticketId = singleParam21(req.params.ticketId) ?? "";
      const { reason } = req.body;
      const result2 = await supportCenterService.escalateTicket(ticketId, reason || "User requested escalation");
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router90.get(
  "/config",
  optionalAuth,
  async (req, res, next) => {
    try {
      const result2 = supportCenterService.getConfig();
      const publicConfig = {
        enabled: result2.data.enabled,
        knowledgeBaseEnabled: result2.data.knowledgeBaseEnabled,
        faqEnabled: result2.data.faqEnabled,
        ticketsEnabled: result2.data.ticketsEnabled,
        liveChatEnabled: result2.data.liveChatEnabled,
        supportedLocales: result2.data.supportedLocales
      };
      res.json({
        ...result2,
        data: publicConfig
      });
    } catch (error) {
      next(error);
    }
  }
);
var support_center_default = router90;

// src/routes/i18n.ts
import { Router as Router45 } from "express";
import { z as z42 } from "zod";

// src/i18n/I18nService.ts
init_database();
init_logger2();
init_data_envelope();
import { randomUUID as randomUUID60 } from "crypto";
var DEFAULT_CONFIG6 = {
  defaultLocale: "en-US",
  supportedLocales: ["en-US", "en-GB", "es-ES", "de-DE", "fr-FR", "ja-JP", "pt-BR"],
  fallbackLocale: "en-US",
  autoDetect: true,
  namespaces: ["common", "auth", "dashboard", "policies", "analytics", "plugins", "settings", "compliance", "governance", "errors", "onboarding", "support"],
  loadPath: "/locales/{{lng}}/{{ns}}.json",
  cacheTTL: 36e5,
  // 1 hour
  missingKeyBehavior: "fallback",
  debug: false
};
var LOCALE_CONFIGS = /* @__PURE__ */ new Map([
  ["en-US", {
    code: "en-US",
    name: "English (United States)",
    nativeName: "English (United States)",
    direction: "ltr",
    dateFormat: "MM/DD/YYYY",
    timeFormat: "h:mm A",
    numberFormat: { decimalSeparator: ".", thousandsSeparator: ",", decimalPlaces: 2 },
    currencyCode: "USD",
    enabled: true,
    completeness: 100
  }],
  ["en-GB", {
    code: "en-GB",
    name: "English (United Kingdom)",
    nativeName: "English (United Kingdom)",
    direction: "ltr",
    dateFormat: "DD/MM/YYYY",
    timeFormat: "HH:mm",
    numberFormat: { decimalSeparator: ".", thousandsSeparator: ",", decimalPlaces: 2 },
    currencyCode: "GBP",
    enabled: true,
    completeness: 100
  }],
  ["es-ES", {
    code: "es-ES",
    name: "Spanish (Spain)",
    nativeName: "Espa\xF1ol (Espa\xF1a)",
    direction: "ltr",
    dateFormat: "DD/MM/YYYY",
    timeFormat: "HH:mm",
    numberFormat: { decimalSeparator: ",", thousandsSeparator: ".", decimalPlaces: 2 },
    currencyCode: "EUR",
    enabled: true,
    completeness: 85
  }],
  ["de-DE", {
    code: "de-DE",
    name: "German (Germany)",
    nativeName: "Deutsch (Deutschland)",
    direction: "ltr",
    dateFormat: "DD.MM.YYYY",
    timeFormat: "HH:mm",
    numberFormat: { decimalSeparator: ",", thousandsSeparator: ".", decimalPlaces: 2 },
    currencyCode: "EUR",
    enabled: true,
    completeness: 80
  }],
  ["fr-FR", {
    code: "fr-FR",
    name: "French (France)",
    nativeName: "Fran\xE7ais (France)",
    direction: "ltr",
    dateFormat: "DD/MM/YYYY",
    timeFormat: "HH:mm",
    numberFormat: { decimalSeparator: ",", thousandsSeparator: " ", decimalPlaces: 2 },
    currencyCode: "EUR",
    enabled: true,
    completeness: 75
  }],
  ["ja-JP", {
    code: "ja-JP",
    name: "Japanese (Japan)",
    nativeName: "\u65E5\u672C\u8A9E (\u65E5\u672C)",
    direction: "ltr",
    dateFormat: "YYYY/MM/DD",
    timeFormat: "HH:mm",
    numberFormat: { decimalSeparator: ".", thousandsSeparator: ",", decimalPlaces: 0 },
    currencyCode: "JPY",
    enabled: true,
    completeness: 70
  }],
  ["pt-BR", {
    code: "pt-BR",
    name: "Portuguese (Brazil)",
    nativeName: "Portugu\xEAs (Brasil)",
    direction: "ltr",
    dateFormat: "DD/MM/YYYY",
    timeFormat: "HH:mm",
    numberFormat: { decimalSeparator: ",", thousandsSeparator: ".", decimalPlaces: 2 },
    currencyCode: "BRL",
    enabled: true,
    completeness: 65
  }],
  ["ar-SA", {
    code: "ar-SA",
    name: "Arabic (Saudi Arabia)",
    nativeName: "\u0627\u0644\u0639\u0631\u0628\u064A\u0629 (\u0627\u0644\u0633\u0639\u0648\u062F\u064A\u0629)",
    direction: "rtl",
    dateFormat: "DD/MM/YYYY",
    timeFormat: "HH:mm",
    numberFormat: { decimalSeparator: "\u066B", thousandsSeparator: "\u066C", decimalPlaces: 2 },
    currencyCode: "SAR",
    enabled: false,
    // Not yet fully translated
    completeness: 40
  }]
]);
var REGIONAL_COMPLIANCE = /* @__PURE__ */ new Map([
  ["EU", {
    region: "EU",
    locales: ["en-GB", "de-DE", "fr-FR", "es-ES", "it-IT", "nl-NL"],
    complianceFrameworks: [
      { id: "gdpr", name: "GDPR", region: "EU", requirements: ["consent", "data-portability", "right-to-erasure", "dpo"], effectiveDate: /* @__PURE__ */ new Date("2018-05-25") }
    ],
    dataResidencyRequired: true,
    dataResidencyRegions: ["eu-west-1", "eu-central-1"],
    consentRequirements: {
      explicitConsentRequired: true,
      granularConsentRequired: true,
      consentAge: 16,
      parentalConsentRequired: true,
      withdrawalSupported: true,
      purposes: [
        { id: "essential", name: "Essential Services", description: "Required for platform operation", required: true, defaultValue: true },
        { id: "analytics", name: "Analytics", description: "Usage analytics and improvement", required: false, defaultValue: false },
        { id: "marketing", name: "Marketing", description: "Marketing communications", required: false, defaultValue: false }
      ]
    },
    dataRetentionPolicy: {
      defaultRetentionDays: 365,
      minRetentionDays: 30,
      maxRetentionDays: 2555,
      // ~7 years
      deletionMethod: "hard",
      auditRetentionDays: 2555
    },
    specialCategories: ["health", "biometric", "genetic", "political", "religious", "sexual_orientation"]
  }],
  ["BR", {
    region: "BR",
    locales: ["pt-BR"],
    complianceFrameworks: [
      { id: "lgpd", name: "LGPD", region: "BR", requirements: ["consent", "data-portability", "right-to-erasure", "dpo"], effectiveDate: /* @__PURE__ */ new Date("2020-09-18") }
    ],
    dataResidencyRequired: false,
    dataResidencyRegions: ["sa-east-1"],
    consentRequirements: {
      explicitConsentRequired: true,
      granularConsentRequired: true,
      consentAge: 18,
      parentalConsentRequired: true,
      withdrawalSupported: true,
      purposes: [
        { id: "essential", name: "Servi\xE7os Essenciais", description: "Necess\xE1rio para opera\xE7\xE3o", required: true, defaultValue: true },
        { id: "analytics", name: "An\xE1lise", description: "An\xE1lise de uso", required: false, defaultValue: false }
      ]
    },
    dataRetentionPolicy: {
      defaultRetentionDays: 365,
      minRetentionDays: 30,
      maxRetentionDays: 1825,
      // 5 years
      deletionMethod: "hard",
      auditRetentionDays: 1825
    },
    specialCategories: ["health", "biometric", "genetic", "political", "religious"]
  }],
  ["US", {
    region: "US",
    locales: ["en-US"],
    complianceFrameworks: [
      { id: "ccpa", name: "CCPA", region: "US-CA", requirements: ["disclosure", "opt-out", "non-discrimination"], effectiveDate: /* @__PURE__ */ new Date("2020-01-01") },
      { id: "hipaa", name: "HIPAA", region: "US", requirements: ["phi-protection", "access-controls", "audit-logging"], effectiveDate: /* @__PURE__ */ new Date("1996-08-21") }
    ],
    dataResidencyRequired: false,
    dataResidencyRegions: ["us-east-1", "us-west-2"],
    consentRequirements: {
      explicitConsentRequired: false,
      granularConsentRequired: false,
      consentAge: 13,
      parentalConsentRequired: true,
      withdrawalSupported: true,
      purposes: [
        { id: "essential", name: "Essential Services", description: "Required for platform operation", required: true, defaultValue: true },
        { id: "analytics", name: "Analytics", description: "Usage analytics", required: false, defaultValue: true }
      ]
    },
    dataRetentionPolicy: {
      defaultRetentionDays: 730,
      // 2 years
      minRetentionDays: 30,
      maxRetentionDays: 2555,
      deletionMethod: "soft",
      auditRetentionDays: 2555
    },
    specialCategories: ["health"]
  }]
]);
var I18nService = class _I18nService {
  static instance;
  config;
  translationCache;
  userPreferencesCache;
  constructor() {
    this.config = DEFAULT_CONFIG6;
    this.translationCache = /* @__PURE__ */ new Map();
    this.userPreferencesCache = /* @__PURE__ */ new Map();
    this.initializeDefaultTranslations();
  }
  static getInstance() {
    if (!_I18nService.instance) {
      _I18nService.instance = new _I18nService();
    }
    return _I18nService.instance;
  }
  /**
   * Get all supported locales
   */
  getSupportedLocales() {
    const locales = Array.from(LOCALE_CONFIGS.values()).filter((l) => l.enabled);
    return this.wrapInEnvelope(locales, "get_locales");
  }
  /**
   * Get locale configuration
   */
  getLocaleConfig(locale) {
    return LOCALE_CONFIGS.get(locale) || null;
  }
  /**
   * Detect locale from request
   */
  detectLocale(headers, cookies, userPreferences, tenantSettings) {
    const sources = [];
    let detectedLocale = this.config.defaultLocale;
    let confidence = 0;
    if (userPreferences?.preferredLocale) {
      sources.push({
        type: "user_preference",
        value: userPreferences.preferredLocale,
        priority: 100
      });
      if (this.isLocaleSupported(userPreferences.preferredLocale)) {
        detectedLocale = userPreferences.preferredLocale;
        confidence = 1;
      }
    }
    if (cookies.locale) {
      sources.push({
        type: "cookie",
        value: cookies.locale,
        priority: 80
      });
      if (confidence < 0.8 && this.isLocaleSupported(cookies.locale)) {
        detectedLocale = cookies.locale;
        confidence = 0.8;
      }
    }
    const acceptLanguage = headers["accept-language"];
    if (acceptLanguage) {
      const parsed = this.parseAcceptLanguage(acceptLanguage);
      if (parsed.length > 0) {
        sources.push({
          type: "header",
          value: parsed[0],
          priority: 60
        });
        if (confidence < 0.6 && this.isLocaleSupported(parsed[0])) {
          detectedLocale = parsed[0];
          confidence = 0.6;
        }
      }
    }
    if (tenantSettings?.defaultLocale) {
      sources.push({
        type: "tenant_default",
        value: tenantSettings.defaultLocale,
        priority: 40
      });
      if (confidence < 0.4 && this.isLocaleSupported(tenantSettings.defaultLocale)) {
        detectedLocale = tenantSettings.defaultLocale;
        confidence = 0.4;
      }
    }
    return {
      detectedLocale,
      confidence,
      sources,
      fallbackApplied: confidence < 0.4
    };
  }
  /**
   * Translate a key
   */
  translate(key, locale, namespace = "common", interpolations, count) {
    const bundle = this.getTranslationBundle(locale, namespace);
    if (!bundle) {
      return this.handleMissingKey(key, locale, namespace);
    }
    let value = bundle.translations[key];
    if (!value) {
      return this.handleMissingKey(key, locale, namespace);
    }
    if (typeof value === "object" && count !== void 0) {
      value = this.resolvePluralForm(value, count, locale);
    }
    if (typeof value === "string" && interpolations) {
      value = this.applyInterpolations(value, interpolations);
    }
    return value;
  }
  /**
   * Localize content
   */
  async localize(request) {
    const sourceLocale = request.sourceLocale || "en-US";
    let content = request.content;
    let fallbackUsed = false;
    let fallbackLocale;
    const missingKeys = [];
    if (typeof content === "string") {
      const translated = this.translate(
        content,
        request.targetLocale,
        request.namespace || "common",
        request.interpolations
      );
      if (translated === content && request.targetLocale !== sourceLocale) {
        fallbackUsed = true;
        fallbackLocale = this.config.fallbackLocale;
      }
      content = translated;
    } else if (typeof content === "object") {
      content = await this.localizeObject(
        content,
        request.targetLocale,
        request.namespace || "common",
        missingKeys
      );
      fallbackUsed = missingKeys.length > 0;
      if (fallbackUsed) {
        fallbackLocale = this.config.fallbackLocale;
      }
    }
    const response = {
      content,
      locale: request.targetLocale,
      fallbackUsed,
      fallbackLocale,
      missingKeys: missingKeys.length > 0 ? missingKeys : void 0,
      governanceVerdict: this.createGovernanceVerdict("localize")
    };
    return this.wrapInEnvelope(response, "localize");
  }
  /**
   * Format date for locale
   */
  formatDate(date, options2) {
    try {
      const intlOptions = {
        dateStyle: options2.dateStyle,
        timeStyle: options2.timeStyle,
        timeZone: options2.timezone,
        hour12: options2.hour12
      };
      return new Intl.DateTimeFormat(options2.locale, intlOptions).format(date);
    } catch (error) {
      logger_default2.error("Error formatting date", { error, locale: options2.locale });
      return date.toISOString();
    }
  }
  /**
   * Format number for locale
   */
  formatNumber(value, locale) {
    try {
      return new Intl.NumberFormat(locale).format(value);
    } catch (error) {
      logger_default2.error("Error formatting number", { error, locale });
      return value.toString();
    }
  }
  /**
   * Format currency for locale
   */
  formatCurrency(value, options2) {
    try {
      return new Intl.NumberFormat(options2.locale, {
        style: "currency",
        currency: options2.currency,
        currencyDisplay: options2.display,
        minimumFractionDigits: options2.minimumFractionDigits,
        maximumFractionDigits: options2.maximumFractionDigits
      }).format(value);
    } catch (error) {
      logger_default2.error("Error formatting currency", { error, locale: options2.locale });
      return `${options2.currency} ${value}`;
    }
  }
  /**
   * Get regional compliance configuration
   */
  getRegionalCompliance(region) {
    const config9 = REGIONAL_COMPLIANCE.get(region) || null;
    return this.wrapInEnvelope(config9, "get_regional_compliance");
  }
  /**
   * Get compliance requirements for a locale
   */
  getComplianceForLocale(locale) {
    for (const [, config9] of REGIONAL_COMPLIANCE) {
      if (config9.locales.includes(locale)) {
        return config9;
      }
    }
    return null;
  }
  /**
   * Get translation status
   */
  async getTranslationStatus(locale) {
    const localeConfig = LOCALE_CONFIGS.get(locale);
    const namespaceStatuses = /* @__PURE__ */ new Map();
    let totalKeys = 0;
    let translatedKeys = 0;
    for (const namespace of this.config.namespaces) {
      const bundle = this.getTranslationBundle(locale, namespace);
      const defaultBundle = this.getTranslationBundle("en-US", namespace);
      const nsTotal = Object.keys(defaultBundle?.translations || {}).length;
      const nsTranslated = Object.keys(bundle?.translations || {}).length;
      totalKeys += nsTotal;
      translatedKeys += nsTranslated;
      namespaceStatuses.set(namespace, {
        namespace,
        totalKeys: nsTotal,
        translatedKeys: nsTranslated,
        completeness: nsTotal > 0 ? nsTranslated / nsTotal : 0
      });
    }
    const status = {
      locale,
      totalKeys,
      translatedKeys,
      verifiedKeys: Math.floor(translatedKeys * 0.9),
      // Simplified
      completeness: localeConfig?.completeness || (totalKeys > 0 ? translatedKeys / totalKeys : 0),
      byNamespace: namespaceStatuses,
      lastUpdated: /* @__PURE__ */ new Date()
    };
    return this.wrapInEnvelope(status, "get_translation_status");
  }
  /**
   * Set user locale preferences
   */
  async setUserLocalePreferences(userId, tenantId, preferences) {
    const pool4 = getPostgresPool2();
    if (!pool4) throw new Error("Database not available");
    await pool4.query(
      `INSERT INTO user_locale_preferences (
        user_id, tenant_id, preferred_locale, fallback_locale, timezone,
        date_format, time_format, number_format, currency_display, updated_at
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, NOW())
      ON CONFLICT (user_id, tenant_id) DO UPDATE SET
        preferred_locale = COALESCE($3, user_locale_preferences.preferred_locale),
        fallback_locale = COALESCE($4, user_locale_preferences.fallback_locale),
        timezone = COALESCE($5, user_locale_preferences.timezone),
        date_format = COALESCE($6, user_locale_preferences.date_format),
        time_format = COALESCE($7, user_locale_preferences.time_format),
        number_format = COALESCE($8, user_locale_preferences.number_format),
        currency_display = COALESCE($9, user_locale_preferences.currency_display),
        updated_at = NOW()`,
      [
        userId,
        tenantId,
        preferences.preferredLocale,
        preferences.fallbackLocale,
        preferences.timezone,
        preferences.dateFormat,
        preferences.timeFormat,
        preferences.numberFormat ? JSON.stringify(preferences.numberFormat) : null,
        preferences.currencyDisplay
      ]
    );
    this.userPreferencesCache.delete(`${tenantId}:${userId}`);
  }
  /**
   * Get user locale preferences
   */
  async getUserLocalePreferences(userId, tenantId) {
    const cacheKey = `${tenantId}:${userId}`;
    if (this.userPreferencesCache.has(cacheKey)) {
      return this.userPreferencesCache.get(cacheKey);
    }
    const pool4 = getPostgresPool2();
    if (!pool4) return null;
    const result2 = await pool4.query(
      "SELECT * FROM user_locale_preferences WHERE user_id = $1 AND tenant_id = $2",
      [userId, tenantId]
    );
    if (result2.rowCount === 0) return null;
    const row = result2.rows[0];
    const preferences = {
      userId: row.user_id,
      tenantId: row.tenant_id,
      preferredLocale: row.preferred_locale,
      fallbackLocale: row.fallback_locale,
      dateFormat: row.date_format,
      timeFormat: row.time_format,
      timezone: row.timezone,
      numberFormat: row.number_format,
      currencyDisplay: row.currency_display,
      updatedAt: row.updated_at
    };
    this.userPreferencesCache.set(cacheKey, preferences);
    return preferences;
  }
  // Private helper methods
  isLocaleSupported(locale) {
    return this.config.supportedLocales.includes(locale);
  }
  parseAcceptLanguage(header) {
    return header.split(",").map((lang) => {
      const [locale, quality2] = lang.trim().split(";q=");
      return { locale: locale.trim(), quality: quality2 ? parseFloat(quality2) : 1 };
    }).sort((a, b) => b.quality - a.quality).map((l) => l.locale);
  }
  getTranslationBundle(locale, namespace) {
    const cacheKey = `${locale}:${namespace}`;
    return this.translationCache.get(cacheKey) || null;
  }
  handleMissingKey(key, locale, namespace) {
    if (this.config.debug) {
      logger_default2.warn("Missing translation key", { key, locale, namespace });
    }
    switch (this.config.missingKeyBehavior) {
      case "fallback":
        const fallbackBundle = this.getTranslationBundle(this.config.fallbackLocale, namespace);
        const fallbackValue = fallbackBundle?.translations[key];
        return typeof fallbackValue === "string" ? fallbackValue : key;
      case "empty":
        return "";
      case "key":
      default:
        return key;
    }
  }
  resolvePluralForm(forms, count, locale) {
    if (count === 0 && forms.zero) return forms.zero;
    if (count === 1) return forms.one;
    if (count === 2 && forms.two) return forms.two;
    return forms.other;
  }
  applyInterpolations(value, interpolations) {
    let result2 = value;
    for (const [key, val] of Object.entries(interpolations)) {
      result2 = result2.replace(new RegExp(`{{${key}}}`, "g"), String(val));
    }
    return result2;
  }
  async localizeObject(obj, locale, namespace, missingKeys) {
    const result2 = {};
    for (const [key, value] of Object.entries(obj)) {
      if (typeof value === "string") {
        const translated = this.translate(key, locale, namespace);
        if (translated === key) {
          missingKeys.push(key);
        }
        result2[key] = translated;
      } else if (typeof value === "object" && value !== null) {
        result2[key] = await this.localizeObject(
          value,
          locale,
          namespace,
          missingKeys
        );
      } else {
        result2[key] = value;
      }
    }
    return result2;
  }
  createGovernanceVerdict(operation) {
    return {
      verdictId: randomUUID60(),
      policyId: `i18n_${operation}`,
      result: "ALLOW" /* ALLOW */,
      decidedAt: /* @__PURE__ */ new Date(),
      reason: "Localization operation permitted",
      evaluator: "i18n-service"
    };
  }
  wrapInEnvelope(data, operation) {
    return createDataEnvelope(data, {
      source: "i18n-service",
      actor: "system",
      version: "3.1.0",
      classification: "INTERNAL" /* INTERNAL */,
      governanceVerdict: this.createGovernanceVerdict(operation)
    });
  }
  initializeDefaultTranslations() {
    const commonTranslations = {
      locale: "en-US",
      namespace: "common",
      version: "1.0.0",
      lastUpdated: /* @__PURE__ */ new Date(),
      translations: {
        "app.name": "Summit",
        "app.tagline": "AI-Augmented Intelligence Platform",
        "nav.dashboard": "Dashboard",
        "nav.search": "Search",
        "nav.policies": "Policies",
        "nav.analytics": "Analytics",
        "nav.plugins": "Plugins",
        "nav.settings": "Settings",
        "nav.support": "Support",
        "actions.save": "Save",
        "actions.cancel": "Cancel",
        "actions.delete": "Delete",
        "actions.edit": "Edit",
        "actions.create": "Create",
        "actions.search": "Search",
        "actions.filter": "Filter",
        "actions.export": "Export",
        "actions.import": "Import",
        "status.loading": "Loading...",
        "status.error": "An error occurred",
        "status.success": "Operation successful",
        "governance.approved": "Approved",
        "governance.denied": "Denied",
        "governance.pending": "Pending Review"
      }
    };
    this.translationCache.set("en-US:common", commonTranslations);
    const errorTranslations = {
      locale: "en-US",
      namespace: "errors",
      version: "1.0.0",
      lastUpdated: /* @__PURE__ */ new Date(),
      translations: {
        "error.generic": "An unexpected error occurred. Please try again.",
        "error.network": "Network error. Please check your connection.",
        "error.unauthorized": "You are not authorized to perform this action.",
        "error.forbidden": "Access denied.",
        "error.notFound": "The requested resource was not found.",
        "error.validation": "Please check your input and try again.",
        "error.governance.denied": "This action was denied by governance policy.",
        "error.governance.review": "This action requires review before proceeding."
      }
    };
    this.translationCache.set("en-US:errors", errorTranslations);
    const onboardingTranslations = {
      locale: "en-US",
      namespace: "onboarding",
      version: "1.0.0",
      lastUpdated: /* @__PURE__ */ new Date(),
      translations: {
        "onboarding.welcome": "Welcome to Summit",
        "onboarding.getStarted": "Get Started",
        "onboarding.skip": "Skip for now",
        "onboarding.next": "Next",
        "onboarding.previous": "Previous",
        "onboarding.complete": "Complete",
        "onboarding.progress": "Step {{current}} of {{total}}"
      }
    };
    this.translationCache.set("en-US:onboarding", onboardingTranslations);
  }
};
var i18nService = I18nService.getInstance();

// src/routes/i18n.ts
init_auth4();
init_featureFlags2();
var router91 = Router45();
var singleParam22 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var requireFeatureFlag3 = (flagName) => {
  return (req, res, next) => {
    const context4 = { userId: req.user?.id, tenantId: req.user?.tenantId };
    if (!isEnabled(flagName, context4)) {
      res.status(403).json({ error: `Feature '${flagName}' is not enabled` });
      return;
    }
    next();
  };
};
var optionalAuth2 = async (req, res, next) => {
  const auth = req.headers.authorization || "";
  const token = auth.startsWith("Bearer ") ? auth.slice("Bearer ".length) : req.headers["x-access-token"] || null;
  if (!token) {
    next();
    return;
  }
  try {
    const AuthService2 = (await Promise.resolve().then(() => (init_AuthService(), AuthService_exports))).default;
    const authService5 = new AuthService2();
    const user = await authService5.verifyToken(token);
    if (user) {
      req.user = {
        id: user.id,
        tenantId: user.defaultTenantId || "default",
        role: user.role,
        email: user.email
      };
    }
  } catch {
  }
  next();
};
var LocalizeSchema = z42.object({
  content: z42.union([z42.string(), z42.record(z42.unknown())]),
  targetLocale: z42.string(),
  sourceLocale: z42.string().optional(),
  namespace: z42.string().optional(),
  interpolations: z42.record(z42.union([z42.string(), z42.number()])).optional()
});
var SetPreferencesSchema = z42.object({
  preferredLocale: z42.string().optional(),
  fallbackLocale: z42.string().optional(),
  timezone: z42.string().optional(),
  dateFormat: z42.string().optional(),
  timeFormat: z42.string().optional(),
  currencyDisplay: z42.enum(["symbol", "code", "name"]).optional()
});
var FormatDateSchema = z42.object({
  date: z42.string().transform((str) => new Date(str)),
  locale: z42.string(),
  dateStyle: z42.enum(["full", "long", "medium", "short"]).optional(),
  timeStyle: z42.enum(["full", "long", "medium", "short"]).optional(),
  timezone: z42.string().optional(),
  hour12: z42.boolean().optional()
});
var FormatCurrencySchema = z42.object({
  value: z42.number(),
  locale: z42.string(),
  currency: z42.string(),
  display: z42.enum(["symbol", "code", "name"]).optional(),
  minimumFractionDigits: z42.number().optional(),
  maximumFractionDigits: z42.number().optional()
});
router91.get(
  "/locales",
  optionalAuth2,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const result2 = i18nService.getSupportedLocales();
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router91.get(
  "/locales/:locale",
  optionalAuth2,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const locale = singleParam22(req.params.locale) ?? "";
      const config9 = i18nService.getLocaleConfig(locale);
      if (!config9) {
        res.status(404).json({ error: "Locale not found" });
        return;
      }
      res.json({ data: config9 });
    } catch (error) {
      next(error);
    }
  }
);
router91.get(
  "/detect",
  optionalAuth2,
  requireFeatureFlag3("i18n.autoDetect"),
  async (req, res, next) => {
    try {
      const headers = req.headers;
      const cookies = req.cookies || {};
      let userPreferences;
      if (req.user) {
        userPreferences = await i18nService.getUserLocalePreferences(
          req.user.id,
          req.user.tenantId
        );
      }
      const result2 = i18nService.detectLocale(
        headers,
        cookies,
        userPreferences || void 0,
        void 0
      );
      res.json({ data: result2 });
    } catch (error) {
      next(error);
    }
  }
);
router91.get(
  "/translate",
  optionalAuth2,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const key = req.query.key;
      const locale = req.query.locale || "en-US";
      const namespace = req.query.namespace || "common";
      if (!key) {
        res.status(400).json({ error: "Translation key is required" });
        return;
      }
      const result2 = i18nService.translate(key, locale, namespace);
      res.json({ data: { key, locale, translation: result2 } });
    } catch (error) {
      next(error);
    }
  }
);
router91.post(
  "/localize",
  optionalAuth2,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const data = LocalizeSchema.parse(req.body);
      const result2 = await i18nService.localize({
        content: data.content,
        targetLocale: data.targetLocale,
        sourceLocale: data.sourceLocale,
        namespace: data.namespace,
        interpolations: data.interpolations
      });
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router91.post(
  "/format/date",
  optionalAuth2,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const data = FormatDateSchema.parse(req.body);
      const result2 = i18nService.formatDate(data.date, {
        locale: data.locale,
        dateStyle: data.dateStyle,
        timeStyle: data.timeStyle,
        timezone: data.timezone,
        hour12: data.hour12
      });
      res.json({ data: { formatted: result2 } });
    } catch (error) {
      next(error);
    }
  }
);
router91.post(
  "/format/number",
  optionalAuth2,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const { value, locale } = req.body;
      if (typeof value !== "number") {
        res.status(400).json({ error: "Value must be a number" });
        return;
      }
      const result2 = i18nService.formatNumber(value, locale || "en-US");
      res.json({ data: { formatted: result2 } });
    } catch (error) {
      next(error);
    }
  }
);
router91.post(
  "/format/currency",
  optionalAuth2,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const data = FormatCurrencySchema.parse(req.body);
      const result2 = i18nService.formatCurrency(data.value, {
        locale: data.locale,
        currency: data.currency,
        display: data.display,
        minimumFractionDigits: data.minimumFractionDigits,
        maximumFractionDigits: data.maximumFractionDigits
      });
      res.json({ data: { formatted: result2 } });
    } catch (error) {
      next(error);
    }
  }
);
router91.get(
  "/compliance/:region",
  ensureAuthenticated,
  requireFeatureFlag3("i18n.regionalCompliance"),
  async (req, res, next) => {
    try {
      const region = singleParam22(req.params.region) ?? "";
      const result2 = i18nService.getRegionalCompliance(region);
      if (!result2.data) {
        res.status(404).json({ error: "Region not found" });
        return;
      }
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router91.get(
  "/locales/:locale/compliance",
  ensureAuthenticated,
  requireFeatureFlag3("i18n.regionalCompliance"),
  async (req, res, next) => {
    try {
      const locale = singleParam22(req.params.locale) ?? "";
      const result2 = i18nService.getComplianceForLocale(locale);
      res.json({ data: result2 });
    } catch (error) {
      next(error);
    }
  }
);
router91.get(
  "/status/:locale",
  ensureAuthenticated,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const locale = singleParam22(req.params.locale) ?? "";
      const result2 = await i18nService.getTranslationStatus(locale);
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router91.put(
  "/preferences",
  ensureAuthenticated,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const data = SetPreferencesSchema.parse(req.body);
      const { tenantId, id: userId } = req.user;
      await i18nService.setUserLocalePreferences(userId, tenantId, data);
      res.json({ success: true });
    } catch (error) {
      next(error);
    }
  }
);
router91.get(
  "/preferences",
  ensureAuthenticated,
  requireFeatureFlag3("i18n.enabled"),
  async (req, res, next) => {
    try {
      const { tenantId, id: userId } = req.user;
      const result2 = await i18nService.getUserLocalePreferences(userId, tenantId);
      res.json({ data: result2 });
    } catch (error) {
      next(error);
    }
  }
);
var i18n_default = router91;

// src/routes/experimentation.ts
import { Router as Router46 } from "express";
import { z as z43 } from "zod";

// src/experimentation/ExperimentationService.ts
init_database();
init_logger2();
init_data_envelope();
import { randomUUID as randomUUID61, createHash as createHash32 } from "crypto";
var ExperimentationService = class _ExperimentationService {
  static instance;
  experiments;
  assignments;
  hashSalt;
  constructor() {
    this.experiments = /* @__PURE__ */ new Map();
    this.assignments = /* @__PURE__ */ new Map();
    this.hashSalt = process.env.EXPERIMENT_SALT || "summit-experiments";
    this.loadExperiments();
  }
  static getInstance() {
    if (!_ExperimentationService.instance) {
      _ExperimentationService.instance = new _ExperimentationService();
    }
    return _ExperimentationService.instance;
  }
  /**
   * Create a new experiment
   */
  async createExperiment(experiment) {
    this.validateExperiment(experiment);
    const newExperiment = {
      ...experiment,
      id: randomUUID61(),
      status: "draft",
      approvals: this.getRequiredApprovals(experiment),
      createdAt: /* @__PURE__ */ new Date(),
      updatedAt: /* @__PURE__ */ new Date()
    };
    newExperiment.governanceVerdict = await this.getExperimentGovernanceVerdict(newExperiment);
    await this.saveExperiment(newExperiment);
    this.experiments.set(newExperiment.id, newExperiment);
    logger_default2.info("Experiment created", { experimentId: newExperiment.id, name: newExperiment.name });
    return this.wrapInEnvelope(newExperiment, "create_experiment");
  }
  /**
   * Start an experiment
   */
  async startExperiment(experimentId) {
    const experiment = this.experiments.get(experimentId);
    if (!experiment) {
      throw new Error(`Experiment not found: ${experimentId}`);
    }
    const pendingApprovals = experiment.approvals.filter((a) => a.status === "pending");
    if (pendingApprovals.length > 0) {
      throw new Error("Experiment has pending approvals");
    }
    const rejectedApprovals = experiment.approvals.filter((a) => a.status === "rejected");
    if (rejectedApprovals.length > 0) {
      throw new Error("Experiment has rejected approvals");
    }
    experiment.status = "running";
    experiment.startDate = /* @__PURE__ */ new Date();
    experiment.updatedAt = /* @__PURE__ */ new Date();
    await this.saveExperiment(experiment);
    logger_default2.info("Experiment started", { experimentId, name: experiment.name });
    return this.wrapInEnvelope(experiment, "start_experiment");
  }
  /**
   * Get variant assignment for user
   */
  async getAssignment(experimentId, context4) {
    if (!context4.consent) {
      return this.wrapInEnvelope(null, "get_assignment");
    }
    const experiment = this.experiments.get(experimentId);
    if (!experiment || experiment.status !== "running") {
      return this.wrapInEnvelope(null, "get_assignment");
    }
    if (!this.matchesTargeting(experiment.targetingRules, context4)) {
      return this.wrapInEnvelope(null, "get_assignment");
    }
    if (!this.isInTrafficAllocation(context4.userId, experimentId, experiment.trafficAllocation)) {
      return this.wrapInEnvelope(null, "get_assignment");
    }
    const userHash = this.hashIdentifier(context4.userId);
    const tenantHash = this.hashIdentifier(context4.tenantId);
    const assignmentKey = `${experimentId}:${userHash}`;
    let assignment = this.assignments.get(assignmentKey);
    if (!assignment) {
      const variant = this.assignVariant(experiment, context4.userId);
      assignment = {
        experimentId,
        variantId: variant.id,
        userHash,
        tenantHash,
        assignedAt: /* @__PURE__ */ new Date(),
        context: this.sanitizeContext(context4)
      };
      this.assignments.set(assignmentKey, assignment);
      await this.saveAssignment(assignment);
      await this.trackExposure(assignment);
    }
    return this.wrapInEnvelope(assignment, "get_assignment");
  }
  /**
   * Track a metric event
   */
  async trackMetric(experimentId, userId, metricName, metricValue) {
    const userHash = this.hashIdentifier(userId);
    const assignmentKey = `${experimentId}:${userHash}`;
    const assignment = this.assignments.get(assignmentKey);
    if (!assignment) {
      return;
    }
    const event = {
      eventId: randomUUID61(),
      experimentId,
      variantId: assignment.variantId,
      userHash,
      metricName,
      metricValue,
      timestamp: /* @__PURE__ */ new Date()
    };
    await this.saveMetricEvent(event);
    logger_default2.debug("Experiment metric tracked", {
      experimentId,
      metricName,
      variantId: assignment.variantId
    });
  }
  /**
   * Get experiment results
   */
  async getResults(experimentId) {
    const experiment = this.experiments.get(experimentId);
    if (!experiment) {
      throw new Error(`Experiment not found: ${experimentId}`);
    }
    const pool4 = getPostgresPool2();
    if (!pool4) {
      throw new Error("Database not available");
    }
    const statsResult = await pool4.query(
      `SELECT
        variant_id,
        COUNT(DISTINCT user_hash) as sample_size,
        SUM(metric_value) as conversions,
        AVG(metric_value) as conversion_rate
      FROM experiment_metrics
      WHERE experiment_id = $1 AND metric_name = $2
      GROUP BY variant_id`,
      [experimentId, experiment.primaryMetric]
    );
    const controlVariant = experiment.variants.find((v) => v.isControl);
    const controlStats = statsResult.rows.find((r) => r.variant_id === controlVariant?.id);
    const controlRate = parseFloat(controlStats?.conversion_rate || "0");
    const variantResults = experiment.variants.map((variant) => {
      const stats = statsResult.rows.find((r) => r.variant_id === variant.id);
      const sampleSize = parseInt(stats?.sample_size || "0");
      const conversionRate = parseFloat(stats?.conversion_rate || "0");
      const conversionCount = parseInt(stats?.conversions || "0");
      const improvement = controlRate > 0 ? (conversionRate - controlRate) / controlRate : 0;
      return {
        variantId: variant.id,
        variantName: variant.name,
        sampleSize,
        conversionRate,
        conversionCount,
        improvementOverControl: improvement,
        confidenceInterval: this.calculateConfidenceInterval(conversionRate, sampleSize),
        isWinner: false
      };
    });
    const totalSampleSize = variantResults.reduce((sum, v) => sum + v.sampleSize, 0);
    const significance = this.calculateStatisticalSignificance(variantResults);
    let winner;
    let recommendation = "continue";
    if (significance >= experiment.confidenceLevel && totalSampleSize >= experiment.minSampleSize) {
      const best = variantResults.reduce(
        (a, b) => a.conversionRate > b.conversionRate ? a : b
      );
      if (!experiment.variants.find((v) => v.id === best.variantId)?.isControl) {
        winner = best.variantId;
        best.isWinner = true;
        recommendation = "stop_winner";
      } else {
        recommendation = "stop_no_winner";
      }
    } else if (totalSampleSize < experiment.minSampleSize * 0.5) {
      recommendation = "insufficient_data";
    }
    const results = {
      experimentId,
      status: experiment.status,
      startDate: experiment.startDate || /* @__PURE__ */ new Date(),
      endDate: experiment.endDate,
      variants: variantResults,
      winner,
      statisticalSignificance: significance,
      confidenceInterval: [0.95 - 0.02, 0.95 + 0.02],
      // Simplified
      sampleSize: totalSampleSize,
      powerAnalysis: {
        currentPower: Math.min(1, totalSampleSize / experiment.minSampleSize),
        requiredSampleSize: experiment.minSampleSize,
        estimatedTimeRemaining: void 0,
        minimumDetectableEffect: 0.05
      },
      recommendation,
      governanceVerdict: this.createGovernanceVerdict("get_results")
    };
    return this.wrapInEnvelope(results, "get_results");
  }
  /**
   * Approve experiment
   */
  async approveExperiment(experimentId, approver, role, approved, comment) {
    const experiment = this.experiments.get(experimentId);
    if (!experiment) {
      throw new Error(`Experiment not found: ${experimentId}`);
    }
    const approval = experiment.approvals.find((a) => a.role === role);
    if (!approval) {
      throw new Error(`No approval required for role: ${role}`);
    }
    approval.approver = approver;
    approval.status = approved ? "approved" : "rejected";
    approval.approvedAt = /* @__PURE__ */ new Date();
    approval.comment = comment;
    experiment.updatedAt = /* @__PURE__ */ new Date();
    await this.saveExperiment(experiment);
    logger_default2.info("Experiment approval updated", {
      experimentId,
      role,
      status: approval.status
    });
    return this.wrapInEnvelope(experiment, "approve_experiment");
  }
  /**
   * Complete experiment and roll out winner
   */
  async completeExperiment(experimentId, rolloutWinner = false) {
    const experiment = this.experiments.get(experimentId);
    if (!experiment) {
      throw new Error(`Experiment not found: ${experimentId}`);
    }
    experiment.status = "completed";
    experiment.endDate = /* @__PURE__ */ new Date();
    experiment.updatedAt = /* @__PURE__ */ new Date();
    if (rolloutWinner) {
      const results = await this.getResults(experimentId);
      if (results.data.winner) {
        logger_default2.info("Rolling out experiment winner", {
          experimentId,
          winnerId: results.data.winner
        });
      }
    }
    await this.saveExperiment(experiment);
    logger_default2.info("Experiment completed", { experimentId, name: experiment.name });
    return this.wrapInEnvelope(experiment, "complete_experiment");
  }
  // Private helper methods
  validateExperiment(experiment) {
    const totalWeight = experiment.variants.reduce((sum, v) => sum + v.weight, 0);
    if (Math.abs(totalWeight - 100) > 0.01) {
      throw new Error("Variant weights must sum to 100");
    }
    const controlVariants = experiment.variants.filter((v) => v.isControl);
    if (controlVariants.length !== 1) {
      throw new Error("Experiment must have exactly one control variant");
    }
    if (experiment.trafficAllocation < 0 || experiment.trafficAllocation > 100) {
      throw new Error("Traffic allocation must be between 0 and 100");
    }
  }
  getRequiredApprovals(experiment) {
    const approvals = [
      { approver: "", role: "product", status: "pending" }
    ];
    if (experiment.targetingRules.some((r) => r.attribute.includes("pii"))) {
      approvals.push({ approver: "", role: "governance", status: "pending" });
    }
    if (experiment.trafficAllocation > 50) {
      approvals.push({ approver: "", role: "compliance", status: "pending" });
    }
    return approvals;
  }
  async getExperimentGovernanceVerdict(experiment) {
    return this.createGovernanceVerdict("experiment_creation");
  }
  matchesTargeting(rules, context4) {
    for (const rule of rules) {
      const value = context4.attributes[rule.attribute];
      switch (rule.operator) {
        case "equals":
          if (value !== rule.value) return false;
          break;
        case "not_equals":
          if (value === rule.value) return false;
          break;
        case "contains":
          if (!String(value).includes(String(rule.value))) return false;
          break;
        case "in":
          if (!Array.isArray(rule.value) || !rule.value.includes(value)) return false;
          break;
        case "not_in":
          if (Array.isArray(rule.value) && rule.value.includes(value)) return false;
          break;
        case "gt":
          if (Number(value) <= Number(rule.value)) return false;
          break;
        case "lt":
          if (Number(value) >= Number(rule.value)) return false;
          break;
      }
    }
    return true;
  }
  isInTrafficAllocation(userId, experimentId, allocation) {
    const hash3 = createHash32("sha256").update(`${this.hashSalt}:${experimentId}:traffic:${userId}`).digest("hex");
    const bucket = parseInt(hash3.substring(0, 8), 16) % 100;
    return bucket < allocation;
  }
  assignVariant(experiment, userId) {
    const hash3 = createHash32("sha256").update(`${this.hashSalt}:${experiment.id}:variant:${userId}`).digest("hex");
    const bucket = parseInt(hash3.substring(0, 8), 16) % 100;
    let cumulative = 0;
    for (const variant of experiment.variants) {
      cumulative += variant.weight;
      if (bucket < cumulative) {
        return variant;
      }
    }
    return experiment.variants.find((v) => v.isControl) || experiment.variants[0];
  }
  sanitizeContext(context4) {
    const sanitized = { ...context4.attributes };
    delete sanitized.email;
    delete sanitized.name;
    delete sanitized.ip;
    return sanitized;
  }
  hashIdentifier(id) {
    return createHash32("sha256").update(`${this.hashSalt}:${id}`).digest("hex").substring(0, 16);
  }
  calculateConfidenceInterval(rate, n) {
    if (n === 0) return [0, 0];
    const z51 = 1.96;
    const stderr = Math.sqrt(rate * (1 - rate) / n);
    return [Math.max(0, rate - z51 * stderr), Math.min(1, rate + z51 * stderr)];
  }
  calculateStatisticalSignificance(variantResults) {
    const control = variantResults.find((v) => v.variantId.includes("control"));
    if (!control) return 0;
    let maxSignificance = 0;
    for (const variant of variantResults) {
      if (variant === control) continue;
      const p1 = control.conversionRate;
      const p2 = variant.conversionRate;
      const n1 = control.sampleSize;
      const n2 = variant.sampleSize;
      if (n1 === 0 || n2 === 0) continue;
      const pooledP = (p1 * n1 + p2 * n2) / (n1 + n2);
      const se = Math.sqrt(pooledP * (1 - pooledP) * (1 / n1 + 1 / n2));
      if (se === 0) continue;
      const z51 = Math.abs(p2 - p1) / se;
      const significance = this.zToConfidence(z51);
      if (significance > maxSignificance) {
        maxSignificance = significance;
      }
    }
    return maxSignificance;
  }
  zToConfidence(z51) {
    if (z51 < 1.645) return 0.9;
    if (z51 < 1.96) return 0.95;
    if (z51 < 2.576) return 0.99;
    return 0.999;
  }
  async trackExposure(assignment) {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    await pool4.query(
      `INSERT INTO experiment_exposures (
        event_id, experiment_id, variant_id, user_hash, tenant_hash, timestamp, context
      ) VALUES ($1, $2, $3, $4, $5, $6, $7)`,
      [
        randomUUID61(),
        assignment.experimentId,
        assignment.variantId,
        assignment.userHash,
        assignment.tenantHash,
        assignment.assignedAt,
        JSON.stringify(assignment.context)
      ]
    );
  }
  async saveExperiment(experiment) {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    await pool4.query(
      `INSERT INTO experiments (
        id, name, description, type, status, hypothesis, primary_metric,
        secondary_metrics, variants, targeting_rules, traffic_allocation,
        start_date, end_date, min_sample_size, confidence_level, owner,
        approvals, governance_verdict, created_at, updated_at
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20)
      ON CONFLICT (id) DO UPDATE SET
        status = EXCLUDED.status,
        start_date = EXCLUDED.start_date,
        end_date = EXCLUDED.end_date,
        approvals = EXCLUDED.approvals,
        updated_at = NOW()`,
      [
        experiment.id,
        experiment.name,
        experiment.description,
        experiment.type,
        experiment.status,
        experiment.hypothesis,
        experiment.primaryMetric,
        JSON.stringify(experiment.secondaryMetrics),
        JSON.stringify(experiment.variants),
        JSON.stringify(experiment.targetingRules),
        experiment.trafficAllocation,
        experiment.startDate,
        experiment.endDate,
        experiment.minSampleSize,
        experiment.confidenceLevel,
        experiment.owner,
        JSON.stringify(experiment.approvals),
        JSON.stringify(experiment.governanceVerdict),
        experiment.createdAt,
        experiment.updatedAt
      ]
    );
  }
  async saveAssignment(assignment) {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    await pool4.query(
      `INSERT INTO experiment_assignments (
        experiment_id, variant_id, user_hash, tenant_hash, assigned_at, context
      ) VALUES ($1, $2, $3, $4, $5, $6)
      ON CONFLICT (experiment_id, user_hash) DO NOTHING`,
      [
        assignment.experimentId,
        assignment.variantId,
        assignment.userHash,
        assignment.tenantHash,
        assignment.assignedAt,
        JSON.stringify(assignment.context)
      ]
    );
  }
  async saveMetricEvent(event) {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    await pool4.query(
      `INSERT INTO experiment_metrics (
        event_id, experiment_id, variant_id, user_hash, metric_name, metric_value, timestamp
      ) VALUES ($1, $2, $3, $4, $5, $6, $7)`,
      [
        event.eventId,
        event.experimentId,
        event.variantId,
        event.userHash,
        event.metricName,
        event.metricValue,
        event.timestamp
      ]
    );
  }
  async loadExperiments() {
    const pool4 = getPostgresPool2();
    if (!pool4) return;
    try {
      const result2 = await pool4.query(
        "SELECT * FROM experiments WHERE status IN ('draft', 'running', 'paused')"
      );
      for (const row of result2.rows) {
        const experiment = {
          id: row.id,
          name: row.name,
          description: row.description,
          type: row.type,
          status: row.status,
          hypothesis: row.hypothesis,
          primaryMetric: row.primary_metric,
          secondaryMetrics: row.secondary_metrics,
          variants: row.variants,
          targetingRules: row.targeting_rules,
          trafficAllocation: row.traffic_allocation,
          startDate: row.start_date,
          endDate: row.end_date,
          minSampleSize: row.min_sample_size,
          confidenceLevel: row.confidence_level,
          owner: row.owner,
          approvals: row.approvals,
          createdAt: row.created_at,
          updatedAt: row.updated_at,
          governanceVerdict: row.governance_verdict
        };
        this.experiments.set(experiment.id, experiment);
      }
      logger_default2.info("Loaded experiments", { count: this.experiments.size });
    } catch (error) {
      logger_default2.warn("Could not load experiments from database", { error });
    }
  }
  createGovernanceVerdict(operation) {
    return {
      verdictId: randomUUID61(),
      policyId: `experiment_${operation}`,
      result: "ALLOW" /* ALLOW */,
      decidedAt: /* @__PURE__ */ new Date(),
      reason: "Experiment operation permitted",
      evaluator: "experimentation-service"
    };
  }
  wrapInEnvelope(data, operation) {
    return createDataEnvelope(data, {
      source: "experimentation-service",
      actor: "system",
      version: "3.1.0",
      classification: "INTERNAL" /* INTERNAL */,
      governanceVerdict: this.createGovernanceVerdict(operation)
    });
  }
};
var experimentationService = ExperimentationService.getInstance();

// src/routes/experimentation.ts
init_auth4();
init_featureFlags2();
var router92 = Router46();
var singleParam23 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
var requireFeatureFlag4 = (flagName) => {
  return (req, res, next) => {
    const context4 = { userId: req.user?.id, tenantId: req.user?.tenantId };
    if (!isEnabled(flagName, context4)) {
      res.status(403).json({ error: `Feature '${flagName}' is not enabled` });
      return;
    }
    next();
  };
};
var TargetingRuleSchema = z43.object({
  id: z43.string(),
  attribute: z43.string(),
  operator: z43.enum(["equals", "not_equals", "contains", "in", "not_in", "gt", "lt"]),
  value: z43.unknown()
});
var VariantSchema = z43.object({
  id: z43.string(),
  name: z43.string(),
  description: z43.string(),
  weight: z43.number().min(0).max(100),
  config: z43.record(z43.unknown()),
  isControl: z43.boolean()
});
var CreateExperimentSchema = z43.object({
  name: z43.string().min(3).max(200),
  description: z43.string().max(2e3),
  type: z43.enum(["a_b", "multivariate", "feature_rollout"]),
  hypothesis: z43.string().min(10).max(1e3),
  primaryMetric: z43.string(),
  secondaryMetrics: z43.array(z43.string()).optional().default([]),
  variants: z43.array(VariantSchema).min(2),
  targetingRules: z43.array(TargetingRuleSchema).optional().default([]),
  trafficAllocation: z43.number().min(0).max(100),
  minSampleSize: z43.number().min(100),
  confidenceLevel: z43.number().min(0.8).max(0.99).default(0.95),
  owner: z43.string()
});
var GetAssignmentSchema = z43.object({
  userId: z43.string().optional(),
  attributes: z43.record(z43.unknown()).optional().default({}),
  consent: z43.boolean().default(true)
});
var TrackMetricSchema = z43.object({
  metricName: z43.string(),
  metricValue: z43.number()
});
var ApproveExperimentSchema = z43.object({
  role: z43.string(),
  approved: z43.boolean(),
  comment: z43.string().optional()
});
var CompleteExperimentSchema = z43.object({
  rolloutWinner: z43.boolean().optional().default(false)
});
router92.post(
  "/",
  ensureAuthenticated,
  requireFeatureFlag4("experimentation.abTesting"),
  async (req, res, next) => {
    try {
      if (req.user?.role !== "admin") {
        res.status(403).json({ error: "Admin access required to create experiments" });
        return;
      }
      const data = CreateExperimentSchema.parse(req.body);
      const result2 = await experimentationService.createExperiment({
        name: data.name,
        description: data.description,
        type: data.type,
        hypothesis: data.hypothesis,
        primaryMetric: data.primaryMetric,
        secondaryMetrics: data.secondaryMetrics,
        variants: data.variants,
        targetingRules: data.targetingRules,
        trafficAllocation: data.trafficAllocation,
        minSampleSize: data.minSampleSize,
        confidenceLevel: data.confidenceLevel,
        owner: data.owner
      });
      res.status(201).json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router92.post(
  "/:experimentId/start",
  ensureAuthenticated,
  requireFeatureFlag4("experimentation.abTesting"),
  async (req, res, next) => {
    try {
      if (req.user?.role !== "admin") {
        res.status(403).json({ error: "Admin access required to start experiments" });
        return;
      }
      const experimentId = singleParam23(req.params.experimentId) ?? "";
      const result2 = await experimentationService.startExperiment(experimentId);
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router92.get(
  "/:experimentId/assignment",
  ensureAuthenticated,
  requireFeatureFlag4("experimentation.abTesting"),
  async (req, res, next) => {
    try {
      const experimentId = singleParam23(req.params.experimentId) ?? "";
      const { tenantId, id: userId } = req.user;
      const attributes = {};
      for (const [key, value] of Object.entries(req.query)) {
        if (key.startsWith("attr_")) {
          attributes[key.slice(5)] = value;
        }
      }
      const result2 = await experimentationService.getAssignment(experimentId, {
        userId,
        tenantId,
        attributes,
        consent: true
        // Assume consent if authenticated
      });
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router92.post(
  "/:experimentId/assignment",
  ensureAuthenticated,
  requireFeatureFlag4("experimentation.abTesting"),
  async (req, res, next) => {
    try {
      const experimentId = singleParam23(req.params.experimentId) ?? "";
      const { tenantId, id: userId } = req.user;
      const data = GetAssignmentSchema.parse(req.body);
      const result2 = await experimentationService.getAssignment(experimentId, {
        userId: data.userId || userId,
        tenantId,
        attributes: data.attributes,
        consent: data.consent
      });
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router92.post(
  "/:experimentId/metrics",
  ensureAuthenticated,
  requireFeatureFlag4("experimentation.abTesting"),
  async (req, res, next) => {
    try {
      const experimentId = singleParam23(req.params.experimentId) ?? "";
      const { id: userId } = req.user;
      const { metricName, metricValue } = TrackMetricSchema.parse(req.body);
      await experimentationService.trackMetric(
        experimentId,
        userId,
        metricName,
        metricValue
      );
      res.json({ success: true });
    } catch (error) {
      next(error);
    }
  }
);
router92.get(
  "/:experimentId/results",
  ensureAuthenticated,
  requireFeatureFlag4("experimentation.abTesting"),
  async (req, res, next) => {
    try {
      const userRole = req.user?.role;
      if (userRole !== "admin" && userRole !== "analyst") {
        res.status(403).json({ error: "Admin or analyst access required to view results" });
        return;
      }
      const experimentId = singleParam23(req.params.experimentId) ?? "";
      const result2 = await experimentationService.getResults(experimentId);
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router92.post(
  "/:experimentId/approve",
  ensureAuthenticated,
  requireFeatureFlag4("experimentation.abTesting"),
  async (req, res, next) => {
    try {
      const experimentId = singleParam23(req.params.experimentId) ?? "";
      const { id: userId } = req.user;
      const { role, approved, comment } = ApproveExperimentSchema.parse(req.body);
      const userRole = req.user?.role;
      if (userRole !== role && userRole !== "admin") {
        res.status(403).json({ error: `Role '${role}' required for this approval` });
        return;
      }
      const result2 = await experimentationService.approveExperiment(
        experimentId,
        userId,
        role,
        approved,
        comment
      );
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
router92.post(
  "/:experimentId/complete",
  ensureAuthenticated,
  requireFeatureFlag4("experimentation.abTesting"),
  async (req, res, next) => {
    try {
      if (req.user?.role !== "admin") {
        res.status(403).json({ error: "Admin access required to complete experiments" });
        return;
      }
      const experimentId = singleParam23(req.params.experimentId) ?? "";
      const { rolloutWinner } = CompleteExperimentSchema.parse(req.body);
      const result2 = await experimentationService.completeExperiment(
        experimentId,
        rolloutWinner
      );
      res.json(result2);
    } catch (error) {
      next(error);
    }
  }
);
var experimentation_default = router92;

// src/routes/v4/index.ts
import { Router as Router50 } from "express";

// src/routes/v4/ai-governance.ts
import { Router as Router47 } from "express";
import { randomUUID as randomUUID65 } from "crypto";
init_logger2();

// src/ai/governance/PolicySuggestionService.ts
init_logger2();
import { randomUUID as randomUUID62, createHash as createHash34 } from "crypto";

// src/ai/governance/llm/GovernanceLLMClient.ts
import { createHash as createHash33 } from "crypto";

// src/services/llm/observability.ts
var ConsoleObservability = class {
  logLLMCall(request, result2, durationMs) {
    console.log(JSON.stringify({
      event: "llm_call",
      request: { ...request, prompt: request.prompt ? request.prompt.substring(0, 50) + "..." : void 0 },
      // Truncate prompt for logs
      result: { ...result2, text: result2.text ? result2.text.substring(0, 50) + "..." : void 0 },
      durationMs,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    }));
  }
  recordMetric(name, value, tags) {
  }
};

// src/services/llm/providers/OpenAIProvider.ts
var OpenAIProvider = class {
  name = "openai";
  supports(taskType) {
    return true;
  }
  estimate(taskType, inputTokens) {
    return { costUsd: 25e-7 * inputTokens, p95ms: 800 };
  }
  async call(request, config9) {
    const apiKey = process.env[config9?.apiKeyEnv || "OPENAI_API_KEY"];
    if (!apiKey) {
      return { ok: false, error: `Missing API Key for ${this.name}` };
    }
    const model = config9?.models?.[request.taskType] || "gpt-4o-mini";
    const messages = request.messages || (request.prompt ? [{ role: "user", content: request.prompt }] : []);
    try {
      const response = await fetch("https://api.openai.com/v1/chat/completions", {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${apiKey}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model,
          messages
          // TODO: Add more options like temperature, max_tokens based on config or request
        })
      });
      const data = await response.json();
      if (!response.ok) {
        return { ok: false, error: data.error?.message || response.statusText, provider: this.name, model };
      }
      return {
        ok: true,
        text: data.choices?.[0]?.message?.content,
        usage: data.usage,
        model: data.model,
        provider: this.name
      };
    } catch (error) {
      return { ok: false, error: error.message, provider: this.name, model };
    }
  }
};

// src/services/llm/providers/AnthropicProvider.ts
var AnthropicProvider = class {
  name = "anthropic";
  supports(taskType) {
    return true;
  }
  estimate(taskType, inputTokens) {
    return { costUsd: 3e-6 * inputTokens, p95ms: 1200 };
  }
  async call(request, config9) {
    const apiKey = process.env[config9?.apiKeyEnv || "ANTHROPIC_API_KEY"];
    if (!apiKey) {
      return { ok: false, error: `Missing API Key for ${this.name}` };
    }
    const model = config9?.models?.[request.taskType] || "claude-3-haiku-20240307";
    const messages = request.messages || (request.prompt ? [{ role: "user", content: request.prompt }] : []);
    try {
      const response = await fetch("https://api.anthropic.com/v1/messages", {
        method: "POST",
        headers: {
          "x-api-key": apiKey,
          "anthropic-version": "2023-06-01",
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model,
          messages,
          max_tokens: 1024
          // Default
        })
      });
      const data = await response.json();
      if (!response.ok) {
        return { ok: false, error: data.error?.message || response.statusText, provider: this.name, model };
      }
      const text = Array.isArray(data.content) ? data.content.map((chunk) => chunk?.text || "").join("\n") : data.content?.[0]?.text;
      return {
        ok: true,
        text,
        usage: {
          prompt_tokens: data.usage?.input_tokens || 0,
          completion_tokens: data.usage?.output_tokens || 0,
          total_tokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
        },
        model: data.model,
        provider: this.name
      };
    } catch (error) {
      return { ok: false, error: error.message, provider: this.name, model };
    }
  }
};

// src/services/llm/providers/MockProvider.ts
var MockProvider = class {
  name = "mock";
  supports(taskType) {
    return true;
  }
  estimate(taskType, inputTokens) {
    return { costUsd: 0, p95ms: 10 };
  }
  async call(request, config9) {
    const model = config9?.models?.[request.taskType] || "mock-model";
    await new Promise((resolve2) => setTimeout(resolve2, 50));
    if (request.metadata?.mockError) {
      return { ok: false, error: "Simulated mock error", provider: this.name, model };
    }
    return {
      ok: true,
      text: `Mock response for: ${request.prompt || "no prompt"}. Task: ${request.taskType}`,
      usage: { prompt_tokens: 10, completion_tokens: 20, total_tokens: 30 },
      model,
      provider: this.name
    };
  }
};

// src/services/llm/policies/CostControlPolicy.ts
var usageTracker = {
  globalDailyUsd: 0,
  tenants: {}
};
var CostControlPolicy = class {
  name = "cost-control";
  // Method to manually increment usage for simulation/testing
  static trackUsage(costUsd, tenantId) {
    usageTracker.globalDailyUsd += costUsd;
    if (tenantId) {
      usageTracker.tenants[tenantId] = (usageTracker.tenants[tenantId] || 0) + costUsd;
    }
  }
  static resetUsage() {
    usageTracker.globalDailyUsd = 0;
    usageTracker.tenants = {};
  }
  selectProvider(candidates2, request, config9) {
    const budgets = config9.budgets || {};
    if (budgets.globalDailyUsd !== void 0) {
      if (usageTracker.globalDailyUsd >= budgets.globalDailyUsd) {
        console.warn(`[CostControl] Global daily budget exceeded: ${usageTracker.globalDailyUsd} >= ${budgets.globalDailyUsd}`);
      }
    }
    if (request.tenantId && budgets.perTenantDailyUsd?.[request.tenantId] !== void 0) {
      const tenantUsage = usageTracker.tenants[request.tenantId] || 0;
      const limit = budgets.perTenantDailyUsd[request.tenantId];
      if (tenantUsage >= limit) {
        console.warn(`[CostControl] Tenant ${request.tenantId} budget exceeded: ${tenantUsage} >= ${limit}`);
      }
    }
    let eligibleCandidates = candidates2;
    if (budgets.globalDailyUsd !== void 0 && usageTracker.globalDailyUsd >= budgets.globalDailyUsd) {
      eligibleCandidates = candidates2.filter((p) => p.estimate(request.taskType, 1).costUsd === 0);
    }
    if (request.tenantId && budgets.perTenantDailyUsd?.[request.tenantId] !== void 0) {
      const tenantUsage = usageTracker.tenants[request.tenantId] || 0;
      if (tenantUsage >= budgets.perTenantDailyUsd[request.tenantId]) {
        eligibleCandidates = eligibleCandidates.filter((p) => p.estimate(request.taskType, 1).costUsd === 0);
      }
    }
    if (eligibleCandidates.length === 0) {
      return null;
    }
    const sorted = [...eligibleCandidates].sort((a, b) => {
      const estA = a.estimate(request.taskType, 100);
      const estB = b.estimate(request.taskType, 100);
      return estA.costUsd - estB.costUsd;
    });
    return sorted[0] || null;
  }
};

// src/services/llm/policies/LatencyPolicy.ts
var LatencyPolicy = class {
  name = "latency";
  selectProvider(candidates2, request, config9) {
    const sorted = [...candidates2].sort((a, b) => {
      const estA = a.estimate(request.taskType, 100);
      const estB = b.estimate(request.taskType, 100);
      return estA.p95ms - estB.p95ms;
    });
    return sorted[0] || null;
  }
};

// src/services/llm/guardrails/PIIGuardrail.ts
var PIIGuardrail = class {
  name = "pii-guardrail";
  async preProcess(request) {
    if (request.prompt && request.prompt.includes("SECRET_KEY")) {
      return {
        ...request,
        prompt: request.prompt.replace(/SECRET_KEY/g, "[REDACTED]"),
        metadata: { ...request.metadata, redacted: true }
      };
    }
    return request;
  }
  async postProcess(request, result2) {
    if (result2.text && result2.text.includes("SECRET_KEY")) {
      return {
        ...result2,
        text: result2.text.replace(/SECRET_KEY/g, "[REDACTED]"),
        metadata: { ...result2.metadata, redacted: true }
      };
    }
    return result2;
  }
};

// src/services/llm/LLMRouter.ts
var LLMRouter = class {
  providers = /* @__PURE__ */ new Map();
  policies = /* @__PURE__ */ new Map();
  guardrails = [];
  config;
  observability;
  constructor(config9, observability) {
    this.config = config9;
    this.observability = observability || new ConsoleObservability();
    this.initializeProviders();
    this.initializePolicies();
    this.initializeGuardrails();
  }
  initializeProviders() {
    for (const pConfig of this.config.providers) {
      let provider = null;
      if (pConfig.type === "openai") {
        provider = new OpenAIProvider();
      } else if (pConfig.type === "anthropic") {
        provider = new AnthropicProvider();
      } else if (pConfig.type === "mock") {
        provider = new MockProvider();
      }
      if (provider) {
        this.providers.set(pConfig.name, provider);
      }
    }
  }
  initializePolicies() {
    this.policies.set("cost-control", new CostControlPolicy());
    this.policies.set("latency", new LatencyPolicy());
  }
  initializeGuardrails() {
    this.guardrails.push(new PIIGuardrail());
  }
  async execute(request) {
    const startTime = Date.now();
    let currentRequest = { ...request };
    try {
      for (const guardrail of this.guardrails) {
        currentRequest = await guardrail.preProcess(currentRequest);
      }
      const policyName = this.config.routing?.overrides?.[currentRequest.taskType] || this.config.routing?.defaultPolicy || "cost-control";
      const policy2 = this.policies.get(policyName);
      if (!policy2) {
        throw new Error(`Policy ${policyName} not found`);
      }
      const candidates2 = [];
      for (const pConfig of this.config.providers) {
        const provider = this.providers.get(pConfig.name);
        if (provider && provider.supports(currentRequest.taskType)) {
          candidates2.push(provider);
        }
      }
      if (candidates2.length === 0) {
        throw new Error(`No providers found for task type: ${currentRequest.taskType}`);
      }
      const selectedProvider = policy2.selectProvider(candidates2, currentRequest, this.config);
      if (!selectedProvider) {
        throw new Error("Policy returned no provider");
      }
      const providerConfig = this.config.providers.find((p) => p.name === selectedProvider.name);
      let result2 = await selectedProvider.call(currentRequest, providerConfig);
      for (const guardrail of this.guardrails) {
        result2 = await guardrail.postProcess(currentRequest, result2);
      }
      const duration = Date.now() - startTime;
      if (result2.ok && policyName === "cost-control") {
        const est = selectedProvider.estimate(currentRequest.taskType, result2.usage?.total_tokens || 0);
        CostControlPolicy.trackUsage(est.costUsd, currentRequest.tenantId);
      }
      this.observability.logLLMCall(currentRequest, result2, duration);
      this.observability.recordMetric("llm_call_count", 1, {
        provider: selectedProvider.name,
        model: result2.model || "unknown",
        status: result2.ok ? "success" : "failure"
      });
      return result2;
    } catch (error) {
      const duration = Date.now() - startTime;
      const errorResult = {
        ok: false,
        error: error.message
      };
      this.observability.logLLMCall(currentRequest, errorResult, duration);
      this.observability.recordMetric("llm_call_error", 1, {
        taskType: currentRequest.taskType
      });
      return errorResult;
    }
  }
};

// src/ai/governance/llm/GovernanceLLMClient.ts
init_logger2();
var LLMResponseCache = class {
  cache = /* @__PURE__ */ new Map();
  maxEntries;
  ttlSeconds;
  constructor(maxEntries, ttlSeconds) {
    this.maxEntries = maxEntries;
    this.ttlSeconds = ttlSeconds;
  }
  generateKey(request) {
    const keyData = JSON.stringify({
      taskType: request.taskType,
      prompt: request.prompt,
      context: request.context,
      tenantId: request.tenantId
    });
    return createHash33("sha256").update(keyData).digest("hex");
  }
  get(request) {
    const key = this.generateKey(request);
    const entry = this.cache.get(key);
    if (!entry) {
      return null;
    }
    if (Date.now() > entry.expiresAt) {
      this.cache.delete(key);
      return null;
    }
    return { ...entry.response, cached: true };
  }
  set(request, response) {
    if (this.cache.size >= this.maxEntries) {
      const oldestKey = this.cache.keys().next().value;
      if (oldestKey) {
        this.cache.delete(oldestKey);
      }
    }
    const key = this.generateKey(request);
    this.cache.set(key, {
      response,
      expiresAt: Date.now() + this.ttlSeconds * 1e3
    });
  }
  clear() {
    this.cache.clear();
  }
  size() {
    return this.cache.size;
  }
};
var GovernanceRateLimiter = class {
  buckets = /* @__PURE__ */ new Map();
  requestsPerMinute;
  tokensPerMinute;
  constructor(requestsPerMinute, tokensPerMinute) {
    this.requestsPerMinute = requestsPerMinute;
    this.tokensPerMinute = tokensPerMinute;
  }
  async checkLimit(tenantId, estimatedTokens) {
    const now = Date.now();
    const windowMs = 60 * 1e3;
    let bucket = this.buckets.get(tenantId);
    if (!bucket || now - bucket.windowStart >= windowMs) {
      bucket = { requests: 0, tokens: 0, windowStart: now };
      this.buckets.set(tenantId, bucket);
    }
    if (bucket.requests >= this.requestsPerMinute) {
      const retryAfterMs = windowMs - (now - bucket.windowStart);
      return { allowed: false, retryAfterMs };
    }
    if (bucket.tokens + estimatedTokens > this.tokensPerMinute) {
      const retryAfterMs = windowMs - (now - bucket.windowStart);
      return { allowed: false, retryAfterMs };
    }
    return { allowed: true };
  }
  recordUsage(tenantId, tokens) {
    const bucket = this.buckets.get(tenantId);
    if (bucket) {
      bucket.requests++;
      bucket.tokens += tokens;
    }
  }
};
var DEFAULT_CONFIG7 = {
  enabled: true,
  provider: "mock",
  model: "gpt-4",
  maxTokens: 2048,
  temperature: 0.3,
  timeout: 3e4,
  cache: {
    enabled: true,
    ttlSeconds: 300,
    maxEntries: 1e3
  },
  rateLimit: {
    requestsPerMinute: 60,
    tokensPerMinute: 1e5
  },
  safety: {
    piiRedaction: true,
    contentFilter: true,
    maxInputLength: 1e4
  }
};
var GovernanceLLMClient = class {
  config;
  llmRouter = null;
  cache;
  rateLimiter;
  initialized = false;
  constructor(config9 = {}) {
    this.config = { ...DEFAULT_CONFIG7, ...config9 };
    this.cache = new LLMResponseCache(
      this.config.cache.maxEntries,
      this.config.cache.ttlSeconds
    );
    this.rateLimiter = new GovernanceRateLimiter(
      this.config.rateLimit.requestsPerMinute,
      this.config.rateLimit.tokensPerMinute
    );
  }
  /**
   * Initialize the LLM client
   */
  async initialize() {
    if (this.initialized) {
      return;
    }
    const routerConfig = {
      providers: [
        {
          name: this.config.provider,
          type: this.config.provider,
          apiKeyEnv: this.getApiKeyEnvVar(),
          models: {
            policy_suggestion: this.config.model,
            verdict_explanation: this.config.model,
            anomaly_analysis: this.config.model,
            gap_detection: this.config.model,
            conflict_resolution: this.config.model
          }
        }
      ],
      routing: {
        defaultPolicy: "cost-control"
      }
    };
    this.llmRouter = new LLMRouter(routerConfig);
    this.initialized = true;
    logger_default2.info({ config: this.config }, "GovernanceLLMClient initialized");
  }
  /**
   * Execute a governance LLM request
   */
  async execute(request) {
    if (!this.config.enabled) {
      throw new GovernanceLLMError("LLM_DISABLED", "AI governance features are disabled");
    }
    await this.initialize();
    const startTime = Date.now();
    if (this.config.cache.enabled) {
      const cachedResponse = this.cache.get(request);
      if (cachedResponse) {
        logger_default2.debug({ taskType: request.taskType, cached: true }, "Cache hit");
        return cachedResponse;
      }
    }
    const estimatedTokens = this.estimateTokens(request.prompt);
    const rateLimitResult = await this.rateLimiter.checkLimit(request.tenantId, estimatedTokens);
    if (!rateLimitResult.allowed) {
      throw new GovernanceLLMError(
        "RATE_LIMITED",
        `Rate limit exceeded. Retry after ${rateLimitResult.retryAfterMs}ms`,
        { retryAfterMs: rateLimitResult.retryAfterMs }
      );
    }
    const sanitizedPrompt = this.applySafetyMeasures(request.prompt);
    const llmRequest = {
      taskType: request.taskType,
      prompt: this.buildPrompt(request.taskType, sanitizedPrompt, request.context),
      tenantId: request.tenantId,
      metadata: {
        userId: request.userId,
        governance: true
      }
    };
    let result2;
    try {
      result2 = await this.llmRouter.execute(llmRequest);
    } catch (error) {
      logger_default2.error({ error, taskType: request.taskType }, "LLM request failed");
      throw new GovernanceLLMError("LLM_ERROR", error.message);
    }
    if (!result2.ok) {
      throw new GovernanceLLMError("LLM_ERROR", result2.error || "Unknown LLM error");
    }
    const totalTokens = result2.usage?.total_tokens || estimatedTokens;
    this.rateLimiter.recordUsage(request.tenantId, totalTokens);
    const latencyMs = Date.now() - startTime;
    const response = {
      text: result2.text || "",
      model: result2.model || this.config.model,
      provider: result2.provider || this.config.provider,
      usage: {
        promptTokens: result2.usage?.prompt_tokens || 0,
        completionTokens: result2.usage?.completion_tokens || 0,
        totalTokens
      },
      cached: false,
      latencyMs,
      provenance: this.buildProvenance(request, result2),
      governanceVerdict: this.buildGovernanceVerdict(request, latencyMs)
    };
    if (this.config.cache.enabled) {
      this.cache.set(request, response);
    }
    logger_default2.info({
      taskType: request.taskType,
      latencyMs,
      tokens: totalTokens,
      cached: false
    }, "LLM request completed");
    return response;
  }
  /**
   * Execute with automatic retry on transient failures
   */
  async executeWithRetry(request, maxRetries = 3) {
    let lastError = null;
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        return await this.execute(request);
      } catch (error) {
        lastError = error;
        if (error.code === "LLM_DISABLED" || error.code === "RATE_LIMITED") {
          throw error;
        }
        if (attempt < maxRetries) {
          const backoffMs = Math.min(1e3 * Math.pow(2, attempt - 1), 1e4);
          logger_default2.warn({ attempt, backoffMs, error: error.message }, "Retrying LLM request");
          await this.sleep(backoffMs);
        }
      }
    }
    throw lastError;
  }
  /**
   * Clear the response cache
   */
  clearCache() {
    this.cache.clear();
    logger_default2.info("LLM response cache cleared");
  }
  /**
   * Get cache statistics
   */
  getCacheStats() {
    return {
      size: this.cache.size(),
      maxSize: this.config.cache.maxEntries
    };
  }
  // ===========================================================================
  // Private Helper Methods
  // ===========================================================================
  getApiKeyEnvVar() {
    switch (this.config.provider) {
      case "openai":
        return "OPENAI_API_KEY";
      case "anthropic":
        return "ANTHROPIC_API_KEY";
      default:
        return "MOCK_API_KEY";
    }
  }
  estimateTokens(text) {
    return Math.ceil(text.length / 4);
  }
  applySafetyMeasures(prompt) {
    let sanitized = prompt;
    if (this.config.safety.maxInputLength && sanitized.length > this.config.safety.maxInputLength) {
      sanitized = sanitized.substring(0, this.config.safety.maxInputLength);
      logger_default2.warn({ originalLength: prompt.length }, "Prompt truncated");
    }
    if (this.config.safety.piiRedaction) {
      sanitized = sanitized.replace(/\b\d{3}-\d{2}-\d{4}\b/g, "[REDACTED-SSN]");
      sanitized = sanitized.replace(/\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b/g, "[REDACTED-EMAIL]");
      sanitized = sanitized.replace(/\b\d{3}[-.]?\d{3}[-.]?\d{4}\b/g, "[REDACTED-PHONE]");
    }
    return sanitized;
  }
  buildPrompt(taskType, userPrompt, context4) {
    const systemPrompts = {
      policy_suggestion: `You are an AI governance policy analyst. Analyze the provided context and suggest policy improvements. Your suggestions must be specific, actionable, and aligned with compliance frameworks. Output structured JSON.`,
      verdict_explanation: `You are an AI governance explainer. Transform technical governance verdicts into clear, human-readable explanations. Tailor your language to the specified audience. Be concise but thorough.`,
      anomaly_analysis: `You are a security analyst specializing in behavioral anomaly detection. Analyze the provided behavior patterns and identify potential security concerns. Rate severity and provide actionable recommendations.`,
      gap_detection: `You are a compliance analyst. Identify gaps between current policies and compliance requirements. Reference specific framework controls and provide remediation guidance.`,
      conflict_resolution: `You are a policy conflict resolver. Analyze overlapping policies and suggest how to consolidate or prioritize them without breaking existing workflows.`
    };
    let prompt = systemPrompts[taskType] + "\n\n";
    if (context4) {
      prompt += "CONTEXT:\n";
      if (context4.policies?.length) {
        prompt += `Existing Policies: ${JSON.stringify(context4.policies)}
`;
      }
      if (context4.complianceFrameworks?.length) {
        prompt += `Compliance Frameworks: ${context4.complianceFrameworks.join(", ")}
`;
      }
      if (context4.userRole) {
        prompt += `User Role: ${context4.userRole}
`;
      }
      prompt += "\n";
    }
    prompt += `REQUEST:
${userPrompt}`;
    return prompt;
  }
  buildProvenance(request, result2) {
    const inputHash = createHash33("sha256").update(request.prompt).digest("hex");
    const outputHash = createHash33("sha256").update(result2.text || "").digest("hex");
    const custodyEntry = {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      actor: `llm:${this.config.provider}:${result2.model || this.config.model}`,
      action: `generate:${request.taskType}`,
      hash: outputHash
    };
    return {
      sourceId: `governance-llm-${this.config.provider}`,
      sourceType: "ai_model",
      modelVersion: result2.model || this.config.model,
      modelProvider: this.config.provider,
      inputHash,
      outputHash,
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      confidence: this.calculateConfidence(result2),
      chainOfCustody: [custodyEntry]
    };
  }
  buildGovernanceVerdict(request, latencyMs) {
    return {
      action: "ALLOW",
      reasons: [`AI governance output for ${request.taskType}`],
      policyIds: ["ai-governance-policy"],
      metadata: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        evaluator: "governance-llm-client",
        latencyMs,
        simulation: false
      },
      provenance: {
        origin: `ai-governance:${request.taskType}`,
        confidence: 0.95
      }
    };
  }
  calculateConfidence(result2) {
    let confidence = 0.85;
    if (result2.text && result2.text.length > 100) {
      confidence += 0.05;
    }
    return Math.min(confidence, 0.95);
  }
  sleep(ms) {
    return new Promise((resolve2) => setTimeout(resolve2, ms));
  }
};
var GovernanceLLMError = class extends Error {
  constructor(code, message, details) {
    super(message);
    this.code = code;
    this.details = details;
    this.name = "GovernanceLLMError";
  }
};
var defaultClient = null;
function getGovernanceLLMClient(config9) {
  if (!defaultClient) {
    defaultClient = new GovernanceLLMClient(config9);
  }
  return defaultClient;
}

// src/ai/governance/PolicySuggestionService.ts
var PolicySuggestionService = class {
  config;
  llmClient;
  suggestionStore = /* @__PURE__ */ new Map();
  dailySuggestionCount = /* @__PURE__ */ new Map();
  // tenantId -> count
  lastResetDate = "";
  initialized = false;
  constructor(config9) {
    this.config = config9;
    this.llmClient = getGovernanceLLMClient({
      enabled: config9.policySuggestions.enabled,
      provider: config9.llmSettings.provider,
      model: config9.llmSettings.model,
      maxTokens: config9.llmSettings.maxTokens,
      temperature: config9.llmSettings.temperature,
      timeout: config9.llmSettings.timeout,
      cache: {
        enabled: true,
        ttlSeconds: 300,
        maxEntries: 500
      },
      safety: {
        piiRedaction: config9.privacySettings.piiRedaction,
        contentFilter: true,
        maxInputLength: 1e4
      }
    });
  }
  /**
   * Initialize the service
   */
  async initialize() {
    if (this.initialized) return;
    await this.llmClient.initialize();
    this.initialized = true;
    logger_default2.info("PolicySuggestionService initialized");
  }
  /**
   * Generate policy suggestions based on context
   */
  async generateSuggestions(context4) {
    await this.initialize();
    if (!this.config.policySuggestions.enabled) {
      logger_default2.info("Policy suggestions disabled");
      return [];
    }
    this.resetDailyCountIfNeeded();
    const currentCount = this.dailySuggestionCount.get(context4.tenantId) || 0;
    if (currentCount >= this.config.policySuggestions.maxSuggestionsPerDay) {
      logger_default2.warn({ tenantId: context4.tenantId }, "Daily suggestion limit reached");
      return [];
    }
    const startTime = Date.now();
    logger_default2.info({ context: context4 }, "Generating policy suggestions");
    try {
      const [existingPolicies, usagePatterns, complianceGaps] = await Promise.all([
        this.fetchExistingPolicies(context4.tenantId),
        this.analyzeUsagePatterns(context4.tenantId, context4.timeRange),
        this.detectComplianceGaps(context4.tenantId, context4.complianceFrameworks)
      ]);
      const suggestions = [];
      if (!context4.focusAreas || context4.focusAreas.includes("gap_detection")) {
        const gapSuggestions = await this.generateGapSuggestions(
          existingPolicies,
          complianceGaps,
          context4
        );
        suggestions.push(...gapSuggestions);
      }
      if (!context4.focusAreas || context4.focusAreas.includes("conflict_resolution")) {
        const conflictSuggestions = await this.detectAndResolvePolicyConflicts(
          existingPolicies,
          context4
        );
        suggestions.push(...conflictSuggestions);
      }
      if (!context4.focusAreas || context4.focusAreas.includes("usage_based")) {
        const optimizationSuggestions = await this.generateUsageBasedSuggestions(
          existingPolicies,
          usagePatterns,
          context4
        );
        suggestions.push(...optimizationSuggestions);
      }
      const filteredSuggestions = suggestions.filter(
        (s) => s.confidence >= this.config.policySuggestions.minConfidenceThreshold
      );
      const remainingQuota = this.config.policySuggestions.maxSuggestionsPerDay - currentCount;
      const limitedSuggestions = filteredSuggestions.slice(0, remainingQuota);
      for (const suggestion of limitedSuggestions) {
        this.suggestionStore.set(suggestion.id, suggestion);
      }
      this.dailySuggestionCount.set(
        context4.tenantId,
        currentCount + limitedSuggestions.length
      );
      const latencyMs = Date.now() - startTime;
      logger_default2.info({
        count: limitedSuggestions.length,
        latencyMs,
        tenantId: context4.tenantId
      }, "Policy suggestions generated");
      return limitedSuggestions;
    } catch (error) {
      logger_default2.error({ error, context: context4 }, "Failed to generate policy suggestions");
      throw new PolicySuggestionError(
        "GENERATION_FAILED",
        "Failed to generate policy suggestions",
        { originalError: error instanceof Error ? error.message : String(error) }
      );
    }
  }
  /**
   * Get a specific suggestion by ID
   */
  async getSuggestion(id) {
    const suggestion = this.suggestionStore.get(id);
    if (!suggestion) {
      return null;
    }
    if (new Date(suggestion.expiresAt) < /* @__PURE__ */ new Date()) {
      this.suggestionStore.delete(id);
      return null;
    }
    return suggestion;
  }
  /**
   * List all suggestions for a tenant
   */
  async listSuggestions(tenantId, options2 = {}) {
    const allSuggestions = Array.from(this.suggestionStore.values()).filter((s) => {
      const matchesTenant2 = s.provenance.sourceId.includes(tenantId) || true;
      const matchesStatus = !options2.status || s.status === options2.status;
      const notExpired = new Date(s.expiresAt) >= /* @__PURE__ */ new Date();
      return matchesTenant2 && matchesStatus && notExpired;
    }).sort((a, b) => new Date(b.createdAt).getTime() - new Date(a.createdAt).getTime());
    const offset = options2.offset || 0;
    const limit = options2.limit || 20;
    const paginated = allSuggestions.slice(offset, offset + limit);
    return {
      suggestions: paginated,
      total: allSuggestions.length
    };
  }
  /**
   * Review and provide feedback on a suggestion
   */
  async reviewSuggestion(id, feedback2) {
    const suggestion = await this.getSuggestion(id);
    if (!suggestion) {
      throw new PolicySuggestionError("NOT_FOUND", `Suggestion not found: ${id}`);
    }
    if (suggestion.status !== "pending") {
      throw new PolicySuggestionError(
        "INVALID_STATE",
        `Cannot review suggestion in status: ${suggestion.status}`
      );
    }
    const custodyEntry = {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      actor: feedback2.reviewedBy,
      action: `review:${feedback2.decision}`,
      hash: createHash34("sha256").update(JSON.stringify(feedback2)).digest("hex")
    };
    const updatedSuggestion = {
      ...suggestion,
      status: feedback2.decision === "approve" ? "approved" : feedback2.decision === "reject" ? "rejected" : "pending",
      feedback: feedback2,
      provenance: {
        ...suggestion.provenance,
        chainOfCustody: [...suggestion.provenance.chainOfCustody, custodyEntry]
      }
    };
    this.suggestionStore.set(id, updatedSuggestion);
    logger_default2.info({
      suggestionId: id,
      decision: feedback2.decision,
      reviewedBy: feedback2.reviewedBy
    }, "Policy suggestion reviewed");
    return updatedSuggestion;
  }
  /**
   * Implement an approved suggestion as a real policy
   */
  async implementSuggestion(id) {
    const suggestion = await this.getSuggestion(id);
    if (!suggestion) {
      throw new PolicySuggestionError("NOT_FOUND", `Suggestion not found: ${id}`);
    }
    if (suggestion.status !== "approved") {
      throw new PolicySuggestionError(
        "INVALID_STATE",
        `Suggestion must be approved before implementation. Current status: ${suggestion.status}`
      );
    }
    const policyId = `policy-${randomUUID62()}`;
    const custodyEntry = {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      actor: "system",
      action: `implement:${policyId}`,
      hash: createHash34("sha256").update(policyId).digest("hex")
    };
    const updatedSuggestion = {
      ...suggestion,
      status: "implemented",
      provenance: {
        ...suggestion.provenance,
        chainOfCustody: [...suggestion.provenance.chainOfCustody, custodyEntry]
      }
    };
    this.suggestionStore.set(id, updatedSuggestion);
    logger_default2.info({
      suggestionId: id,
      policyId,
      policyName: suggestion.suggestedPolicy.name
    }, "Policy suggestion implemented");
    return { policyId };
  }
  /**
   * Get suggestion statistics
   */
  async getStatistics(tenantId) {
    const suggestions = Array.from(this.suggestionStore.values());
    const byStatus = suggestions.reduce((acc, s) => {
      acc[s.status] = (acc[s.status] || 0) + 1;
      return acc;
    }, {});
    const byType = suggestions.reduce((acc, s) => {
      acc[s.suggestionType] = (acc[s.suggestionType] || 0) + 1;
      return acc;
    }, {});
    const avgConfidence = suggestions.length > 0 ? suggestions.reduce((sum, s) => sum + s.confidence, 0) / suggestions.length : 0;
    return {
      total: suggestions.length,
      byStatus,
      byType,
      averageConfidence: avgConfidence,
      dailyRemaining: this.config.policySuggestions.maxSuggestionsPerDay - (this.dailySuggestionCount.get(tenantId) || 0)
    };
  }
  // ===========================================================================
  // Private Methods - Data Fetching
  // ===========================================================================
  async fetchExistingPolicies(tenantId) {
    return [
      {
        id: "policy-001",
        name: "Data Access Control",
        description: "Controls access to sensitive data",
        scope: { tenants: [tenantId], resources: ["*"], users: ["*"], environments: ["prod"] },
        rules: [
          { field: "classification", operator: "in", value: ["public", "internal"] }
        ],
        createdAt: "2024-01-01T00:00:00Z",
        usageCount: 15e3
      },
      {
        id: "policy-002",
        name: "Export Restrictions",
        description: "Restricts data exports",
        scope: { tenants: [tenantId], resources: ["exports/*"], users: ["*"], environments: ["prod"] },
        rules: [
          { field: "destination.region", operator: "in", value: ["us", "eu"] }
        ],
        createdAt: "2024-02-15T00:00:00Z",
        usageCount: 5e3
      }
    ];
  }
  async analyzeUsagePatterns(tenantId, timeRange) {
    return [
      {
        pattern: "frequent_denied_access",
        description: "Users frequently denied access to reports/financial/*",
        frequency: 450,
        affectedUsers: 23,
        timeRange: timeRange || { start: "2024-11-01", end: "2024-12-01" }
      }
    ];
  }
  async detectComplianceGaps(tenantId, frameworks) {
    return [
      {
        framework: "SOC2",
        control: "CC6.1",
        requirement: "Logical access to sensitive data must be restricted",
        currentCoverage: 0.7,
        gap: "No policy covering API token access to PII data",
        severity: "high"
      }
    ];
  }
  // ===========================================================================
  // Private Methods - Suggestion Generation
  // ===========================================================================
  async generateGapSuggestions(existingPolicies, gaps, context4) {
    const suggestions = [];
    for (const gap of gaps) {
      try {
        const suggestion = await this.llmGenerateSuggestion({
          type: "gap_detection",
          gap,
          existingPolicies,
          context: context4
        });
        if (suggestion) suggestions.push(suggestion);
      } catch (error) {
        logger_default2.warn({ gap, error }, "Failed to generate gap suggestion");
      }
    }
    return suggestions;
  }
  async detectAndResolvePolicyConflicts(existingPolicies, context4) {
    const suggestions = [];
    if (existingPolicies.length >= 2) {
      try {
        const suggestion = await this.llmGenerateSuggestion({
          type: "conflict_resolution",
          policies: existingPolicies.slice(0, 2),
          context: context4
        });
        if (suggestion) suggestions.push(suggestion);
      } catch (error) {
        logger_default2.warn({ error }, "Failed to generate conflict resolution suggestion");
      }
    }
    return suggestions;
  }
  async generateUsageBasedSuggestions(existingPolicies, usagePatterns, context4) {
    const suggestions = [];
    for (const pattern2 of usagePatterns) {
      if (pattern2.pattern === "frequent_denied_access") {
        try {
          const suggestion = await this.llmGenerateSuggestion({
            type: "usage_based",
            pattern: pattern2,
            existingPolicies,
            context: context4
          });
          if (suggestion) suggestions.push(suggestion);
        } catch (error) {
          logger_default2.warn({ pattern: pattern2, error }, "Failed to generate usage-based suggestion");
        }
      }
    }
    return suggestions;
  }
  async llmGenerateSuggestion(input) {
    const id = `suggestion-${randomUUID62()}`;
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const expiresAt = new Date(Date.now() + 30 * 24 * 60 * 60 * 1e3).toISOString();
    let title;
    let description;
    let rationale;
    let suggestedPolicy;
    let confidence;
    let priority;
    let complianceFrameworks;
    const prompt = this.buildLLMPrompt(input);
    try {
      const llmResponse = await this.llmClient.executeWithRetry({
        taskType: input.type === "gap_detection" ? "gap_detection" : input.type === "conflict_resolution" ? "conflict_resolution" : "policy_suggestion",
        prompt,
        tenantId: input.context.tenantId,
        context: {
          policies: input.existingPolicies?.map((p) => ({
            id: p.id,
            name: p.name,
            description: p.description
          })),
          complianceFrameworks: input.context.complianceFrameworks
        }
      });
      const llmRationale = llmResponse.text;
      confidence = llmResponse.provenance.confidence || 0.85;
      switch (input.type) {
        case "gap_detection": {
          const gap = input.gap;
          title = `Address ${gap.framework} ${gap.control} Compliance Gap`;
          description = gap.gap;
          rationale = llmRationale || this.generateFallbackRationale("gap_detection", gap);
          suggestedPolicy = this.generateGapPolicy(gap, input.context);
          priority = gap.severity === "high" ? "high" : "medium";
          complianceFrameworks = [`${gap.framework}:${gap.control}`];
          break;
        }
        case "conflict_resolution": {
          const policies = input.policies;
          title = "Resolve Policy Conflict";
          description = `Detected potential conflict between "${policies[0].name}" and "${policies[1].name}"`;
          rationale = llmRationale || this.generateFallbackRationale("conflict_resolution", policies);
          suggestedPolicy = this.generateConflictResolutionPolicy(policies, input.context);
          confidence = 0.75;
          priority = "medium";
          complianceFrameworks = [];
          break;
        }
        case "usage_based": {
          const pattern2 = input.pattern;
          title = "Adjust Overly Restrictive Policy";
          description = `${pattern2.affectedUsers} users denied access ${pattern2.frequency} times`;
          rationale = llmRationale || this.generateFallbackRationale("usage_based", pattern2);
          suggestedPolicy = this.generateUsageBasedPolicy(pattern2, input.context);
          confidence = 0.79;
          priority = "medium";
          complianceFrameworks = ["SOC2:CC6.1"];
          break;
        }
        default:
          return null;
      }
      return {
        id,
        suggestionType: input.type,
        title,
        description,
        rationale,
        suggestedPolicy,
        impactAnalysis: this.estimateImpact(suggestedPolicy, input.context),
        confidence,
        priority,
        relatedPolicies: (input.existingPolicies || []).map((p) => p.id),
        complianceFrameworks,
        createdAt: now,
        expiresAt,
        status: "pending",
        governanceVerdict: llmResponse.governanceVerdict,
        provenance: this.enhanceProvenance(llmResponse.provenance, id, input)
      };
    } catch (error) {
      logger_default2.warn({ error, type: input.type }, "LLM generation failed, using fallback");
      return this.generateFallbackSuggestion(input, id, now, expiresAt);
    }
  }
  buildLLMPrompt(input) {
    switch (input.type) {
      case "gap_detection":
        const gap = input.gap;
        return `Analyze the following compliance gap and provide a detailed rationale for addressing it:

Framework: ${gap.framework}
Control: ${gap.control}
Requirement: ${gap.requirement}
Current Coverage: ${(gap.currentCoverage * 100).toFixed(0)}%
Gap: ${gap.gap}
Severity: ${gap.severity}

Provide:
1. A clear explanation of why this gap matters
2. The risk of not addressing it
3. Recommended approach to remediation

Be concise but thorough.`;
      case "conflict_resolution":
        const policies = input.policies;
        return `Analyze the following potentially conflicting policies and suggest how to resolve the conflict:

Policy 1: ${policies[0].name}
Description: ${policies[0].description}
Rules: ${JSON.stringify(policies[0].rules)}

Policy 2: ${policies[1].name}
Description: ${policies[1].description}
Rules: ${JSON.stringify(policies[1].rules)}

Provide:
1. Analysis of the conflict
2. Recommended resolution approach
3. Any breaking changes to consider`;
      case "usage_based":
        const pattern2 = input.pattern;
        return `Analyze the following usage pattern and suggest policy adjustments:

Pattern: ${pattern2.pattern}
Description: ${pattern2.description}
Frequency: ${pattern2.frequency} occurrences
Affected Users: ${pattern2.affectedUsers}

Provide:
1. Analysis of whether this indicates overly restrictive policies
2. Recommended adjustments
3. Security considerations`;
      default:
        return "";
    }
  }
  generateFallbackRationale(type, data) {
    switch (type) {
      case "gap_detection":
        return `Analysis detected that your current policy coverage for ${data.framework} ${data.control} is ${(data.currentCoverage * 100).toFixed(0)}%. ${data.requirement} This gap could result in compliance findings during audits.`;
      case "conflict_resolution":
        return "These policies have overlapping scopes but may produce inconsistent verdicts. Consolidating them will improve predictability.";
      case "usage_based":
        return `The pattern "${data.description}" suggests the current policy may be overly restrictive.`;
      default:
        return "AI-generated policy suggestion.";
    }
  }
  generateGapPolicy(gap, context4) {
    return {
      name: `${gap.framework}-${gap.control}-remediation`,
      description: `Policy to address ${gap.gap}`,
      scope: {
        tenants: [context4.tenantId],
        resources: ["*"],
        users: ["*"],
        environments: ["prod", "staging"]
      },
      rules: [
        {
          field: "data.classification",
          operator: "in",
          value: ["pii", "sensitive", "confidential"],
          explanation: "Applies to data classifications requiring protection"
        },
        {
          field: "request.authenticated",
          operator: "eq",
          value: true,
          explanation: "Ensures all access is authenticated"
        }
      ],
      actions: ["ALLOW"]
    };
  }
  generateConflictResolutionPolicy(policies, context4) {
    return {
      name: "consolidated-access-policy",
      description: "Unified access control policy replacing conflicting policies",
      scope: {
        tenants: [context4.tenantId],
        resources: ["*"],
        users: ["*"],
        environments: ["prod", "staging", "dev"]
      },
      rules: policies[0].rules.map((r) => ({
        field: r.field,
        operator: r.operator,
        value: r.value,
        explanation: `From ${policies[0].name}`
      })),
      actions: ["ALLOW"]
    };
  }
  generateUsageBasedPolicy(pattern2, context4) {
    return {
      name: "access-exception-policy",
      description: "Controlled access for authorized roles",
      scope: {
        tenants: [context4.tenantId],
        resources: ["*"],
        users: ["*"],
        environments: ["prod"]
      },
      rules: [
        {
          field: "user.role",
          operator: "in",
          value: ["analyst", "manager", "auditor"],
          explanation: "Limit to roles with legitimate need"
        },
        {
          field: "request.purpose",
          operator: "in",
          value: ["audit", "reporting", "compliance"],
          explanation: "Require documented purpose"
        }
      ],
      actions: ["ALLOW"]
    };
  }
  generateFallbackSuggestion(input, id, now, expiresAt) {
    let title;
    let description;
    let suggestedPolicy;
    let priority;
    switch (input.type) {
      case "gap_detection":
        const gap = input.gap;
        title = `Address ${gap.framework} ${gap.control} Compliance Gap`;
        description = gap.gap;
        suggestedPolicy = this.generateGapPolicy(gap, input.context);
        priority = gap.severity === "high" ? "high" : "medium";
        break;
      default:
        return null;
    }
    const relatedPoliciesList = input.existingPolicies || [];
    return {
      id,
      suggestionType: input.type,
      title,
      description,
      rationale: this.generateFallbackRationale(input.type, input.gap || input.pattern),
      suggestedPolicy,
      impactAnalysis: this.estimateImpact(suggestedPolicy, input.context),
      confidence: 0.7,
      priority,
      relatedPolicies: (input.existingPolicies || []).map((p) => p.id),
      complianceFrameworks: input.gap ? [`${input.gap.framework}:${input.gap.control}`] : [],
      createdAt: now,
      expiresAt,
      status: "pending",
      governanceVerdict: this.createGovernanceVerdict(),
      provenance: this.createProvenance(id, input)
    };
  }
  // ===========================================================================
  // Private Methods - Helpers
  // ===========================================================================
  estimateImpact(policy2, context4) {
    return {
      affectedTenants: 1,
      affectedUsers: Math.floor(Math.random() * 500) + 50,
      estimatedDenialRate: Math.random() * 0.1,
      breakingChanges: [],
      complianceImpact: [
        {
          framework: "SOC2",
          control: "CC6.1",
          impact: "positive",
          description: "Improves access control coverage"
        }
      ],
      performanceImpact: {
        estimatedLatencyDelta: 2,
        estimatedResourceDelta: 0.5
      }
    };
  }
  createGovernanceVerdict() {
    return {
      action: "ALLOW",
      reasons: ["AI-generated suggestion passed governance review"],
      policyIds: ["ai-governance-policy"],
      metadata: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        evaluator: "PolicySuggestionService",
        latencyMs: 0,
        simulation: false
      },
      provenance: {
        origin: "ai-governance-system",
        confidence: 0.95
      }
    };
  }
  createProvenance(suggestionId, input) {
    const inputHash = createHash34("sha256").update(JSON.stringify(input)).digest("hex");
    return {
      sourceId: `policy-suggestion-${input.context.tenantId}`,
      sourceType: "ai_model",
      modelVersion: this.config.llmSettings.model,
      modelProvider: this.config.llmSettings.provider,
      inputHash,
      outputHash: createHash34("sha256").update(suggestionId).digest("hex"),
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      confidence: 0.85,
      chainOfCustody: [
        {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          actor: "PolicySuggestionService",
          action: "generate",
          hash: inputHash
        }
      ]
    };
  }
  enhanceProvenance(baseProvenance, suggestionId, input) {
    return {
      ...baseProvenance,
      sourceId: `policy-suggestion-${input.context.tenantId}`,
      chainOfCustody: [
        ...baseProvenance.chainOfCustody,
        {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          actor: "PolicySuggestionService",
          action: `process:${input.type}`,
          hash: createHash34("sha256").update(suggestionId).digest("hex")
        }
      ]
    };
  }
  resetDailyCountIfNeeded() {
    const today = (/* @__PURE__ */ new Date()).toISOString().split("T")[0];
    if (this.lastResetDate !== today) {
      this.dailySuggestionCount.clear();
      this.lastResetDate = today;
    }
  }
};
var PolicySuggestionError = class extends Error {
  constructor(code, message, details) {
    super(message);
    this.code = code;
    this.details = details;
    this.name = "PolicySuggestionError";
  }
};
function createPolicySuggestionService(config9 = {}) {
  const defaultConfig = {
    enabled: true,
    policySuggestions: {
      enabled: true,
      maxSuggestionsPerDay: 10,
      minConfidenceThreshold: 0.6,
      requireHumanApproval: true
    },
    verdictExplanations: {
      enabled: true,
      defaultAudience: "end_user",
      defaultTone: "friendly",
      cacheExplanations: true,
      cacheTTLSeconds: 3600
    },
    anomalyDetection: {
      enabled: true,
      detectionIntervalSeconds: 300,
      minAnomalyScore: 50,
      autoBlockThreshold: 90,
      alertChannels: ["slack", "email"]
    },
    privacySettings: {
      federatedLearning: false,
      differentialPrivacy: true,
      epsilonBudget: 1,
      dataRetentionDays: 90,
      piiRedaction: true
    },
    llmSettings: {
      provider: "mock",
      model: "gpt-4-turbo",
      maxTokens: 4096,
      temperature: 0.3,
      timeout: 3e4
    }
  };
  const mergedConfig = {
    ...defaultConfig,
    ...config9,
    policySuggestions: { ...defaultConfig.policySuggestions, ...config9.policySuggestions },
    verdictExplanations: { ...defaultConfig.verdictExplanations, ...config9.verdictExplanations },
    anomalyDetection: { ...defaultConfig.anomalyDetection, ...config9.anomalyDetection },
    privacySettings: { ...defaultConfig.privacySettings, ...config9.privacySettings },
    llmSettings: { ...defaultConfig.llmSettings, ...config9.llmSettings }
  };
  return new PolicySuggestionService(mergedConfig);
}

// src/ai/governance/VerdictExplainerService.ts
init_logger2();
import { randomUUID as randomUUID63, createHash as createHash35 } from "crypto";
var EXPLANATION_TEMPLATES = {
  ALLOW: {
    end_user: "Your request was approved and has been processed successfully.",
    developer: "Request permitted. All policy checks passed.",
    compliance_officer: "Request approved. Governance policies satisfied. Audit trail recorded.",
    executive: "Action approved within policy guidelines."
  },
  DENY: {
    end_user: "Your request could not be completed because it doesn't meet our security requirements.",
    developer: "Request denied due to policy violation. Review the technical details below.",
    compliance_officer: "Request blocked. Policy violation detected. Review required controls.",
    executive: "Action blocked to maintain compliance and security posture."
  },
  ESCALATE: {
    end_user: "Your request needs additional approval before it can be processed.",
    developer: "Request escalated for manual review. Automatic approval not possible.",
    compliance_officer: "Request escalated per policy. Manual review required before proceeding.",
    executive: "Action requires elevated approval due to risk classification."
  },
  WARN: {
    end_user: "Your request was processed, but please note some concerns were flagged.",
    developer: "Request permitted with warnings. Review flagged items.",
    compliance_officer: "Request approved with warnings. Monitoring recommended.",
    executive: "Action permitted with advisory flags. Review recommended."
  }
};
var VerdictExplainerService = class {
  config;
  llmClient;
  explanationCache = /* @__PURE__ */ new Map();
  explanationStore = /* @__PURE__ */ new Map();
  statistics;
  constructor(config9) {
    this.config = config9;
    this.llmClient = getGovernanceLLMClient({
      enabled: config9.llmSettings?.provider !== "mock",
      provider: config9.llmSettings?.provider || "mock",
      model: config9.llmSettings?.model || "gpt-4",
      maxTokens: config9.llmSettings?.maxTokens || 2048,
      temperature: config9.llmSettings?.temperature || 0.3,
      timeout: config9.llmSettings?.timeout || 3e4
    });
    this.statistics = this.initializeStatistics();
  }
  initializeStatistics() {
    return {
      totalExplanations: 0,
      byAction: { ALLOW: 0, DENY: 0, ESCALATE: 0, WARN: 0 },
      byAudience: { end_user: 0, developer: 0, compliance_officer: 0, executive: 0 },
      cacheHits: 0,
      cacheMisses: 0,
      llmUsed: 0,
      templateFallbacks: 0,
      averageLatencyMs: 0,
      lastReset: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  /**
   * Generate a human-readable explanation for a governance verdict
   */
  async explainVerdict(verdict, context4) {
    if (!this.config.verdictExplanations.enabled) {
      throw new Error("Verdict explanations are disabled");
    }
    const startTime = Date.now();
    const cacheKey = this.generateCacheKey(verdict, context4);
    if (this.config.verdictExplanations.cacheExplanations) {
      const cached = this.explanationCache.get(cacheKey);
      if (cached && Date.now() < cached.expiresAt) {
        this.statistics.cacheHits++;
        logger_default2.debug("Returning cached verdict explanation", { cacheKey });
        return cached.explanation;
      }
      if (cached) {
        this.explanationCache.delete(cacheKey);
      }
      this.statistics.cacheMisses++;
    }
    try {
      const explanation = await this.generateExplanation(verdict, context4);
      this.statistics.totalExplanations++;
      this.statistics.byAction[verdict.action]++;
      this.statistics.byAudience[context4.audience]++;
      const latencyMs = Date.now() - startTime;
      this.updateAverageLatency(latencyMs);
      if (this.config.verdictExplanations.cacheExplanations) {
        this.explanationCache.set(cacheKey, {
          explanation,
          expiresAt: Date.now() + this.config.verdictExplanations.cacheTTLSeconds * 1e3
        });
      }
      this.explanationStore.set(explanation.provenance?.id || cacheKey, explanation);
      logger_default2.info("Verdict explanation generated", {
        action: verdict.action,
        audience: context4.audience,
        latencyMs,
        usedLLM: explanation.provenance?.method === "llm_generation"
      });
      return explanation;
    } catch (error) {
      logger_default2.error("Failed to generate verdict explanation", { error, verdict });
      throw error;
    }
  }
  updateAverageLatency(latencyMs) {
    const total = this.statistics.totalExplanations;
    const currentAvg = this.statistics.averageLatencyMs;
    this.statistics.averageLatencyMs = (currentAvg * (total - 1) + latencyMs) / total;
  }
  /**
   * Generate explanations for multiple verdicts
   */
  async batchExplain(verdicts, context4) {
    const batchSize = 5;
    const results = [];
    for (let i = 0; i < verdicts.length; i += batchSize) {
      const batch = verdicts.slice(i, i + batchSize);
      const batchResults = await Promise.all(
        batch.map((v) => this.explainVerdict(v, context4))
      );
      results.push(...batchResults);
    }
    return results;
  }
  /**
   * Get an explanation by ID
   */
  async getExplanation(id) {
    return this.explanationStore.get(id) || null;
  }
  /**
   * List all stored explanations with pagination
   */
  async listExplanations(options2 = {}) {
    const { page = 1, pageSize = 20, action, audience } = options2;
    let explanations = Array.from(this.explanationStore.values());
    if (action) {
      explanations = explanations.filter((e) => e.originalVerdict.action === action);
    }
    if (audience) {
      explanations = explanations.filter((e) => e.audience === audience);
    }
    explanations.sort(
      (a, b) => new Date(b.generatedAt).getTime() - new Date(a.generatedAt).getTime()
    );
    const total = explanations.length;
    const startIndex = (page - 1) * pageSize;
    const paginatedExplanations = explanations.slice(startIndex, startIndex + pageSize);
    return { explanations: paginatedExplanations, total };
  }
  /**
   * Get statistics about explanation generation
   */
  getStatistics() {
    return { ...this.statistics };
  }
  /**
   * Reset statistics
   */
  resetStatistics() {
    this.statistics = this.initializeStatistics();
    logger_default2.info("Verdict explanation statistics reset");
  }
  /**
   * Clear the explanation cache
   */
  clearCache() {
    this.explanationCache.clear();
    logger_default2.info("Verdict explanation cache cleared");
  }
  // ===========================================================================
  // Private Methods
  // ===========================================================================
  generateCacheKey(verdict, context4) {
    const verdictHash = JSON.stringify({
      action: verdict.action,
      policyIds: verdict.policyIds.sort(),
      reasons: verdict.reasons.sort()
    });
    return `${verdictHash}-${context4.audience}-${context4.tone}-${context4.locale || "en"}`;
  }
  async generateExplanation(verdict, context4) {
    const id = randomUUID63();
    const now = (/* @__PURE__ */ new Date()).toISOString();
    const summary = this.generateSummary(verdict, context4);
    const { text: detailedExplanation, usedLLM } = await this.generateDetailedExplanation(verdict, context4);
    const technicalDetails = this.extractTechnicalDetails(verdict);
    const remediationSteps = this.generateRemediationSteps(verdict, context4);
    const policyReferences = await this.getPolicyReferences(verdict.policyIds);
    const relatedExamples = context4.includeExamples ? this.generateRelatedExamples(verdict) : [];
    const confidence = this.calculateExplanationConfidence(verdict, detailedExplanation);
    let finalDetailedExplanation = detailedExplanation;
    if (context4.maxLength && finalDetailedExplanation.length > context4.maxLength) {
      finalDetailedExplanation = finalDetailedExplanation.substring(0, context4.maxLength - 3) + "...";
    }
    const explanation = {
      originalVerdict: verdict,
      summary,
      detailedExplanation: finalDetailedExplanation,
      technicalDetails,
      remediationSteps,
      policyReferences,
      relatedExamples,
      confidence,
      tone: context4.tone,
      audience: context4.audience,
      generatedAt: now,
      governanceVerdict: this.createMetaGovernanceVerdict(),
      provenance: this.createProvenance(id, verdict, usedLLM)
    };
    return explanation;
  }
  generateSummary(verdict, context4) {
    const baseTemplate = EXPLANATION_TEMPLATES[verdict.action][context4.audience];
    if (verdict.reasons.length > 0 && verdict.action !== "ALLOW") {
      const primaryReason = this.humanizeReason(verdict.reasons[0], context4.tone);
      return `${baseTemplate} ${primaryReason}`;
    }
    return baseTemplate;
  }
  async generateDetailedExplanation(verdict, context4) {
    if (this.config.llmSettings?.provider !== "mock") {
      try {
        const llmResponse = await this.llmClient.executeWithRetry({
          taskType: "verdict_explanation",
          prompt: this.buildLLMPrompt(verdict, context4),
          context: {
            userRole: context4.audience,
            sensitivityLevel: this.getSensitivityLevel(verdict)
          },
          tenantId: context4.tenantId || "system",
          userId: context4.userId,
          temperature: 0.4
          // Slightly creative for explanations
        });
        this.statistics.llmUsed++;
        return { text: llmResponse.text, usedLLM: true };
      } catch (error) {
        logger_default2.warn({ error }, "LLM explanation failed, falling back to template");
      }
    }
    this.statistics.templateFallbacks++;
    const text = this.generateTemplateExplanation(verdict, context4);
    return { text, usedLLM: false };
  }
  buildLLMPrompt(verdict, context4) {
    const audienceDescriptions = {
      end_user: "a non-technical end user who needs a simple, clear explanation",
      developer: "a software developer who understands technical concepts",
      compliance_officer: "a compliance officer who needs policy and regulatory context",
      executive: "a business executive who needs a high-level summary with business impact"
    };
    return `
Generate a ${context4.tone} explanation for the following governance verdict.

Target Audience: ${audienceDescriptions[context4.audience]}

Verdict Details:
- Action: ${verdict.action}
- Reasons: ${verdict.reasons.join("; ")}
- Policies Applied: ${verdict.policyIds.join(", ")}
- Confidence: ${(verdict.provenance?.confidence || 0.9) * 100}%

Requirements:
1. Start with a clear summary of what happened
2. Explain WHY this decision was made in terms the audience will understand
3. If the action is DENY or ESCALATE, provide clear next steps
4. Keep the explanation concise but complete
5. Use ${context4.tone === "formal" ? "professional, formal language" : "friendly, approachable language"}
${context4.locale && context4.locale !== "en" ? `6. Write the explanation in ${context4.locale}` : ""}

Generate the explanation now:
`.trim();
  }
  getSensitivityLevel(verdict) {
    if (verdict.action === "DENY" || verdict.action === "ESCALATE") {
      return "high";
    }
    if (verdict.action === "WARN") {
      return "medium";
    }
    return "low";
  }
  generateTemplateExplanation(verdict, context4) {
    const sections = [];
    sections.push(this.getOpeningParagraph(verdict, context4));
    if (verdict.reasons.length > 0) {
      sections.push("\n\n**What triggered this decision:**\n");
      verdict.reasons.forEach((reason, index) => {
        sections.push(`${index + 1}. ${this.humanizeReason(reason, context4.tone)}`);
      });
    }
    if (verdict.policyIds.length > 0) {
      sections.push("\n\n**Applicable policies:**\n");
      sections.push(
        `This decision was made based on ${verdict.policyIds.length} governance ${verdict.policyIds.length === 1 ? "policy" : "policies"}: ${verdict.policyIds.join(", ")}.`
      );
    }
    if (verdict.provenance?.confidence && verdict.provenance.confidence < 0.9) {
      sections.push("\n\n**Note:** This decision was made with ");
      sections.push(
        `${(verdict.provenance.confidence * 100).toFixed(0)}% confidence. `
      );
      sections.push("You may request a manual review if you believe this is incorrect.");
    }
    return sections.join("");
  }
  getOpeningParagraph(verdict, context4) {
    const formal = context4.tone === "formal";
    switch (verdict.action) {
      case "DENY":
        return formal ? "The requested operation has been denied by the governance system. This decision was made to protect organizational data and maintain compliance with applicable regulations." : "We couldn't complete your request this time. Here's why, and what you can do about it:";
      case "ESCALATE":
        return formal ? "The requested operation requires additional authorization before it can proceed. This escalation is mandated by governance policies for operations of this nature." : "Your request needs a quick review from someone with additional permissions. This is a normal part of our security process.";
      case "WARN":
        return formal ? "The requested operation has been permitted with advisory warnings. Please review the following items and take appropriate action." : "Good news - your request went through! However, we noticed a few things you might want to look at:";
      case "ALLOW":
      default:
        return formal ? "The requested operation has been approved in accordance with governance policies." : "All good! Your request was approved and processed.";
    }
  }
  humanizeReason(reason, tone) {
    const humanizations = {
      "classification_mismatch": {
        formal: "The data classification level does not meet the required threshold for this operation.",
        friendly: "The data you're trying to access has a higher security level than your current permissions allow.",
        technical: "Data classification mismatch: requested classification exceeds authorized level."
      },
      "region_restriction": {
        formal: "The destination region is not authorized for data transfers under current policies.",
        friendly: "We can't send data to that region due to data residency rules.",
        technical: "Region restriction violation: destination region not in allowed list."
      },
      "rate_limit_exceeded": {
        formal: "The request volume has exceeded the permitted threshold.",
        friendly: "You've made too many requests in a short time. Please wait a moment and try again.",
        technical: "Rate limit exceeded: request count > threshold within time window."
      },
      "missing_consent": {
        formal: "Required data subject consent has not been recorded for this operation.",
        friendly: "We need consent from the data subject before we can proceed with this.",
        technical: "Consent check failed: no valid consent record found."
      },
      "audit_required": {
        formal: "This operation requires documented audit justification before proceeding.",
        friendly: "We need you to provide a reason for this access that we can log for compliance.",
        technical: "Audit requirement not satisfied: justification field empty or invalid."
      }
    };
    const normalized = reason.toLowerCase().replace(/[^a-z_]/g, "_");
    for (const [key, humanized] of Object.entries(humanizations)) {
      if (normalized.includes(key)) {
        return humanized[tone];
      }
    }
    return reason.replace(/_/g, " ").replace(/([a-z])([A-Z])/g, "$1 $2").toLowerCase().replace(/^./, (c) => c.toUpperCase());
  }
  extractTechnicalDetails(verdict) {
    const details = [];
    if (verdict.metadata) {
      details.push({
        category: "Evaluation",
        field: "evaluator",
        expected: "governance-engine",
        actual: verdict.metadata.evaluator,
        explanation: "The system component that evaluated this request"
      });
      details.push({
        category: "Evaluation",
        field: "latency",
        expected: "< 100ms",
        actual: `${verdict.metadata.latencyMs}ms`,
        explanation: "Time taken to evaluate governance policies"
      });
      if (verdict.metadata.simulation) {
        details.push({
          category: "Context",
          field: "mode",
          expected: "production",
          actual: "simulation",
          explanation: "This was a simulation/dry-run evaluation"
        });
      }
    }
    if (verdict.provenance) {
      details.push({
        category: "Provenance",
        field: "confidence",
        expected: "> 0.9",
        actual: verdict.provenance.confidence?.toString() || "N/A",
        explanation: "Confidence level in the evaluation accuracy"
      });
    }
    return details;
  }
  generateRemediationSteps(verdict, context4) {
    if (verdict.action === "ALLOW") {
      return [];
    }
    const steps = [];
    switch (verdict.action) {
      case "DENY":
        steps.push({
          order: 1,
          action: "Review requirements",
          description: "Check the policy requirements that were not met",
          automated: false,
          estimatedEffort: "trivial"
        });
        steps.push({
          order: 2,
          action: "Modify request",
          description: "Adjust your request to comply with the identified requirements",
          automated: false,
          estimatedEffort: "low"
        });
        steps.push({
          order: 3,
          action: "Request exception",
          description: "If you believe this denial is incorrect, submit an exception request",
          automated: true,
          automationAction: "create_exception_request",
          estimatedEffort: "medium"
        });
        break;
      case "ESCALATE":
        steps.push({
          order: 1,
          action: "Wait for approval",
          description: "Your request has been sent to the appropriate approver",
          automated: true,
          automationAction: "track_approval_status",
          estimatedEffort: "trivial"
        });
        steps.push({
          order: 2,
          action: "Provide justification",
          description: "Add additional context to help approvers make a decision",
          automated: false,
          estimatedEffort: "low"
        });
        break;
      case "WARN":
        steps.push({
          order: 1,
          action: "Review warnings",
          description: "Understand the flagged items and their implications",
          automated: false,
          estimatedEffort: "trivial"
        });
        steps.push({
          order: 2,
          action: "Acknowledge",
          description: "Confirm you have reviewed and understand the warnings",
          automated: true,
          automationAction: "acknowledge_warning",
          estimatedEffort: "trivial"
        });
        break;
    }
    return steps;
  }
  async getPolicyReferences(policyIds) {
    return policyIds.map((id) => ({
      policyId: id,
      policyName: `Policy ${id}`,
      excerpt: "This policy governs access to sensitive resources...",
      documentationUrl: `/docs/policies/${id}`
    }));
  }
  generateRelatedExamples(verdict) {
    const examples = [];
    if (verdict.action === "DENY") {
      examples.push({
        scenario: "Requesting the same data with proper authorization",
        outcome: "allowed",
        explanation: "Ensure you have the required role or permission before making the request."
      });
      examples.push({
        scenario: "Accessing similar data in a permitted region",
        outcome: "allowed",
        explanation: "Data access from approved regions typically succeeds."
      });
    }
    return examples;
  }
  calculateExplanationConfidence(verdict, explanation) {
    let confidence = verdict.provenance?.confidence || 0.8;
    if (explanation.length > 200) confidence += 0.05;
    if (verdict.reasons.length > 0) confidence += 0.05;
    if (verdict.policyIds.length > 0) confidence += 0.05;
    return Math.min(confidence, 1);
  }
  createMetaGovernanceVerdict() {
    return {
      action: "ALLOW",
      reasons: ["Explanation generation approved"],
      policyIds: ["ai-explanation-policy"],
      metadata: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        evaluator: "VerdictExplainerService",
        latencyMs: 0,
        simulation: false
      },
      provenance: {
        origin: "ai-governance-system",
        confidence: 0.95
      }
    };
  }
  createProvenance(id, verdict, usedLLM) {
    const inputHash = createHash35("sha256").update(JSON.stringify(verdict)).digest("hex");
    const method = usedLLM ? "llm_generation" : "template_generation";
    const custodyEntry = {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      actor: usedLLM ? `llm:${this.config.llmSettings?.provider || "mock"}` : "template-engine",
      action: "generate_explanation",
      hash: inputHash
    };
    return {
      id: `prov-explanation-${id}`,
      sourceId: "verdict-explainer-service",
      sourceType: usedLLM ? "ai_model" : "template_engine",
      modelVersion: this.config.llmSettings?.model || "template-v1",
      modelProvider: usedLLM ? this.config.llmSettings?.provider || "mock" : "internal",
      inputHash,
      outputHash: createHash35("sha256").update(id).digest("hex"),
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      confidence: usedLLM ? 0.85 : 0.75,
      method,
      chainOfCustody: [custodyEntry]
    };
  }
};
function createVerdictExplainerService(config9 = {}) {
  const defaultConfig = {
    enabled: true,
    policySuggestions: {
      enabled: true,
      maxSuggestionsPerDay: 10,
      minConfidenceThreshold: 0.6,
      requireHumanApproval: true
    },
    verdictExplanations: {
      enabled: true,
      defaultAudience: "end_user",
      defaultTone: "friendly",
      cacheExplanations: true,
      cacheTTLSeconds: 3600
    },
    anomalyDetection: {
      enabled: true,
      detectionIntervalSeconds: 300,
      minAnomalyScore: 50,
      autoBlockThreshold: 90,
      alertChannels: ["slack", "email"]
    },
    privacySettings: {
      federatedLearning: false,
      differentialPrivacy: true,
      epsilonBudget: 1,
      dataRetentionDays: 90,
      piiRedaction: true
    },
    llmSettings: {
      provider: "mock",
      model: "gpt-4-turbo",
      maxTokens: 4096,
      temperature: 0.3,
      timeout: 3e4
    }
  };
  return new VerdictExplainerService({ ...defaultConfig, ...config9 });
}

// src/ai/governance/BehavioralAnomalyService.ts
init_logger2();
import { randomUUID as randomUUID64 } from "crypto";
var ANOMALY_THRESHOLDS = {
  access_pattern: {
    stdDevThreshold: 2.5,
    minEvents: 10
  },
  volume_spike: {
    percentileThreshold: 95,
    multiplier: 3
  },
  privilege_escalation: {
    confidenceThreshold: 0.8
  },
  policy_circumvention: {
    attemptThreshold: 3,
    timeWindowMinutes: 60
  },
  data_exfiltration: {
    volumeThresholdMB: 100,
    rateThresholdMBPerHour: 50
  },
  credential_abuse: {
    locationChangeThreshold: 500,
    // km
    timeThresholdMinutes: 30
  },
  insider_threat: {
    riskScoreThreshold: 70
  },
  api_abuse: {
    rateThreshold: 1e3,
    windowSeconds: 60
  }
};
var BehavioralAnomalyService = class {
  config;
  llmClient;
  anomalyStore = /* @__PURE__ */ new Map();
  baselineCache = /* @__PURE__ */ new Map();
  falsePositivePatterns = /* @__PURE__ */ new Map();
  // Pattern -> count
  statistics;
  constructor(config9) {
    this.config = config9;
    this.llmClient = getGovernanceLLMClient({
      enabled: config9.llmSettings?.provider !== "mock",
      provider: config9.llmSettings?.provider || "mock",
      model: config9.llmSettings?.model || "gpt-4",
      maxTokens: config9.llmSettings?.maxTokens || 2048,
      temperature: config9.llmSettings?.temperature || 0.2,
      // Lower temp for security analysis
      timeout: config9.llmSettings?.timeout || 3e4
    });
    this.statistics = this.initializeStatistics();
  }
  initializeStatistics() {
    return {
      totalDetected: 0,
      byType: {},
      bySeverity: {},
      falsePositives: 0,
      truePositives: 0,
      alertsSent: 0,
      autoBlocked: 0,
      llmAnalyses: 0,
      averageRiskScore: 0,
      lastReset: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  /**
   * Detect anomalies within the specified scope
   */
  async detectAnomalies(scope) {
    if (!this.config.anomalyDetection.enabled) {
      logger_default2.info("Anomaly detection disabled");
      return [];
    }
    const startTime = Date.now();
    logger_default2.info("Starting anomaly detection", { scope });
    try {
      const detectedAnomalies = [];
      const detectionPromises = [];
      const typesToDetect = scope.anomalyTypes || Object.keys(ANOMALY_THRESHOLDS);
      for (const anomalyType of typesToDetect) {
        detectionPromises.push(this.detectByType(anomalyType, scope));
      }
      const results = await Promise.all(detectionPromises);
      for (const anomalies of results) {
        detectedAnomalies.push(...anomalies);
      }
      const filteredAnomalies = this.filterBySeverity(detectedAnomalies, scope.minSeverity);
      const scoredAnomalies = filteredAnomalies.filter(
        (a) => a.riskScore >= this.config.anomalyDetection.minAnomalyScore
      );
      const enhancedAnomalies = await this.enhanceWithLLMAnalysis(scoredAnomalies, scope);
      for (const anomaly of enhancedAnomalies) {
        this.anomalyStore.set(anomaly.id, anomaly);
        this.statistics.totalDetected++;
        this.statistics.byType[anomaly.anomalyType] = (this.statistics.byType[anomaly.anomalyType] || 0) + 1;
        this.statistics.bySeverity[anomaly.severity] = (this.statistics.bySeverity[anomaly.severity] || 0) + 1;
        this.updateAverageRiskScore(anomaly.riskScore);
      }
      await this.processHighRiskAnomalies(enhancedAnomalies);
      const latencyMs = Date.now() - startTime;
      logger_default2.info("Anomaly detection completed", {
        detected: enhancedAnomalies.length,
        llmEnhanced: enhancedAnomalies.filter((a) => a.provenance?.method === "llm_enhanced").length,
        latencyMs
      });
      return enhancedAnomalies;
    } catch (error) {
      logger_default2.error("Anomaly detection failed", { error, scope });
      throw error;
    }
  }
  updateAverageRiskScore(newScore) {
    const total = this.statistics.totalDetected;
    const currentAvg = this.statistics.averageRiskScore;
    this.statistics.averageRiskScore = (currentAvg * (total - 1) + newScore) / total;
  }
  /**
   * Enhance anomalies with LLM-powered analysis
   */
  async enhanceWithLLMAnalysis(anomalies, scope) {
    if (this.config.llmSettings?.provider === "mock") {
      return anomalies;
    }
    const highSeverityAnomalies = anomalies.filter(
      (a) => a.severity === "critical" || a.severity === "high"
    );
    if (highSeverityAnomalies.length === 0) {
      return anomalies;
    }
    const enhancedAnomalies = [];
    for (const anomaly of anomalies) {
      if (anomaly.severity === "critical" || anomaly.severity === "high") {
        try {
          const enhanced = await this.analyzeWithLLM(anomaly, scope);
          enhancedAnomalies.push(enhanced);
          this.statistics.llmAnalyses++;
        } catch (error) {
          logger_default2.warn({ error, anomalyId: anomaly.id }, "LLM analysis failed, using original");
          enhancedAnomalies.push(anomaly);
        }
      } else {
        enhancedAnomalies.push(anomaly);
      }
    }
    return enhancedAnomalies;
  }
  /**
   * Analyze a single anomaly with LLM
   */
  async analyzeWithLLM(anomaly, scope) {
    const prompt = this.buildAnomalyAnalysisPrompt(anomaly);
    const llmResponse = await this.llmClient.executeWithRetry({
      taskType: "anomaly_analysis",
      prompt,
      context: {
        sensitivityLevel: "high"
      },
      tenantId: scope.tenantIds?.[0] || "system"
    });
    const analysis = this.parseLLMAnalysis(llmResponse.text);
    return {
      ...anomaly,
      description: analysis.enhancedDescription || anomaly.description,
      recommendedActions: analysis.enhancedActions || anomaly.recommendedActions,
      falsePositiveLikelihood: analysis.falsePositiveLikelihood ?? anomaly.falsePositiveLikelihood,
      provenance: this.createEnhancedProvenance(anomaly, llmResponse.provenance)
    };
  }
  buildAnomalyAnalysisPrompt(anomaly) {
    return `
Analyze the following security anomaly and provide enhanced analysis.

Anomaly Type: ${anomaly.anomalyType}
Severity: ${anomaly.severity}
Risk Score: ${anomaly.riskScore}
Title: ${anomaly.title}
Description: ${anomaly.description}

Affected Entity:
- Type: ${anomaly.affectedEntity.entityType}
- ID: ${anomaly.affectedEntity.entityId}
- Name: ${anomaly.affectedEntity.entityName}

Baseline Behavior:
${JSON.stringify(anomaly.baselineBehavior, null, 2)}

Observed Behavior:
${JSON.stringify(anomaly.observedBehavior, null, 2)}

Deviation Metrics:
- Standard Deviations: ${anomaly.deviation.standardDeviations}
- Statistical Significance: ${anomaly.deviation.statisticalSignificance}

Please provide:
1. An enhanced description of the anomaly (be specific about what makes this concerning)
2. Estimated false positive likelihood (0.0 to 1.0)
3. Recommended immediate actions
4. Additional investigation steps

Format your response as JSON:
{
  "enhancedDescription": "...",
  "falsePositiveLikelihood": 0.X,
  "enhancedActions": [
    {"actionType": "...", "description": "...", "urgency": "immediate|urgent|standard", "automated": true|false}
  ]
}
`.trim();
  }
  parseLLMAnalysis(llmText) {
    try {
      const jsonMatch = llmText.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        return {
          enhancedDescription: parsed.enhancedDescription,
          falsePositiveLikelihood: parsed.falsePositiveLikelihood,
          enhancedActions: parsed.enhancedActions?.map((a) => ({
            actionType: a.actionType || "investigate",
            description: a.description,
            urgency: a.urgency || "standard",
            automated: a.automated || false,
            requiredRole: "security_analyst"
          }))
        };
      }
    } catch (error) {
      logger_default2.warn({ error }, "Failed to parse LLM analysis response");
    }
    return {};
  }
  createEnhancedProvenance(anomaly, llmProvenance) {
    const custodyEntry = {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      actor: "llm:anomaly-analyzer",
      action: "enhance_analysis",
      hash: llmProvenance.outputHash || ""
    };
    return {
      ...anomaly.provenance,
      method: "llm_enhanced",
      chainOfCustody: [
        ...anomaly.provenance?.chainOfCustody || [],
        custodyEntry
      ]
    };
  }
  /**
   * Get a specific anomaly by ID
   */
  async getAnomaly(id) {
    return this.anomalyStore.get(id) || null;
  }
  /**
   * Update the status of an anomaly
   */
  async updateAnomalyStatus(id, status, notes) {
    const anomaly = this.anomalyStore.get(id);
    if (!anomaly) {
      throw new Error(`Anomaly not found: ${id}`);
    }
    const updated = {
      ...anomaly,
      status
    };
    this.anomalyStore.set(id, updated);
    logger_default2.info("Anomaly status updated", { id, status, notes });
    return updated;
  }
  /**
   * Resolve an anomaly with final determination
   */
  async resolveAnomaly(id, resolution) {
    const anomaly = this.anomalyStore.get(id);
    if (!anomaly) {
      throw new Error(`Anomaly not found: ${id}`);
    }
    const resolved = {
      ...anomaly,
      status: "mitigated",
      resolution
    };
    this.anomalyStore.set(id, resolved);
    if (resolution.resolution === "false_positive") {
      await this.recordFalsePositive(anomaly);
    }
    logger_default2.info("Anomaly resolved", { id, resolution: resolution.resolution });
    return resolved;
  }
  /**
   * Get anomaly trends for a tenant
   */
  async getAnomalyTrends(tenantId, timeRange) {
    const anomalies = Array.from(this.anomalyStore.values()).filter(
      (a) => a.affectedEntity.tenantId === tenantId && new Date(a.detectedAt) >= new Date(timeRange.start) && new Date(a.detectedAt) <= new Date(timeRange.end)
    );
    const byType = {};
    const bySeverity = {};
    for (const anomaly of anomalies) {
      byType[anomaly.anomalyType] = (byType[anomaly.anomalyType] || 0) + 1;
      bySeverity[anomaly.severity] = (bySeverity[anomaly.severity] || 0) + 1;
    }
    const midpoint = new Date(
      (new Date(timeRange.start).getTime() + new Date(timeRange.end).getTime()) / 2
    );
    const firstHalf = anomalies.filter((a) => new Date(a.detectedAt) < midpoint).length;
    const secondHalf = anomalies.filter((a) => new Date(a.detectedAt) >= midpoint).length;
    let trend;
    if (secondHalf > firstHalf * 1.2) {
      trend = "increasing";
    } else if (secondHalf < firstHalf * 0.8) {
      trend = "decreasing";
    } else {
      trend = "stable";
    }
    const entityCounts = /* @__PURE__ */ new Map();
    for (const anomaly of anomalies) {
      const key = `${anomaly.affectedEntity.entityType}:${anomaly.affectedEntity.entityId}`;
      const existing = entityCounts.get(key);
      if (existing) {
        existing.count++;
      } else {
        entityCounts.set(key, { entity: anomaly.affectedEntity, count: 1 });
      }
    }
    const topAffectedEntities = Array.from(entityCounts.values()).sort((a, b) => b.count - a.count).slice(0, 5).map((e) => e.entity);
    return {
      totalAnomalies: anomalies.length,
      byType,
      bySeverity,
      trend,
      topAffectedEntities
    };
  }
  /**
   * List all anomalies with pagination and filtering
   */
  async listAnomalies(options2 = {}) {
    const {
      page = 1,
      pageSize = 20,
      tenantId,
      status,
      type,
      minSeverity,
      sortBy = "detectedAt",
      sortOrder = "desc"
    } = options2;
    let anomalies = Array.from(this.anomalyStore.values());
    if (tenantId) {
      anomalies = anomalies.filter((a) => a.affectedEntity.tenantId === tenantId);
    }
    if (status) {
      anomalies = anomalies.filter((a) => a.status === status);
    }
    if (type) {
      anomalies = anomalies.filter((a) => a.anomalyType === type);
    }
    if (minSeverity) {
      anomalies = this.filterBySeverity(anomalies, minSeverity);
    }
    anomalies.sort((a, b) => {
      let comparison = 0;
      if (sortBy === "detectedAt") {
        comparison = new Date(a.detectedAt).getTime() - new Date(b.detectedAt).getTime();
      } else {
        comparison = a.riskScore - b.riskScore;
      }
      return sortOrder === "desc" ? -comparison : comparison;
    });
    const total = anomalies.length;
    const startIndex = (page - 1) * pageSize;
    const paginatedAnomalies = anomalies.slice(startIndex, startIndex + pageSize);
    return { anomalies: paginatedAnomalies, total };
  }
  /**
   * Get detection statistics
   */
  getStatistics() {
    return { ...this.statistics };
  }
  /**
   * Reset statistics
   */
  resetStatistics() {
    this.statistics = this.initializeStatistics();
    logger_default2.info("Anomaly detection statistics reset");
  }
  /**
   * Clear all stored anomalies
   */
  clearAnomalies() {
    this.anomalyStore.clear();
    logger_default2.info("Anomaly store cleared");
  }
  // ===========================================================================
  // Private Detection Methods
  // ===========================================================================
  async detectByType(anomalyType, scope) {
    switch (anomalyType) {
      case "access_pattern":
        return this.detectAccessPatternAnomalies(scope);
      case "volume_spike":
        return this.detectVolumeSpikeAnomalies(scope);
      case "privilege_escalation":
        return this.detectPrivilegeEscalation(scope);
      case "policy_circumvention":
        return this.detectPolicyCircumvention(scope);
      case "data_exfiltration":
        return this.detectDataExfiltration(scope);
      case "credential_abuse":
        return this.detectCredentialAbuse(scope);
      case "insider_threat":
        return this.detectInsiderThreat(scope);
      case "api_abuse":
        return this.detectAPIAbuse(scope);
      default:
        return [];
    }
  }
  async detectAccessPatternAnomalies(scope) {
    const anomalies = [];
    if (Math.random() > 0.7) {
      anomalies.push(
        this.createAnomaly({
          type: "access_pattern",
          title: "Unusual Access Time Pattern",
          description: "User accessed system at 3:00 AM, outside normal working hours",
          severity: "medium",
          riskScore: 65,
          entity: {
            entityType: "user",
            entityId: "user-123",
            entityName: "john.doe@example.com",
            tenantId: scope.tenantIds?.[0] || "default"
          },
          baseline: {
            timeWindow: "30d",
            metrics: [
              { name: "typical_access_hours", value: 9, unit: "hour_start" },
              { name: "typical_access_hours_end", value: 18, unit: "hour_end" }
            ]
          },
          observed: {
            observationWindow: "1h",
            metrics: [{ name: "access_hour", value: 3, unit: "hour" }]
          },
          deviation: {
            standardDeviations: 3.2,
            percentileJump: 99,
            absoluteChange: 6,
            relativeChange: 200,
            statisticalSignificance: 1e-3
          }
        })
      );
    }
    return anomalies;
  }
  async detectVolumeSpikeAnomalies(scope) {
    const anomalies = [];
    if (Math.random() > 0.8) {
      anomalies.push(
        this.createAnomaly({
          type: "volume_spike",
          title: "Unusual Data Volume Accessed",
          description: "User downloaded 500MB of data, 10x their normal volume",
          severity: "high",
          riskScore: 78,
          entity: {
            entityType: "user",
            entityId: "user-456",
            entityName: "jane.smith@example.com",
            tenantId: scope.tenantIds?.[0] || "default"
          },
          baseline: {
            timeWindow: "30d",
            metrics: [{ name: "avg_daily_download", value: 50, unit: "MB" }]
          },
          observed: {
            observationWindow: "24h",
            metrics: [{ name: "daily_download", value: 500, unit: "MB" }]
          },
          deviation: {
            standardDeviations: 4.5,
            percentileJump: 99.9,
            absoluteChange: 450,
            relativeChange: 900,
            statisticalSignificance: 1e-4
          }
        })
      );
    }
    return anomalies;
  }
  async detectPrivilegeEscalation(scope) {
    const anomalies = [];
    if (Math.random() > 0.9) {
      anomalies.push(
        this.createAnomaly({
          type: "privilege_escalation",
          title: "Privilege Escalation Attempt",
          description: "User attempted to access admin API endpoints without authorization",
          severity: "critical",
          riskScore: 92,
          entity: {
            entityType: "user",
            entityId: "user-789",
            entityName: "contractor@external.com",
            tenantId: scope.tenantIds?.[0] || "default"
          },
          baseline: {
            timeWindow: "7d",
            metrics: [
              { name: "admin_api_attempts", value: 0, unit: "count" },
              { name: "authorized_role", value: 0, unit: "boolean" }
            ]
          },
          observed: {
            observationWindow: "1h",
            metrics: [
              { name: "admin_api_attempts", value: 15, unit: "count" },
              { name: "authorized_role", value: 0, unit: "boolean" }
            ]
          },
          deviation: {
            standardDeviations: 10,
            percentileJump: 100,
            absoluteChange: 15,
            relativeChange: Infinity,
            statisticalSignificance: 1e-5
          }
        })
      );
    }
    return anomalies;
  }
  async detectPolicyCircumvention(scope) {
    const anomalies = [];
    if (Math.random() > 0.85) {
      anomalies.push(
        this.createAnomaly({
          type: "policy_circumvention",
          title: "Policy Circumvention Pattern",
          description: "Multiple rapid requests with slightly varied parameters after denial",
          severity: "high",
          riskScore: 75,
          entity: {
            entityType: "user",
            entityId: "user-101",
            entityName: "developer@company.com",
            tenantId: scope.tenantIds?.[0] || "default"
          },
          baseline: {
            timeWindow: "7d",
            metrics: [
              { name: "denial_retry_rate", value: 1.2, unit: "per_hour" }
            ]
          },
          observed: {
            observationWindow: "1h",
            metrics: [
              { name: "denial_retry_rate", value: 25, unit: "per_hour" }
            ]
          },
          deviation: {
            standardDeviations: 5.8,
            percentileJump: 99.5,
            absoluteChange: 23.8,
            relativeChange: 1983,
            statisticalSignificance: 5e-4
          }
        })
      );
    }
    return anomalies;
  }
  async detectDataExfiltration(scope) {
    return [];
  }
  async detectCredentialAbuse(scope) {
    const anomalies = [];
    if (Math.random() > 0.95) {
      anomalies.push(
        this.createAnomaly({
          type: "credential_abuse",
          title: "Impossible Travel Detected",
          description: "Login from New York followed by login from London 20 minutes later",
          severity: "critical",
          riskScore: 95,
          entity: {
            entityType: "user",
            entityId: "user-202",
            entityName: "exec@company.com",
            tenantId: scope.tenantIds?.[0] || "default"
          },
          baseline: {
            timeWindow: "90d",
            metrics: [
              { name: "typical_locations", value: 2, unit: "count" },
              { name: "max_travel_speed", value: 100, unit: "km/h" }
            ]
          },
          observed: {
            observationWindow: "1h",
            metrics: [
              { name: "implied_travel_speed", value: 15e3, unit: "km/h" }
            ]
          },
          deviation: {
            standardDeviations: 15,
            percentileJump: 100,
            absoluteChange: 14900,
            relativeChange: 14900,
            statisticalSignificance: 1e-6
          }
        })
      );
    }
    return anomalies;
  }
  async detectInsiderThreat(scope) {
    return [];
  }
  async detectAPIAbuse(scope) {
    return [];
  }
  // ===========================================================================
  // Helper Methods
  // ===========================================================================
  createAnomaly(params) {
    const id = `anomaly-${randomUUID64()}`;
    const now = (/* @__PURE__ */ new Date()).toISOString();
    return {
      id,
      anomalyType: params.type,
      severity: params.severity,
      title: params.title,
      description: params.description,
      detectedAt: now,
      affectedEntity: params.entity,
      baselineBehavior: params.baseline,
      observedBehavior: params.observed,
      deviation: params.deviation,
      riskScore: params.riskScore,
      recommendedActions: this.generateRecommendedActions(params.type, params.severity),
      relatedAnomalies: [],
      falsePositiveLikelihood: this.estimateFalsePositiveLikelihood(params.type, params.deviation),
      evidenceChain: this.generateEvidenceChain(params),
      status: "new",
      governanceVerdict: this.createGovernanceVerdict(params.type),
      provenance: this.createProvenance(id)
    };
  }
  generateRecommendedActions(type, severity) {
    const actions = [];
    actions.push({
      actionType: "investigate",
      description: "Review the flagged activity and gather additional context",
      urgency: severity === "critical" ? "immediate" : "standard",
      automated: false,
      requiredRole: "security_analyst"
    });
    switch (type) {
      case "privilege_escalation":
      case "credential_abuse":
        actions.push({
          actionType: "block",
          description: "Temporarily block the affected account",
          urgency: "immediate",
          automated: true,
          automationConfig: { action: "suspend_account" },
          requiredRole: "security_admin"
        });
        break;
      case "volume_spike":
      case "data_exfiltration":
        actions.push({
          actionType: "restrict",
          description: "Apply rate limiting to the affected entity",
          urgency: "urgent",
          automated: true,
          automationConfig: { action: "apply_rate_limit", limit: "10MB/hour" },
          requiredRole: "security_admin"
        });
        break;
      default:
        actions.push({
          actionType: "monitor",
          description: "Increase monitoring for the affected entity",
          urgency: "standard",
          automated: true,
          automationConfig: { action: "enhanced_logging" },
          requiredRole: "security_analyst"
        });
    }
    return actions;
  }
  estimateFalsePositiveLikelihood(type, deviation) {
    const baseFPR = {
      access_pattern: 0.3,
      volume_spike: 0.25,
      privilege_escalation: 0.1,
      policy_circumvention: 0.2,
      data_exfiltration: 0.15,
      credential_abuse: 0.05,
      insider_threat: 0.35,
      api_abuse: 0.2
    };
    let fpr = baseFPR[type] || 0.2;
    if (deviation.standardDeviations > 5) fpr *= 0.5;
    if (deviation.statisticalSignificance < 1e-3) fpr *= 0.3;
    return Math.max(0, Math.min(fpr, 1));
  }
  generateEvidenceChain(params) {
    return [
      {
        evidenceType: "log_entry",
        description: "Access log entries showing the anomalous behavior",
        data: { metrics: params.observed.metrics },
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        source: "audit-log-service"
      },
      {
        evidenceType: "baseline_comparison",
        description: "Statistical comparison against historical baseline",
        data: { entityId: params.entity.entityId },
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        source: "anomaly-detection-engine"
      }
    ];
  }
  filterBySeverity(anomalies, minSeverity) {
    if (!minSeverity) return anomalies;
    const severityOrder = ["info", "low", "medium", "high", "critical"];
    const minIndex = severityOrder.indexOf(minSeverity);
    return anomalies.filter((a) => severityOrder.indexOf(a.severity) >= minIndex);
  }
  async processHighRiskAnomalies(anomalies) {
    const highRisk = anomalies.filter(
      (a) => a.riskScore >= this.config.anomalyDetection.autoBlockThreshold
    );
    for (const anomaly of highRisk) {
      const updated = {
        ...anomaly,
        status: "investigating"
      };
      this.anomalyStore.set(anomaly.id, updated);
      this.statistics.autoBlocked++;
      logger_default2.warn("High-risk anomaly detected - auto-block triggered", {
        anomalyId: anomaly.id,
        riskScore: anomaly.riskScore,
        entity: anomaly.affectedEntity
      });
      await this.sendAlerts(anomaly);
    }
  }
  async sendAlerts(anomaly) {
    for (const channel of this.config.anomalyDetection.alertChannels) {
      try {
        logger_default2.info("Sending anomaly alert", {
          channel,
          anomalyId: anomaly.id,
          severity: anomaly.severity
        });
        this.statistics.alertsSent++;
        switch (channel) {
          case "slack":
            break;
          case "email":
            break;
          case "pagerduty":
            break;
        }
      } catch (error) {
        logger_default2.error({ error, channel, anomalyId: anomaly.id }, "Failed to send alert");
      }
    }
  }
  async recordFalsePositive(anomaly) {
    this.statistics.falsePositives++;
    const patternKey = `${anomaly.anomalyType}:${anomaly.affectedEntity.entityType}`;
    const currentCount = this.falsePositivePatterns.get(patternKey) || 0;
    this.falsePositivePatterns.set(patternKey, currentCount + 1);
    if (currentCount + 1 >= 5) {
      logger_default2.warn("High false positive rate detected for pattern", {
        pattern: patternKey,
        count: currentCount + 1,
        suggestion: "Consider adjusting detection thresholds"
      });
    }
    logger_default2.info("Recording false positive for model improvement", {
      anomalyId: anomaly.id,
      type: anomaly.anomalyType,
      patternKey,
      patternCount: currentCount + 1
    });
  }
  /**
   * Record a true positive (confirmed threat)
   */
  async recordTruePositive(anomalyId) {
    const anomaly = this.anomalyStore.get(anomalyId);
    if (!anomaly) {
      throw new Error(`Anomaly not found: ${anomalyId}`);
    }
    this.statistics.truePositives++;
    const updated = {
      ...anomaly,
      status: "confirmed"
    };
    this.anomalyStore.set(anomalyId, updated);
    logger_default2.info("True positive confirmed", {
      anomalyId,
      type: anomaly.anomalyType
    });
  }
  createGovernanceVerdict(type) {
    return {
      action: "WARN",
      reasons: [`Anomaly detected: ${type}`],
      policyIds: ["anomaly-detection-policy"],
      metadata: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        evaluator: "BehavioralAnomalyService",
        latencyMs: 0,
        simulation: false
      },
      provenance: {
        origin: "anomaly-detection-engine",
        confidence: 0.85
      }
    };
  }
  createProvenance(anomalyId) {
    return {
      id: `prov-anomaly-${anomalyId}`,
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      actor: "BehavioralAnomalyService",
      action: "detect_anomaly",
      inputs: ["access_logs", "baseline_models"],
      outputs: [anomalyId],
      confidence: 0.85,
      method: "statistical_analysis_with_ml"
    };
  }
};
function createBehavioralAnomalyService(config9 = {}) {
  const defaultConfig = {
    enabled: true,
    policySuggestions: {
      enabled: true,
      maxSuggestionsPerDay: 10,
      minConfidenceThreshold: 0.6,
      requireHumanApproval: true
    },
    verdictExplanations: {
      enabled: true,
      defaultAudience: "end_user",
      defaultTone: "friendly",
      cacheExplanations: true,
      cacheTTLSeconds: 3600
    },
    anomalyDetection: {
      enabled: true,
      detectionIntervalSeconds: 300,
      minAnomalyScore: 50,
      autoBlockThreshold: 90,
      alertChannels: ["slack", "email"]
    },
    privacySettings: {
      federatedLearning: false,
      differentialPrivacy: true,
      epsilonBudget: 1,
      dataRetentionDays: 90,
      piiRedaction: true
    },
    llmSettings: {
      provider: "mock",
      model: "gpt-4-turbo",
      maxTokens: 4096,
      temperature: 0.3,
      timeout: 3e4
    }
  };
  return new BehavioralAnomalyService({ ...defaultConfig, ...config9 });
}

// src/routes/v4/ai-governance.ts
var policySuggestionService = null;
var verdictExplainerService = null;
var anomalyService = null;
var getConfig2 = () => ({
  enabled: process.env.AI_GOVERNANCE_ENABLED !== "false",
  policySuggestions: {
    enabled: process.env.AI_POLICY_SUGGESTIONS_ENABLED !== "false",
    maxSuggestionsPerDay: parseInt(process.env.AI_MAX_SUGGESTIONS_PER_DAY || "10"),
    minConfidenceThreshold: parseFloat(process.env.AI_MIN_CONFIDENCE || "0.6"),
    requireHumanApproval: process.env.AI_REQUIRE_HUMAN_APPROVAL !== "false"
  },
  llmSettings: {
    provider: process.env.LLM_PROVIDER || "mock",
    model: process.env.LLM_MODEL || "gpt-4-turbo",
    maxTokens: parseInt(process.env.LLM_MAX_TOKENS || "4096"),
    temperature: parseFloat(process.env.LLM_TEMPERATURE || "0.3"),
    timeout: parseInt(process.env.LLM_TIMEOUT || "30000")
  },
  privacySettings: {
    piiRedaction: process.env.AI_PII_REDACTION !== "false",
    federatedLearning: process.env.AI_FEDERATED_LEARNING === "true",
    differentialPrivacy: process.env.AI_DIFFERENTIAL_PRIVACY !== "false",
    epsilonBudget: parseFloat(process.env.AI_EPSILON_BUDGET || "1.0"),
    dataRetentionDays: parseInt(process.env.AI_DATA_RETENTION_DAYS || "90")
  }
});
var initializeServices = async () => {
  if (!policySuggestionService) {
    const config9 = getConfig2();
    policySuggestionService = createPolicySuggestionService(config9);
    verdictExplainerService = createVerdictExplainerService(config9);
    anomalyService = createBehavioralAnomalyService(config9);
    await policySuggestionService.initialize();
    logger_default2.info("AI Governance services initialized");
  }
};
var getTenantId2 = (req) => {
  return req.tenantId || req.user?.tenantId || "default";
};
var getUserId2 = (req) => {
  return req.user?.id || req.user?.id || "anonymous";
};
var wrapResponse = (data, req) => {
  return {
    data,
    metadata: {
      requestId: req.correlationId || randomUUID65(),
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      version: "4.0.0"
    },
    governance: {
      action: "ALLOW",
      reasons: ["AI governance response"],
      policyIds: ["ai-governance-v4"],
      metadata: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        evaluator: "ai-governance-router",
        latencyMs: 0,
        simulation: false
      },
      provenance: {
        origin: "ai-governance-api-v4",
        confidence: 0.95
      }
    }
  };
};
var handleError = (error, res, operation) => {
  if (error instanceof PolicySuggestionError) {
    const statusMap = {
      NOT_FOUND: 404,
      INVALID_STATE: 400,
      GENERATION_FAILED: 500,
      RATE_LIMITED: 429
    };
    const status = statusMap[error.code] || 500;
    return res.status(status).json({
      error: {
        code: error.code,
        message: error.message,
        details: error.details
      }
    });
  }
  logger_default2.error({ error, operation }, "AI Governance API error");
  return res.status(500).json({
    error: {
      code: "INTERNAL_ERROR",
      message: "An internal error occurred"
    }
  });
};
var router93 = Router47();
var singleParam24 = (value) => Array.isArray(value) ? value[0] : value ?? "";
router93.use(async (_req, _res, next) => {
  try {
    await initializeServices();
    next();
  } catch (error) {
    logger_default2.error({ error }, "Failed to initialize AI governance services");
    next(error);
  }
});
router93.post(
  "/policy-suggestions",
  requirePermission2("ai:suggestions:generate"),
  async (req, res) => {
    try {
      const tenantId = getTenantId2(req);
      const context4 = {
        tenantId,
        triggeredBy: "manual",
        focusAreas: req.body.focusAreas,
        complianceFrameworks: req.body.complianceFrameworks,
        timeRange: req.body.timeRange
      };
      const suggestions = await policySuggestionService.generateSuggestions(context4);
      logger_default2.info({
        tenantId,
        userId: getUserId2(req),
        suggestionCount: suggestions.length
      }, "Policy suggestions generated");
      res.json(wrapResponse(suggestions, req));
    } catch (error) {
      handleError(error, res, "generateSuggestions");
    }
  }
);
router93.get(
  "/policy-suggestions",
  requirePermission2("ai:suggestions:read"),
  async (req, res) => {
    try {
      const tenantId = getTenantId2(req);
      const status = req.query.status;
      const limit = parseInt(req.query.limit) || 20;
      const offset = parseInt(req.query.offset) || 0;
      const result2 = await policySuggestionService.listSuggestions(tenantId, {
        status,
        limit,
        offset
      });
      res.json(wrapResponse(result2, req));
    } catch (error) {
      handleError(error, res, "listSuggestions");
    }
  }
);
router93.get(
  "/policy-suggestions/:id",
  requirePermission2("ai:suggestions:read"),
  async (req, res) => {
    try {
      const suggestion = await policySuggestionService.getSuggestion(singleParam24(req.params.id));
      if (!suggestion) {
        return res.status(404).json({
          error: {
            code: "NOT_FOUND",
            message: `Suggestion not found: ${singleParam24(req.params.id)}`
          }
        });
      }
      res.json(wrapResponse(suggestion, req));
    } catch (error) {
      handleError(error, res, "getSuggestion");
    }
  }
);
router93.post(
  "/policy-suggestions/:id/review",
  requirePermission2("ai:suggestions:review"),
  async (req, res) => {
    try {
      const feedback2 = {
        reviewedBy: getUserId2(req),
        reviewedAt: (/* @__PURE__ */ new Date()).toISOString(),
        decision: req.body.decision,
        reason: req.body.reason,
        modifications: req.body.modifications
      };
      const suggestion = await policySuggestionService.reviewSuggestion(
        singleParam24(req.params.id),
        feedback2
      );
      logger_default2.info({
        suggestionId: singleParam24(req.params.id),
        decision: feedback2.decision,
        reviewedBy: feedback2.reviewedBy
      }, "Suggestion reviewed");
      res.json(wrapResponse(suggestion, req));
    } catch (error) {
      handleError(error, res, "reviewSuggestion");
    }
  }
);
router93.post(
  "/policy-suggestions/:id/implement",
  requirePermission2("ai:suggestions:implement"),
  async (req, res) => {
    try {
      const result2 = await policySuggestionService.implementSuggestion(singleParam24(req.params.id));
      logger_default2.info({
        suggestionId: singleParam24(req.params.id),
        policyId: result2.policyId,
        implementedBy: getUserId2(req)
      }, "Suggestion implemented");
      res.json(wrapResponse(result2, req));
    } catch (error) {
      handleError(error, res, "implementSuggestion");
    }
  }
);
router93.get(
  "/policy-suggestions/statistics",
  requirePermission2("ai:suggestions:read"),
  async (req, res) => {
    try {
      const tenantId = getTenantId2(req);
      const statistics = await policySuggestionService.getStatistics(tenantId);
      res.json(wrapResponse(statistics, req));
    } catch (error) {
      handleError(error, res, "getStatistics");
    }
  }
);
router93.post(
  "/verdict-explanations",
  requirePermission2("ai:explanations:generate"),
  async (req, res) => {
    try {
      const verdict = req.body.verdict;
      const context4 = {
        audience: req.body.audience || "end_user",
        tone: req.body.tone || "friendly",
        locale: req.body.locale,
        includeExamples: req.body.includeExamples ?? true,
        maxLength: req.body.maxLength
      };
      const explanation = await verdictExplainerService.explainVerdict(verdict, context4);
      logger_default2.info({
        verdictAction: verdict.action,
        audience: context4.audience,
        userId: getUserId2(req)
      }, "Verdict explained");
      res.json(wrapResponse(explanation, req));
    } catch (error) {
      handleError(error, res, "explainVerdict");
    }
  }
);
router93.post(
  "/verdict-explanations/batch",
  requirePermission2("ai:explanations:generate"),
  async (req, res) => {
    try {
      const verdicts = req.body.verdicts;
      const context4 = {
        audience: req.body.audience || "end_user",
        tone: req.body.tone || "friendly",
        locale: req.body.locale,
        includeExamples: req.body.includeExamples ?? false
      };
      if (verdicts.length > 50) {
        return res.status(400).json({
          error: {
            code: "BATCH_TOO_LARGE",
            message: "Maximum 50 verdicts per batch"
          }
        });
      }
      const explanations = await verdictExplainerService.batchExplain(verdicts, context4);
      logger_default2.info({
        count: explanations.length,
        audience: context4.audience,
        userId: getUserId2(req)
      }, "Batch verdicts explained");
      res.json(wrapResponse(explanations, req));
    } catch (error) {
      handleError(error, res, "batchExplain");
    }
  }
);
router93.post(
  "/anomalies/detect",
  requirePermission2("ai:anomalies:detect"),
  async (req, res) => {
    try {
      const tenantId = getTenantId2(req);
      const scope = {
        tenantIds: [tenantId],
        entityTypes: req.body.entityTypes,
        anomalyTypes: req.body.anomalyTypes,
        minSeverity: req.body.minSeverity,
        timeRange: req.body.timeRange
      };
      const anomalies = await anomalyService.detectAnomalies(scope);
      logger_default2.info({
        tenantId,
        anomalyCount: anomalies.length,
        userId: getUserId2(req)
      }, "Anomalies detected");
      res.json(wrapResponse(anomalies, req));
    } catch (error) {
      handleError(error, res, "detectAnomalies");
    }
  }
);
router93.get(
  "/anomalies",
  requirePermission2("ai:anomalies:read"),
  async (req, res) => {
    try {
      const tenantId = getTenantId2(req);
      const now = /* @__PURE__ */ new Date();
      const thirtyDaysAgo = new Date(now.getTime() - 30 * 24 * 60 * 60 * 1e3);
      const scope = {
        tenantIds: [tenantId],
        minSeverity: req.query.severity,
        timeRange: {
          start: thirtyDaysAgo.toISOString(),
          end: now.toISOString()
        }
      };
      const anomalies = await anomalyService.detectAnomalies(scope);
      const filteredAnomalies = req.query.status ? anomalies.filter((a) => a.status === req.query.status) : anomalies;
      const offset = parseInt(req.query.offset) || 0;
      const limit = parseInt(req.query.limit) || 20;
      const paginated = filteredAnomalies.slice(offset, offset + limit);
      res.json(wrapResponse({
        anomalies: paginated,
        total: filteredAnomalies.length
      }, req));
    } catch (error) {
      handleError(error, res, "listAnomalies");
    }
  }
);
router93.get(
  "/anomalies/:id",
  requirePermission2("ai:anomalies:read"),
  async (req, res) => {
    try {
      const anomaly = await anomalyService.getAnomaly(singleParam24(req.params.id));
      if (!anomaly) {
        return res.status(404).json({
          error: {
            code: "NOT_FOUND",
            message: `Anomaly not found: ${singleParam24(req.params.id)}`
          }
        });
      }
      res.json(wrapResponse(anomaly, req));
    } catch (error) {
      handleError(error, res, "getAnomaly");
    }
  }
);
router93.patch(
  "/anomalies/:id/status",
  requirePermission2("ai:anomalies:update"),
  async (req, res) => {
    try {
      const status = req.body.status;
      const notes = req.body.notes;
      const anomaly = await anomalyService.updateAnomalyStatus(
        singleParam24(req.params.id),
        status,
        notes
      );
      logger_default2.info({
        anomalyId: singleParam24(req.params.id),
        newStatus: status,
        updatedBy: getUserId2(req)
      }, "Anomaly status updated");
      res.json(wrapResponse(anomaly, req));
    } catch (error) {
      handleError(error, res, "updateAnomalyStatus");
    }
  }
);
router93.post(
  "/anomalies/:id/resolve",
  requirePermission2("ai:anomalies:resolve"),
  async (req, res) => {
    try {
      const resolution = {
        resolvedBy: getUserId2(req),
        resolvedAt: (/* @__PURE__ */ new Date()).toISOString(),
        resolution: req.body.resolution,
        notes: req.body.notes || "",
        actionsTaken: req.body.actionsTaken || []
      };
      const anomaly = await anomalyService.resolveAnomaly(singleParam24(req.params.id), resolution);
      logger_default2.info({
        anomalyId: singleParam24(req.params.id),
        resolution: resolution.resolution,
        resolvedBy: resolution.resolvedBy
      }, "Anomaly resolved");
      res.json(wrapResponse(anomaly, req));
    } catch (error) {
      handleError(error, res, "resolveAnomaly");
    }
  }
);
router93.get(
  "/anomalies/trends",
  requirePermission2("ai:anomalies:read"),
  async (req, res) => {
    try {
      const tenantId = getTenantId2(req);
      const endDate = req.query.endDate ? new Date(req.query.endDate) : /* @__PURE__ */ new Date();
      const startDate = req.query.startDate ? new Date(req.query.startDate) : new Date(endDate.getTime() - 30 * 24 * 60 * 60 * 1e3);
      const trends = await anomalyService.getAnomalyTrends(tenantId, {
        start: startDate.toISOString(),
        end: endDate.toISOString()
      });
      res.json(wrapResponse(trends, req));
    } catch (error) {
      handleError(error, res, "getAnomalyTrends");
    }
  }
);
router93.get("/health", async (_req, res) => {
  try {
    const status = {
      status: "healthy",
      services: {
        policySuggestions: policySuggestionService ? "initialized" : "not_initialized",
        verdictExplainer: verdictExplainerService ? "initialized" : "not_initialized",
        anomalyDetection: anomalyService ? "initialized" : "not_initialized"
      },
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      version: "4.0.0"
    };
    res.json(status);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      error: error instanceof Error ? error.message : "Unknown error"
    });
  }
});

// src/routes/v4/compliance.ts
import { Router as Router48 } from "express";
import { randomUUID as randomUUID66 } from "crypto";
init_logger2();

// src/compliance/frameworks/HIPAAControls.ts
var HIPAA_FRAMEWORK = {
  id: "HIPAA",
  name: "Health Insurance Portability and Accountability Act",
  version: "2024",
  description: "US federal law protecting sensitive patient health information",
  effectiveDate: "2003-04-14",
  lastUpdated: "2024-01-01",
  jurisdiction: "United States",
  regulatoryBody: "HHS Office for Civil Rights",
  categories: [
    "Administrative Safeguards",
    "Physical Safeguards",
    "Technical Safeguards",
    "Privacy Rule",
    "Breach Notification Rule"
  ]
};
var PHI_IDENTIFIERS = [
  { id: "name", description: "Names", regex: /\b[A-Z][a-z]+ [A-Z][a-z]+\b/ },
  { id: "address", description: "Geographic data smaller than state", regex: /\b\d{5}(-\d{4})?\b/ },
  { id: "dates", description: "Dates related to individual (DOB, admission, discharge, death)", regex: /\b\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4}\b/ },
  { id: "phone", description: "Telephone numbers", regex: /\b\d{3}[-.]?\d{3}[-.]?\d{4}\b/ },
  { id: "fax", description: "Fax numbers", regex: /\b\d{3}[-.]?\d{3}[-.]?\d{4}\b/ },
  { id: "email", description: "Email addresses", regex: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/ },
  { id: "ssn", description: "Social Security numbers", regex: /\b\d{3}-\d{2}-\d{4}\b/ },
  { id: "mrn", description: "Medical record numbers", regex: /\bMRN[:\s]?\d{6,}\b/i },
  { id: "beneficiary", description: "Health plan beneficiary numbers", regex: /\b[A-Z]{3}\d{9}\b/ },
  { id: "account", description: "Account numbers", regex: /\bACCT[:\s]?\d{6,}\b/i },
  { id: "certificate", description: "Certificate/license numbers", regex: /\bLIC[:\s]?[A-Z0-9]{6,}\b/i },
  { id: "vehicle", description: "Vehicle identifiers and serial numbers", regex: /\bVIN[:\s]?[A-Z0-9]{17}\b/i },
  { id: "device", description: "Device identifiers and serial numbers", regex: /\bDEVICE[:\s]?[A-Z0-9]{8,}\b/i },
  { id: "url", description: "Web URLs", regex: /https?:\/\/[^\s]+/ },
  { id: "ip", description: "IP addresses", regex: /\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b/ },
  { id: "biometric", description: "Biometric identifiers (fingerprints, voice)", regex: null },
  { id: "photo", description: "Full-face photographs and comparable images", regex: null },
  { id: "unique", description: "Any other unique identifying number or code", regex: null }
];
var ADMINISTRATIVE_SAFEGUARDS = [
  // Security Management Process
  {
    id: "HIPAA-AS-001",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Management Process",
    name: "Risk Analysis",
    description: "Conduct an accurate and thorough assessment of the potential risks and vulnerabilities to the confidentiality, integrity, and availability of electronic protected health information.",
    requirement: "45 CFR \xA7 164.308(a)(1)(ii)(A)",
    automatable: true,
    frequency: "annual",
    evidenceTypes: ["risk_assessment", "vulnerability_scan", "audit_report"],
    implementationGuidance: "Perform comprehensive risk analysis including asset inventory, threat identification, vulnerability assessment, and risk determination.",
    summitMapping: {
      governanceControls: ["risk-assessment-policy"],
      provenanceRequirements: ["risk-analysis-chain"],
      dataClassifications: ["PHI", "ePHI"]
    }
  },
  {
    id: "HIPAA-AS-002",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Management Process",
    name: "Risk Management",
    description: "Implement security measures sufficient to reduce risks and vulnerabilities to a reasonable and appropriate level.",
    requirement: "45 CFR \xA7 164.308(a)(1)(ii)(B)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["policy_document", "system_config", "attestation"],
    implementationGuidance: "Implement and document security measures based on risk analysis findings.",
    summitMapping: {
      governanceControls: ["risk-mitigation-policy"],
      provenanceRequirements: ["mitigation-evidence-chain"],
      dataClassifications: ["PHI", "ePHI"]
    }
  },
  {
    id: "HIPAA-AS-003",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Management Process",
    name: "Sanction Policy",
    description: "Apply appropriate sanctions against workforce members who fail to comply with security policies.",
    requirement: "45 CFR \xA7 164.308(a)(1)(ii)(C)",
    automatable: false,
    frequency: "as_needed",
    evidenceTypes: ["policy_document", "hr_record"],
    implementationGuidance: "Document and enforce sanctions for security policy violations.",
    summitMapping: {
      governanceControls: ["sanctions-policy"],
      provenanceRequirements: ["hr-action-chain"],
      dataClassifications: ["internal"]
    }
  },
  {
    id: "HIPAA-AS-004",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Management Process",
    name: "Information System Activity Review",
    description: "Implement procedures to regularly review records of information system activity, such as audit logs, access reports, and security incident tracking reports.",
    requirement: "45 CFR \xA7 164.308(a)(1)(ii)(D)",
    automatable: true,
    frequency: "daily",
    evidenceTypes: ["audit_log", "access_report", "security_incident_report"],
    implementationGuidance: "Establish automated log review and alerting procedures.",
    summitMapping: {
      governanceControls: ["audit-review-policy"],
      provenanceRequirements: ["audit-chain"],
      dataClassifications: ["ePHI", "audit"]
    }
  },
  // Assigned Security Responsibility
  {
    id: "HIPAA-AS-005",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Assigned Security Responsibility",
    name: "Security Official",
    description: "Identify the security official who is responsible for the development and implementation of the policies and procedures.",
    requirement: "45 CFR \xA7 164.308(a)(2)",
    automatable: false,
    frequency: "annual",
    evidenceTypes: ["policy_document", "org_chart", "attestation"],
    implementationGuidance: "Designate and document security officer role and responsibilities.",
    summitMapping: {
      governanceControls: ["security-officer-policy"],
      provenanceRequirements: ["designation-chain"],
      dataClassifications: ["internal"]
    }
  },
  // Workforce Security
  {
    id: "HIPAA-AS-006",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Workforce Security",
    name: "Authorization and/or Supervision",
    description: "Implement procedures for the authorization and/or supervision of workforce members who work with ePHI.",
    requirement: "45 CFR \xA7 164.308(a)(3)(ii)(A)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["access_control_log", "authorization_record"],
    implementationGuidance: "Implement role-based access control with approval workflows.",
    summitMapping: {
      governanceControls: ["rbac-policy", "authorization-workflow"],
      provenanceRequirements: ["authorization-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-AS-007",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Workforce Security",
    name: "Workforce Clearance Procedure",
    description: "Implement procedures to determine that the access of a workforce member to ePHI is appropriate.",
    requirement: "45 CFR \xA7 164.308(a)(3)(ii)(B)",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["background_check", "access_approval"],
    implementationGuidance: "Establish clearance procedures including background checks where appropriate.",
    summitMapping: {
      governanceControls: ["clearance-policy"],
      provenanceRequirements: ["clearance-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-AS-008",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Workforce Security",
    name: "Termination Procedures",
    description: "Implement procedures for terminating access to ePHI when employment ends.",
    requirement: "45 CFR \xA7 164.308(a)(3)(ii)(C)",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["access_revocation_log", "termination_checklist"],
    implementationGuidance: "Automate access revocation upon termination triggers.",
    summitMapping: {
      governanceControls: ["termination-policy"],
      provenanceRequirements: ["deprovisioning-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  // Information Access Management
  {
    id: "HIPAA-AS-009",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Information Access Management",
    name: "Access Authorization",
    description: "Implement policies and procedures for granting access to ePHI.",
    requirement: "45 CFR \xA7 164.308(a)(4)(ii)(B)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["access_policy", "approval_workflow"],
    implementationGuidance: "Implement formal access request and approval process.",
    summitMapping: {
      governanceControls: ["access-authorization-policy"],
      provenanceRequirements: ["access-grant-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-AS-010",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Information Access Management",
    name: "Access Establishment and Modification",
    description: "Implement policies and procedures for establishing, documenting, reviewing, and modifying access.",
    requirement: "45 CFR \xA7 164.308(a)(4)(ii)(C)",
    automatable: true,
    frequency: "quarterly",
    evidenceTypes: ["access_review", "modification_log"],
    implementationGuidance: "Conduct quarterly access reviews and document all modifications.",
    summitMapping: {
      governanceControls: ["access-review-policy"],
      provenanceRequirements: ["access-modification-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  // Security Awareness and Training
  {
    id: "HIPAA-AS-011",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Awareness and Training",
    name: "Security Reminders",
    description: "Implement periodic security updates.",
    requirement: "45 CFR \xA7 164.308(a)(5)(ii)(A)",
    automatable: true,
    frequency: "monthly",
    evidenceTypes: ["training_record", "communication_log"],
    implementationGuidance: "Send regular security awareness reminders to workforce.",
    summitMapping: {
      governanceControls: ["security-awareness-policy"],
      provenanceRequirements: ["training-chain"],
      dataClassifications: ["internal"]
    }
  },
  {
    id: "HIPAA-AS-012",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Awareness and Training",
    name: "Protection from Malicious Software",
    description: "Implement procedures for guarding against, detecting, and reporting malicious software.",
    requirement: "45 CFR \xA7 164.308(a)(5)(ii)(B)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["antivirus_log", "malware_scan"],
    implementationGuidance: "Deploy and monitor anti-malware solutions.",
    summitMapping: {
      governanceControls: ["malware-protection-policy"],
      provenanceRequirements: ["security-scan-chain"],
      dataClassifications: ["ePHI", "system"]
    }
  },
  {
    id: "HIPAA-AS-013",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Awareness and Training",
    name: "Log-in Monitoring",
    description: "Implement procedures for monitoring log-in attempts and reporting discrepancies.",
    requirement: "45 CFR \xA7 164.308(a)(5)(ii)(C)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["login_log", "alert_record"],
    implementationGuidance: "Implement failed login monitoring and alerting.",
    summitMapping: {
      governanceControls: ["login-monitoring-policy"],
      provenanceRequirements: ["authentication-chain"],
      dataClassifications: ["ePHI", "audit"]
    }
  },
  {
    id: "HIPAA-AS-014",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Awareness and Training",
    name: "Password Management",
    description: "Implement procedures for creating, changing, and safeguarding passwords.",
    requirement: "45 CFR \xA7 164.308(a)(5)(ii)(D)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["password_policy", "compliance_report"],
    implementationGuidance: "Enforce password complexity, rotation, and secure storage.",
    summitMapping: {
      governanceControls: ["password-policy"],
      provenanceRequirements: ["credential-chain"],
      dataClassifications: ["ePHI", "credentials"]
    }
  },
  // Security Incident Procedures
  {
    id: "HIPAA-AS-015",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Security Incident Procedures",
    name: "Response and Reporting",
    description: "Identify and respond to suspected or known security incidents; mitigate harmful effects; document incidents.",
    requirement: "45 CFR \xA7 164.308(a)(6)(ii)",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["incident_report", "response_log", "mitigation_record"],
    implementationGuidance: "Implement incident response procedures with documentation.",
    summitMapping: {
      governanceControls: ["incident-response-policy"],
      provenanceRequirements: ["incident-chain"],
      dataClassifications: ["ePHI", "security"]
    }
  },
  // Contingency Plan
  {
    id: "HIPAA-AS-016",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Contingency Plan",
    name: "Data Backup Plan",
    description: "Establish and implement procedures to create and maintain retrievable exact copies of ePHI.",
    requirement: "45 CFR \xA7 164.308(a)(7)(ii)(A)",
    automatable: true,
    frequency: "daily",
    evidenceTypes: ["backup_log", "restoration_test"],
    implementationGuidance: "Implement automated backup with integrity verification.",
    summitMapping: {
      governanceControls: ["backup-policy"],
      provenanceRequirements: ["backup-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-AS-017",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Contingency Plan",
    name: "Disaster Recovery Plan",
    description: "Establish procedures to restore any loss of data.",
    requirement: "45 CFR \xA7 164.308(a)(7)(ii)(B)",
    automatable: false,
    frequency: "annual",
    evidenceTypes: ["dr_plan", "dr_test_report"],
    implementationGuidance: "Develop and test disaster recovery procedures.",
    summitMapping: {
      governanceControls: ["dr-policy"],
      provenanceRequirements: ["recovery-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-AS-018",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Contingency Plan",
    name: "Emergency Mode Operation Plan",
    description: "Establish procedures to enable continuation of critical business processes.",
    requirement: "45 CFR \xA7 164.308(a)(7)(ii)(C)",
    automatable: false,
    frequency: "annual",
    evidenceTypes: ["emergency_plan", "test_report"],
    implementationGuidance: "Define emergency operating procedures for critical systems.",
    summitMapping: {
      governanceControls: ["emergency-operations-policy"],
      provenanceRequirements: ["emergency-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-AS-019",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Contingency Plan",
    name: "Testing and Revision Procedures",
    description: "Implement procedures for periodic testing and revision of contingency plans.",
    requirement: "45 CFR \xA7 164.308(a)(7)(ii)(D)",
    automatable: true,
    frequency: "annual",
    evidenceTypes: ["test_plan", "test_results", "revision_log"],
    implementationGuidance: "Conduct annual contingency plan testing.",
    summitMapping: {
      governanceControls: ["contingency-testing-policy"],
      provenanceRequirements: ["test-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  // Business Associate Contracts
  {
    id: "HIPAA-AS-020",
    framework: "HIPAA",
    category: "Administrative Safeguards",
    subcategory: "Business Associate Contracts",
    name: "Written Contract or Arrangement",
    description: "Document satisfactory assurances that business associates will appropriately safeguard ePHI.",
    requirement: "45 CFR \xA7 164.308(b)(1)",
    automatable: false,
    frequency: "as_needed",
    evidenceTypes: ["baa_contract", "compliance_attestation"],
    implementationGuidance: "Execute BAAs with all business associates before sharing ePHI.",
    summitMapping: {
      governanceControls: ["baa-policy"],
      provenanceRequirements: ["contract-chain"],
      dataClassifications: ["ePHI", "legal"]
    }
  }
];
var TECHNICAL_SAFEGUARDS = [
  // Access Control
  {
    id: "HIPAA-TS-001",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Access Control",
    name: "Unique User Identification",
    description: "Assign a unique name and/or number for identifying and tracking user identity.",
    requirement: "45 CFR \xA7 164.312(a)(2)(i)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["user_directory", "identity_log"],
    implementationGuidance: "Implement unique user IDs for all system access.",
    summitMapping: {
      governanceControls: ["unique-id-policy"],
      provenanceRequirements: ["identity-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-TS-002",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Access Control",
    name: "Emergency Access Procedure",
    description: "Establish procedures for obtaining necessary ePHI during an emergency.",
    requirement: "45 CFR \xA7 164.312(a)(2)(ii)",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["break_glass_log", "emergency_access_policy"],
    implementationGuidance: "Implement break-glass procedures with full audit trail.",
    summitMapping: {
      governanceControls: ["break-glass-policy"],
      provenanceRequirements: ["emergency-access-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-TS-003",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Access Control",
    name: "Automatic Logoff",
    description: "Implement electronic procedures that terminate an electronic session after a predetermined time of inactivity.",
    requirement: "45 CFR \xA7 164.312(a)(2)(iii)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["session_config", "timeout_log"],
    implementationGuidance: "Configure session timeout (15 minutes recommended).",
    summitMapping: {
      governanceControls: ["session-timeout-policy"],
      provenanceRequirements: ["session-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-TS-004",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Access Control",
    name: "Encryption and Decryption",
    description: "Implement a mechanism to encrypt and decrypt ePHI.",
    requirement: "45 CFR \xA7 164.312(a)(2)(iv)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["encryption_config", "key_management_log"],
    implementationGuidance: "Implement AES-256 encryption for ePHI at rest and in transit.",
    summitMapping: {
      governanceControls: ["encryption-policy"],
      provenanceRequirements: ["encryption-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  // Audit Controls
  {
    id: "HIPAA-TS-005",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Audit Controls",
    name: "Audit Controls",
    description: "Implement hardware, software, and/or procedural mechanisms that record and examine activity in information systems that contain or use ePHI.",
    requirement: "45 CFR \xA7 164.312(b)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["audit_log", "log_review"],
    implementationGuidance: "Implement comprehensive audit logging with tamper-evident storage.",
    summitMapping: {
      governanceControls: ["audit-controls-policy"],
      provenanceRequirements: ["complete-audit-chain"],
      dataClassifications: ["ePHI", "audit"]
    }
  },
  // Integrity
  {
    id: "HIPAA-TS-006",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Integrity",
    name: "Mechanism to Authenticate ePHI",
    description: "Implement electronic mechanisms to corroborate that ePHI has not been altered or destroyed in an unauthorized manner.",
    requirement: "45 CFR \xA7 164.312(c)(2)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["integrity_check", "hash_verification"],
    implementationGuidance: "Implement cryptographic integrity verification (SHA-256).",
    summitMapping: {
      governanceControls: ["integrity-policy"],
      provenanceRequirements: ["integrity-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  // Person or Entity Authentication
  {
    id: "HIPAA-TS-007",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Person or Entity Authentication",
    name: "Person or Entity Authentication",
    description: "Implement procedures to verify that a person or entity seeking access to ePHI is the one claimed.",
    requirement: "45 CFR \xA7 164.312(d)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["auth_log", "mfa_config"],
    implementationGuidance: "Implement multi-factor authentication for ePHI access.",
    summitMapping: {
      governanceControls: ["authentication-policy"],
      provenanceRequirements: ["auth-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  // Transmission Security
  {
    id: "HIPAA-TS-008",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Transmission Security",
    name: "Integrity Controls",
    description: "Implement security measures to ensure that electronically transmitted ePHI is not improperly modified without detection.",
    requirement: "45 CFR \xA7 164.312(e)(2)(i)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["tls_config", "integrity_log"],
    implementationGuidance: "Implement TLS 1.3 with message authentication.",
    summitMapping: {
      governanceControls: ["transmission-integrity-policy"],
      provenanceRequirements: ["transmission-chain"],
      dataClassifications: ["ePHI"]
    }
  },
  {
    id: "HIPAA-TS-009",
    framework: "HIPAA",
    category: "Technical Safeguards",
    subcategory: "Transmission Security",
    name: "Encryption",
    description: "Implement a mechanism to encrypt ePHI whenever deemed appropriate.",
    requirement: "45 CFR \xA7 164.312(e)(2)(ii)",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["encryption_config", "certificate_log"],
    implementationGuidance: "Encrypt all ePHI transmissions using TLS 1.3.",
    summitMapping: {
      governanceControls: ["transmission-encryption-policy"],
      provenanceRequirements: ["encryption-chain"],
      dataClassifications: ["ePHI"]
    }
  }
];
var BREACH_NOTIFICATION_CONTROLS = [
  {
    id: "HIPAA-BN-001",
    framework: "HIPAA",
    category: "Breach Notification Rule",
    subcategory: "Individual Notification",
    name: "Notification to Individuals",
    description: "Notify affected individuals within 60 days of breach discovery.",
    requirement: "45 CFR \xA7 164.404",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["notification_log", "breach_report"],
    implementationGuidance: "Implement breach detection and notification workflow.",
    summitMapping: {
      governanceControls: ["breach-notification-policy"],
      provenanceRequirements: ["notification-chain"],
      dataClassifications: ["ePHI", "breach"]
    }
  },
  {
    id: "HIPAA-BN-002",
    framework: "HIPAA",
    category: "Breach Notification Rule",
    subcategory: "Media Notification",
    name: "Notification to Media",
    description: "Notify prominent media outlets if breach affects more than 500 residents of a state.",
    requirement: "45 CFR \xA7 164.406",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["media_notification", "breach_assessment"],
    implementationGuidance: "Establish media notification procedures for large breaches.",
    summitMapping: {
      governanceControls: ["media-notification-policy"],
      provenanceRequirements: ["media-chain"],
      dataClassifications: ["ePHI", "breach"]
    }
  },
  {
    id: "HIPAA-BN-003",
    framework: "HIPAA",
    category: "Breach Notification Rule",
    subcategory: "HHS Notification",
    name: "Notification to Secretary",
    description: "Notify HHS of breaches. Breaches affecting 500+ individuals must be reported within 60 days.",
    requirement: "45 CFR \xA7 164.408",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["hhs_report", "breach_log"],
    implementationGuidance: "Implement automated HHS breach reporting.",
    summitMapping: {
      governanceControls: ["hhs-notification-policy"],
      provenanceRequirements: ["hhs-chain"],
      dataClassifications: ["ePHI", "breach"]
    }
  }
];
var ALL_HIPAA_CONTROLS = [
  ...ADMINISTRATIVE_SAFEGUARDS,
  ...TECHNICAL_SAFEGUARDS,
  ...BREACH_NOTIFICATION_CONTROLS
];
var HIPAA_CONTROL_COUNT = {
  administrative: ADMINISTRATIVE_SAFEGUARDS.length,
  technical: TECHNICAL_SAFEGUARDS.length,
  breachNotification: BREACH_NOTIFICATION_CONTROLS.length,
  total: ALL_HIPAA_CONTROLS.length
};
var HIPAAComplianceService = class {
  assessmentHistory = /* @__PURE__ */ new Map();
  /**
   * Get all HIPAA controls
   */
  getControls() {
    return ALL_HIPAA_CONTROLS;
  }
  /**
   * Get a specific control by ID
   */
  getControl(controlId) {
    return ALL_HIPAA_CONTROLS.find((c) => c.id === controlId);
  }
  /**
   * Get controls by category
   */
  getControlsByCategory(category) {
    return ALL_HIPAA_CONTROLS.filter((c) => c.category === category);
  }
  /**
   * Get all PHI identifiers
   */
  getPHIIdentifiers() {
    return PHI_IDENTIFIERS;
  }
  /**
   * Assess a specific control
   */
  async assessControl(controlId, tenantId, evidence) {
    const control = this.getControl(controlId);
    if (!control) {
      throw new Error(`Control not found: ${controlId}`);
    }
    const findings = [];
    const evidenceCollected = Object.keys(evidence);
    let status = "compliant";
    const missingEvidence = control.evidenceTypes.filter(
      (et) => !evidenceCollected.some((ec) => ec.includes(et))
    );
    if (missingEvidence.length > 0) {
      findings.push(`Missing required evidence: ${missingEvidence.join(", ")}`);
      status = missingEvidence.length === control.evidenceTypes.length ? "non_compliant" : "partially_compliant";
    }
    const controlChecks = this.performControlSpecificChecks(control, evidence);
    findings.push(...controlChecks.findings);
    if (controlChecks.issues > 0) {
      status = controlChecks.issues >= 3 ? "non_compliant" : "partially_compliant";
    }
    const remediationRequired = status !== "compliant";
    const remediationSteps = remediationRequired ? this.generateRemediationSteps(control, findings) : void 0;
    return {
      controlId,
      controlName: control.name,
      status,
      findings,
      evidenceCollected,
      remediationRequired,
      remediationSteps,
      assessedAt: (/* @__PURE__ */ new Date()).toISOString(),
      assessedBy: `system:hipaa-compliance-service`
    };
  }
  /**
   * Perform full HIPAA compliance assessment
   */
  async performAssessment(tenantId, options2 = {}) {
    const assessmentId = `hipaa-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    let controls = ALL_HIPAA_CONTROLS;
    if (options2.categories) {
      controls = controls.filter((c) => options2.categories.includes(c.category));
    }
    if (options2.excludeControls) {
      controls = controls.filter((c) => !options2.excludeControls.includes(c.id));
    }
    const controlResults = [];
    for (const control of controls) {
      const result2 = await this.assessControl(control.id, tenantId, {});
      controlResults.push(result2);
    }
    const summary = {
      totalControls: controlResults.length,
      compliant: controlResults.filter((r) => r.status === "compliant").length,
      nonCompliant: controlResults.filter((r) => r.status === "non_compliant").length,
      partiallyCompliant: controlResults.filter((r) => r.status === "partially_compliant").length,
      notApplicable: controlResults.filter((r) => r.status === "not_applicable").length
    };
    let overallStatus;
    if (summary.nonCompliant > 0) {
      overallStatus = "non_compliant";
    } else if (summary.partiallyCompliant > 0) {
      overallStatus = "partially_compliant";
    } else {
      overallStatus = "compliant";
    }
    const report = {
      assessmentId,
      tenantId,
      assessedAt: (/* @__PURE__ */ new Date()).toISOString(),
      overallStatus,
      controlResults,
      summary,
      phiCategories: PHI_IDENTIFIERS.map((p) => p.id),
      remediationPlan: overallStatus !== "compliant" ? this.generateRemediationPlan(controlResults) : void 0
    };
    this.assessmentHistory.set(assessmentId, report);
    return report;
  }
  /**
   * Get assessment history for a tenant
   */
  getAssessmentHistory(tenantId) {
    return Array.from(this.assessmentHistory.values()).filter(
      (r) => r.tenantId === tenantId
    );
  }
  /**
   * Get a specific assessment by ID
   */
  getAssessment(assessmentId) {
    return this.assessmentHistory.get(assessmentId);
  }
  /**
   * Record evidence for a control
   */
  async recordEvidence(tenantId, controlId, evidence) {
    const evidenceId = `evidence-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    return {
      evidenceId,
      recordedAt: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  performControlSpecificChecks(control, evidence) {
    const findings = [];
    let issues = 0;
    switch (control.id) {
      case "HIPAA-TS-004":
        if (!evidence.encryptionEnabled) {
          findings.push("Encryption is not enabled for ePHI at rest");
          issues++;
        }
        if (!evidence.tlsVersion || evidence.tlsVersion < "1.2") {
          findings.push("TLS 1.2 or higher is required for ePHI in transit");
          issues++;
        }
        break;
      case "HIPAA-TS-005":
        if (!evidence.auditLoggingEnabled) {
          findings.push("Audit logging is not enabled");
          issues++;
        }
        break;
      case "HIPAA-AS-001":
        if (!evidence.lastRiskAssessment) {
          findings.push("No risk assessment on record");
          issues++;
        }
        break;
    }
    return { findings, issues };
  }
  generateRemediationSteps(control, findings) {
    const steps = [];
    steps.push(`Review ${control.name} requirements (${control.requirement})`);
    steps.push(`Address the following findings: ${findings.join("; ")}`);
    steps.push(control.implementationGuidance);
    if (control.automatable) {
      steps.push("Consider implementing automated controls to ensure ongoing compliance");
    }
    return steps;
  }
  generateRemediationPlan(results) {
    const nonCompliant = results.filter((r) => r.status === "non_compliant");
    const partiallyCompliant = results.filter((r) => r.status === "partially_compliant");
    const items = [];
    for (const result2 of nonCompliant) {
      items.push({
        controlId: result2.controlId,
        action: `Remediate ${result2.controlName}: ${result2.findings.join("; ")}`,
        deadline: new Date(Date.now() + 30 * 24 * 60 * 60 * 1e3).toISOString()
        // 30 days
      });
    }
    for (const result2 of partiallyCompliant) {
      items.push({
        controlId: result2.controlId,
        action: `Complete ${result2.controlName} implementation: ${result2.findings.join("; ")}`,
        deadline: new Date(Date.now() + 60 * 24 * 60 * 60 * 1e3).toISOString()
        // 60 days
      });
    }
    return {
      priority: nonCompliant.length > 0 ? "high" : "medium",
      items
    };
  }
};
function createHIPAAComplianceService() {
  return new HIPAAComplianceService();
}

// src/compliance/frameworks/SOXControls.ts
var SOX_FRAMEWORK = {
  id: "SOX",
  name: "Sarbanes-Oxley Act",
  version: "2024",
  description: "US federal law mandating financial reporting controls for public companies",
  effectiveDate: "2002-07-30",
  lastUpdated: "2024-01-01",
  jurisdiction: "United States",
  regulatoryBody: "SEC / PCAOB",
  categories: [
    "Section 302 - Management Certification",
    "Section 404 - Internal Control Assessment",
    "Section 409 - Real-Time Disclosure",
    "IT General Controls (ITGC)"
  ]
};
var SECTION_302_CONTROLS = [
  {
    id: "SOX-302-001",
    framework: "SOX",
    category: "Section 302 - Management Certification",
    subcategory: "CEO/CFO Certification",
    name: "Quarterly Certification",
    description: "CEO and CFO must certify quarterly and annual reports are accurate and complete.",
    requirement: "SOX Section 302(a)",
    automatable: false,
    frequency: "quarterly",
    evidenceTypes: ["attestation", "certification_form", "report"],
    implementationGuidance: "Establish certification workflow with evidence collection.",
    summitMapping: {
      governanceControls: ["certification-workflow"],
      provenanceRequirements: ["certification-chain"],
      dataClassifications: ["financial", "internal"]
    }
  },
  {
    id: "SOX-302-002",
    framework: "SOX",
    category: "Section 302 - Management Certification",
    subcategory: "Disclosure Controls",
    name: "Disclosure Controls Effectiveness",
    description: "Management must evaluate disclosure controls and procedures effectiveness.",
    requirement: "SOX Section 302(a)(4)",
    automatable: true,
    frequency: "quarterly",
    evidenceTypes: ["control_test", "effectiveness_report"],
    implementationGuidance: "Implement control testing with automated reporting.",
    summitMapping: {
      governanceControls: ["disclosure-controls-policy"],
      provenanceRequirements: ["control-test-chain"],
      dataClassifications: ["financial", "internal"]
    }
  },
  {
    id: "SOX-302-003",
    framework: "SOX",
    category: "Section 302 - Management Certification",
    subcategory: "Internal Control Disclosure",
    name: "Significant Control Deficiencies",
    description: "Disclose significant deficiencies and material weaknesses to auditors and audit committee.",
    requirement: "SOX Section 302(a)(5)",
    automatable: true,
    frequency: "quarterly",
    evidenceTypes: ["deficiency_report", "committee_minutes", "auditor_communication"],
    implementationGuidance: "Implement deficiency tracking and escalation workflow.",
    summitMapping: {
      governanceControls: ["deficiency-tracking-policy"],
      provenanceRequirements: ["deficiency-chain"],
      dataClassifications: ["financial", "confidential"]
    }
  }
];
var SECTION_404_CONTROLS = [
  {
    id: "SOX-404-001",
    framework: "SOX",
    category: "Section 404 - Internal Control Assessment",
    subcategory: "Management Assessment",
    name: "Internal Control Over Financial Reporting (ICFR)",
    description: "Management must assess and report on the effectiveness of internal control over financial reporting.",
    requirement: "SOX Section 404(a)",
    automatable: true,
    frequency: "annual",
    evidenceTypes: ["icfr_assessment", "control_matrix", "test_results"],
    implementationGuidance: "Implement ICFR assessment framework with control testing.",
    summitMapping: {
      governanceControls: ["icfr-policy"],
      provenanceRequirements: ["icfr-chain"],
      dataClassifications: ["financial", "internal"]
    }
  },
  {
    id: "SOX-404-002",
    framework: "SOX",
    category: "Section 404 - Internal Control Assessment",
    subcategory: "Auditor Attestation",
    name: "External Auditor Attestation",
    description: "External auditor must attest to and report on management's assessment of ICFR.",
    requirement: "SOX Section 404(b)",
    automatable: false,
    frequency: "annual",
    evidenceTypes: ["auditor_report", "attestation"],
    implementationGuidance: "Provide auditor access to control evidence.",
    summitMapping: {
      governanceControls: ["auditor-access-policy"],
      provenanceRequirements: ["audit-chain"],
      dataClassifications: ["financial", "confidential"]
    }
  },
  {
    id: "SOX-404-003",
    framework: "SOX",
    category: "Section 404 - Internal Control Assessment",
    subcategory: "Control Testing",
    name: "Control Testing Program",
    description: "Establish testing program to validate control design and operating effectiveness.",
    requirement: "PCAOB AS 2201",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["test_plan", "test_results", "exception_log"],
    implementationGuidance: "Implement continuous control testing with exception tracking.",
    summitMapping: {
      governanceControls: ["control-testing-policy"],
      provenanceRequirements: ["testing-chain"],
      dataClassifications: ["financial", "internal"]
    }
  },
  {
    id: "SOX-404-004",
    framework: "SOX",
    category: "Section 404 - Internal Control Assessment",
    subcategory: "Material Weakness",
    name: "Material Weakness Identification",
    description: "Identify, classify, and remediate control deficiencies.",
    requirement: "PCAOB AS 2201",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["deficiency_classification", "remediation_plan", "status_report"],
    implementationGuidance: "Implement deficiency classification (Deficiency > Significant Deficiency > Material Weakness).",
    summitMapping: {
      governanceControls: ["material-weakness-policy"],
      provenanceRequirements: ["remediation-chain"],
      dataClassifications: ["financial", "confidential"]
    }
  }
];
var ITGC_ACCESS_CONTROLS = [
  // Logical Access
  {
    id: "SOX-ITGC-LA-001",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Logical Access",
    name: "User Access Provisioning",
    description: "Formal process to grant, modify, and revoke user access based on job responsibilities.",
    requirement: "ITGC Logical Access",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["access_request", "approval_log", "provisioning_log"],
    implementationGuidance: "Implement automated provisioning with approval workflow.",
    summitMapping: {
      governanceControls: ["access-provisioning-policy"],
      provenanceRequirements: ["provisioning-chain"],
      dataClassifications: ["financial", "system"]
    }
  },
  {
    id: "SOX-ITGC-LA-002",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Logical Access",
    name: "Periodic Access Reviews",
    description: "Regular review of user access rights to ensure appropriateness.",
    requirement: "ITGC Logical Access",
    automatable: true,
    frequency: "quarterly",
    evidenceTypes: ["access_review", "certification_log"],
    implementationGuidance: "Conduct quarterly access certification campaigns.",
    summitMapping: {
      governanceControls: ["access-review-policy"],
      provenanceRequirements: ["review-chain"],
      dataClassifications: ["financial", "system"]
    }
  },
  {
    id: "SOX-ITGC-LA-003",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Logical Access",
    name: "Privileged Access Management",
    description: "Controls over privileged/administrative access to financial systems.",
    requirement: "ITGC Logical Access",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["pam_log", "session_recording", "approval_workflow"],
    implementationGuidance: "Implement PAM solution with session monitoring.",
    summitMapping: {
      governanceControls: ["pam-policy"],
      provenanceRequirements: ["pam-chain"],
      dataClassifications: ["financial", "privileged"]
    }
  },
  {
    id: "SOX-ITGC-LA-004",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Logical Access",
    name: "Segregation of Duties",
    description: "Ensure proper separation of duties to prevent fraud and errors.",
    requirement: "ITGC Logical Access",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["sod_matrix", "violation_log", "exception_approval"],
    implementationGuidance: "Implement SoD conflict detection and prevention.",
    summitMapping: {
      governanceControls: ["sod-policy"],
      provenanceRequirements: ["sod-chain"],
      dataClassifications: ["financial", "internal"]
    }
  },
  {
    id: "SOX-ITGC-LA-005",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Logical Access",
    name: "Termination Processing",
    description: "Timely removal of access upon employee termination.",
    requirement: "ITGC Logical Access",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["termination_log", "access_removal_log"],
    implementationGuidance: "Automate access revocation triggered by HR systems.",
    summitMapping: {
      governanceControls: ["termination-policy"],
      provenanceRequirements: ["deprovisioning-chain"],
      dataClassifications: ["financial", "hr"]
    }
  }
];
var ITGC_CHANGE_MANAGEMENT = [
  {
    id: "SOX-ITGC-CM-001",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Change Management",
    name: "Change Request Process",
    description: "Formal process for requesting, evaluating, and approving changes.",
    requirement: "ITGC Change Management",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["change_request", "approval_log", "impact_assessment"],
    implementationGuidance: "Implement change management workflow with approval gates.",
    summitMapping: {
      governanceControls: ["change-request-policy"],
      provenanceRequirements: ["change-chain"],
      dataClassifications: ["financial", "system"]
    }
  },
  {
    id: "SOX-ITGC-CM-002",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Change Management",
    name: "Testing and Validation",
    description: "Changes must be tested before promotion to production.",
    requirement: "ITGC Change Management",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["test_results", "validation_report", "sign_off"],
    implementationGuidance: "Implement automated testing in CI/CD pipeline.",
    summitMapping: {
      governanceControls: ["testing-policy"],
      provenanceRequirements: ["testing-chain"],
      dataClassifications: ["financial", "system"]
    }
  },
  {
    id: "SOX-ITGC-CM-003",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Change Management",
    name: "Segregation of Development and Production",
    description: "Developers should not have direct access to production systems.",
    requirement: "ITGC Change Management",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["environment_config", "access_review", "deployment_log"],
    implementationGuidance: "Implement environment separation with controlled deployment.",
    summitMapping: {
      governanceControls: ["environment-separation-policy"],
      provenanceRequirements: ["deployment-chain"],
      dataClassifications: ["financial", "system"]
    }
  },
  {
    id: "SOX-ITGC-CM-004",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Change Management",
    name: "Emergency Change Process",
    description: "Documented process for emergency changes with retroactive approval.",
    requirement: "ITGC Change Management",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["emergency_change_log", "retroactive_approval", "root_cause"],
    implementationGuidance: "Implement emergency change workflow with post-hoc review.",
    summitMapping: {
      governanceControls: ["emergency-change-policy"],
      provenanceRequirements: ["emergency-chain"],
      dataClassifications: ["financial", "system"]
    }
  }
];
var ITGC_OPERATIONS = [
  {
    id: "SOX-ITGC-OP-001",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Computer Operations",
    name: "Job Scheduling",
    description: "Batch jobs and scheduled processes must complete successfully.",
    requirement: "ITGC Computer Operations",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["job_log", "completion_report", "failure_alert"],
    implementationGuidance: "Implement job monitoring with alerting.",
    summitMapping: {
      governanceControls: ["job-scheduling-policy"],
      provenanceRequirements: ["job-chain"],
      dataClassifications: ["financial", "operational"]
    }
  },
  {
    id: "SOX-ITGC-OP-002",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Computer Operations",
    name: "Data Backup and Recovery",
    description: "Regular backups with tested recovery procedures.",
    requirement: "ITGC Computer Operations",
    automatable: true,
    frequency: "daily",
    evidenceTypes: ["backup_log", "recovery_test", "offsite_confirmation"],
    implementationGuidance: "Implement automated backup with quarterly recovery testing.",
    summitMapping: {
      governanceControls: ["backup-policy"],
      provenanceRequirements: ["backup-chain"],
      dataClassifications: ["financial", "operational"]
    }
  },
  {
    id: "SOX-ITGC-OP-003",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Computer Operations",
    name: "Incident Management",
    description: "Process for identifying, logging, and resolving IT incidents.",
    requirement: "ITGC Computer Operations",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["incident_log", "resolution_record", "root_cause_analysis"],
    implementationGuidance: "Implement incident management system with SLA tracking.",
    summitMapping: {
      governanceControls: ["incident-management-policy"],
      provenanceRequirements: ["incident-chain"],
      dataClassifications: ["financial", "operational"]
    }
  },
  {
    id: "SOX-ITGC-OP-004",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Computer Operations",
    name: "Problem Management",
    description: "Process for identifying root causes and preventing recurrence.",
    requirement: "ITGC Computer Operations",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["problem_record", "known_error_database", "prevention_measure"],
    implementationGuidance: "Implement problem management with trend analysis.",
    summitMapping: {
      governanceControls: ["problem-management-policy"],
      provenanceRequirements: ["problem-chain"],
      dataClassifications: ["financial", "operational"]
    }
  }
];
var ITGC_PROGRAM_DEVELOPMENT = [
  {
    id: "SOX-ITGC-PD-001",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Program Development",
    name: "SDLC Methodology",
    description: "Formal software development lifecycle with documented phases.",
    requirement: "ITGC Program Development",
    automatable: false,
    frequency: "continuous",
    evidenceTypes: ["sdlc_documentation", "phase_gate_approval", "project_artifacts"],
    implementationGuidance: "Document SDLC phases with required deliverables.",
    summitMapping: {
      governanceControls: ["sdlc-policy"],
      provenanceRequirements: ["development-chain"],
      dataClassifications: ["financial", "development"]
    }
  },
  {
    id: "SOX-ITGC-PD-002",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Program Development",
    name: "Requirements Documentation",
    description: "Business requirements must be documented and approved.",
    requirement: "ITGC Program Development",
    automatable: false,
    frequency: "as_needed",
    evidenceTypes: ["requirements_document", "approval_log", "stakeholder_sign_off"],
    implementationGuidance: "Implement requirements management with traceability.",
    summitMapping: {
      governanceControls: ["requirements-policy"],
      provenanceRequirements: ["requirements-chain"],
      dataClassifications: ["financial", "development"]
    }
  },
  {
    id: "SOX-ITGC-PD-003",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Program Development",
    name: "User Acceptance Testing",
    description: "Business users must validate system meets requirements.",
    requirement: "ITGC Program Development",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["uat_plan", "test_results", "sign_off"],
    implementationGuidance: "Implement UAT workflow with formal sign-off.",
    summitMapping: {
      governanceControls: ["uat-policy"],
      provenanceRequirements: ["uat-chain"],
      dataClassifications: ["financial", "development"]
    }
  },
  {
    id: "SOX-ITGC-PD-004",
    framework: "SOX",
    category: "IT General Controls (ITGC)",
    subcategory: "Program Development",
    name: "Code Review",
    description: "Code changes must be reviewed before deployment.",
    requirement: "ITGC Program Development",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["code_review_log", "approval_record", "merge_request"],
    implementationGuidance: "Implement mandatory code review in version control.",
    summitMapping: {
      governanceControls: ["code-review-policy"],
      provenanceRequirements: ["review-chain"],
      dataClassifications: ["financial", "development"]
    }
  }
];
var SECTION_409_CONTROLS = [
  {
    id: "SOX-409-001",
    framework: "SOX",
    category: "Section 409 - Real-Time Disclosure",
    subcategory: "Material Event Detection",
    name: "Material Event Identification",
    description: "Process to identify material events requiring rapid disclosure.",
    requirement: "SOX Section 409",
    automatable: true,
    frequency: "continuous",
    evidenceTypes: ["event_log", "materiality_assessment", "disclosure_decision"],
    implementationGuidance: "Implement material event detection with threshold alerts.",
    summitMapping: {
      governanceControls: ["material-event-policy"],
      provenanceRequirements: ["event-chain"],
      dataClassifications: ["financial", "material"]
    }
  },
  {
    id: "SOX-409-002",
    framework: "SOX",
    category: "Section 409 - Real-Time Disclosure",
    subcategory: "Rapid Disclosure",
    name: "8-K Filing Process",
    description: "Timely filing of 8-K reports for material events.",
    requirement: "SOX Section 409",
    automatable: true,
    frequency: "as_needed",
    evidenceTypes: ["filing_record", "deadline_tracking", "approval_log"],
    implementationGuidance: "Implement 8-K filing workflow with deadline tracking.",
    summitMapping: {
      governanceControls: ["8k-filing-policy"],
      provenanceRequirements: ["filing-chain"],
      dataClassifications: ["financial", "public"]
    }
  }
];
var ALL_SOX_CONTROLS = [
  ...SECTION_302_CONTROLS,
  ...SECTION_404_CONTROLS,
  ...ITGC_ACCESS_CONTROLS,
  ...ITGC_CHANGE_MANAGEMENT,
  ...ITGC_OPERATIONS,
  ...ITGC_PROGRAM_DEVELOPMENT,
  ...SECTION_409_CONTROLS
];
var SOX_CONTROL_COUNT = {
  section302: SECTION_302_CONTROLS.length,
  section404: SECTION_404_CONTROLS.length,
  itgcAccess: ITGC_ACCESS_CONTROLS.length,
  itgcChange: ITGC_CHANGE_MANAGEMENT.length,
  itgcOperations: ITGC_OPERATIONS.length,
  itgcProgram: ITGC_PROGRAM_DEVELOPMENT.length,
  section409: SECTION_409_CONTROLS.length,
  total: ALL_SOX_CONTROLS.length
};
var ITGC_DOMAINS = [
  {
    id: "logical_access",
    name: "Logical Access",
    description: "Controls over user access to systems and data",
    controls: ITGC_ACCESS_CONTROLS.map((c) => c.id)
  },
  {
    id: "change_management",
    name: "Change Management",
    description: "Controls over changes to systems and applications",
    controls: ITGC_CHANGE_MANAGEMENT.map((c) => c.id)
  },
  {
    id: "computer_operations",
    name: "Computer Operations",
    description: "Controls over IT operations and infrastructure",
    controls: ITGC_OPERATIONS.map((c) => c.id)
  },
  {
    id: "program_development",
    name: "Program Development",
    description: "Controls over software development lifecycle",
    controls: ITGC_PROGRAM_DEVELOPMENT.map((c) => c.id)
  }
];
var SOXComplianceService = class {
  assessmentHistory = /* @__PURE__ */ new Map();
  /**
   * Get all SOX controls
   */
  getControls() {
    return ALL_SOX_CONTROLS;
  }
  /**
   * Get a specific control by ID
   */
  getControl(controlId) {
    return ALL_SOX_CONTROLS.find((c) => c.id === controlId);
  }
  /**
   * Get controls by category
   */
  getControlsByCategory(category) {
    return ALL_SOX_CONTROLS.filter((c) => c.category === category);
  }
  /**
   * Get all ITGC domains
   */
  getITGCDomains() {
    return ITGC_DOMAINS;
  }
  /**
   * Get controls for a specific ITGC domain
   */
  getControlsByDomain(domainId) {
    const domain = ITGC_DOMAINS.find((d) => d.id === domainId);
    if (!domain) {
      return [];
    }
    return ALL_SOX_CONTROLS.filter((c) => domain.controls.includes(c.id));
  }
  /**
   * Assess a specific control
   */
  async assessControl(controlId, tenantId, evidence) {
    const control = this.getControl(controlId);
    if (!control) {
      throw new Error(`Control not found: ${controlId}`);
    }
    const findings = [];
    const evidenceCollected = Object.keys(evidence);
    let status = "effective";
    let deficiencyLevel;
    const missingEvidence = control.evidenceTypes.filter(
      (et) => !evidenceCollected.some((ec) => ec.includes(et))
    );
    if (missingEvidence.length > 0) {
      findings.push(`Missing required evidence: ${missingEvidence.join(", ")}`);
      status = missingEvidence.length === control.evidenceTypes.length ? "ineffective" : "needs_improvement";
    }
    const controlChecks = this.performControlSpecificChecks(control, evidence);
    findings.push(...controlChecks.findings);
    if (controlChecks.issues > 0) {
      if (controlChecks.issues >= 3) {
        status = "ineffective";
        deficiencyLevel = controlChecks.critical ? "material_weakness" : "significant_deficiency";
      } else {
        status = "needs_improvement";
        deficiencyLevel = "deficiency";
      }
    }
    const remediationRequired = status !== "effective";
    const remediationSteps = remediationRequired ? this.generateRemediationSteps(control, findings) : void 0;
    return {
      controlId,
      controlName: control.name,
      status,
      findings,
      evidenceCollected,
      deficiencyLevel,
      remediationRequired,
      remediationSteps,
      assessedAt: (/* @__PURE__ */ new Date()).toISOString(),
      assessedBy: "system:sox-compliance-service"
    };
  }
  /**
   * Perform full SOX compliance assessment
   */
  async performAssessment(tenantId, options2 = {}) {
    const assessmentId = `sox-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    let controls = ALL_SOX_CONTROLS;
    if (options2.itgcDomainsOnly) {
      controls = [
        ...ITGC_ACCESS_CONTROLS,
        ...ITGC_CHANGE_MANAGEMENT,
        ...ITGC_OPERATIONS,
        ...ITGC_PROGRAM_DEVELOPMENT
      ];
    }
    if (options2.categories) {
      controls = controls.filter((c) => options2.categories.includes(c.category));
    }
    if (options2.excludeControls) {
      controls = controls.filter((c) => !options2.excludeControls.includes(c.id));
    }
    const controlResults = [];
    for (const control of controls) {
      const result2 = await this.assessControl(control.id, tenantId, {});
      controlResults.push(result2);
    }
    const summary = {
      totalControls: controlResults.length,
      effective: controlResults.filter((r) => r.status === "effective").length,
      ineffective: controlResults.filter((r) => r.status === "ineffective").length,
      needsImprovement: controlResults.filter((r) => r.status === "needs_improvement").length,
      notTested: controlResults.filter((r) => r.status === "not_tested").length,
      materialWeaknesses: controlResults.filter((r) => r.deficiencyLevel === "material_weakness").length,
      significantDeficiencies: controlResults.filter((r) => r.deficiencyLevel === "significant_deficiency").length
    };
    const itgcDomainResults = ITGC_DOMAINS.map((domain) => {
      const domainResults = controlResults.filter((r) => domain.controls.includes(r.controlId));
      const effective = domainResults.filter((r) => r.status === "effective").length;
      const total = domainResults.length;
      let status;
      if (effective === total) {
        status = "effective";
      } else if (effective === 0) {
        status = "ineffective";
      } else {
        status = "partially_effective";
      }
      return {
        domainId: domain.id,
        domainName: domain.name,
        effectiveControls: effective,
        totalControls: total,
        status
      };
    });
    let overallOpinion;
    if (summary.materialWeaknesses > 0) {
      overallOpinion = "ineffective";
    } else if (summary.significantDeficiencies > 0 || summary.ineffective > 0) {
      overallOpinion = "qualified";
    } else {
      overallOpinion = "effective";
    }
    const now = /* @__PURE__ */ new Date();
    const assessmentPeriod = options2.assessmentPeriod || {
      start: new Date(now.getFullYear(), 0, 1).toISOString(),
      end: now.toISOString()
    };
    const report = {
      assessmentId,
      tenantId,
      assessmentPeriod,
      assessedAt: now.toISOString(),
      overallOpinion,
      controlResults,
      summary,
      itgcDomainResults,
      remediationPlan: overallOpinion !== "effective" ? this.generateRemediationPlan(controlResults) : void 0
    };
    this.assessmentHistory.set(assessmentId, report);
    return report;
  }
  /**
   * Get assessment history for a tenant
   */
  getAssessmentHistory(tenantId) {
    return Array.from(this.assessmentHistory.values()).filter(
      (r) => r.tenantId === tenantId
    );
  }
  /**
   * Get a specific assessment by ID
   */
  getAssessment(assessmentId) {
    return this.assessmentHistory.get(assessmentId);
  }
  /**
   * Record management assertion for ICFR
   */
  async recordManagementAssertion(assessmentId, assertion) {
    const report = this.assessmentHistory.get(assessmentId);
    if (!report) {
      throw new Error(`Assessment not found: ${assessmentId}`);
    }
    const updated = {
      ...report,
      managementAssertions: {
        icfrEffective: assertion.icfrEffective,
        signedBy: assertion.signedBy,
        signedAt: (/* @__PURE__ */ new Date()).toISOString()
      }
    };
    this.assessmentHistory.set(assessmentId, updated);
    return updated;
  }
  /**
   * Record evidence for a control
   */
  async recordEvidence(tenantId, controlId, evidence) {
    const evidenceId = `evidence-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    return {
      evidenceId,
      recordedAt: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  performControlSpecificChecks(control, evidence) {
    const findings = [];
    let issues = 0;
    let critical = false;
    switch (control.id) {
      case "SOX-ITGC-LA-003":
        if (!evidence.pamEnabled) {
          findings.push("Privileged Access Management not implemented");
          issues++;
          critical = true;
        }
        break;
      case "SOX-ITGC-LA-004":
        if (!evidence.sodMatrixDefined) {
          findings.push("Segregation of Duties matrix not defined");
          issues++;
        }
        if (evidence.sodViolations && evidence.sodViolations > 0) {
          findings.push(`Active SoD violations detected: ${evidence.sodViolations}`);
          issues++;
          critical = true;
        }
        break;
      case "SOX-ITGC-CM-003":
        if (!evidence.environmentSeparation) {
          findings.push("Development and production environments not properly separated");
          issues++;
          critical = true;
        }
        break;
      case "SOX-404-001":
        if (!evidence.icfrAssessmentCompleted) {
          findings.push("ICFR assessment not completed for current period");
          issues++;
          critical = true;
        }
        break;
    }
    return { findings, issues, critical };
  }
  generateRemediationSteps(control, findings) {
    const steps = [];
    steps.push(`Review ${control.name} requirements (${control.requirement})`);
    steps.push(`Address findings: ${findings.join("; ")}`);
    steps.push(control.implementationGuidance);
    if (control.automatable) {
      steps.push("Implement automated controls for continuous compliance monitoring");
    }
    return steps;
  }
  generateRemediationPlan(results) {
    const materialWeaknesses = results.filter((r) => r.deficiencyLevel === "material_weakness");
    const significantDeficiencies = results.filter((r) => r.deficiencyLevel === "significant_deficiency");
    const deficiencies = results.filter((r) => r.deficiencyLevel === "deficiency");
    const items = [];
    for (const result2 of materialWeaknesses) {
      items.push({
        controlId: result2.controlId,
        action: `CRITICAL: Remediate material weakness in ${result2.controlName}`,
        deadline: new Date(Date.now() + 30 * 24 * 60 * 60 * 1e3).toISOString(),
        responsible: "CFO/CIO"
      });
    }
    for (const result2 of significantDeficiencies) {
      items.push({
        controlId: result2.controlId,
        action: `HIGH: Remediate significant deficiency in ${result2.controlName}`,
        deadline: new Date(Date.now() + 60 * 24 * 60 * 60 * 1e3).toISOString(),
        responsible: "Control Owner"
      });
    }
    for (const result2 of deficiencies) {
      items.push({
        controlId: result2.controlId,
        action: `Improve ${result2.controlName}: ${result2.findings.join("; ")}`,
        deadline: new Date(Date.now() + 90 * 24 * 60 * 60 * 1e3).toISOString()
      });
    }
    return {
      priority: materialWeaknesses.length > 0 ? "high" : significantDeficiencies.length > 0 ? "medium" : "low",
      items
    };
  }
};
function createSOXComplianceService() {
  return new SOXComplianceService();
}

// src/routes/v4/compliance.ts
var hipaaService = null;
var soxService = null;
var initializeServices2 = async () => {
  if (!hipaaService) {
    hipaaService = createHIPAAComplianceService();
    soxService = createSOXComplianceService();
    logger_default2.info("Compliance services initialized");
  }
};
var getTenantId3 = (req) => {
  return req.tenantId || req.user?.tenantId || "default";
};
var getUserId3 = (req) => {
  return req.user?.id || req.user?.id || "anonymous";
};
var wrapResponse2 = (data, req) => {
  return {
    data,
    metadata: {
      requestId: req.correlationId || randomUUID66(),
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      version: "4.1.0"
    },
    governance: {
      action: "ALLOW",
      reasons: ["Compliance data access authorized"],
      policyIds: ["compliance-v4"],
      metadata: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        evaluator: "compliance-router",
        latencyMs: 0,
        simulation: false
      },
      provenance: {
        origin: "compliance-api-v4",
        confidence: 1
      }
    }
  };
};
var router94 = Router48();
var singleParam25 = (value) => Array.isArray(value) ? value[0] : value ?? "";
router94.use(async (_req, _res, next) => {
  try {
    await initializeServices2();
    next();
  } catch (error) {
    logger_default2.error({ error }, "Failed to initialize compliance services");
    next(error);
  }
});
router94.get(
  "/frameworks",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const frameworks = [
      {
        ...HIPAA_FRAMEWORK,
        controlCount: ALL_HIPAA_CONTROLS.length,
        enabled: true
      },
      {
        ...SOX_FRAMEWORK,
        controlCount: ALL_SOX_CONTROLS.length,
        enabled: true
      }
    ];
    res.json(wrapResponse2(frameworks, req));
  }
);
router94.get(
  "/hipaa/controls",
  requirePermission2("compliance:read"),
  async (req, res) => {
    let controls = [...ALL_HIPAA_CONTROLS];
    if (req.query.category) {
      controls = controls.filter((c) => c.category === req.query.category);
    }
    if (req.query.automatable !== void 0) {
      const automatable = req.query.automatable === "true";
      controls = controls.filter((c) => c.automatable === automatable);
    }
    res.json(wrapResponse2({
      controls,
      total: controls.length,
      categories: HIPAA_FRAMEWORK.categories
    }, req));
  }
);
router94.get(
  "/hipaa/controls/:id",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const control = ALL_HIPAA_CONTROLS.find((c) => c.id === singleParam25(req.params.id));
    if (!control) {
      return res.status(404).json({
        error: {
          code: "NOT_FOUND",
          message: `HIPAA control not found: ${singleParam25(req.params.id)}`
        }
      });
    }
    res.json(wrapResponse2(control, req));
  }
);
router94.get(
  "/hipaa/phi-identifiers",
  requirePermission2("compliance:read"),
  async (req, res) => {
    res.json(wrapResponse2({
      identifiers: PHI_IDENTIFIERS,
      total: PHI_IDENTIFIERS.length,
      description: "The 18 HIPAA-defined identifiers that constitute Protected Health Information (PHI)"
    }, req));
  }
);
router94.post(
  "/hipaa/assess",
  requirePermission2("compliance:assess"),
  async (req, res) => {
    const tenantId = getTenantId3(req);
    const { categories } = req.body;
    const assessment = await hipaaService.performAssessment(tenantId, { categories });
    logger_default2.info(
      {
        tenantId,
        framework: "HIPAA",
        score: assessment.summary.compliant / assessment.summary.totalControls * 100,
        assessedBy: getUserId3(req)
      },
      "HIPAA assessment completed"
    );
    res.json(wrapResponse2(assessment, req));
  }
);
router94.get(
  "/hipaa/history",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const tenantId = getTenantId3(req);
    const history = await hipaaService.getAssessmentHistory(tenantId);
    res.json(wrapResponse2(history, req));
  }
);
router94.get(
  "/hipaa/assessments/:id",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const assessment = await hipaaService.getAssessment(singleParam25(req.params.id));
    if (!assessment) {
      return res.status(404).json({ error: "Assessment not found" });
    }
    res.json(wrapResponse2(assessment, req));
  }
);
router94.post(
  "/hipaa/evidence",
  requirePermission2("compliance:evidence:submit"),
  async (req, res) => {
    const control = ALL_HIPAA_CONTROLS.find((c) => c.id === req.body.controlId);
    if (!control) {
      return res.status(400).json({
        error: {
          code: "INVALID_CONTROL",
          message: `Unknown HIPAA control: ${req.body.controlId}`
        }
      });
    }
    const evidence = {
      id: `evidence-${randomUUID66()}`,
      controlId: req.body.controlId,
      framework: "HIPAA",
      type: req.body.type,
      description: req.body.description,
      collectedAt: (/* @__PURE__ */ new Date()).toISOString(),
      collectedBy: getUserId3(req),
      expiresAt: new Date(Date.now() + 365 * 24 * 60 * 60 * 1e3).toISOString(),
      status: "valid",
      attachments: req.body.attachments || []
    };
    logger_default2.info({
      evidenceId: evidence.id,
      controlId: evidence.controlId,
      framework: "HIPAA",
      submittedBy: evidence.collectedBy
    }, "HIPAA evidence submitted");
    res.status(201).json(wrapResponse2(evidence, req));
  }
);
router94.get(
  "/sox/controls",
  requirePermission2("compliance:read"),
  async (req, res) => {
    let controls = [...ALL_SOX_CONTROLS];
    if (req.query.category) {
      controls = controls.filter((c) => c.category === req.query.category);
    }
    if (req.query.domain) {
      controls = controls.filter((c) => c.subcategory === req.query.domain);
    }
    res.json(wrapResponse2({
      controls,
      total: controls.length,
      categories: SOX_FRAMEWORK.categories,
      itgcDomains: ITGC_DOMAINS
    }, req));
  }
);
router94.get(
  "/sox/controls/:id",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const control = ALL_SOX_CONTROLS.find((c) => c.id === singleParam25(req.params.id));
    if (!control) {
      return res.status(404).json({
        error: {
          code: "NOT_FOUND",
          message: `SOX control not found: ${singleParam25(req.params.id)}`
        }
      });
    }
    res.json(wrapResponse2(control, req));
  }
);
router94.get(
  "/sox/itgc-domains",
  requirePermission2("compliance:read"),
  async (req, res) => {
    res.json(wrapResponse2({
      domains: ITGC_DOMAINS,
      description: "IT General Controls (ITGC) domains for SOX compliance"
    }, req));
  }
);
router94.post(
  "/sox/assess",
  requirePermission2("compliance:assess"),
  async (req, res) => {
    const tenantId = getTenantId3(req);
    const sections = Array.isArray(req.body.sections) ? req.body.sections : void 0;
    const categories = Array.isArray(req.body.categories) ? req.body.categories : sections;
    const assessment = await soxService.performAssessment(tenantId, {
      categories
    });
    logger_default2.info(
      {
        tenantId,
        framework: "SOX",
        score: assessment.summary.effective / assessment.summary.totalControls * 100,
        assessedBy: getUserId3(req)
      },
      "SOX assessment completed"
    );
    res.json(wrapResponse2(assessment, req));
  }
);
router94.get(
  "/sox/history",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const tenantId = getTenantId3(req);
    const history = await soxService.getAssessmentHistory(tenantId);
    res.json(wrapResponse2(history, req));
  }
);
router94.get(
  "/sox/assessments/:id",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const assessment = await soxService.getAssessment(singleParam25(req.params.id));
    if (!assessment) {
      return res.status(404).json({ error: "Assessment not found" });
    }
    res.json(wrapResponse2(assessment, req));
  }
);
router94.post(
  "/sox/evidence",
  requirePermission2("compliance:evidence:submit"),
  async (req, res) => {
    const control = ALL_SOX_CONTROLS.find((c) => c.id === req.body.controlId);
    if (!control) {
      return res.status(400).json({
        error: {
          code: "INVALID_CONTROL",
          message: `Unknown SOX control: ${req.body.controlId}`
        }
      });
    }
    const evidence = {
      id: `evidence-${randomUUID66()}`,
      controlId: req.body.controlId,
      framework: "SOX",
      type: req.body.type,
      description: req.body.description,
      collectedAt: (/* @__PURE__ */ new Date()).toISOString(),
      collectedBy: getUserId3(req),
      expiresAt: new Date(Date.now() + 365 * 24 * 60 * 60 * 1e3).toISOString(),
      status: "valid",
      attachments: req.body.attachments || []
    };
    logger_default2.info({
      evidenceId: evidence.id,
      controlId: evidence.controlId,
      framework: "SOX",
      submittedBy: evidence.collectedBy
    }, "SOX evidence submitted");
    res.status(201).json(wrapResponse2(evidence, req));
  }
);
router94.get(
  "/mappings",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const mappings = [
      {
        source: { framework: "HIPAA", control: "HIPAA-AS-001" },
        target: { framework: "SOC2", control: "CC6.1" },
        relationship: "equivalent",
        description: "Access control requirements align"
      },
      {
        source: { framework: "SOX", control: "SOX-ITGC-AC-001" },
        target: { framework: "SOC2", control: "CC6.1" },
        relationship: "partial",
        description: "Logical access controls overlap"
      },
      {
        source: { framework: "HIPAA", control: "HIPAA-TS-003" },
        target: { framework: "ISO27001", control: "A.10.1.1" },
        relationship: "equivalent",
        description: "Encryption requirements align"
      }
    ];
    let filteredMappings = mappings;
    if (req.query.sourceFramework) {
      filteredMappings = filteredMappings.filter(
        (m) => m.source.framework === req.query.sourceFramework
      );
    }
    if (req.query.targetFramework) {
      filteredMappings = filteredMappings.filter(
        (m) => m.target.framework === req.query.targetFramework
      );
    }
    res.json(wrapResponse2({
      mappings: filteredMappings,
      total: filteredMappings.length
    }, req));
  }
);
router94.get(
  "/dashboard",
  requirePermission2("compliance:read"),
  async (req, res) => {
    const tenantId = getTenantId3(req);
    const hipaaLatest = await hipaaService.getAssessmentHistory(tenantId);
    const soxLatest = await soxService.getAssessmentHistory(tenantId);
    const hipaa = hipaaLatest[0];
    const sox = soxLatest[0];
    const dashboard = {
      tenantId,
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      frameworks: [
        {
          id: "HIPAA",
          name: "HIPAA",
          enabled: true,
          score: hipaa ? hipaa.summary.compliant / hipaa.summary.totalControls * 100 : 0,
          trend: "stable",
          lastAssessment: hipaa?.assessedAt || null,
          criticalGaps: hipaa?.summary.nonCompliant || 0
        },
        {
          id: "SOX",
          name: "SOX",
          enabled: true,
          score: sox ? sox.summary.effective / sox.summary.totalControls * 100 : 0,
          trend: "stable",
          lastAssessment: sox?.assessedAt || null,
          criticalGaps: sox?.summary.ineffective || 0
        }
      ],
      upcomingDeadlines: [
        {
          date: new Date(Date.now() + 30 * 24 * 60 * 60 * 1e3).toISOString(),
          framework: "SOX",
          description: "Q4 ITGC testing deadline"
        },
        {
          date: new Date(Date.now() + 60 * 24 * 60 * 60 * 1e3).toISOString(),
          framework: "HIPAA",
          description: "Annual risk assessment due"
        }
      ],
      recentActivity: [
        ...hipaaLatest.slice(0, 3).map((h) => ({
          timestamp: h.assessedAt,
          action: "Assessment completed",
          framework: "HIPAA",
          control: null
        })),
        ...soxLatest.slice(0, 3).map((s) => ({
          timestamp: s.assessedAt,
          action: "Assessment completed",
          framework: "SOX",
          control: null
        }))
      ].sort((a, b) => new Date(b.timestamp).getTime() - new Date(a.timestamp).getTime()).slice(0, 5)
    };
    res.json(wrapResponse2(dashboard, req));
  }
);
router94.get("/health", async (_req, res) => {
  res.json({
    status: "healthy",
    frameworks: {
      HIPAA: {
        status: "active",
        controls: ALL_HIPAA_CONTROLS.length
      },
      SOX: {
        status: "active",
        controls: ALL_SOX_CONTROLS.length
      }
    },
    timestamp: (/* @__PURE__ */ new Date()).toISOString(),
    version: "4.1.0"
  });
});

// src/routes/v4/zero-trust.ts
import { Router as Router49 } from "express";
import { randomUUID as randomUUID69 } from "crypto";
init_logger2();

// src/security/zero-trust/HSMService.ts
import { randomUUID as randomUUID67 } from "crypto";
var HSMServiceImpl = class {
  constructor(config9 = {}) {
    this.config = config9;
  }
  providers = /* @__PURE__ */ new Map();
  keys = /* @__PURE__ */ new Map();
  initialized = false;
  /**
   * Initialize HSM service and connect to configured providers
   */
  async initialize() {
    if (this.initialized) {
      return;
    }
    for (const providerConfig of this.config.providers || []) {
      const provider = await this.connectProvider(providerConfig);
      this.providers.set(provider.id, provider);
    }
    if (!this.providers.has("software-hsm")) {
      this.providers.set("software-hsm", {
        id: "software-hsm",
        name: "Software HSM (Development)",
        type: "software_hsm",
        status: "active"
      });
    }
    this.initialized = true;
  }
  /**
   * Get a specific HSM provider
   */
  async getProvider(providerId) {
    this.ensureInitialized();
    return this.providers.get(providerId) || null;
  }
  /**
   * Generate a new cryptographic key in the HSM
   */
  async generateKey(spec) {
    this.ensureInitialized();
    const providerId = this.selectProvider(spec);
    const keyId = `key-${randomUUID67()}`;
    this.validateKeySpec(spec);
    const keyHandle = {
      id: keyId,
      providerId,
      label: spec.labels?.name || `key-${Date.now()}`,
      spec,
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      expiresAt: this.calculateExpiry(spec),
      version: 1,
      status: "active",
      attestation: await this.generateAttestation(keyId, providerId)
    };
    this.keys.set(keyId, keyHandle);
    return keyHandle;
  }
  /**
   * Get an existing key handle
   */
  async getKey(keyId) {
    this.ensureInitialized();
    return this.keys.get(keyId) || null;
  }
  /**
   * Sign data using an HSM-protected key
   */
  async sign(keyId, data, algorithm) {
    this.ensureInitialized();
    const key = await this.getKey(keyId);
    if (!key) {
      throw new HSMError("KEY_NOT_FOUND", `Key ${keyId} not found`);
    }
    if (!key.spec.purpose.includes("sign")) {
      throw new HSMError("INVALID_OPERATION", "Key cannot be used for signing");
    }
    if (key.status !== "active") {
      throw new HSMError("KEY_INACTIVE", `Key ${keyId} is not active`);
    }
    const crypto53 = await import("crypto");
    const alg = algorithm || this.getDefaultSignAlgorithm(key.spec);
    const signature = crypto53.createHash("sha256").update(data).update(keyId).digest();
    return signature;
  }
  /**
   * Verify a signature using an HSM-protected key
   */
  async verify(keyId, data, signature) {
    this.ensureInitialized();
    const key = await this.getKey(keyId);
    if (!key) {
      throw new HSMError("KEY_NOT_FOUND", `Key ${keyId} not found`);
    }
    if (!key.spec.purpose.includes("verify")) {
      throw new HSMError("INVALID_OPERATION", "Key cannot be used for verification");
    }
    const expectedSignature = await this.sign(keyId, data);
    return signature.equals(expectedSignature);
  }
  /**
   * Encrypt data using an HSM-protected key
   */
  async encrypt(keyId, plaintext) {
    this.ensureInitialized();
    const key = await this.getKey(keyId);
    if (!key) {
      throw new HSMError("KEY_NOT_FOUND", `Key ${keyId} not found`);
    }
    if (!key.spec.purpose.includes("encrypt")) {
      throw new HSMError("INVALID_OPERATION", "Key cannot be used for encryption");
    }
    const crypto53 = await import("crypto");
    const iv = crypto53.randomBytes(16);
    const cipher = crypto53.createCipheriv(
      "aes-256-gcm",
      crypto53.createHash("sha256").update(keyId).digest(),
      iv
    );
    const encrypted = Buffer.concat([
      iv,
      cipher.update(plaintext),
      cipher.final(),
      cipher.getAuthTag()
    ]);
    return encrypted;
  }
  /**
   * Decrypt data using an HSM-protected key
   */
  async decrypt(keyId, ciphertext) {
    this.ensureInitialized();
    const key = await this.getKey(keyId);
    if (!key) {
      throw new HSMError("KEY_NOT_FOUND", `Key ${keyId} not found`);
    }
    if (!key.spec.purpose.includes("decrypt")) {
      throw new HSMError("INVALID_OPERATION", "Key cannot be used for decryption");
    }
    const crypto53 = await import("crypto");
    const iv = ciphertext.subarray(0, 16);
    const authTag = ciphertext.subarray(ciphertext.length - 16);
    const encrypted = ciphertext.subarray(16, ciphertext.length - 16);
    const decipher = crypto53.createDecipheriv(
      "aes-256-gcm",
      crypto53.createHash("sha256").update(keyId).digest(),
      iv
    );
    decipher.setAuthTag(authTag);
    const decrypted = Buffer.concat([
      decipher.update(encrypted),
      decipher.final()
    ]);
    return decrypted;
  }
  /**
   * Wrap (encrypt) another key for secure transport
   */
  async wrapKey(wrappingKeyId, keyToWrap) {
    this.ensureInitialized();
    const wrappingKey = await this.getKey(wrappingKeyId);
    if (!wrappingKey) {
      throw new HSMError("KEY_NOT_FOUND", `Wrapping key ${wrappingKeyId} not found`);
    }
    if (!wrappingKey.spec.purpose.includes("wrap")) {
      throw new HSMError("INVALID_OPERATION", "Key cannot be used for wrapping");
    }
    if (!keyToWrap.spec.extractable) {
      throw new HSMError("KEY_NOT_EXTRACTABLE", "Target key is not extractable");
    }
    const keyData = Buffer.from(JSON.stringify({
      id: keyToWrap.id,
      spec: keyToWrap.spec,
      version: keyToWrap.version
    }));
    return this.encrypt(wrappingKeyId, keyData);
  }
  /**
   * Unwrap (decrypt) a wrapped key
   */
  async unwrapKey(wrappingKeyId, wrappedKey, spec) {
    this.ensureInitialized();
    const wrappingKey = await this.getKey(wrappingKeyId);
    if (!wrappingKey) {
      throw new HSMError("KEY_NOT_FOUND", `Wrapping key ${wrappingKeyId} not found`);
    }
    if (!wrappingKey.spec.purpose.includes("unwrap")) {
      throw new HSMError("INVALID_OPERATION", "Key cannot be used for unwrapping");
    }
    const keyData = await this.decrypt(wrappingKeyId, wrappedKey);
    const unwrappedData = JSON.parse(keyData.toString());
    const keyId = `key-${randomUUID67()}`;
    const keyHandle = {
      id: keyId,
      providerId: wrappingKey.providerId,
      label: spec.labels?.name || `imported-${Date.now()}`,
      spec,
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      version: 1,
      status: "active"
    };
    this.keys.set(keyId, keyHandle);
    return keyHandle;
  }
  /**
   * Rotate a key (create new version)
   */
  async rotateKey(keyId) {
    this.ensureInitialized();
    const existingKey = await this.getKey(keyId);
    if (!existingKey) {
      throw new HSMError("KEY_NOT_FOUND", `Key ${keyId} not found`);
    }
    existingKey.status = "rotated";
    const newKey = await this.generateKey(existingKey.spec);
    newKey.version = existingKey.version + 1;
    return newKey;
  }
  /**
   * Destroy a key (secure deletion)
   */
  async destroyKey(keyId) {
    this.ensureInitialized();
    const key = await this.getKey(keyId);
    if (!key) {
      throw new HSMError("KEY_NOT_FOUND", `Key ${keyId} not found`);
    }
    key.status = "destroyed";
    this.keys.delete(keyId);
  }
  /**
   * Generate attestation proof for a key
   */
  async attestKey(keyId) {
    this.ensureInitialized();
    const key = await this.getKey(keyId);
    if (!key) {
      throw new HSMError("KEY_NOT_FOUND", `Key ${keyId} not found`);
    }
    return this.generateAttestation(keyId, key.providerId);
  }
  // ===========================================================================
  // Private Helper Methods
  // ===========================================================================
  ensureInitialized() {
    if (!this.initialized) {
      throw new HSMError("NOT_INITIALIZED", "HSM service not initialized");
    }
  }
  async connectProvider(config9) {
    return {
      id: config9.id || randomUUID67(),
      name: config9.name,
      type: config9.type,
      endpoint: config9.endpoint,
      region: config9.region,
      partition: config9.partition,
      status: "active"
    };
  }
  selectProvider(spec) {
    return "software-hsm";
  }
  validateKeySpec(spec) {
    if (spec.keyType === "RSA" && spec.keySize) {
      if (![2048, 3072, 4096].includes(spec.keySize)) {
        throw new HSMError("INVALID_KEY_SIZE", "RSA key size must be 2048, 3072, or 4096");
      }
    }
    if (spec.keyType === "EC" && spec.curve) {
      if (!["P-256", "P-384", "P-521", "Ed25519"].includes(spec.curve)) {
        throw new HSMError("INVALID_CURVE", "Invalid elliptic curve");
      }
    }
    if (spec.keyType === "AES" && spec.keySize) {
      if (![128, 192, 256].includes(spec.keySize)) {
        throw new HSMError("INVALID_KEY_SIZE", "AES key size must be 128, 192, or 256");
      }
    }
  }
  calculateExpiry(spec) {
    const hasSigningPurpose = spec.purpose.some((p) => ["sign", "verify"].includes(p));
    const yearsValid = hasSigningPurpose ? 1 : 2;
    const expiry = /* @__PURE__ */ new Date();
    expiry.setFullYear(expiry.getFullYear() + yearsValid);
    return expiry.toISOString();
  }
  getDefaultSignAlgorithm(spec) {
    switch (spec.keyType) {
      case "RSA":
        return "RSA-SHA256";
      case "EC":
        return spec.curve === "Ed25519" ? "Ed25519" : "ECDSA-SHA256";
      default:
        return "HMAC-SHA256";
    }
  }
  async generateAttestation(keyId, providerId) {
    const provider = this.providers.get(providerId);
    const attestationType = this.getAttestationType(provider?.type);
    return {
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      attestationType,
      providerCertChain: [
        // In production, this would be actual certificate chain
        "MIIC...provider-root-cert",
        "MIIC...provider-intermediate-cert"
      ],
      keyProperties: {
        generatedInHSM: provider?.type !== "software_hsm",
        neverExported: true,
        fipsCompliant: this.isFIPSCompliant(provider?.type),
        tamperResistant: provider?.type !== "software_hsm"
      },
      signature: `attestation-sig-${keyId}-${Date.now()}`
    };
  }
  getAttestationType(providerType) {
    switch (providerType) {
      case "aws_cloudhsm":
      case "azure_managed_hsm":
      case "gcp_cloud_hsm":
      case "thales_luna":
      case "yubihsm":
        return "hsm_native";
      default:
        return "hsm_native";
    }
  }
  isFIPSCompliant(providerType) {
    return providerType !== "software_hsm";
  }
};
var HSMError = class extends Error {
  constructor(code, message) {
    super(message);
    this.code = code;
    this.name = "HSMError";
  }
};
function createHSMService(config9) {
  return new HSMServiceImpl(config9);
}

// src/security/zero-trust/ImmutableAuditService.ts
import { createHash as createHash37, randomUUID as randomUUID68 } from "crypto";
var ImmutableAuditServiceImpl = class {
  constructor(config9 = {}) {
    this.config = config9;
    this.signingKeyId = config9.signingKeyId;
  }
  entries = /* @__PURE__ */ new Map();
  entrySequence = [];
  currentSequence = 0;
  merkleTreeCache = /* @__PURE__ */ new Map();
  signingKeyId;
  /**
   * Record a new audit event with integrity proof
   */
  async recordEvent(entry) {
    const id = `audit-${randomUUID68()}`;
    const sequence = ++this.currentSequence;
    const previousHash = this.entrySequence.length > 0 ? this.entrySequence[this.entrySequence.length - 1].integrity.entryHash : "0000000000000000000000000000000000000000000000000000000000000000";
    const entryHash = this.calculateEntryHash({
      ...entry,
      id,
      sequence,
      previousHash
    });
    const integrity = {
      previousHash,
      entryHash,
      signature: await this.signEntry(entryHash),
      signedBy: this.signingKeyId || "system-key"
    };
    const auditEntry = {
      id,
      sequence,
      timestamp: entry.timestamp,
      entryType: entry.entryType,
      payload: entry.payload,
      metadata: entry.metadata,
      integrity
    };
    this.entries.set(id, auditEntry);
    this.entrySequence.push(auditEntry);
    if (this.entrySequence.length % (this.config.merkleTreeBatchSize || 100) === 0) {
      await this.rebuildMerkleTree();
    }
    return auditEntry;
  }
  /**
   * Get a specific audit entry
   */
  async getEntry(entryId) {
    return this.entries.get(entryId) || null;
  }
  /**
   * Query audit entries with filters
   */
  async queryEntries(query3) {
    let results = Array.from(this.entries.values());
    if (query3.tenantId) {
      results = results.filter((e) => e.metadata.tenantId === query3.tenantId);
    }
    if (query3.actorId) {
      results = results.filter((e) => e.metadata.actorId === query3.actorId);
    }
    if (query3.resourceType) {
      results = results.filter((e) => e.metadata.resourceType === query3.resourceType);
    }
    if (query3.resourceId) {
      results = results.filter((e) => e.metadata.resourceId === query3.resourceId);
    }
    if (query3.entryTypes && query3.entryTypes.length > 0) {
      results = results.filter((e) => query3.entryTypes.includes(e.entryType));
    }
    if (query3.startTime) {
      results = results.filter((e) => e.timestamp >= query3.startTime);
    }
    if (query3.endTime) {
      results = results.filter((e) => e.timestamp <= query3.endTime);
    }
    results.sort((a, b) => a.sequence - b.sequence);
    const offset = query3.offset || 0;
    const limit = query3.limit || 100;
    return results.slice(offset, offset + limit);
  }
  /**
   * Verify integrity of a single entry
   */
  async verifyEntry(entryId) {
    const entry = await this.getEntry(entryId);
    if (!entry) {
      throw new AuditError("ENTRY_NOT_FOUND", `Entry ${entryId} not found`);
    }
    const expectedHash = this.calculateEntryHash({
      ...entry,
      previousHash: entry.integrity.previousHash
    });
    const hashValid = expectedHash === entry.integrity.entryHash;
    const signatureValid = await this.verifySignature(
      entry.integrity.entryHash,
      entry.integrity.signature
    );
    let chainValid = true;
    if (entry.sequence > 1) {
      const previousEntry = this.entrySequence[entry.sequence - 2];
      if (previousEntry) {
        chainValid = previousEntry.integrity.entryHash === entry.integrity.previousHash;
      }
    }
    let anchorValid;
    if (entry.integrity.anchorInfo) {
      anchorValid = await this.verifyBlockchainAnchor(entry.integrity.anchorInfo);
    }
    return {
      entryId,
      valid: hashValid && signatureValid && chainValid && (anchorValid ?? true),
      hashValid,
      signatureValid,
      chainValid,
      anchorValid,
      verifiedAt: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  /**
   * Verify integrity of a chain of entries
   */
  async verifyChain(startSequence, endSequence) {
    const entries = this.entrySequence.filter(
      (e) => e.sequence >= startSequence && e.sequence <= endSequence
    );
    let entriesVerified = 0;
    let brokenAt;
    for (let i = 0; i < entries.length; i++) {
      const entry = entries[i];
      const verification2 = await this.verifyEntry(entry.id);
      if (!verification2.valid) {
        brokenAt = entry.sequence;
        break;
      }
      if (i < entries.length - 1) {
        const nextEntry = entries[i + 1];
        if (nextEntry.integrity.previousHash !== entry.integrity.entryHash) {
          brokenAt = nextEntry.sequence;
          break;
        }
      }
      entriesVerified++;
    }
    return {
      startSequence,
      endSequence,
      entriesVerified,
      valid: brokenAt === void 0,
      brokenAt,
      verifiedAt: (/* @__PURE__ */ new Date()).toISOString()
    };
  }
  /**
   * Get Merkle proof for an entry
   */
  async getMerkleProof(entryId) {
    const entry = await this.getEntry(entryId);
    if (!entry) {
      throw new AuditError("ENTRY_NOT_FOUND", `Entry ${entryId} not found`);
    }
    const tree = await this.getMerkleTree();
    const leafIndex = this.entrySequence.findIndex((e) => e.id === entryId);
    if (leafIndex === -1) {
      throw new AuditError("ENTRY_NOT_IN_TREE", "Entry not found in Merkle tree");
    }
    const proof = this.generateMerkleProof(tree, leafIndex);
    return {
      entryId,
      leafHash: entry.integrity.entryHash,
      proof,
      root: tree.root,
      treeHeight: tree.height,
      valid: this.verifyMerkleProof(entry.integrity.entryHash, proof, tree.root)
    };
  }
  /**
   * Anchor Merkle root to blockchain
   */
  async anchorToBlockchain(merkleRoot) {
    const anchor = {
      chainType: this.config.blockchainType || "rfc3161",
      chainId: this.config.blockchainChainId,
      transactionId: `tx-${randomUUID68()}`,
      blockNumber: Math.floor(Date.now() / 1e3),
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      anchorHash: merkleRoot
    };
    const recentEntries = this.entrySequence.slice(-100);
    for (const entry of recentEntries) {
      if (!entry.integrity.anchorInfo) {
        entry.integrity.anchorInfo = anchor;
      }
    }
    return anchor;
  }
  /**
   * Export audit bundle for compliance/legal
   */
  async exportAuditBundle(query3) {
    const entries = await this.queryEntries(query3);
    if (entries.length === 0) {
      throw new AuditError("NO_ENTRIES", "No entries match the query");
    }
    const leaves = entries.map((e) => e.integrity.entryHash);
    const merkleRoot = this.buildMerkleRoot(leaves);
    const signature = await this.signEntry(merkleRoot);
    const lastEntry = entries[entries.length - 1];
    const anchor = lastEntry.integrity.anchorInfo;
    return {
      id: `bundle-${randomUUID68()}`,
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      query: query3,
      entries,
      merkleRoot,
      signature,
      anchor
    };
  }
  // ===========================================================================
  // Private Helper Methods
  // ===========================================================================
  calculateEntryHash(data) {
    const content = JSON.stringify({
      id: data.id,
      sequence: data.sequence,
      timestamp: data.timestamp,
      entryType: data.entryType,
      payload: data.payload,
      metadata: data.metadata,
      previousHash: data.previousHash
    });
    return createHash37("sha256").update(content).digest("hex");
  }
  async signEntry(hash3) {
    return createHash37("sha256").update(hash3).update(this.signingKeyId || "system-key").digest("hex");
  }
  async verifySignature(hash3, signature) {
    const expectedSignature = await this.signEntry(hash3);
    return signature === expectedSignature;
  }
  async verifyBlockchainAnchor(anchor) {
    return !!(anchor.anchorHash && anchor.transactionId && anchor.timestamp);
  }
  async getMerkleTree() {
    const cacheKey = `tree-${this.entrySequence.length}`;
    if (!this.merkleTreeCache.has(cacheKey)) {
      await this.rebuildMerkleTree();
    }
    return this.merkleTreeCache.get(cacheKey);
  }
  async rebuildMerkleTree() {
    const leaves = this.entrySequence.map((e) => e.integrity.entryHash);
    const root = this.buildMerkleRoot(leaves);
    const height = Math.ceil(Math.log2(leaves.length || 1));
    const tree = {
      root,
      leaves,
      height,
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      periodStart: this.entrySequence[0]?.timestamp || (/* @__PURE__ */ new Date()).toISOString(),
      periodEnd: this.entrySequence[this.entrySequence.length - 1]?.timestamp || (/* @__PURE__ */ new Date()).toISOString(),
      entryCount: this.entrySequence.length
    };
    this.merkleTreeCache.set(`tree-${this.entrySequence.length}`, tree);
  }
  buildMerkleRoot(leaves) {
    if (leaves.length === 0) {
      return createHash37("sha256").update("empty").digest("hex");
    }
    if (leaves.length === 1) {
      return leaves[0];
    }
    const nextLevel = [];
    for (let i = 0; i < leaves.length; i += 2) {
      const left = leaves[i];
      const right = leaves[i + 1] || left;
      const combined = createHash37("sha256").update(left).update(right).digest("hex");
      nextLevel.push(combined);
    }
    return this.buildMerkleRoot(nextLevel);
  }
  generateMerkleProof(tree, leafIndex) {
    const proof = [];
    let index = leafIndex;
    let level = [...tree.leaves];
    while (level.length > 1) {
      const siblingIndex = index % 2 === 0 ? index + 1 : index - 1;
      if (siblingIndex < level.length) {
        proof.push(level[siblingIndex]);
      } else {
        proof.push(level[index]);
      }
      const nextLevel = [];
      for (let i = 0; i < level.length; i += 2) {
        const left = level[i];
        const right = level[i + 1] || left;
        nextLevel.push(createHash37("sha256").update(left).update(right).digest("hex"));
      }
      level = nextLevel;
      index = Math.floor(index / 2);
    }
    return proof;
  }
  verifyMerkleProof(leafHash, proof, root) {
    let currentHash = leafHash;
    for (const sibling of proof) {
      const hash1 = createHash37("sha256").update(currentHash).update(sibling).digest("hex");
      const hash22 = createHash37("sha256").update(sibling).update(currentHash).digest("hex");
      currentHash = hash1;
    }
    return true;
  }
};
var AuditError = class extends Error {
  constructor(code, message) {
    super(message);
    this.code = code;
    this.name = "AuditError";
  }
};
function createImmutableAuditService(config9) {
  return new ImmutableAuditServiceImpl(config9);
}

// src/security/zero-trust/index.ts
var ZeroTrustService = class {
  constructor(config9 = {}) {
    this.config = config9;
    this.hsm = createHSMService(config9.hsm);
    this.audit = createImmutableAuditService(config9.audit);
  }
  hsm;
  audit;
  initialized = false;
  /**
   * Initialize all zero-trust services
   */
  async initialize() {
    if (this.initialized) {
      return;
    }
    await this.hsm.initialize();
    const signingKey = await this.hsm.generateKey({
      keyType: "EC",
      curve: "P-256",
      purpose: ["sign", "verify"],
      extractable: false,
      persistent: true,
      labels: { name: "audit-signing-key", usage: "audit-ledger" }
    });
    await this.audit.recordEvent({
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      entryType: "security_event",
      payload: {
        event: "zero_trust_initialized",
        signingKeyId: signingKey.id,
        hsmProvider: signingKey.providerId
      },
      metadata: {
        actorId: "system",
        actorType: "system",
        tenantId: "system",
        resourceType: "zero-trust-service",
        resourceId: "initialization",
        action: "initialize",
        outcome: "success"
      }
    });
    this.initialized = true;
  }
  /**
   * Check if services are initialized
   */
  isInitialized() {
    return this.initialized;
  }
  /**
   * Record a security-relevant event
   */
  async recordSecurityEvent(actorId, actorType, tenantId, action, details) {
    const entry = await this.audit.recordEvent({
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      entryType: "security_event",
      payload: details,
      metadata: {
        actorId,
        actorType,
        tenantId,
        resourceType: "security",
        resourceId: details.resourceId || "unknown",
        action,
        outcome: "success"
      }
    });
    return entry.id;
  }
  /**
   * Verify audit chain integrity
   */
  async verifyAuditIntegrity(startTime, endTime) {
    const entries = await this.audit.queryEntries({
      startTime,
      endTime,
      limit: 1e4
    });
    if (entries.length === 0) {
      return { valid: true, entriesVerified: 0 };
    }
    const startSequence = entries[0].sequence;
    const endSequence = entries[entries.length - 1].sequence;
    const result2 = await this.audit.verifyChain(startSequence, endSequence);
    return {
      valid: result2.valid,
      entriesVerified: result2.entriesVerified,
      brokenAt: result2.brokenAt
    };
  }
};
function createZeroTrustService(config9) {
  return new ZeroTrustService(config9);
}

// src/routes/v4/zero-trust.ts
var zeroTrustService = null;
var initializeServices3 = async () => {
  if (!zeroTrustService) {
    zeroTrustService = createZeroTrustService({
      hsm: {
        providers: [
          {
            name: process.env.HSM_PROVIDER || "Software HSM",
            type: process.env.HSM_TYPE || "software_hsm"
          }
        ]
      },
      audit: {
        merkleTreeBatchSize: 100,
        blockchainType: process.env.BLOCKCHAIN_TYPE || "rfc3161"
      }
    });
    await zeroTrustService.initialize();
    logger_default2.info("Zero-Trust services initialized");
  }
};
var getTenantId4 = (req) => {
  return req.tenantId || req.user?.tenantId || "default";
};
var getUserId4 = (req) => {
  return req.user?.id || req.user?.id || "anonymous";
};
var wrapResponse3 = (data, req) => {
  return {
    data,
    metadata: {
      requestId: req.correlationId || randomUUID69(),
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      version: "4.2.0"
    },
    governance: {
      action: "ALLOW",
      reasons: ["Zero-trust operation authorized"],
      policyIds: ["zero-trust-v4"],
      metadata: {
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        evaluator: "zero-trust-router",
        latencyMs: 0,
        simulation: false
      },
      provenance: {
        origin: "zero-trust-api-v4",
        confidence: 1
      }
    }
  };
};
var router95 = Router49();
var singleParam26 = (value) => Array.isArray(value) ? value[0] : value ?? "";
router95.use(async (_req, _res, next) => {
  try {
    await initializeServices3();
    next();
  } catch (error) {
    logger_default2.error({ error }, "Failed to initialize zero-trust services");
    next(error);
  }
});
router95.post(
  "/hsm/keys",
  requirePermission2("security:keys:create"),
  async (req, res) => {
    try {
      const spec = {
        keyType: req.body.keyType,
        keySize: req.body.keySize,
        curve: req.body.curve,
        purpose: req.body.purpose,
        extractable: req.body.extractable ?? false,
        persistent: req.body.persistent ?? true,
        labels: {
          ...req.body.labels,
          tenantId: getTenantId4(req),
          createdBy: getUserId4(req)
        }
      };
      const keyHandle = await zeroTrustService.hsm.generateKey(spec);
      logger_default2.info({
        keyId: keyHandle.id,
        keyType: spec.keyType,
        tenantId: getTenantId4(req),
        createdBy: getUserId4(req)
      }, "HSM key generated");
      await zeroTrustService.recordSecurityEvent(
        getUserId4(req),
        "user",
        getTenantId4(req),
        "key:generate",
        { keyId: keyHandle.id, keyType: spec.keyType }
      );
      res.status(201).json(wrapResponse3(keyHandle, req));
    } catch (error) {
      logger_default2.error({ error }, "Failed to generate HSM key");
      res.status(500).json({
        error: {
          code: error.code || "HSM_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.get(
  "/hsm/keys/:id",
  requirePermission2("security:keys:read"),
  async (req, res) => {
    try {
      const keyHandle = await zeroTrustService.hsm.getKey(singleParam26(req.params.id));
      if (!keyHandle) {
        return res.status(404).json({
          error: {
            code: "NOT_FOUND",
            message: `Key not found: ${singleParam26(req.params.id)}`
          }
        });
      }
      res.json(wrapResponse3(keyHandle, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: error.code || "HSM_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.post(
  "/hsm/keys/:id/sign",
  requirePermission2("security:keys:use"),
  async (req, res) => {
    try {
      const data = Buffer.from(req.body.data, "base64");
      const signature = await zeroTrustService.hsm.sign(
        singleParam26(req.params.id),
        data,
        req.body.algorithm
      );
      await zeroTrustService.recordSecurityEvent(
        getUserId4(req),
        "user",
        getTenantId4(req),
        "key:sign",
        { keyId: singleParam26(req.params.id) }
      );
      res.json(wrapResponse3({
        keyId: singleParam26(req.params.id),
        signature: signature.toString("base64"),
        algorithm: req.body.algorithm,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      }, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: error.code || "SIGN_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.post(
  "/hsm/keys/:id/verify",
  requirePermission2("security:keys:use"),
  async (req, res) => {
    try {
      const data = Buffer.from(req.body.data, "base64");
      const signature = Buffer.from(req.body.signature, "base64");
      const valid = await zeroTrustService.hsm.verify(singleParam26(req.params.id), data, signature);
      res.json(wrapResponse3({
        keyId: singleParam26(req.params.id),
        valid,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      }, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: error.code || "VERIFY_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.post(
  "/hsm/keys/:id/rotate",
  requirePermission2("security:keys:rotate"),
  async (req, res) => {
    try {
      const newKeyHandle = await zeroTrustService.hsm.rotateKey(singleParam26(req.params.id));
      logger_default2.info({
        oldKeyId: singleParam26(req.params.id),
        newKeyId: newKeyHandle.id,
        rotatedBy: getUserId4(req)
      }, "HSM key rotated");
      await zeroTrustService.recordSecurityEvent(
        getUserId4(req),
        "user",
        getTenantId4(req),
        "key:rotate",
        { oldKeyId: singleParam26(req.params.id), newKeyId: newKeyHandle.id }
      );
      res.json(wrapResponse3(newKeyHandle, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: error.code || "ROTATE_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.get(
  "/hsm/keys/:id/attest",
  requirePermission2("security:keys:read"),
  async (req, res) => {
    try {
      const attestation = await zeroTrustService.hsm.attestKey(singleParam26(req.params.id));
      res.json(wrapResponse3(attestation, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: error.code || "ATTEST_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.post(
  "/audit/events",
  requirePermission2("audit:write"),
  async (req, res) => {
    try {
      const entry = await zeroTrustService.audit.recordEvent({
        timestamp: (/* @__PURE__ */ new Date()).toISOString(),
        entryType: req.body.entryType,
        payload: req.body.payload,
        metadata: {
          actorId: getUserId4(req),
          actorType: "user",
          tenantId: getTenantId4(req),
          resourceType: req.body.resourceType || "unknown",
          resourceId: req.body.resourceId || "unknown",
          action: req.body.action || "record",
          outcome: "success",
          correlationId: req.correlationId
        }
      });
      res.status(201).json(wrapResponse3(entry, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: "AUDIT_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.get(
  "/audit/events",
  requirePermission2("audit:read"),
  async (req, res) => {
    try {
      const query3 = {
        tenantId: getTenantId4(req),
        actorId: req.query.actorId,
        resourceType: req.query.resourceType,
        resourceId: req.query.resourceId,
        entryTypes: req.query.entryType ? [req.query.entryType] : void 0,
        startTime: req.query.startTime,
        endTime: req.query.endTime,
        limit: parseInt(req.query.limit) || 100,
        offset: parseInt(req.query.offset) || 0
      };
      const entries = await zeroTrustService.audit.queryEntries(query3);
      res.json(wrapResponse3({
        entries,
        total: entries.length,
        query: query3
      }, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: "QUERY_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.get(
  "/audit/events/:id",
  requirePermission2("audit:read"),
  async (req, res) => {
    try {
      const entry = await zeroTrustService.audit.getEntry(singleParam26(req.params.id));
      if (!entry) {
        return res.status(404).json({
          error: {
            code: "NOT_FOUND",
            message: `Audit entry not found: ${singleParam26(req.params.id)}`
          }
        });
      }
      res.json(wrapResponse3(entry, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: "QUERY_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.get(
  "/audit/events/:id/verify",
  requirePermission2("audit:verify"),
  async (req, res) => {
    try {
      const verification2 = await zeroTrustService.audit.verifyEntry(singleParam26(req.params.id));
      res.json(wrapResponse3(verification2, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: "VERIFY_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.get(
  "/audit/events/:id/merkle-proof",
  requirePermission2("audit:read"),
  async (req, res) => {
    try {
      const proof = await zeroTrustService.audit.getMerkleProof(singleParam26(req.params.id));
      res.json(wrapResponse3(proof, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: "PROOF_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.post(
  "/audit/chain/verify",
  requirePermission2("audit:verify"),
  async (req, res) => {
    try {
      const result2 = await zeroTrustService.verifyAuditIntegrity(
        req.body.startTime,
        req.body.endTime
      );
      logger_default2.info({
        entriesVerified: result2.entriesVerified,
        valid: result2.valid,
        verifiedBy: getUserId4(req)
      }, "Audit chain verified");
      res.json(wrapResponse3(result2, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: "CHAIN_VERIFY_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.post(
  "/audit/export",
  requirePermission2("audit:export"),
  async (req, res) => {
    try {
      const query3 = {
        tenantId: getTenantId4(req),
        startTime: req.body.startTime,
        endTime: req.body.endTime,
        resourceType: req.body.resourceType
      };
      const bundle = await zeroTrustService.audit.exportAuditBundle(query3);
      logger_default2.info({
        bundleId: bundle.id,
        entryCount: bundle.entries.length,
        exportedBy: getUserId4(req)
      }, "Audit bundle exported");
      res.json(wrapResponse3(bundle, req));
    } catch (error) {
      res.status(500).json({
        error: {
          code: "EXPORT_ERROR",
          message: error.message
        }
      });
    }
  }
);
router95.get("/health", async (_req, res) => {
  try {
    const status = {
      status: zeroTrustService?.isInitialized() ? "healthy" : "initializing",
      services: {
        hsm: "active",
        auditLedger: "active"
      },
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      version: "4.2.0"
    };
    res.json(status);
  } catch (error) {
    res.status(503).json({
      status: "unhealthy",
      error: error instanceof Error ? error.message : "Unknown error"
    });
  }
});
router95.get(
  "/providers",
  requirePermission2("security:read"),
  async (req, res) => {
    const providers = [
      {
        id: "software-hsm",
        name: "Software HSM (Development)",
        type: "software_hsm",
        status: "active",
        fipsCompliant: false
      },
      {
        id: "aws-cloudhsm",
        name: "AWS CloudHSM",
        type: "aws_cloudhsm",
        status: process.env.AWS_HSM_ENABLED === "true" ? "active" : "disabled",
        fipsCompliant: true
      },
      {
        id: "azure-managed-hsm",
        name: "Azure Managed HSM",
        type: "azure_managed_hsm",
        status: process.env.AZURE_HSM_ENABLED === "true" ? "active" : "disabled",
        fipsCompliant: true
      }
    ];
    res.json(wrapResponse3(providers, req));
  }
);

// src/routes/v4/index.ts
var router96 = Router50();
router96.use("/ai", router93);
router96.use("/compliance", router94);
router96.use("/zero-trust", router95);
router96.get("/version", (_req, res) => {
  res.json({
    version: "4.0.0",
    pillars: {
      "ai-governance": { version: "4.0.0", status: "stable" },
      "compliance": { version: "4.1.0", status: "beta" },
      "zero-trust": { version: "4.2.0", status: "alpha" }
    },
    releaseDate: "2025-01-15",
    documentation: "/api-docs#/v4"
  });
});

// src/routes/vector-store.ts
import express51 from "express";

// src/services/VectorStoreService.ts
init_postgres();
init_EmbeddingService();
init_logger2();
import { v4 as uuidv434 } from "uuid";
var VectorStoreService = class _VectorStoreService {
  static instance;
  embeddingService;
  constructor() {
    this.embeddingService = new EmbeddingService_default();
  }
  static getInstance() {
    if (!_VectorStoreService.instance) {
      _VectorStoreService.instance = new _VectorStoreService();
    }
    return _VectorStoreService.instance;
  }
  /**
   * Ingest a document and chunk it
   * For now, a simple character splitter is used if content is provided.
   * In a real app, use a proper text splitter (RecursiveCharacterTextSplitter).
   */
  async ingestDocument(tenantId, data) {
    const pool4 = getPostgresPool();
    const documentId = uuidv434();
    const chunks = this.chunkText(data.content);
    const chunkEmbeddings = await this.embeddingService.generateEmbeddings(chunks);
    let storedCount = 0;
    await pool4.withTransaction(async (client6) => {
      await client6.query(
        `INSERT INTO documents (id, tenant_id, source_id, title, content, metadata)
           VALUES ($1, $2, $3, $4, $5, $6)`,
        [
          documentId,
          tenantId,
          data.sourceId,
          data.title,
          data.content,
          data.metadata || {}
        ]
      );
      for (let i = 0; i < chunks.length; i++) {
        const chunkContent = chunks[i];
        const embedding = chunkEmbeddings[i];
        const vectorString = `[${embedding.join(",")}]`;
        await client6.query(
          `INSERT INTO document_chunks (
                  document_id, tenant_id, chunk_index, content, metadata, embedding
               ) VALUES ($1, $2, $3, $4, $5, $6::vector)`,
          [
            documentId,
            tenantId,
            i,
            chunkContent,
            data.metadata || {},
            // Inherit doc metadata + specific chunk meta if needed
            vectorString
          ]
        );
        storedCount++;
      }
    });
    logger_default2.info({ tenantId, documentId, chunkCount: storedCount }, "Document ingested");
    return { documentId, chunkCount: storedCount };
  }
  /**
   * Semantic Search
   */
  async search(query3, options2) {
    const { limit = 5, threshold = 0.7, metadataFilter, tenantId } = options2;
    const pool4 = getPostgresPool();
    const queryEmbedding = await this.embeddingService.generateEmbedding({ text: query3 });
    const vectorString = `[${queryEmbedding.join(",")}]`;
    let filterClause = "tenant_id = $2";
    const params = [vectorString, tenantId];
    let paramIndex = 3;
    if (metadataFilter) {
      filterClause += ` AND metadata @> $${paramIndex}`;
      params.push(metadataFilter);
      paramIndex++;
    }
    const sql = `
      SELECT
        id,
        document_id,
        tenant_id,
        chunk_index,
        content,
        metadata,
        1 - (embedding <=> $1::vector) as similarity
      FROM document_chunks
      WHERE ${filterClause}
      AND 1 - (embedding <=> $1::vector) > $${paramIndex}
      ORDER BY similarity DESC
      LIMIT $${paramIndex + 1}
    `;
    params.push(threshold, limit);
    const result2 = await pool4.read(sql, params);
    return result2.rows.map((row) => ({
      id: row.id,
      documentId: row.document_id,
      tenantId: row.tenant_id,
      chunkIndex: row.chunk_index,
      content: row.content,
      metadata: row.metadata,
      score: row.similarity
    }));
  }
  /**
   * Delete Document
   */
  async deleteDocument(documentId, tenantId) {
    const pool4 = getPostgresPool();
    const result2 = await pool4.write(
      "DELETE FROM documents WHERE id = $1 AND tenant_id = $2",
      [documentId, tenantId]
    );
    return (result2.rowCount ?? 0) > 0;
  }
  /**
   * Simple chunking helper
   * Split by double newline (paragraphs) first, then ensure max chunk size.
   * This is a naive implementation.
   */
  chunkText(text, maxChars = 1e3, overlap = 100) {
    const chunks = [];
    let rawChunks = text.split(/\n\s*\n/);
    let currentChunk = "";
    for (const raw of rawChunks) {
      if (currentChunk.length + raw.length < maxChars) {
        currentChunk += (currentChunk ? "\n\n" : "") + raw;
      } else {
        if (currentChunk) chunks.push(currentChunk);
        if (raw.length > maxChars) {
          let start = 0;
          while (start < raw.length) {
            const end = Math.min(start + maxChars, raw.length);
            chunks.push(raw.slice(start, end));
            start = end - overlap;
          }
          currentChunk = "";
        } else {
          currentChunk = raw;
        }
      }
    }
    if (currentChunk) chunks.push(currentChunk);
    return chunks;
  }
};

// src/routes/vector-store.ts
init_auth4();
import { z as z44 } from "zod";
var router97 = express51.Router();
var IngestSchema = z44.object({
  content: z44.string().min(1),
  title: z44.string().optional(),
  sourceId: z44.string().optional(),
  metadata: z44.record(z44.any()).optional()
});
var SearchSchema2 = z44.object({
  query: z44.string().min(1),
  limit: z44.coerce.number().min(1).max(100).optional().default(5),
  threshold: z44.coerce.number().min(0).max(1).optional().default(0.7),
  metadataFilter: z44.record(z44.any()).optional()
});
var asyncHandler4 = (fn) => (req, res, next) => Promise.resolve(fn(req, res, next)).catch(next);
var vectorStore = VectorStoreService.getInstance();
router97.post(
  "/ingest",
  ensureAuthenticated,
  asyncHandler4(async (req, res) => {
    const user = req.user;
    if (!user || !user.tenantId) {
      return res.status(401).json({ error: "Unauthorized: No tenant context" });
    }
    const validation = IngestSchema.safeParse(req.body);
    if (!validation.success) {
      return res.status(400).json({ error: validation.error.format() });
    }
    const result2 = await vectorStore.ingestDocument(
      user.tenantId,
      validation.data
    );
    res.json(result2);
  })
);
router97.post(
  "/search",
  ensureAuthenticated,
  asyncHandler4(async (req, res) => {
    const user = req.user;
    if (!user || !user.tenantId) {
      return res.status(401).json({ error: "Unauthorized: No tenant context" });
    }
    const validation = SearchSchema2.safeParse(req.body);
    if (!validation.success) {
      return res.status(400).json({ error: validation.error.format() });
    }
    const results = await vectorStore.search(validation.data.query, {
      limit: validation.data.limit,
      threshold: validation.data.threshold,
      metadataFilter: validation.data.metadataFilter,
      tenantId: user.tenantId
    });
    res.json({ results });
  })
);
router97.delete(
  "/documents/:id",
  ensureAuthenticated,
  asyncHandler4(async (req, res) => {
    const user = req.user;
    const { id } = req.params;
    if (!user || !user.tenantId) {
      return res.status(401).json({ error: "Unauthorized: No tenant context" });
    }
    const success = await vectorStore.deleteDocument(id, user.tenantId);
    if (!success) {
      return res.status(404).json({ error: "Document not found" });
    }
    res.json({ success: true });
  })
);
var vector_store_default = router97;

// src/routes/intel-graph.ts
import express52 from "express";
import { z as z46 } from "zod/v4";

// src/services/IntelGraphService.ts
init_database();
init_errors();
init_ledger();
import neo4j4 from "neo4j-driver";
import { randomUUID as randomUUID70 } from "crypto";
import { Counter as Counter19, Histogram as Histogram15 } from "prom-client";
import { z as z45 } from "zod";
var CreateEntitySchema = z45.object({ name: z45.string().min(1), description: z45.string().optional() });
var CreateClaimSchema = z45.object({ statement: z45.string().min(1), confidence: z45.number().min(0).max(1), entityId: z45.string().uuid() });
var AttachEvidenceSchema = z45.object({ claimId: z45.string().uuid(), sourceURI: z45.string().url(), hash: z45.string().min(1), content: z45.string().min(1) });
var TagPolicySchema = z45.object({ label: z45.string().min(1), sensitivity: z45.enum(["public", "internal", "confidential", "secret", "top-secret"]) });
var CreateDecisionSchema = z45.object({ question: z45.string().min(1), recommendation: z45.string().min(1), rationale: z45.string().min(1) });
var intelGraphRequestLatency = new Histogram15({
  name: "intelgraph_service_request_latency_seconds",
  help: "Latency of IntelGraphService methods",
  labelNames: ["method"]
});
var intelGraphOperationsCounter = new Counter19({
  name: "intelgraph_service_operations_total",
  help: "Total number of operations handled by IntelGraphService",
  labelNames: ["method", "status"]
  // status can be 'success' or 'error'
});
var IntelGraphService = class _IntelGraphService {
  static instance;
  driver;
  ledger;
  constructor() {
    this.driver = getNeo4jDriver2();
    if (!this.driver) {
      throw new AppError("Neo4j driver not initialized", 500);
    }
    this.ledger = provenanceLedger;
  }
  static getInstance() {
    if (!_IntelGraphService.instance) {
      _IntelGraphService.instance = new _IntelGraphService();
    }
    return _IntelGraphService.instance;
  }
  /**
   * A higher-order function to wrap method execution with metrics and error handling.
   * @param {string} methodName - The name of the method being executed.
   * @param {Function} fn - The function to execute.
   * @returns {Promise<T>} The result of the function.
   */
  async measure(methodName, fn) {
    const end = intelGraphRequestLatency.startTimer({ method: methodName });
    const session = this.driver.session();
    try {
      const result2 = await fn(session);
      intelGraphOperationsCounter.inc({ method: methodName, status: "success" });
      return result2;
    } catch (error) {
      intelGraphOperationsCounter.inc({ method: methodName, status: "error" });
      if (error instanceof AppError) {
        throw error;
      }
      throw new DatabaseError(`IntelGraphService.${methodName} failed: ${error.message}`);
    } finally {
      end();
      await session.close();
    }
  }
  async createEntity(entityData, owner, tenantId) {
    return this.measure("createEntity", async (session) => {
      const { name, description } = CreateEntitySchema.parse(entityData);
      const now = (/* @__PURE__ */ new Date()).toISOString();
      const newEntity = { id: randomUUID70(), createdAt: now, updatedAt: now, owner, name, description: description || "" };
      const result2 = await session.run("CREATE (e:Entity $props) RETURN e", { props: { ...newEntity, tenantId } });
      const createdEntity = result2.records[0]?.get("e").properties;
      await this.ledger.appendEntry({ tenantId, timestamp: new Date(now), actionType: "CREATE", resourceType: "Entity", resourceId: createdEntity.id, actorId: owner, actorType: "user", payload: { mutationType: "CREATE", entityId: createdEntity.id, entityType: "Entity", name, description }, metadata: {} });
      return createdEntity;
    });
  }
  async createClaim(claimData, owner, tenantId) {
    return this.measure("createClaim", async (session) => {
      const { statement, confidence, entityId } = CreateClaimSchema.parse(claimData);
      const now = (/* @__PURE__ */ new Date()).toISOString();
      const newClaim = { id: randomUUID70(), createdAt: now, updatedAt: now, owner, statement, confidence, entityId };
      const result2 = await session.run(
        `MATCH (e:Entity {id: $entityId, tenantId: $tenantId})
             CREATE (c:Claim $props)
             CREATE (c)-[:RELATES_TO]->(e)
             RETURN c`,
        { entityId, tenantId, props: { ...newClaim, tenantId } }
      );
      if (result2.records.length === 0) throw new NotFoundError(`Entity with ID ${entityId} not found for this tenant.`);
      const createdClaim = result2.records[0].get("c").properties;
      await this.ledger.appendEntry({ tenantId, timestamp: new Date(now), actionType: "CREATE", resourceType: "Claim", resourceId: createdClaim.id, actorId: owner, actorType: "user", payload: { mutationType: "CREATE", entityId: createdClaim.id, entityType: "Claim", statement, confidence, parentEntityId: entityId }, metadata: {} });
      return createdClaim;
    });
  }
  async attachEvidence(evidenceData, owner, tenantId) {
    return this.measure("attachEvidence", async (session) => {
      const { claimId, sourceURI, hash: hash3, content } = AttachEvidenceSchema.parse(evidenceData);
      const now = (/* @__PURE__ */ new Date()).toISOString();
      const newEvidence = { id: randomUUID70(), createdAt: now, updatedAt: now, owner, sourceURI, hash: hash3, content, claimId };
      const result2 = await session.run(
        `MATCH (c:Claim {id: $claimId, tenantId: $tenantId})
               CREATE (ev:Evidence $props)
               CREATE (ev)-[:SUPPORTS]->(c)
               RETURN ev`,
        { claimId, tenantId, props: { ...newEvidence, tenantId } }
      );
      if (result2.records.length === 0) throw new NotFoundError(`Claim with ID ${claimId} not found for this tenant.`);
      const attachedEvidence = result2.records[0].get("ev").properties;
      await this.ledger.appendEntry({ tenantId, timestamp: new Date(now), actionType: "ATTACH", resourceType: "Evidence", resourceId: attachedEvidence.id, actorId: owner, actorType: "user", payload: { mutationType: "CREATE", entityId: attachedEvidence.id, entityType: "Evidence", claimId, sourceURI, hash: hash3 }, metadata: {} });
      return attachedEvidence;
    });
  }
  async tagPolicy(policyData, targetNodeId, owner, tenantId) {
    return this.measure("tagPolicy", async (session) => {
      const { label, sensitivity } = TagPolicySchema.parse(policyData);
      const now = (/* @__PURE__ */ new Date()).toISOString();
      const newPolicy = { id: randomUUID70(), createdAt: now, updatedAt: now, owner, label, sensitivity };
      const result2 = await session.run(
        `MATCH (n {id: $targetNodeId, tenantId: $tenantId})
               CREATE (p:PolicyLabel $props)
               CREATE (n)-[:HAS_POLICY]->(p)
               RETURN p`,
        { targetNodeId, tenantId, props: { ...newPolicy, tenantId } }
      );
      if (result2.records.length === 0) throw new NotFoundError(`Node with ID ${targetNodeId} not found for this tenant.`);
      const taggedPolicy = result2.records[0].get("p").properties;
      await this.ledger.appendEntry({ tenantId, timestamp: new Date(now), actionType: "TAG", resourceType: "PolicyLabel", resourceId: taggedPolicy.id, actorId: owner, actorType: "user", payload: { mutationType: "CREATE", entityId: taggedPolicy.id, entityType: "PolicyLabel", targetNodeId, label, sensitivity }, metadata: {} });
      return taggedPolicy;
    });
  }
  async getDecisionProvenance(decisionId, tenantId) {
    return this.measure("getDecisionProvenance", async (session) => {
      const result2 = await session.run(
        `MATCH (d:Decision {id: $decisionId, tenantId: $tenantId})
               OPTIONAL MATCH (d)-[:INFORMED_BY]->(c:Claim)
               OPTIONAL MATCH (c)<-[:SUPPORTS]-(ev:Evidence)
               WITH d, c, COLLECT(ev.properties) AS evidences
               WITH d, COLLECT({claim: c.properties, evidences: evidences}) AS claims
               RETURN {decision: d.properties, claims: claims} AS provenance`,
        { decisionId, tenantId }
      );
      if (result2.records.length === 0 || !result2.records[0].get("provenance").decision) {
        throw new NotFoundError(`Decision with ID ${decisionId} not found for this tenant.`);
      }
      return result2.records[0].get("provenance");
    });
  }
  async getEntityClaims(entityId, tenantId) {
    return this.measure("getEntityClaims", async (session) => {
      const result2 = await session.run(
        `MATCH (e:Entity {id: $entityId, tenantId: $tenantId})
               OPTIONAL MATCH (e)<-[:RELATES_TO]-(c:Claim)
               OPTIONAL MATCH (c)-[:HAS_POLICY]->(p:PolicyLabel)
               WITH e, c, COLLECT(p.properties) AS policies
               WITH e, COLLECT({claim: c.properties, policies: policies}) AS claims
               RETURN {entity: e.properties, claims: claims} AS entityClaims`,
        { entityId, tenantId }
      );
      if (result2.records.length === 0 || !result2.records[0].get("entityClaims").entity) {
        throw new NotFoundError(`Entity with ID ${entityId} not found for this tenant.`);
      }
      return result2.records[0].get("entityClaims");
    });
  }
  // --- Generic Graph Methods used by other services (e.g. EntityResolver) ---
  async getNodeById(tenantId, nodeId) {
    return this.measure("getNodeById", async (session) => {
      const result2 = await session.run("MATCH (n {id: $nodeId, tenantId: $tenantId}) RETURN n", { nodeId, tenantId });
      return result2.records[0]?.get("n").properties;
    });
  }
  async ensureNode(tenantId, label, properties) {
    return this.measure("ensureNode", async (session) => {
      const props = { ...properties, tenantId };
      if (!props.id) props.id = randomUUID70();
      const result2 = await session.run(
        `MERGE (n:${label} {id: $id, tenantId: $tenantId}) 
         SET n += $props 
         RETURN n`,
        { id: props.id, tenantId, props }
      );
      return result2.records[0]?.get("n").properties;
    });
  }
  async createEdge(tenantId, fromNodeId, toNodeId, relationshipType, properties = {}) {
    return this.measure("createEdge", async (session) => {
      const props = { ...properties, tenantId };
      const result2 = await session.run(
        `MATCH (a {id: $fromNodeId, tenantId: $tenantId}), (b {id: $toNodeId, tenantId: $tenantId})
         MERGE (a)-[r:${relationshipType}]->(b)
         SET r += $props
         RETURN r`,
        { fromNodeId, toNodeId, tenantId, props }
      );
      return result2.records[0]?.get("r").properties;
    });
  }
  async findSimilarNodes(tenantId, label, properties, limit = 100) {
    return this.measure("findSimilarNodes", async (session) => {
      const whereClauses = ["n.tenantId = $tenantId"];
      const params = { tenantId, limit: neo4j4.int(limit) };
      Object.entries(properties).forEach(([key, value], index) => {
        whereClauses.push(`n.${key} = $val${index}`);
        params[`val${index}`] = value;
      });
      const cypher = `MATCH (n:${label}) WHERE ${whereClauses.join(" AND ")} RETURN n LIMIT $limit`;
      const result2 = await session.run(cypher, params);
      return result2.records.map((r) => r.get("n").properties);
    });
  }
  async createDecision(decisionData, informedByClaimIds, owner, tenantId) {
    return this.measure("createDecision", async (session) => {
      const { question, recommendation, rationale } = CreateDecisionSchema.parse(decisionData);
      const now = (/* @__PURE__ */ new Date()).toISOString();
      const newDecision = { id: randomUUID70(), createdAt: now, updatedAt: now, owner, question, recommendation, rationale };
      const props = { ...newDecision, tenantId };
      const result2 = await session.run(
        `CREATE (d:Decision $props)
               WITH d
               UNWIND $informedByClaimIds AS claimId
               MATCH (c:Claim {id: claimId, tenantId: $tenantId})
               CREATE (d)-[:INFORMED_BY]->(c)
               RETURN d`,
        { props, informedByClaimIds, tenantId }
      );
      const createdDecision = result2.records[0]?.get("d").properties;
      if (!createdDecision) throw new DatabaseError(`Failed to create decision.`);
      await this.ledger.appendEntry({ tenantId, timestamp: new Date(now), actionType: "CREATE", resourceType: "Decision", resourceId: createdDecision.id, actorId: owner, actorType: "user", payload: { mutationType: "CREATE", entityId: createdDecision.id, entityType: "Decision", question, recommendation, informedByClaimIds }, metadata: {} });
      return createdDecision;
    });
  }
  static _resetForTesting() {
    _IntelGraphService.instance = null;
  }
};

// src/routes/intel-graph.ts
init_auth4();
var router98 = express52.Router();
var createEntitySchema = z46.object({
  name: z46.string().min(1, "Name is required"),
  description: z46.string().optional()
});
var createClaimSchema2 = z46.object({
  statement: z46.string().min(1, "Statement is required"),
  confidence: z46.number().min(0).max(1),
  entityId: z46.string().uuid("Invalid Entity ID format")
});
var attachEvidenceSchema = z46.object({
  claimId: z46.string().uuid("Invalid Claim ID format"),
  sourceURI: z46.string().url("Invalid source URI"),
  hash: z46.string().min(1, "Hash is required"),
  content: z46.string().min(1, "Content is required")
});
var tagPolicySchema = z46.object({
  targetNodeId: z46.string().uuid("Invalid Target Node ID format"),
  label: z46.string().min(1, "Label is required"),
  sensitivity: z46.enum(["public", "internal", "confidential", "secret", "top-secret"])
});
var intelGraphServiceMiddleware = (req, res, next) => {
  req.intelGraphService = IntelGraphService.getInstance();
  next();
};
router98.use(ensureAuthenticated, tenantContextMiddleware, intelGraphServiceMiddleware);
router98.post("/entities", async (req, res, next) => {
  try {
    const { name, description } = createEntitySchema.parse(req.body);
    const owner = req.user.id;
    const tenantId = req.user.tenantId;
    const entity = await req.intelGraphService.createEntity({ name, description }, owner, tenantId);
    res.status(201).json(entity);
  } catch (error) {
    next(error);
  }
});
router98.post("/claims", async (req, res, next) => {
  try {
    const { statement, confidence, entityId } = createClaimSchema2.parse(req.body);
    const owner = req.user.id;
    const tenantId = req.user.tenantId;
    const claim = await req.intelGraphService.createClaim({ statement, confidence, entityId }, owner, tenantId);
    res.status(201).json(claim);
  } catch (error) {
    next(error);
  }
});
router98.post("/evidence", async (req, res, next) => {
  try {
    const { claimId, sourceURI, hash: hash3, content } = attachEvidenceSchema.parse(req.body);
    const owner = req.user.id;
    const tenantId = req.user.tenantId;
    const evidence = await req.intelGraphService.attachEvidence({ claimId, sourceURI, hash: hash3, content }, owner, tenantId);
    res.status(201).json(evidence);
  } catch (error) {
    next(error);
  }
});
router98.post("/policies", async (req, res, next) => {
  try {
    const { targetNodeId, label, sensitivity } = tagPolicySchema.parse(req.body);
    const owner = req.user.id;
    const tenantId = req.user.tenantId;
    const policy2 = await req.intelGraphService.tagPolicy({ label, sensitivity }, targetNodeId, owner, tenantId);
    res.status(201).json(policy2);
  } catch (error) {
    next(error);
  }
});
router98.get("/decisions/:id/provenance", async (req, res, next) => {
  try {
    const { id } = req.params;
    const tenantId = req.user.tenantId;
    const provenance = await req.intelGraphService.getDecisionProvenance(id, tenantId);
    res.status(200).json(provenance);
  } catch (error) {
    next(error);
  }
});
router98.get("/entities/:id/claims", async (req, res, next) => {
  try {
    const { id } = req.params;
    const tenantId = req.user.tenantId;
    const entityClaims = await req.intelGraphService.getEntityClaims(id, tenantId);
    res.status(200).json(entityClaims);
  } catch (error) {
    next(error);
  }
});
var intel_graph_default = router98;

// src/routes/graphrag.ts
import express53 from "express";

// src/services/GraphRAGQueryService.ts
init_logger2();
init_metrics4();

// src/metering/emitter.ts
init_logger2();

// src/metering/pipeline.ts
init_logger2();

// src/metering/postgres-repository.ts
init_database();
init_logger2();
var PostgresMeterRepository = class {
  async recordEvent(event) {
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      await client6.query("BEGIN");
      const query3 = `
        INSERT INTO usage_events (
          tenant_id,
          principal_id,
          principal_kind,
          kind,
          quantity,
          unit,
          occurred_at,
          metadata,
          correlation_id,
          idempotency_key
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
        ON CONFLICT (tenant_id, idempotency_key) DO NOTHING
        RETURNING id
      `;
      let quantity = 0;
      let unit = "";
      let metadata = event.metadata || {};
      switch (event.kind) {
        case "ingest.units" /* INGEST_UNITS */:
          quantity = event.units;
          unit = "units";
          break;
        case "query.credits" /* QUERY_CREDITS */:
          quantity = event.credits;
          unit = "credits";
          break;
        case "storage.bytes_estimate" /* STORAGE_BYTES_ESTIMATE */:
          quantity = event.bytes;
          unit = "bytes";
          break;
        case "user.seat.active" /* USER_SEAT_ACTIVE */:
          quantity = event.seatCount ?? 1;
          unit = "seats";
          metadata = { ...metadata, userId: event.userId };
          break;
      }
      const values = [
        event.tenantId,
        event.principalId || null,
        event.principalKind || null,
        event.kind,
        quantity,
        unit,
        event.occurredAt || /* @__PURE__ */ new Date(),
        JSON.stringify(metadata),
        event.correlationId || null,
        event.idempotencyKey || null
      ];
      const result2 = await client6.query(query3, values);
      await client6.query("COMMIT");
      return (result2.rowCount ?? 0) > 0;
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error({ error, event }, "Failed to record meter event to Postgres");
      throw error;
    } finally {
      client6.release();
    }
  }
};
var PostgresTenantUsageRepository = class extends TenantUsageDailyRepository {
  async saveAll(rows) {
    const pool4 = getPostgresPool2();
    const client6 = await pool4.connect();
    try {
      await client6.query("BEGIN");
      const query3 = `
        INSERT INTO usage_summaries (
          tenant_id,
          period_start,
          period_end,
          kind,
          total_quantity,
          unit,
          breakdown
        ) VALUES ($1, $2, $3, $4, $5, $6, $7)
        ON CONFLICT (tenant_id, period_start, period_end, kind)
        DO UPDATE SET
          total_quantity = usage_summaries.total_quantity + EXCLUDED.total_quantity,
          breakdown = usage_summaries.breakdown || EXCLUDED.breakdown
      `;
      for (const row of rows) {
        const periodStart = new Date(row.date);
        periodStart.setHours(0, 0, 0, 0);
        const periodEnd = new Date(periodStart);
        periodEnd.setHours(23, 59, 59, 999);
        if (row.ingestUnits > 0) {
          await client6.query(query3, [
            row.tenantId,
            periodStart,
            periodEnd,
            "ingest.units" /* INGEST_UNITS */,
            row.ingestUnits,
            "units",
            "{}"
          ]);
        }
        if (row.queryCredits > 0) {
          await client6.query(query3, [
            row.tenantId,
            periodStart,
            periodEnd,
            "query.credits" /* QUERY_CREDITS */,
            row.queryCredits,
            "credits",
            "{}"
          ]);
        }
        if (row.storageBytesEstimate > 0) {
          await client6.query(query3, [
            row.tenantId,
            periodStart,
            periodEnd,
            "storage.bytes_estimate" /* STORAGE_BYTES_ESTIMATE */,
            row.storageBytesEstimate,
            "bytes",
            "{}"
          ]);
        }
      }
      await client6.query("COMMIT");
    } catch (error) {
      await client6.query("ROLLBACK");
      logger_default2.error({ error }, "Failed to save usage summaries to Postgres");
      throw error;
    } finally {
      client6.release();
    }
  }
  // Override list to fetch from DB if needed, or just rely on super for in-memory if we are mixing strategies.
  // Ideally we should query `usage_summaries`.
};
var postgresMeterRepository = new PostgresMeterRepository();
var postgresUsageRepository = new PostgresTenantUsageRepository();

// src/metering/persistence.ts
import fs37 from "fs";
import path41 from "path";
import crypto43 from "crypto";
import stringify from "fast-json-stable-stringify";
var DATA_DIR = path41.join(process.cwd(), "data", "metering");
if (!fs37.existsSync(DATA_DIR)) {
  fs37.mkdirSync(DATA_DIR, { recursive: true });
}
var FileMeterStore = class {
  logPath = path41.join(DATA_DIR, "events.jsonl");
  lastHash = "";
  constructor() {
    if (fs37.existsSync(this.logPath)) {
    }
  }
  async append(event) {
    const lineContent = stringify(event);
    const newHash = crypto43.createHash("sha256").update(this.lastHash + lineContent).digest("hex");
    const line = stringify({
      data: event,
      hash: newHash,
      prevHash: this.lastHash
    }) + "\n";
    await fs37.promises.appendFile(this.logPath, line, "utf8");
    this.lastHash = newHash;
  }
  async verifyLogIntegrity() {
    if (!fs37.existsSync(this.logPath)) {
      return { valid: true };
    }
    const content = await fs37.promises.readFile(this.logPath, "utf8");
    const lines = content.trim().split("\n");
    let calculatedLastHash = "";
    for (let i = 0; i < lines.length; i++) {
      try {
        if (!lines[i].trim()) continue;
        const record2 = JSON.parse(lines[i]);
        const eventStr = stringify(record2.data);
        if (record2.prevHash !== calculatedLastHash) {
          return { valid: false, brokenAtLine: i + 1 };
        }
        const expectedHash = crypto43.createHash("sha256").update(calculatedLastHash + eventStr).digest("hex");
        if (record2.hash !== expectedHash) {
          return { valid: false, brokenAtLine: i + 1 };
        }
        calculatedLastHash = expectedHash;
      } catch (e) {
        return { valid: false, brokenAtLine: i + 1 };
      }
    }
    return { valid: true };
  }
};
var FileTenantUsageRepository = class extends TenantUsageDailyRepository {
  filePath = path41.join(DATA_DIR, "rollups.json");
  constructor() {
    super();
    this.load();
  }
  load() {
    try {
      if (fs37.existsSync(this.filePath)) {
        const data = fs37.readFileSync(this.filePath, "utf8");
        const rows = JSON.parse(data);
        super.saveAll(rows);
      }
    } catch (err) {
      console.error("Failed to load metering rollups", err);
    }
  }
  async saveAll(rows) {
    await super.saveAll(rows);
    const all = await this.list();
    await fs37.promises.writeFile(this.filePath, JSON.stringify(all, null, 2), "utf8");
  }
};
var meterStore = new FileMeterStore();
var persistentUsageRepository = new FileTenantUsageRepository();

// src/metering/pipeline.ts
var dateKey = (d) => d.toISOString().slice(0, 10);
var MAX_CACHE_SIZE = 1e4;
var MeteringPipeline = class {
  processedKeys = /* @__PURE__ */ new Set();
  deadLetters = [];
  rollups = /* @__PURE__ */ new Map();
  cleanupTimer;
  constructor() {
    this.cleanupTimer = setInterval(
      () => this.cleanupCache(),
      1e3 * 60 * 60
    );
    this.cleanupTimer.unref?.();
  }
  cleanupCache() {
    if (this.processedKeys.size > MAX_CACHE_SIZE) {
      this.processedKeys.clear();
      logger_default2.info("Cleared metering idempotency cache");
    }
  }
  async enqueue(event) {
    try {
      await this.handleEvent(event);
    } catch (error) {
      this.deadLetters.push({
        event,
        reason: error.message
      });
      logger_default2.warn(
        { error, event },
        "MeteringPipeline moved event to DLQ"
      );
    }
  }
  async replayDLQ(transform) {
    const stillDead = [];
    let replayed = 0;
    for (const dlq of this.deadLetters) {
      const event = transform ? transform(dlq.event) : dlq.event;
      try {
        await this.handleEvent(event);
        replayed++;
      } catch (error) {
        stillDead.push({
          event,
          reason: error.message
        });
      }
    }
    this.deadLetters = stillDead;
    return { replayed, remaining: this.deadLetters.length };
  }
  getDeadLetters() {
    return [...this.deadLetters];
  }
  getDailyRollups() {
    return Array.from(this.rollups.values());
  }
  reset() {
    this.processedKeys.clear();
    this.deadLetters = [];
    this.rollups.clear();
  }
  async handleEvent(event) {
    const occurred = event.occurredAt ? new Date(event.occurredAt) : /* @__PURE__ */ new Date();
    const idempotencyKey = event.idempotencyKey || event.correlationId || this.buildSyntheticKey(event);
    if (this.processedKeys.has(idempotencyKey)) {
      return;
    }
    this.validate(event);
    const eventWithKey = { ...event, idempotencyKey };
    try {
      const inserted = await postgresMeterRepository.recordEvent(eventWithKey);
      if (inserted === false) {
        this.processedKeys.add(idempotencyKey);
        return;
      }
    } catch (err) {
      logger_default2.error({ err }, "Failed to persist meter event");
    }
    try {
      await meterStore.append(event);
    } catch (err) {
      logger_default2.error({ err }, "Failed to append to meter store");
      throw err;
    }
    this.processedKeys.add(idempotencyKey);
    const day = dateKey(occurred);
    const key = `${event.tenantId}:${day}`;
    const current = this.rollups.get(key) || {
      tenantId: event.tenantId,
      date: day,
      ingestUnits: 0,
      queryCredits: 0,
      storageBytesEstimate: 0,
      activeSeats: 0,
      llmTokens: 0,
      computeMs: 0,
      apiRequests: 0,
      correlationIds: [],
      lastEventAt: occurred.toISOString()
    };
    switch (event.kind) {
      case "ingest.units" /* INGEST_UNITS */:
        current.ingestUnits += event.units;
        break;
      case "query.credits" /* QUERY_CREDITS */:
        current.queryCredits += event.credits;
        break;
      case "storage.bytes_estimate" /* STORAGE_BYTES_ESTIMATE */:
        current.storageBytesEstimate += event.bytes;
        break;
      case "user.seat.active" /* USER_SEAT_ACTIVE */:
        current.activeSeats += event.seatCount ?? 1;
        break;
      case "llm.tokens" /* LLM_TOKENS */:
        current.llmTokens = (current.llmTokens || 0) + event.tokens;
        break;
      case "maestro.compute.ms" /* MAESTRO_COMPUTE_MS */:
        current.computeMs = (current.computeMs || 0) + event.durationMs;
        break;
      case "api.request" /* API_REQUEST */:
        current.apiRequests = (current.apiRequests || 0) + 1;
        break;
      default:
        break;
    }
    current.lastEventAt = occurred.toISOString();
    if (event.correlationId) {
      const seen = new Set(current.correlationIds);
      seen.add(event.correlationId);
      current.correlationIds = Array.from(seen);
    }
    this.rollups.set(key, current);
  }
  validate(event) {
    if (!event.tenantId) {
      throw new Error("tenantId is required");
    }
    if (event.kind === "ingest.units" /* INGEST_UNITS */ && event.units < 0) {
      throw new Error("ingest units must be non-negative");
    }
    if (event.kind === "query.credits" /* QUERY_CREDITS */ && event.credits < 0) {
      throw new Error("query credits must be non-negative");
    }
    if (event.kind === "storage.bytes_estimate" /* STORAGE_BYTES_ESTIMATE */ && event.bytes < 0) {
      throw new Error("storage bytes must be non-negative");
    }
    if (event.kind === "user.seat.active" /* USER_SEAT_ACTIVE */ && (event.seatCount ?? 0) < 0) {
      throw new Error("seat count must be non-negative");
    }
    if (event.kind === "llm.tokens" /* LLM_TOKENS */ && event.tokens < 0) {
      throw new Error("llm tokens must be non-negative");
    }
    if (event.kind === "maestro.compute.ms" /* MAESTRO_COMPUTE_MS */ && event.durationMs < 0) {
      throw new Error("compute duration must be non-negative");
    }
  }
  buildSyntheticKey(event) {
    const occurred = event.occurredAt ? new Date(event.occurredAt) : /* @__PURE__ */ new Date();
    const unique = Math.random().toString(36).substring(7);
    return `${event.tenantId}:${event.kind}:${event.source}:${occurred.toISOString()}:${unique}`;
  }
};
var meteringPipeline = new MeteringPipeline();

// src/metering/emitter.ts
var MeteringEmitter = class {
  async emit(event) {
    await meteringPipeline.enqueue(event);
  }
  async emitIngestUnits(input) {
    await this.safeEmit({
      kind: "ingest.units" /* INGEST_UNITS */,
      tenantId: input.tenantId,
      units: input.units,
      source: input.source,
      correlationId: input.correlationId,
      idempotencyKey: input.idempotencyKey,
      metadata: input.metadata
    });
  }
  async emitQueryCredits(input) {
    await this.safeEmit({
      kind: "query.credits" /* QUERY_CREDITS */,
      tenantId: input.tenantId,
      credits: input.credits,
      source: input.source,
      correlationId: input.correlationId,
      idempotencyKey: input.idempotencyKey,
      metadata: input.metadata
    });
  }
  async emitStorageEstimate(input) {
    await this.safeEmit({
      kind: "storage.bytes_estimate" /* STORAGE_BYTES_ESTIMATE */,
      tenantId: input.tenantId,
      bytes: input.bytes,
      source: input.source,
      correlationId: input.correlationId,
      idempotencyKey: input.idempotencyKey,
      metadata: input.metadata
    });
  }
  async emitActiveSeat(input) {
    await this.safeEmit({
      kind: "user.seat.active" /* USER_SEAT_ACTIVE */,
      tenantId: input.tenantId,
      source: input.source,
      correlationId: input.correlationId,
      idempotencyKey: input.idempotencyKey,
      metadata: input.metadata,
      seatCount: input.seatCount ?? 1,
      userId: input.userId
    });
  }
  async emitLlmTokens(input) {
    await this.safeEmit({
      kind: "llm.tokens" /* LLM_TOKENS */,
      tenantId: input.tenantId,
      tokens: input.tokens,
      model: input.model,
      provider: input.provider,
      source: input.source,
      correlationId: input.correlationId,
      idempotencyKey: input.idempotencyKey,
      metadata: input.metadata
    });
  }
  async emitComputeMs(input) {
    await this.safeEmit({
      kind: "maestro.compute.ms" /* MAESTRO_COMPUTE_MS */,
      tenantId: input.tenantId,
      durationMs: input.durationMs,
      source: input.source,
      taskId: input.taskId,
      correlationId: input.correlationId,
      idempotencyKey: input.idempotencyKey,
      metadata: input.metadata
    });
  }
  async emitApiRequest(input) {
    await this.safeEmit({
      kind: "api.request" /* API_REQUEST */,
      tenantId: input.tenantId,
      source: input.source,
      endpoint: input.endpoint,
      method: input.method,
      statusCode: input.statusCode,
      correlationId: input.correlationId,
      idempotencyKey: input.idempotencyKey,
      metadata: input.metadata
    });
  }
  async safeEmit(event) {
    try {
      await this.emit(event);
    } catch (error) {
      logger_default2.warn({ error, event }, "Failed to emit meter event");
    }
  }
};
var meteringEmitter = new MeteringEmitter();

// src/prompts/registry.ts
init_logger2();
import fs38 from "fs/promises";
import path42 from "path";
import { fileURLToPath as fileURLToPath4 } from "url";
import yaml3 from "js-yaml";
import AjvModule2 from "ajv";
var Ajv4 = AjvModule2.default || AjvModule2;
var PromptRegistry = class {
  prompts = /* @__PURE__ */ new Map();
  schema = null;
  ajv = new Ajv4();
  promptsDir;
  constructor(promptsDir) {
    if (promptsDir) {
      this.promptsDir = promptsDir;
    } else {
      const __filename5 = fileURLToPath4(import.meta.url);
      const __dirname5 = path42.dirname(__filename5);
      this.promptsDir = __dirname5;
    }
  }
  async initialize() {
    try {
      const schemaPath = path42.join(this.promptsDir, "schema.json");
      try {
        const schemaContent = await fs38.readFile(schemaPath, "utf-8");
        this.schema = JSON.parse(schemaContent);
      } catch (e) {
        logger_default2.warn("Schema file not found or invalid, proceeding without validation");
      }
      await this.loadPromptsRecursively(this.promptsDir);
      logger_default2.info(`Loaded ${this.prompts.size} prompt templates`, {
        templates: Array.from(this.prompts.keys())
      });
    } catch (error) {
      logger_default2.error("Failed to initialize prompt registry", {
        error: error.message
      });
      throw error;
    }
  }
  async loadPromptsRecursively(dir) {
    const entries = await fs38.readdir(dir, { withFileTypes: true });
    for (const entry of entries) {
      const fullPath = path42.join(dir, entry.name);
      if (entry.isDirectory()) {
        await this.loadPromptsRecursively(fullPath);
      } else if (entry.isFile() && (entry.name.endsWith(".yaml") || entry.name.endsWith(".yml"))) {
        await this.loadPrompt(fullPath);
      }
    }
  }
  async loadPrompt(filePath) {
    try {
      const content = await fs38.readFile(filePath, "utf-8");
      const prompt = yaml3.load(content);
      if (!prompt || !prompt.meta || !prompt.meta.id) {
        return;
      }
      if (this.schema) {
        const valid = this.ajv.validate(this.schema, prompt);
        if (!valid) {
          logger_default2.warn(`Invalid prompt schema for ${filePath}: ${this.ajv.errorsText()}`);
        }
      }
      this.prompts.set(prompt.meta.id, prompt);
      logger_default2.debug("Loaded prompt template", {
        id: prompt.meta.id,
        file: path42.basename(filePath)
      });
    } catch (error) {
      logger_default2.error("Failed to load prompt", {
        file: filePath,
        error: error.message
      });
    }
  }
  getPrompt(id) {
    return this.prompts.get(id) || null;
  }
  getAllPrompts() {
    return Array.from(this.prompts.values());
  }
  render(id, inputs) {
    const prompt = this.getPrompt(id);
    if (!prompt) {
      throw new Error(`Prompt not found: ${id}`);
    }
    this.validateInputs(prompt, inputs);
    const sanitizedInputs = this.sanitizeInputs(inputs);
    return this.renderTemplate(prompt.template, sanitizedInputs);
  }
  /**
   * Sanitize inputs to prevent prompt injection attacks.
   * Escapes delimiters and common injection patterns.
   */
  sanitizeInputs(inputs) {
    const sanitized = {};
    for (const [key, value] of Object.entries(inputs)) {
      if (typeof value === "string") {
        sanitized[key] = this.sanitizeString(value);
      } else if (Array.isArray(value)) {
        sanitized[key] = value.map(
          (item) => typeof item === "string" ? this.sanitizeString(item) : item
        );
      } else if (typeof value === "object" && value !== null) {
        sanitized[key] = this.sanitizeInputs(value);
      } else {
        sanitized[key] = value;
      }
    }
    return sanitized;
  }
  sanitizeString(str) {
    return str.replace(/"""/g, '\\"\\"\\"').replace(/```/g, "\\`\\`\\`").replace(/<\|endoftext\|>/g, "[END]").replace(/System:/gi, "System_User_Input:");
  }
  validateInputs(prompt, inputs) {
    const required = Object.keys(prompt.inputs);
    const missing = required.filter((key) => !(key in inputs));
    if (missing.length > 0) {
      throw new Error(`Missing required inputs: ${missing.join(", ")}`);
    }
    for (const [key, expectedType] of Object.entries(prompt.inputs)) {
      const value = inputs[key];
      if (!this.validateType(value, expectedType)) {
        throw new Error(
          `Invalid type for ${key}: expected ${expectedType}, got ${typeof value}`
        );
      }
    }
  }
  validateType(value, expectedType) {
    switch (expectedType) {
      case "string":
        return typeof value === "string";
      case "number":
        return typeof value === "number";
      case "boolean":
        return typeof value === "boolean";
      case "array":
        return Array.isArray(value);
      case "object":
        return typeof value === "object" && !Array.isArray(value);
      default:
        return true;
    }
  }
  renderTemplate(template, inputs) {
    let rendered = template;
    for (const [key, value] of Object.entries(inputs)) {
      const placeholder = new RegExp(`{{\\s*${key}\\s*}}`, "g");
      const stringValue = this.formatValue(value);
      rendered = rendered.replace(placeholder, stringValue);
    }
    rendered = this.renderArrays(rendered, inputs);
    return rendered;
  }
  formatValue(value) {
    if (Array.isArray(value)) {
      return value.join("\n");
    }
    if (typeof value === "object") {
      return JSON.stringify(value, null, 2);
    }
    return String(value);
  }
  renderArrays(template, inputs) {
    return template.replace(
      /{{#(\w+)}}(.*?){{\/\1}}/gs,
      (match, arrayName, itemTemplate) => {
        const array3 = inputs[arrayName];
        if (!Array.isArray(array3)) {
          return "";
        }
        return array3.map((item) => {
          if (typeof item === "object") {
            let rendered = itemTemplate;
            for (const [key, value] of Object.entries(item)) {
              rendered = rendered.replace(
                new RegExp(`{{\\s*${key}\\s*}}`, "g"),
                String(value)
              );
            }
            return rendered;
          } else {
            return itemTemplate.replace(/{{\.}}/g, String(item));
          }
        }).join("");
      }
    );
  }
  async runGoldenTests(promptId) {
    const results = [];
    const promptsToTest = promptId ? [this.getPrompt(promptId)].filter(Boolean) : this.getAllPrompts();
    for (const prompt of promptsToTest) {
      if (!prompt.examples?.length) {
        continue;
      }
      for (const example of prompt.examples) {
        try {
          const rendered = this.render(prompt.meta.id, example.inputs);
          let passed2 = true;
          let missing = [];
          if (example.expected_contains) {
            passed2 = example.expected_contains.every(
              (expected) => rendered.includes(expected)
            );
            if (!passed2) {
              missing = example.expected_contains.filter(
                (expected) => !rendered.includes(expected)
              );
            }
          }
          results.push({
            promptId: prompt.meta.id,
            exampleName: example.name,
            passed: passed2,
            rendered,
            expectedContains: example.expected_contains || [],
            missingExpected: missing
          });
        } catch (error) {
          results.push({
            promptId: prompt.meta.id,
            exampleName: example.name,
            passed: false,
            error: error.message,
            expectedContains: example.expected_contains || [],
            missingExpected: example.expected_contains || []
          });
        }
      }
    }
    const passed = results.filter((r) => r.passed).length;
    const total = results.length;
    logger_default2.info("Golden tests completed", {
      passed,
      total,
      successRate: total > 0 ? (passed / total * 100).toFixed(1) + "%" : "0%"
    });
    return results;
  }
  async reloadPrompts() {
    this.prompts.clear();
    await this.initialize();
  }
};
var promptRegistry = new PromptRegistry();

// src/services/GraphRAGQueryService.ts
init_tracing();
var GraphRAGQueryService = class {
  graphRAGService;
  queryPreviewService;
  glassBoxService;
  pool;
  neo4jDriver;
  constructor(graphRAGService, queryPreviewService, glassBoxService, pool4, neo4jDriver2) {
    this.graphRAGService = graphRAGService;
    this.queryPreviewService = queryPreviewService;
    this.glassBoxService = glassBoxService;
    this.pool = pool4;
    this.neo4jDriver = neo4jDriver2;
  }
  /**
   * Main query method - handles the full flow from NL query to answer with citations
   */
  async query(request) {
    return tracer6.trace("graphrag.query", async (span) => {
      span.setAttribute("graphrag.investigation_id", request.investigationId);
      span.setAttribute("graphrag.tenant_id", request.tenantId);
      span.setAttribute("graphrag.question_length", request.question.length);
      const startTime = Date.now();
      metrics3.featureUsageTotal.inc({
        tenant_id: request.tenantId,
        feature_name: "graphrag_query"
      });
      logger2.info(
        {
          investigationId: request.investigationId,
          question: request.question,
          generatePreview: request.generateQueryPreview,
          autoExecute: request.autoExecute
        },
        "Starting GraphRAG query"
      );
      let systemPromptId = "core.jules-copilot@v4";
      let systemPromptOwner = "jules";
      try {
        const promptConfig = promptRegistry.getPrompt(systemPromptId);
        if (promptConfig) {
          systemPromptOwner = promptConfig.meta.owner;
        }
      } catch (e) {
        logger2.warn({ error: e }, "Failed to load system prompt config for audit metadata");
      }
      const run = await this.glassBoxService.createRun({
        investigationId: request.investigationId,
        tenantId: request.tenantId,
        userId: request.userId,
        type: "graphrag_query",
        prompt: request.question,
        parameters: {
          focusEntityIds: request.focusEntityIds,
          maxHops: request.maxHops,
          generatePreview: request.generateQueryPreview,
          autoExecute: request.autoExecute,
          systemPromptId,
          systemPromptOwner
        }
      });
      try {
        await this.glassBoxService.updateStatus(run.id, "running");
        let preview;
        if (request.generateQueryPreview) {
          const previewStepId = await this.captureStep(
            run.id,
            "Generate query preview from natural language"
          );
          preview = await this.queryPreviewService.createPreview({
            investigationId: request.investigationId,
            tenantId: request.tenantId,
            userId: request.userId,
            naturalLanguageQuery: request.question,
            language: "cypher",
            parameters: {
              focusEntityIds: request.focusEntityIds,
              maxHops: request.maxHops
            },
            focusEntityIds: request.focusEntityIds,
            maxHops: request.maxHops
          });
          await this.glassBoxService.completeStep(run.id, previewStepId, {
            previewId: preview.id,
            costLevel: preview.costEstimate.level,
            riskLevel: preview.riskAssessment.level,
            canExecute: preview.canExecute
          });
          if (!request.autoExecute) {
            await this.glassBoxService.updateStatus(run.id, "completed", {
              previewGenerated: true,
              requiresExplicitExecution: true
            });
            return {
              answer: "",
              confidence: 0,
              citations: [],
              preview: {
                id: preview.id,
                generatedQuery: preview.generatedQuery,
                queryExplanation: preview.queryExplanation,
                costLevel: preview.costEstimate.level,
                riskLevel: preview.riskAssessment.level,
                canExecute: preview.canExecute,
                requiresApproval: preview.requiresApproval
              },
              runId: run.id,
              executionTimeMs: Date.now() - startTime
            };
          }
          if (!preview.canExecute) {
            throw new Error(`Query cannot be executed: ${preview.validationErrors.join(", ")}`);
          }
          if (preview.requiresApproval) {
            logger2.warn(
              {
                previewId: preview.id,
                costLevel: preview.costEstimate.level,
                riskLevel: preview.riskAssessment.level
              },
              "Query requires approval but auto-execute is true"
            );
          }
        }
        const ragStepId = await this.captureStep(
          run.id,
          "Execute GraphRAG retrieval and generation"
        );
        const ragRequest = {
          investigationId: request.investigationId,
          tenantId: request.tenantId,
          question: request.question,
          focusEntityIds: request.focusEntityIds,
          maxHops: request.maxHops || 2
        };
        const ragResponse = await this.graphRAGService.answer(ragRequest);
        await this.glassBoxService.completeStep(run.id, ragStepId, {
          confidence: ragResponse.confidence,
          citationCount: ragResponse.citations.entityIds.length,
          pathCount: ragResponse.why_paths?.length
        });
        const enrichStepId = await this.captureStep(run.id, "Enrich citations with entity details");
        const enrichedCitations = await this.enrichCitations(
          ragResponse.citations.entityIds,
          request.investigationId,
          request.tenantId
        );
        await this.glassBoxService.completeStep(run.id, enrichStepId, {
          enrichedCount: enrichedCitations.length
        });
        if (enrichedCitations.length === 0 && ragResponse.answer.length > 50) {
          const errorMsg = "Publication blocked: Answer generated but lacks required citations.";
          logger2.warn({ runId: run.id }, errorMsg);
          throw new Error(errorMsg);
        }
        const subgraphSize = await this.getSubgraphSize(
          request.investigationId,
          request.focusEntityIds
        );
        const executionTimeMs = Date.now() - startTime;
        const response = {
          answer: ragResponse.answer,
          confidence: ragResponse.confidence,
          citations: enrichedCitations,
          why_paths: ragResponse.why_paths,
          preview: preview ? {
            id: preview.id,
            generatedQuery: preview.generatedQuery,
            queryExplanation: preview.queryExplanation,
            costLevel: preview.costEstimate.level,
            riskLevel: preview.riskAssessment.level,
            canExecute: preview.canExecute,
            requiresApproval: preview.requiresApproval
          } : void 0,
          runId: run.id,
          executionTimeMs,
          subgraphSize
        };
        await this.glassBoxService.updateStatus(run.id, "completed", response);
        metrics3.graphragQueryTotal.inc({
          status: "success",
          hasPreview: preview ? "true" : "false",
          redactionEnabled: "false",
          provenanceEnabled: "false"
        });
        metrics3.graphragQueryDurationMs.observe(
          { hasPreview: preview ? "true" : "false" },
          executionTimeMs
        );
        logger2.info(
          {
            runId: run.id,
            investigationId: request.investigationId,
            confidence: ragResponse.confidence,
            citationCount: enrichedCitations.length,
            executionTimeMs,
            hasPreview: !!preview
          },
          "Completed GraphRAG query"
        );
        try {
          await meteringEmitter.emitQueryCredits({
            tenantId: request.tenantId,
            credits: 1,
            source: "graphrag-query-service",
            correlationId: run.id,
            idempotencyKey: run.id,
            metadata: {
              investigationId: request.investigationId,
              autoExecute: request.autoExecute ?? true
            }
          });
        } catch (meterError) {
          logger2.warn({ meterError, runId: run.id }, "Failed to emit query metering event");
        }
        return response;
      } catch (error) {
        await this.glassBoxService.updateStatus(run.id, "failed", void 0, String(error));
        metrics3.graphragQueryTotal.inc({
          status: "failed",
          hasPreview: "false",
          redactionEnabled: "false",
          provenanceEnabled: "false"
        });
        logger2.error(
          {
            error,
            runId: run.id,
            request
          },
          "Failed to execute GraphRAG query"
        );
        throw error;
      }
    });
  }
  /**
   * Execute a previously created preview
   */
  async executePreview(request) {
    const startTime = Date.now();
    const preview = await this.queryPreviewService.getPreview(request.previewId);
    if (!preview) {
      throw new Error(`Preview ${request.previewId} not found`);
    }
    logger2.info(
      {
        previewId: request.previewId,
        investigationId: preview.investigationId,
        useEditedQuery: request.useEditedQuery,
        dryRun: request.dryRun
      },
      "Executing preview"
    );
    const execResult = await this.queryPreviewService.executePreview({
      previewId: request.previewId,
      userId: request.userId,
      useEditedQuery: request.useEditedQuery,
      dryRun: request.dryRun,
      maxRows: request.maxRows,
      timeout: request.timeout,
      cursor: request.cursor,
      batchSize: request.batchSize,
      stream: request.stream
    });
    if (request.dryRun) {
      return {
        answer: "Dry run - query validated but not executed",
        confidence: 0,
        citations: [],
        runId: execResult.runId,
        executionTimeMs: execResult.executionTimeMs
      };
    }
    const ragRequest = {
      investigationId: preview.investigationId,
      tenantId: preview.tenantId,
      question: preview.naturalLanguageQuery,
      focusEntityIds: preview.parameters.focusEntityIds,
      maxHops: preview.parameters.maxHops
    };
    const ragResponse = await this.graphRAGService.answer(ragRequest);
    const enrichedCitations = await this.enrichCitations(
      ragResponse.citations.entityIds,
      preview.investigationId,
      preview.tenantId
    );
    const response = {
      answer: ragResponse.answer,
      confidence: ragResponse.confidence,
      citations: enrichedCitations,
      why_paths: ragResponse.why_paths,
      preview: {
        id: preview.id,
        generatedQuery: request.useEditedQuery && preview.editedQuery ? preview.editedQuery : preview.generatedQuery,
        queryExplanation: preview.queryExplanation,
        costLevel: preview.costEstimate.level,
        riskLevel: preview.riskAssessment.level,
        canExecute: preview.canExecute,
        requiresApproval: preview.requiresApproval
      },
      runId: execResult.runId,
      rows: execResult.results,
      partialResults: execResult.partialResults,
      rowCount: execResult.rowCount,
      nextCursor: execResult.nextCursor,
      hasMore: execResult.hasMore,
      streamingChannel: execResult.streamingChannel,
      streamedBatches: execResult.streamedBatches,
      executionTimeMs: Date.now() - startTime
    };
    logger2.info(
      {
        previewId: request.previewId,
        runId: execResult.runId,
        citationCount: enrichedCitations.length,
        executionTimeMs: response.executionTimeMs
      },
      "Executed preview successfully"
    );
    return response;
  }
  /**
   * Get a run by ID with full details
   */
  async getRun(runId) {
    return this.glassBoxService.getRun(runId);
  }
  /**
   * Replay a run with optional modifications
   */
  async replayRun(runId, userId, options2) {
    const originalRun = await this.glassBoxService.getRun(runId);
    if (!originalRun) {
      throw new Error(`Run ${runId} not found`);
    }
    logger2.info(
      {
        originalRunId: runId,
        userId,
        hasModifications: !!(options2?.modifiedQuestion || options2?.modifiedParameters)
      },
      "Replaying run"
    );
    const replayRun = await this.glassBoxService.replayRun(runId, userId, {
      modifiedPrompt: options2?.modifiedQuestion,
      modifiedParameters: options2?.modifiedParameters,
      skipCache: options2?.skipCache
    });
    const request = {
      investigationId: originalRun.investigationId,
      tenantId: originalRun.tenantId,
      userId,
      question: options2?.modifiedQuestion || originalRun.prompt,
      focusEntityIds: options2?.modifiedParameters?.focusEntityIds || originalRun.parameters.focusEntityIds,
      maxHops: options2?.modifiedParameters?.maxHops || originalRun.parameters.maxHops,
      generateQueryPreview: originalRun.parameters.generatePreview,
      autoExecute: true
    };
    return this.query(request);
  }
  /**
   * List runs for an investigation
   */
  async listRuns(investigationId, options2) {
    return this.glassBoxService.listRuns(investigationId, {
      type: "graphrag_query",
      ...options2
    });
  }
  /**
   * Get replay history for a run
   */
  async getReplayHistory(runId) {
    return this.glassBoxService.getReplayHistory(runId);
  }
  /**
   * Enrich citations with entity details
   */
  async enrichCitations(entityIds, investigationId, tenantId) {
    if (entityIds.length === 0) {
      return [];
    }
    const tenantFilter = tenantId ? "AND e.tenant_id = $3" : "";
    const query3 = `
      SELECT
        e.id,
        e.kind,
        e.labels,
        e.props->>'name' as name,
        e.props->>'description' as description,
        e.props->>'url' as source_url,
        e.props->>'confidence' as confidence
      FROM entities e
      WHERE e.id = ANY($1)
      AND e.props->>'investigationId' = $2
      ${tenantFilter}
      ORDER BY array_position($1, e.id)
    `;
    try {
      const params = [entityIds, investigationId];
      if (tenantId) params.push(tenantId);
      const result2 = await this.pool.query(query3, params);
      return result2.rows.map((row) => ({
        entityId: row.id,
        entityKind: row.kind,
        entityLabels: row.labels || [],
        entityName: row.name || row.id,
        snippetText: row.description ? this.truncateText(row.description, 200) : void 0,
        confidence: row.confidence ? parseFloat(row.confidence) : void 0,
        sourceUrl: row.source_url
      }));
    } catch (error) {
      logger2.error(
        {
          error,
          entityIds,
          investigationId
        },
        "Failed to enrich citations"
      );
      return entityIds.map((id) => ({
        entityId: id
      }));
    }
  }
  /**
   * Get subgraph size for metadata
   */
  async getSubgraphSize(investigationId, focusEntityIds) {
    const session = this.neo4jDriver.session();
    try {
      let query3;
      let params;
      if (focusEntityIds && focusEntityIds.length > 0) {
        query3 = `
          MATCH (anchor:Entity)
          WHERE anchor.id IN $focusIds
            AND anchor.investigationId = $investigationId
          CALL apoc.path.subgraphAll(anchor, {
            maxLevel: 2,
            relationshipFilter: 'REL>',
            labelFilter: 'Entity'
          })
          YIELD nodes, relationships
          RETURN size(nodes) as nodeCount, size(relationships) as edgeCount
        `;
        params = { focusIds: focusEntityIds, investigationId };
      } else {
        query3 = `
          MATCH (e:Entity {investigationId: $investigationId})
          WITH count(e) as nodeCount
          MATCH ()-[r:REL]->()
          WHERE r.investigationId = $investigationId
          WITH nodeCount, count(r) as edgeCount
          RETURN nodeCount, edgeCount
        `;
        params = { investigationId };
      }
      const result2 = await session.run(query3, params);
      if (result2.records.length > 0) {
        return {
          nodeCount: result2.records[0].get("nodeCount").toNumber(),
          edgeCount: result2.records[0].get("edgeCount").toNumber()
        };
      }
      return void 0;
    } catch (error) {
      logger2.error(
        {
          error,
          investigationId
        },
        "Failed to get subgraph size"
      );
      return void 0;
    } finally {
      await session.close();
    }
  }
  /**
   * Capture a step in the glass-box run
   */
  async captureStep(runId, description) {
    const stepId = __require("uuid").v4();
    await this.glassBoxService.addStep(runId, {
      type: "tool_call",
      description
    });
    return stepId;
  }
  /**
   * Truncate text to max length
   */
  truncateText(text, maxLength) {
    if (text.length <= maxLength) {
      return text;
    }
    return text.substring(0, maxLength - 3) + "...";
  }
};

// src/services/GraphRAGService.ts
init_logger2();
import { z as z48 } from "zod/v4";
import { createHash as createHash38 } from "crypto";

// src/utils/CircuitBreaker.ts
import { default as pino56 } from "pino";
var logger52 = pino56();
var CircuitBreaker2 = class {
  state = "CLOSED" /* CLOSED */;
  failureCount = 0;
  successCount = 0;
  lastFailureTime = 0;
  options;
  metrics;
  /**
   * Creates an instance of CircuitBreaker.
   *
   * @param options - Configuration options for the circuit breaker.
   */
  constructor(options2) {
    this.options = {
      failureThreshold: 5,
      successThreshold: 3,
      resetTimeout: 3e4,
      // 30 seconds
      p95ThresholdMs: 2e3,
      // 2 seconds
      errorRateThreshold: 0.5,
      // 50%
      ...options2
    };
    this.metrics = {
      totalRequests: 0,
      failedRequests: 0,
      latencies: [],
      stateChanges: 0
    };
    logger52.info(
      `Circuit Breaker initialized with options: ${JSON.stringify(this.options)}`
    );
  }
  /**
   * Gets the current state of the circuit breaker.
   *
   * @returns The current CircuitBreakerState.
   */
  getState() {
    return this.state;
  }
  /**
   * Retrieves current metrics for the circuit breaker.
   *
   * @returns An object containing metrics such as total requests, failed requests, P95 latency, error rate, and current state.
   */
  getMetrics() {
    return {
      ...this.metrics,
      p95Latency: this.calculateP95Latency(),
      errorRate: this.metrics.totalRequests > 0 ? this.metrics.failedRequests / this.metrics.totalRequests : 0,
      state: this.state
    };
  }
  /**
   * Gets the number of consecutive failures.
   *
   * @returns The failure count.
   */
  getFailureCount() {
    return this.failureCount;
  }
  /**
   * Gets the timestamp of the last failure.
   *
   * @returns The timestamp (in milliseconds) of the last failure.
   */
  getLastFailureTime() {
    return this.lastFailureTime;
  }
  calculateP95Latency() {
    if (this.metrics.latencies.length === 0) {
      return 0;
    }
    const sortedLatencies = [...this.metrics.latencies].sort((a, b) => a - b);
    const p95Index = Math.ceil(sortedLatencies.length * 0.95) - 1;
    return sortedLatencies[p95Index];
  }
  recordLatency(latency) {
    this.metrics.latencies.push(latency);
    if (this.metrics.latencies.length > 100) {
      this.metrics.latencies.shift();
    }
  }
  evaluateState() {
    const { p95Latency, errorRate } = this.getMetrics();
    if (this.state === "CLOSED" /* CLOSED */) {
      if (this.failureCount >= this.options.failureThreshold || p95Latency > this.options.p95ThresholdMs || errorRate > this.options.errorRateThreshold) {
        this.open();
      }
    } else if (this.state === "OPEN" /* OPEN */) {
      if (Date.now() - this.lastFailureTime > this.options.resetTimeout) {
        this.halfOpen();
      }
    } else if (this.state === "HALF_OPEN" /* HALF_OPEN */) {
    }
  }
  open() {
    this.state = "OPEN" /* OPEN */;
    this.lastFailureTime = Date.now();
    this.metrics.stateChanges++;
    logger52.warn("Circuit Breaker: OPENED");
  }
  halfOpen() {
    this.state = "HALF_OPEN" /* HALF_OPEN */;
    this.successCount = 0;
    this.metrics.stateChanges++;
    logger52.info("Circuit Breaker: HALF_OPEN");
  }
  close() {
    this.state = "CLOSED" /* CLOSED */;
    this.failureCount = 0;
    this.successCount = 0;
    this.metrics.stateChanges++;
    logger52.info("Circuit Breaker: CLOSED");
  }
  /**
   * Executes a command within the context of the circuit breaker.
   *
   * @typeParam T - The return type of the command.
   * @param command - The function to execute.
   * @returns The result of the command.
   * @throws Error if the circuit is OPEN or if the command itself fails.
   */
  async execute(command) {
    this.metrics.totalRequests++;
    this.evaluateState();
    if (this.state === "OPEN" /* OPEN */) {
      logger52.debug("Circuit Breaker: Request rejected (OPEN)");
      throw new Error("CircuitBreaker: Service is currently unavailable");
    }
    try {
      const startTime = Date.now();
      const result2 = await command();
      const latency = Date.now() - startTime;
      this.recordLatency(latency);
      if (this.state === "HALF_OPEN" /* HALF_OPEN */) {
        this.successCount++;
        if (this.successCount >= this.options.successThreshold) {
          this.close();
        }
      } else if (this.state === "CLOSED" /* CLOSED */) {
        this.failureCount = 0;
      }
      return result2;
    } catch (error) {
      this.metrics.failedRequests++;
      this.lastFailureTime = Date.now();
      if (this.state === "HALF_OPEN" /* HALF_OPEN */) {
        this.open();
      } else if (this.state === "CLOSED" /* CLOSED */) {
        this.failureCount++;
        this.evaluateState();
      }
      logger52.error(
        `Circuit Breaker: Command failed. State: ${this.state}, Failure Count: ${this.failureCount}. Error: ${error.message}`
      );
      throw error;
    }
  }
};

// src/services/PathRankingService.ts
function rankPaths(paths, opts = {}) {
  const {
    edgeWeights = {},
    nodeCentrality = {},
    strategy = "v2",
    weights = { length: 0.34, edgeType: 0.33, centrality: 0.33 }
  } = opts;
  const maxCentrality = Math.max(...Object.values(nodeCentrality), 1);
  const maxEdgeWeight = Math.max(...Object.values(edgeWeights), 1);
  const ranked = paths.map((p) => {
    const length = 1;
    const lengthScoreRaw = 1 / length;
    const edgeWeight = edgeWeights[p.type || ""] || 1;
    const edgeScoreRaw = edgeWeight / maxEdgeWeight;
    const centralityVal = ((nodeCentrality[p.from] || 0) + (nodeCentrality[p.to] || 0)) / 2;
    const centralityScoreRaw = centralityVal / maxCentrality;
    if (strategy === "v1") {
      const score = lengthScoreRaw;
      return {
        path: p,
        score,
        score_breakdown: { length: score, edgeType: 0, centrality: 0 }
      };
    }
    const breakdown = {
      length: lengthScoreRaw * weights.length,
      edgeType: edgeScoreRaw * weights.edgeType,
      centrality: centralityScoreRaw * weights.centrality
    };
    return {
      path: p,
      score: breakdown.length + breakdown.edgeType + breakdown.centrality,
      score_breakdown: breakdown
    };
  });
  return ranked.sort((a, b) => b.score - a.score);
}

// src/services/GraphRAGService.ts
init_metrics2();
init_errors();

// src/config/graphrag.ts
init_config3();
import { z as z47 } from "zod";
var defaultPrompt = z47.object({
  question: z47.string().min(3)
});
var defaultOutput = z47.object({
  answer: z47.string(),
  confidence: z47.number(),
  citations: z47.object({ entityIds: z47.array(z47.string()) }),
  why_paths: z47.array(
    z47.object({
      from: z47.string(),
      to: z47.string(),
      relId: z47.string(),
      type: z47.string(),
      supportScore: z47.number().optional()
    })
  )
});
var graphragConfig = {
  redisUrl: config_default.graphrag?.redisUrl,
  useCases: {
    default: {
      promptSchema: defaultPrompt,
      outputSchema: defaultOutput,
      tokenBudget: config_default.graphrag?.tokenBudget ?? 4096,
      latencyBudgetMs: config_default.graphrag?.latencyBudgetMs ?? 5e3
    }
  }
};
var graphrag_default = graphragConfig;

// src/services/GraphRAGService.ts
var resolvedLogger = logger_default2?.default ?? logger_default2;
var baseLogger = resolvedLogger && typeof resolvedLogger.child === "function" ? resolvedLogger : { child: () => console };
var serviceLogger4 = baseLogger.child({ name: "GraphRAGService" });
var GraphRAGRequestSchema = z48.object({
  investigationId: z48.string().min(1),
  tenantId: z48.string().min(1).optional(),
  // Optional for backward compatibility, but recommended
  question: z48.string().min(3),
  focusEntityIds: z48.array(z48.string()).optional(),
  maxHops: z48.number().int().min(1).max(3).optional(),
  temperature: z48.number().min(0).max(1).optional(),
  maxTokens: z48.number().int().min(100).max(2e3).optional(),
  useCase: z48.string().optional().default("default"),
  rankingStrategy: z48.enum(["v1", "v2"]).optional()
});
var EntitySchema = z48.object({
  id: z48.string(),
  type: z48.string(),
  label: z48.string(),
  description: z48.string().optional(),
  properties: z48.record(z48.string(), z48.any()),
  confidence: z48.number().min(0).max(1)
});
var RelationshipSchema = z48.object({
  id: z48.string(),
  type: z48.string(),
  fromEntityId: z48.string(),
  toEntityId: z48.string(),
  label: z48.string().optional(),
  properties: z48.record(z48.string(), z48.any()),
  confidence: z48.number().min(0).max(1)
});
var ScoreBreakdownSchema = z48.object({
  length: z48.number(),
  edgeType: z48.number(),
  centrality: z48.number()
});
var WhyPathSchema = z48.object({
  from: z48.string(),
  to: z48.string(),
  relId: z48.string(),
  type: z48.string(),
  supportScore: z48.number().min(0).max(1).optional(),
  score_breakdown: ScoreBreakdownSchema.optional()
});
var CitationsSchema = z48.object({
  entityIds: z48.array(z48.string())
});
var GraphRAGResponseSchema = z48.object({
  answer: z48.string().min(1),
  confidence: z48.number().min(0).max(1),
  citations: CitationsSchema,
  why_paths: z48.array(WhyPathSchema)
});
var GraphRAGService = class {
  neo4j;
  redis;
  llmService;
  embeddingService;
  config;
  cacheStats = { hits: 0, total: 0 };
  circuitBreaker;
  // Declare circuit breaker instance
  constructor(neo4jDriver2, llmService, embeddingService, redisClient4) {
    this.neo4j = neo4jDriver2;
    this.redis = redisClient4 || null;
    this.llmService = llmService;
    this.embeddingService = embeddingService;
    this.circuitBreaker = new CircuitBreaker2({
      // Initialize circuit breaker
      failureThreshold: 5,
      successThreshold: 3,
      resetTimeout: 3e4,
      // 30 seconds
      p95ThresholdMs: 2e3,
      // 2 seconds
      errorRateThreshold: 0.5
      // 50%
    });
    this.config = {
      maxContextSize: 4e3,
      maxRetrievalDepth: 3,
      minRelevanceScore: 0.7,
      cacheTTL: 300,
      maxCacheTTL: 3600,
      cacheFreqWindow: 600,
      llmModel: "gpt-4",
      embeddingModel: "text-embedding-3-small"
    };
  }
  /**
   * Main GraphRAG query method with explainable output
   */
  async answer(request) {
    return this.circuitBreaker.execute(async () => {
      const validated = GraphRAGRequestSchema.parse(request);
      const useCase = validated.useCase;
      const useCaseConfig = graphrag_default.useCases[useCase] || graphrag_default.useCases.default;
      useCaseConfig.promptSchema.parse({ question: validated.question });
      if (validated.maxTokens && validated.maxTokens > useCaseConfig.tokenBudget) {
        throw new UserFacingError(
          `Token budget exceeded: requested ${validated.maxTokens}, budget ${useCaseConfig.tokenBudget}`,
          400,
          "TOKEN_BUDGET_EXCEEDED"
        );
      }
      const startTime = Date.now();
      try {
        serviceLogger4.info(
          `GraphRAG query initiated. Investigation ID: ${validated.investigationId}, Question Length: ${validated.question.length}, Focus Entities: ${validated.focusEntityIds?.length || 0}`
        );
        const subgraphContext = await this.retrieveSubgraphWithCache(validated);
        const response = await this.generateResponseWithSchema(
          validated.question,
          subgraphContext,
          validated,
          useCaseConfig.outputSchema
        );
        response.why_paths = this.rankWhyPaths(
          response.why_paths,
          subgraphContext,
          validated.rankingStrategy
        );
        const responseTime = Date.now() - startTime;
        if (responseTime > useCaseConfig.latencyBudgetMs) {
          serviceLogger4.warn(
            `Latency budget exceeded for use case ${useCase}: ${responseTime}ms > ${useCaseConfig.latencyBudgetMs}ms`
          );
        }
        serviceLogger4.info(
          `GraphRAG query completed. Investigation ID: ${validated.investigationId}, Response Time: ${responseTime}, Entities Retrieved: ${subgraphContext.entities.length}, Relationships Retrieved: ${subgraphContext.relationships.length}, Confidence: ${response.confidence}`
        );
        if (this.redis && subgraphContext.subgraphHash) {
          const responseCacheKey = `graphrag:response:${subgraphContext.subgraphHash}:${createHash38("sha256").update(validated.question).digest("hex").substring(0, 16)}`;
          try {
            await this.redis.setex(
              responseCacheKey,
              subgraphContext.ttl,
              JSON.stringify(response)
            );
            serviceLogger4.debug(
              `Cached GraphRAG response. Response Cache Key: ${responseCacheKey}`
            );
          } catch (error) {
            serviceLogger4.warn(`Redis response cache write failed. Error: ${error}`);
          }
        }
        return response;
      } catch (error) {
        serviceLogger4.error(
          {
            investigationId: validated.investigationId,
            error: error instanceof Error ? error.message : "Unknown error",
            traceId: error.traceId
          },
          "GraphRAG query failed"
        );
        if (error instanceof UserFacingError) {
          throw error;
        }
        throw new Error(
          `GraphRAG query failed: ${error instanceof Error ? error.message : "Unknown error"}`
        );
      }
    });
  }
  /**
   * Retrieve subgraph with Redis caching based on subgraph hash
   */
  async retrieveSubgraphWithCache(request) {
    const cacheKey = this.createSubgraphCacheKey(request);
    const ttl = await this.getDynamicTTL(cacheKey);
    this.cacheStats.total++;
    if (this.redis) {
      try {
        const cached = await this.redis.get(cacheKey);
        if (cached) {
          this.cacheStats.hits++;
          graphragCacheHitRatio.set(
            this.cacheStats.hits / this.cacheStats.total
          );
          serviceLogger4.debug(`Cache hit for subgraph. Cache Key: ${cacheKey}`);
          await this.redis.expire(cacheKey, ttl);
          return { ...JSON.parse(cached), ttl };
        }
      } catch (error) {
        serviceLogger4.warn(`Redis cache read failed. Error: ${error}`);
      }
    }
    const subgraph = await this.retrieveSubgraph(request);
    const subgraphHash = this.hashSubgraph(subgraph);
    const context4 = {
      ...subgraph,
      subgraphHash,
      ttl
    };
    if (this.redis) {
      try {
        await this.redis.setex(cacheKey, ttl, JSON.stringify(context4));
        serviceLogger4.debug(
          `Cached subgraph. Cache Key: ${cacheKey}, Hash: ${subgraphHash}`
        );
      } catch (error) {
        serviceLogger4.warn(`Redis cache write failed. Error: ${error}`);
      }
    }
    graphragCacheHitRatio.set(this.cacheStats.hits / this.cacheStats.total);
    return context4;
  }
  async getDynamicTTL(cacheKey) {
    if (!this.redis) {
      return this.config.cacheTTL;
    }
    try {
      const freqKey = `graphrag:freq:${cacheKey}`;
      const count = await this.redis.incr(freqKey);
      await this.redis.expire(freqKey, this.config.cacheFreqWindow);
      await this.redis.zincrby("graphrag:popular_subgraphs", 1, cacheKey);
      const ttl = Math.min(
        this.config.maxCacheTTL,
        Math.round(this.config.cacheTTL * Math.log2(count + 1))
      );
      return ttl;
    } catch (error) {
      serviceLogger4.warn(`Redis frequency tracking failed. Error: ${error}`);
      return this.config.cacheTTL;
    }
  }
  /**
   * Retrieve subgraph using k-hop traversal and motif patterns
   */
  async retrieveSubgraph(request) {
    const session = this.neo4j.session();
    try {
      const { investigationId, focusEntityIds = [], maxHops = 2 } = request;
      let cypher;
      let params;
      if (focusEntityIds.length > 0) {
        let whereClause = "WHERE anchor.id IN $focusIds AND anchor.investigationId = $investigationId";
        if (request.tenantId) {
          whereClause += " AND anchor.tenantId = $tenantId";
        }
        cypher = `
          MATCH (anchor:Entity)
          ${whereClause}
          CALL apoc.path.subgraphAll(anchor, {
            maxLevel: $maxHops,
            relationshipFilter: 'RELATIONSHIP>',
            labelFilter: 'Entity'
          }) YIELD nodes, relationships
          WITH collect(DISTINCT nodes) as nodeArrays, collect(DISTINCT relationships) as relArrays
          UNWIND apoc.coll.flatten(nodeArrays) as node
          UNWIND apoc.coll.flatten(relArrays) as rel
          WITH collect(DISTINCT node) as allNodes, collect(DISTINCT rel) as allRels
          RETURN allNodes as nodes, allRels as relationships
        `;
        params = { focusIds: focusEntityIds, investigationId, maxHops, tenantId: request.tenantId };
      } else {
        const matchProps = request.tenantId ? "{investigationId: $investigationId, tenantId: $tenantId}" : "{investigationId: $investigationId}";
        cypher = `
          MATCH (e:Entity ${matchProps})
          WITH e ORDER BY e.confidence DESC, e.createdAt DESC LIMIT 10
          CALL apoc.path.subgraphAll(e, {
            maxLevel: $maxHops,
            relationshipFilter: 'RELATIONSHIP>',
            labelFilter: 'Entity'
          }) YIELD nodes, relationships
          WITH collect(DISTINCT nodes) as nodeArrays, collect(DISTINCT relationships) as relArrays
          UNWIND apoc.coll.flatten(nodeArrays) as node
          UNWIND apoc.coll.flatten(relArrays) as rel
          WITH collect(DISTINCT node) as allNodes, collect(DISTINCT rel) as allRels
          RETURN allNodes as nodes, allRels as relationships
        `;
        params = { investigationId, maxHops, tenantId: request.tenantId };
      }
      const result2 = await session.run(cypher, params);
      if (!result2.records.length) {
        return { entities: [], relationships: [] };
      }
      const record2 = result2.records[0];
      const entities = this.parseEntities(record2.get("nodes") || []);
      const relationships = this.parseRelationships(
        record2.get("relationships") || []
      );
      return { entities, relationships };
    } finally {
      await session.close();
    }
  }
  /**
   * Build concise string from Zod validation issues
   */
  summarizeZodIssues(error) {
    return error.issues.map((i) => `${i.path.join(".")}: ${i.message}`).join("; ");
  }
  /**
   * Generate response with strict JSON schema enforcement and retry logic
   */
  async generateResponseWithSchema(question, context4, request, schema2) {
    const prompt = this.buildContextPrompt(question, context4);
    const callLLMAndValidate = async (temp) => {
      const rawResponse = await this.llmService.complete(prompt, {
        model: request.temperature !== void 0 ? this.config.llmModel : void 0,
        maxTokens: request.maxTokens || 1e3,
        temperature: temp,
        responseFormat: "json"
      });
      let parsedResponse;
      try {
        parsedResponse = JSON.parse(rawResponse);
      } catch (error) {
        throw new Error("LLM returned invalid JSON");
      }
      const validatedResponse = schema2.parse(
        parsedResponse
      );
      this.validateCitations(validatedResponse.citations, context4);
      this.validateWhyPaths(validatedResponse.why_paths, context4);
      return validatedResponse;
    };
    try {
      return await callLLMAndValidate(request.temperature || 0);
    } catch (error) {
      if (error instanceof z48.ZodError || error instanceof Error && error.message.includes("LLM returned invalid JSON")) {
        graphragSchemaFailuresTotal.inc();
        const summary = error instanceof z48.ZodError ? this.summarizeZodIssues(error) : error.message;
        serviceLogger4.warn(
          { issues: summary },
          `LLM schema violation or invalid JSON; retrying with temperature=0`
        );
        try {
          const retryResponse = await callLLMAndValidate(0);
          serviceLogger4.info("LLM response validated on retry.");
          return retryResponse;
        } catch (retryError) {
          graphragSchemaFailuresTotal.inc();
          const mapped = mapGraphRAGError(retryError);
          const retrySummary = retryError instanceof z48.ZodError ? this.summarizeZodIssues(retryError) : retryError instanceof Error ? retryError.message : "Unknown error";
          serviceLogger4.error({
            traceId: mapped.traceId,
            issues: retrySummary
          }, "LLM schema invalid after retry");
          throw mapped;
        }
      }
      throw mapGraphRAGError(error);
    }
  }
  /**
   * Build context prompt for LLM with JSON schema requirements
   */
  buildContextPrompt(question, context4) {
    const entityContext = context4.entities.map(
      (e) => `Entity ${e.id}: ${e.label} (${e.type}) - ${e.description || "No description"}`
    ).join("\n");
    const relationshipContext = context4.relationships.map(
      (r) => `Relationship ${r.id}: ${r.fromEntityId} --[${r.type}]--> ${r.toEntityId}`
    ).join("\n");
    return `You are an **intelligence analysis copilot** running inside the Summit platform.
Your primary objective is to answer multi-step investigative questions by traversing the graph, not by guessing.
You must minimize hallucinations by only asserting facts that are explicitly present in the graph or directly quoted from source documents.

## RETRIEVAL AND REASONING RULES
- Treat the Neo4j graph as the source of truth for: entity identity, relationships, temporal ordering, and network structure.
- Prefer multi-hop graph queries (paths, communities, centrality, patterns) over flat keyword or embedding similarity when building an explanation.
- When evidence is sparse or ambiguous, say so, and return the best-supported hypotheses instead of fabricating details.

## BEHAVIOR FOR MULTI-STEP INTELLIGENCE TASKS
- Clarify the analytical goal (mapping network, finding key intermediaries, tracing money/influence, validating a claim, etc.).
- Compose an analytic narrative that explains both **what** is happening (facts from documents) and **how** actors connect across the network (paths, motifs, communities). Cite graph-level evidence explicitly.

## HALLUCINATION CONTROL AND ANSWER FORMATTING
- Do not infer new relationships that are not present in the graph unless explicitly asked to speculate; clearly label any speculation as hypothesis, not fact.
- When the graph does not contain enough information, say "insufficient graph evidence".
- Prefer concise, structured answers: short summary, then bullet-point findings linked to graph evidence.

CONTEXT ENTITIES:
${entityContext}

CONTEXT RELATIONSHIPS:
${relationshipContext}

USER QUESTION: ${question}

RESPONSE REQUIREMENTS:
- You MUST respond with valid JSON matching this exact schema
- answer: string (comprehensive answer based on context, following the analytic narrative style)
- confidence: number (0-1, based on context completeness and certainty)
- citations: object with entityIds array (entity IDs that support your answer)
- why_paths: array of objects with from, to, relId, type (relationship paths that explain your reasoning)

RESPONSE CONSTRAINTS:
- Only reference entities and relationships from the provided context
- If context is insufficient, state this clearly and set low confidence
- Include specific relationship IDs in why_paths that support your reasoning
- Cite all relevant entity IDs that contribute to your answer

Respond with JSON only:`;
  }
  /**
   * Validate that citations reference entities in the context
   */
  validateCitations(citations, context4) {
    const availableEntityIds = new Set(context4.entities.map((e) => e.id));
    const invalidCitations = citations.entityIds.filter(
      (id) => !availableEntityIds.has(id)
    );
    if (invalidCitations.length > 0) {
      throw new Error(
        `Invalid entity citations: ${invalidCitations.join(", ")}`
      );
    }
  }
  /**
   * Validate that why_paths reference actual relationships in the context
   */
  validateWhyPaths(whyPaths, context4) {
    const availableRelIds = new Set(context4.relationships.map((r) => r.id));
    const invalidPaths = whyPaths.filter(
      (path55) => !availableRelIds.has(path55.relId)
    );
    if (invalidPaths.length > 0) {
      const invalidIds = invalidPaths.map((p) => p.relId);
      throw new Error(
        `Invalid relationship IDs in why_paths: ${invalidIds.join(", ")}`
      );
    }
  }
  /**
   * Create cache key based on investigation, anchors, and hops
   */
  createSubgraphCacheKey(request) {
    const { investigationId, tenantId, focusEntityIds = [], maxHops = 2 } = request;
    const sortedAnchors = [...focusEntityIds].sort();
    const keyData = `${tenantId || "no-tenant"}:${investigationId}:${sortedAnchors.join(",")}:${maxHops}`;
    return `graphrag:subgraph:${createHash38("sha256").update(keyData).digest("hex").substring(0, 16)}`;
  }
  rankWhyPaths(paths, context4, strategy = "v2") {
    const centrality = {};
    for (const rel of context4.relationships) {
      centrality[rel.fromEntityId] = (centrality[rel.fromEntityId] || 0) + 1;
      centrality[rel.toEntityId] = (centrality[rel.toEntityId] || 0) + 1;
    }
    const ranked = rankPaths(paths, {
      nodeCentrality: centrality,
      strategy
    });
    return ranked.map((r) => ({
      ...r.path,
      supportScore: r.score,
      score_breakdown: r.score_breakdown
    }));
  }
  /**
   * Create hash of subgraph content for cache validation
   */
  hashSubgraph(subgraph) {
    const content = JSON.stringify({
      entities: subgraph.entities.map((e) => e.id).sort(),
      relationships: subgraph.relationships.map((r) => r.id).sort()
    });
    return createHash38("sha256").update(content).digest("hex").substring(0, 16);
  }
  /**
   * Parse Neo4j entities into typed Entity objects
   */
  parseEntities(nodes) {
    return nodes.map((node) => {
      const props = node.properties;
      return EntitySchema.parse({
        id: props.id,
        type: props.type,
        label: props.label,
        description: props.description || void 0,
        properties: props.properties ? JSON.parse(props.properties) : {},
        confidence: props.confidence || 1
      });
    });
  }
  /**
   * Parse Neo4j relationships into typed Relationship objects
   */
  parseRelationships(rels) {
    return rels.map((rel) => {
      const props = rel.properties;
      return RelationshipSchema.parse({
        id: props.id,
        type: props.type,
        fromEntityId: props.fromEntityId,
        toEntityId: props.toEntityId,
        label: props.label || void 0,
        properties: props.properties ? JSON.parse(props.properties) : {},
        confidence: props.confidence || 1
      });
    });
  }
  /**
   * Health check method
   */
  async getHealth() {
    let cacheStatus = "disabled";
    if (this.redis) {
      try {
        await this.redis.ping();
        cacheStatus = "healthy";
      } catch (error) {
        cacheStatus = "unhealthy";
      }
    }
    return {
      status: "healthy",
      cacheStatus,
      config: this.config,
      circuitBreaker: this.circuitBreaker.getMetrics()
      // Expose circuit breaker metrics
    };
  }
};

// src/routes/graphrag.ts
init_EmbeddingService();
init_LLMService();

// src/ai/nl-to-cypher/LLMServiceAdapter.ts
var LLMServiceAdapter = class {
  constructor(llmService) {
    this.llmService = llmService;
  }
  async generate(prompt) {
    return this.llmService.complete(prompt, {
      temperature: 0
      // Default model is handled by LLMService config if not specified
      // but we can specify 'gpt-4' or similar if needed.
      // We'll leave it to default or config to be more flexible.
    });
  }
};

// src/routes/graphrag.ts
init_database();
init_auth4();
init_rateLimit2();

// src/middleware/validation.ts
import { ZodError as ZodError2 } from "zod";
var validateRequest2 = (schemas) => {
  return async (req, res, next) => {
    try {
      if (schemas.params) {
        req.params = await schemas.params.parseAsync(req.params);
      }
      if (schemas.body) {
        req.body = await schemas.body.parseAsync(req.body);
      }
      if (schemas.query) {
        req.query = await schemas.query.parseAsync(req.query);
      }
      next();
    } catch (error) {
      if (error instanceof ZodError2) {
        const formattedErrors = error.errors.map((e) => ({
          path: e.path.join("."),
          message: e.message
        }));
        return res.status(400).json({
          error: "Validation failed",
          details: formattedErrors
        });
      }
      next(error);
    }
  };
};

// src/routes/graphrag.ts
init_logger2();
var graphRagRateLimiter = createRateLimiter2("QUERY" /* QUERY */);
var router99 = express53.Router();
var graphRAGQueryService;
function initializeServices4() {
  if (!graphRAGQueryService) {
    const neo4jDriver2 = getNeo4jDriver2();
    const redisClient4 = getRedisClient();
    const pool4 = getPostgresPool2().pool;
    const embeddingService = new EmbeddingService_default();
    const llmService = new LLMService();
    const graphRAGService = new GraphRAGService(
      neo4jDriver2,
      llmService,
      embeddingService,
      redisClient4
    );
    const glassBoxService = new GlassBoxRunService(pool4, redisClient4);
    const modelAdapter = new LLMServiceAdapter(llmService);
    const nlToCypherService = new NlToCypherService(modelAdapter);
    const queryPreviewService = new QueryPreviewService(
      pool4,
      neo4jDriver2,
      nlToCypherService,
      glassBoxService,
      redisClient4
    );
    graphRAGQueryService = new GraphRAGQueryService(
      graphRAGService,
      queryPreviewService,
      glassBoxService,
      pool4,
      neo4jDriver2
    );
  }
}
router99.use(ensureAuthenticated);
router99.use(graphRagRateLimiter);
var querySchema3 = {
  query: { type: "string", required: false },
  // Optional because it might be 'question'
  question: { type: "string", required: false },
  investigationId: { type: "string", required: true }
  // ... other fields are optional or handled dynamically
};
var handleQuery = async (req, res) => {
  initializeServices4();
  try {
    const body4 = req.body;
    let requestPayload;
    const tenantId = req.user?.tenant_id;
    const userId = req.user?.id;
    if (!tenantId || !userId) {
      logger2.warn({ user: req.user }, "Missing user context for GraphRAG query");
      return res.status(401).json({ error: "Unauthorized: Missing user context" });
    }
    if (body4.context || body4.maxResults || body4.depth || body4.model) {
      requestPayload = {
        question: body4.query || body4.question,
        investigationId: body4.investigationId,
        tenantId,
        userId,
        // Map legacy fields
        maxHops: body4.depth || body4.maxHops || 2,
        focusEntityIds: body4.focusEntityIds,
        // Legacy "options" often passed, so we ignore or map what we can
        generateQueryPreview: false,
        // Legacy assumes direct execution
        autoExecute: true
      };
    } else {
      requestPayload = {
        question: body4.query || body4.question,
        investigationId: body4.investigationId,
        tenantId,
        userId,
        focusEntityIds: body4.focusEntityIds,
        maxHops: body4.maxHops,
        generateQueryPreview: body4.generatePreview,
        autoExecute: body4.autoExecute ?? true
      };
    }
    if (!requestPayload.question || !requestPayload.investigationId) {
      return res.status(400).json({ error: "Missing required fields: query/question, investigationId" });
    }
    const result2 = await graphRAGQueryService.query(requestPayload);
    res.json(result2);
  } catch (error) {
    logger2.error({ error, body: req.body }, "GraphRAG query failed");
    res.status(500).json({ error: error.message });
  }
};
router99.post("/query", validateRequest2(querySchema3), handleQuery);
router99.post("/answer", validateRequest2(querySchema3), handleQuery);
router99.post("/preview/execute", async (req, res) => {
  initializeServices4();
  try {
    const { previewId, useEditedQuery, dryRun } = req.body;
    if (!previewId) {
      return res.status(400).json({ error: "Missing previewId" });
    }
    const result2 = await graphRAGQueryService.executePreview({
      previewId,
      userId: req.user?.id || "unknown",
      useEditedQuery,
      dryRun
    });
    res.json(result2);
  } catch (error) {
    logger2.error({ error, body: req.body }, "Preview execution failed");
    res.status(500).json({ error: error.message });
  }
});
var graphrag_default2 = router99;

// src/routes/intent.ts
import express54 from "express";

// src/services/IntentClassificationService.ts
init_logger2();
var IntentClassificationService = class {
  constructor() {
  }
  async classify(query3, context4 = {}) {
    const q = query3.toLowerCase().trim();
    logger2.info({ query: q }, "Classifying intent");
    if (q.split(" ").length < 2 && !["help", "status"].includes(q)) {
      return {
        primary_intent: "clarification",
        confidence: 0.9,
        entities: [],
        reasoning: "Query is too short to determine intent.",
        clarifying_question: `Could you specify what you want to know about "${query3}"?`
      };
    }
    const actionKeywords = ["cancel", "create", "delete", "update", "run", "start", "escalate", "approve", "reject"];
    if (actionKeywords.some((k) => q.startsWith(k))) {
      return this.classifyAction(q);
    }
    const retrievalKeywords = ["what", "who", "when", "where", "how", "list", "show", "describe", "find", "status", "check", "search"];
    if (retrievalKeywords.some((k) => q.startsWith(k)) || q.includes("?")) {
      return this.classifyRetrieval(q);
    }
    if (q.length > 5) {
      return this.classifyRetrieval(q);
    }
    return {
      primary_intent: "clarification",
      confidence: 0.5,
      entities: [],
      reasoning: "Unsure of intent.",
      clarifying_question: "I am not sure how to help. Are you trying to search for information or perform an action?"
    };
  }
  classifyAction(q) {
    const parts = q.split(" ");
    const action = parts[0];
    const entities = parts.slice(1).filter((w) => w.length > 3);
    return {
      primary_intent: "action",
      sub_intent: action,
      confidence: 0.85,
      entities,
      reasoning: `Detected action keyword "${action}"`
    };
  }
  classifyRetrieval(q) {
    const result2 = {
      primary_intent: "retrieval",
      confidence: 0.9,
      entities: [],
      // TODO: Use NER
      allowed_sources: [],
      freshness_requirement: {}
    };
    if (q.includes("policy") || q.includes("compliance") || q.includes("rule")) {
      result2.allowed_sources = ["confluence", "google_drive", "notion"];
      result2.sub_intent = "policy_search";
    } else if (q.includes("incident") || q.includes("status") || q.includes("health")) {
      result2.allowed_sources = ["pagerduty", "statuspage", "jira"];
      result2.sub_intent = "operational_status";
    } else {
      result2.allowed_sources = ["all"];
      result2.sub_intent = "general_knowledge";
    }
    if (q.includes("now") || q.includes("current") || q.includes("latest") || q.includes("today")) {
      result2.freshness_requirement = {
        max_age_days: 1,
        requires_live: true
      };
    } else if (q.includes("history") || q.includes("past") || q.includes("last year")) {
      result2.freshness_requirement = {
        max_age_days: 365
      };
    } else {
      result2.freshness_requirement = {
        max_age_days: 30
        // Default
      };
    }
    return result2;
  }
};

// src/services/IntentRouterService.ts
init_logger2();
init_tracing();
var IntentRouterService = class {
  constructor(classifier, ragService) {
    this.classifier = classifier;
    this.ragService = ragService;
  }
  async route(query3, context4) {
    return tracer6.trace("intent.route", async (span) => {
      span.setAttribute("intent.query_length", query3.length);
      if (context4.tenantId) span.setAttribute("tenant.id", context4.tenantId);
      const intent = await this.classifier.classify(query3, context4);
      logger2.info({ intent }, "Intent classified");
      span.setAttribute("intent.primary", intent.primary_intent);
      if (intent.sub_intent) span.setAttribute("intent.sub", intent.sub_intent);
      if (intent.primary_intent === "clarification") {
        return {
          answer: intent.clarifying_question || "Could you clarify?",
          intent,
          citations: []
        };
      }
      if (intent.primary_intent === "retrieval") {
        return this.handleRetrieval(query3, intent, context4);
      } else if (intent.primary_intent === "action") {
        return this.handleAction(query3, intent, context4);
      }
      return {
        answer: "I'm not sure how to handle this request.",
        intent,
        citations: []
      };
    });
  }
  async handleRetrieval(query3, intent, context4) {
    const requiresLive = intent.freshness_requirement?.requires_live;
    logger2.info({ requiresLive, allowedSources: intent.allowed_sources }, "Retrieval constraints");
    try {
      const ragResponse = await this.ragService.query({
        investigationId: context4.investigationId || "default-investigation",
        tenantId: context4.tenantId || "default-tenant",
        userId: context4.userId || "unknown-user",
        question: query3,
        autoExecute: true
      });
      return {
        answer: ragResponse.answer,
        intent,
        citations: ragResponse.citations,
        freshness_proof: {
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          live_lookup: !!requiresLive,
          data_as_of: (/* @__PURE__ */ new Date()).toISOString()
          // Mock: In real system, get from GraphRAG metadata
        }
      };
    } catch (error) {
      logger2.error({ error }, "GraphRAG query failed in Router");
      return {
        answer: "I encountered an error retrieving that information.",
        intent,
        citations: []
      };
    }
  }
  async handleAction(query3, intent, context4) {
    return {
      answer: `I have noted your request to ${intent.sub_intent} ${intent.entities.join(", ")}. This is a mock action.`,
      intent,
      citations: [],
      next_steps: ["Confirm action", "Undo"]
    };
  }
};

// src/routes/intent.ts
init_EmbeddingService();
init_LLMService();
init_database();
init_auth4();
init_logger2();
var router100 = express54.Router();
var intentRouterService;
function initializeServices5() {
  if (!intentRouterService) {
    const neo4jDriver2 = getNeo4jDriver2();
    const redisClient4 = getRedisClient();
    const pool4 = getPostgresPool2().pool;
    const embeddingService = new EmbeddingService_default();
    const llmService = new LLMService();
    const graphRAGService = new GraphRAGService(
      neo4jDriver2,
      llmService,
      embeddingService,
      redisClient4
    );
    const glassBoxService = new GlassBoxRunService(pool4, redisClient4);
    const modelAdapter = new LLMServiceAdapter(llmService);
    const nlToCypherService = new NlToCypherService(modelAdapter);
    const queryPreviewService = new QueryPreviewService(
      pool4,
      neo4jDriver2,
      nlToCypherService,
      glassBoxService,
      redisClient4
    );
    const graphRAGQueryService2 = new GraphRAGQueryService(
      graphRAGService,
      queryPreviewService,
      glassBoxService,
      pool4,
      neo4jDriver2
    );
    const classificationService = new IntentClassificationService();
    intentRouterService = new IntentRouterService(
      classificationService,
      graphRAGQueryService2
    );
  }
}
router100.post("/query", ensureAuthenticated, async (req, res) => {
  initializeServices5();
  try {
    const { query: query3, context: context4 } = req.body;
    if (!query3) {
      return res.status(400).json({ error: "Query is required" });
    }
    const tenantId = req.user?.tenant_id || req.user?.tenantId;
    const userId = req.user?.id || req.user?.sub;
    const effectiveContext = {
      ...context4,
      tenantId,
      userId,
      investigationId: context4?.investigationId || "default"
    };
    const result2 = await intentRouterService.route(query3, effectiveContext);
    res.json(result2);
  } catch (error) {
    logger2.error({ error }, "Intent routing failed");
    res.status(500).json({ error: error.message });
  }
});
var intent_default = router100;

// src/factflow/routes.ts
import { Router as Router51 } from "express";

// src/factflow/lib/evidence_id.ts
import crypto44 from "node:crypto";
function evidenceIdFromBytes(bytes) {
  const h = crypto44.createHash("sha256").update(bytes).digest("hex").slice(0, 12);
  return `EVD_${h}`;
}
function generateEvidenceId(content) {
  return evidenceIdFromBytes(Buffer.from(content));
}

// src/factflow/validation.ts
import { z as z49 } from "zod";
var EvidenceItemSchema = z49.object({
  source: z49.string(),
  url: z49.string().optional(),
  snippet: z49.string().optional()
});
var TimeRangeSchema = z49.object({
  start: z49.number(),
  end: z49.number()
});
var ClaimSchema = z49.object({
  id: z49.string(),
  text: z49.string(),
  speaker: z49.string().optional(),
  time_range: TimeRangeSchema,
  verdict: z49.enum(["verified", "disputed", "unverified", "needs_review"]),
  confidence: z49.number().min(0).max(1).optional(),
  evidence: z49.array(EvidenceItemSchema).optional()
});
var FactFlowReportSchema = z49.object({
  job_id: z49.string(),
  timestamp: z49.string().datetime(),
  claims: z49.array(ClaimSchema)
});
var FactFlowMetricsSchema = z49.object({
  job_id: z49.string(),
  processing_time_ms: z49.number(),
  audio_duration_sec: z49.number(),
  cache_hit: z49.boolean(),
  claims_count: z49.number(),
  verified_count: z49.number(),
  needs_review_count: z49.number()
});

// src/factflow/engine.ts
var FactFlowEngine = class {
  constructor(transcription2, diarization2, verification2) {
    this.transcription = transcription2;
    this.diarization = diarization2;
    this.verification = verification2;
  }
  async process(jobId, audioBuffer) {
    const startTime = Date.now();
    const transcript = await this.transcription.transcribe(audioBuffer);
    const speakers = await this.diarization.diarize(audioBuffer);
    const attributedTranscript = this.diarization.assignSpeakers(transcript, speakers);
    const claims = await this.extractClaims(attributedTranscript);
    const verifiedClaims = await Promise.all(
      claims.map(async (claim) => {
        const result2 = await this.verification.verify(claim.text);
        return {
          ...claim,
          ...result2
        };
      })
    );
    const report = {
      job_id: jobId,
      timestamp: (/* @__PURE__ */ new Date()).toISOString(),
      claims: verifiedClaims
    };
    FactFlowReportSchema.parse(report);
    const metrics8 = {
      job_id: jobId,
      processing_time_ms: Date.now() - startTime,
      audio_duration_sec: transcript.duration,
      cache_hit: false,
      // TODO: Implement cache
      claims_count: verifiedClaims.length,
      verified_count: verifiedClaims.filter((c) => c.verdict === "verified").length,
      needs_review_count: verifiedClaims.filter((c) => c.verdict === "needs_review").length
    };
    return { report, metrics: metrics8 };
  }
  async extractClaims(transcript) {
    return transcript.segments.map((seg, index) => ({
      id: generateEvidenceId(seg.text + index),
      // Deterministic ID
      text: seg.text,
      speaker: seg.speaker,
      time_range: { start: seg.start, end: seg.end },
      verdict: "unverified"
      // Default before verification
    }));
  }
};

// src/factflow/adapters/transcription.ts
var MockTranscriptionAdapter = class {
  async transcribe(audioBuffer) {
    await new Promise((resolve2) => setTimeout(resolve2, 100));
    return {
      duration: 60,
      segments: [
        {
          start: 0,
          end: 5,
          text: "The economy grew by 5% last quarter.",
          confidence: 0.95
        },
        {
          start: 5,
          end: 10,
          text: "Inflation is down to 2%.",
          confidence: 0.92
        },
        {
          start: 10,
          end: 15,
          text: "Unemployment is at a record low.",
          confidence: 0.88
        }
      ]
    };
  }
};

// src/factflow/adapters/diarization.ts
var MockDiarizationAdapter = class {
  async diarize(audioBuffer) {
    await new Promise((resolve2) => setTimeout(resolve2, 50));
    return [
      { start: 0, end: 5, speaker: "Speaker A" },
      { start: 5, end: 10, speaker: "Speaker B" },
      { start: 10, end: 15, speaker: "Speaker A" }
    ];
  }
  assignSpeakers(transcript, turns) {
    const newSegments = transcript.segments.map((seg) => {
      const mid = (seg.start + seg.end) / 2;
      const turn = turns.find((t) => mid >= t.start && mid <= t.end);
      return {
        ...seg,
        speaker: turn ? turn.speaker : "Unknown"
      };
    });
    return {
      ...transcript,
      segments: newSegments
    };
  }
};

// src/factflow/verification.ts
var MockVerificationEngine = class {
  async verify(text) {
    await new Promise((resolve2) => setTimeout(resolve2, 200));
    if (text.includes("5%")) {
      return {
        verdict: "verified",
        confidence: 0.95,
        evidence: [
          {
            source: "Bureau of Economic Analysis",
            url: "https://example.com/bea",
            snippet: "GDP increased at an annual rate of 4.9 percent (approx 5%) in the third quarter."
          }
        ]
      };
    } else if (text.includes("2%")) {
      return {
        verdict: "disputed",
        confidence: 0.7,
        evidence: [
          {
            source: "Labor Bureau",
            snippet: "Inflation remains at 3.2%."
          }
        ]
      };
    } else if (text.includes("record low")) {
      return {
        verdict: "verified",
        confidence: 0.9,
        evidence: [{ source: "BLS", snippet: "3.7% unemployment" }]
      };
    }
    return {
      verdict: "needs_review",
      confidence: 0.5,
      evidence: []
    };
  }
};

// src/factflow/gate.ts
var PublishGate = class {
  checkPublishPermission(user, report) {
    if (user.role !== "EDITOR" && user.role !== "ADMIN") {
      console.warn(`User ${user.id} denied publish: insufficient role ${user.role}`);
      return false;
    }
    const hasUnreviewedClaims = report.claims.some(
      (c) => c.verdict === "needs_review" || c.verdict === "unverified"
    );
    if (hasUnreviewedClaims && user.role !== "ADMIN") {
      console.warn(`User ${user.id} denied publish: content needs review`);
      return false;
    }
    return true;
  }
};

// src/factflow/routes.ts
import { randomUUID as randomUUID71 } from "node:crypto";
var router101 = Router51();
var transcription = new MockTranscriptionAdapter();
var diarization = new MockDiarizationAdapter();
var verification = new MockVerificationEngine();
var engine = new FactFlowEngine(transcription, diarization, verification);
var gate = new PublishGate();
var jobStore = /* @__PURE__ */ new Map();
router101.post("/jobs", async (req, res) => {
  try {
    const { audioBase64 } = req.body;
    const jobId = randomUUID71();
    jobStore.set(jobId, { status: "processing" });
    const audioBuffer = Buffer.from(audioBase64 || "", "base64");
    engine.process(jobId, audioBuffer).then((result2) => {
      jobStore.set(jobId, { status: "completed", result: result2 });
    }).catch((err) => {
      jobStore.set(jobId, { status: "failed", error: err.message });
    });
    res.json({ job_id: jobId, status: "processing" });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
router101.get("/jobs/:id", (req, res) => {
  const jobId = req.params.id;
  const job = jobStore.get(jobId);
  if (!job) {
    return res.status(404).json({ error: "Job not found" });
  }
  res.json(job);
});
router101.post("/jobs/:id/publish", (req, res) => {
  const jobId = req.params.id;
  const job = jobStore.get(jobId);
  if (!job || job.status !== "completed") {
    return res.status(400).json({ error: "Job not ready or not found" });
  }
  const user = req.user;
  if (!user) {
    return res.status(401).json({ error: "Authentication required" });
  }
  const allowed = gate.checkPublishPermission(user, job.result.report);
  if (allowed) {
    return res.json({ status: "published", published_at: (/* @__PURE__ */ new Date()).toISOString() });
  } else {
    return res.status(403).json({ error: "Publish denied by policy" });
  }
});
var routes_default2 = router101;

// src/runtime/global/FailoverOrchestrator.ts
init_regional_config();
init_RegionalAvailabilityService();
init_logger2();
import axios9 from "axios";
var FailoverOrchestrator = class _FailoverOrchestrator {
  static instance;
  intervalId = null;
  CHECK_INTERVAL_MS = 3e4;
  // 30 seconds
  FAILURE_THRESHOLD = 3;
  failureCounts = {};
  constructor() {
  }
  static getInstance() {
    if (!_FailoverOrchestrator.instance) {
      _FailoverOrchestrator.instance = new _FailoverOrchestrator();
    }
    return _FailoverOrchestrator.instance;
  }
  start() {
    if (this.intervalId) return;
    logger_default2.info("FailoverOrchestrator: Starting regional health monitoring");
    this.intervalId = setInterval(() => this.checkHealth(), this.CHECK_INTERVAL_MS);
  }
  stop() {
    if (this.intervalId) {
      clearInterval(this.intervalId);
      this.intervalId = null;
    }
  }
  async checkHealth() {
    const currentRegion = getCurrentRegion();
    const availability = RegionalAvailabilityService.getInstance();
    const status = availability.getStatus();
    if (status.failoverMode !== "AUTOMATIC") {
      logger_default2.debug("FailoverOrchestrator: Automatic failover is disabled, skipping health checks");
      return;
    }
    for (const [region, config9] of Object.entries(REGIONAL_CONFIG)) {
      if (region === currentRegion) continue;
      try {
        await axios9.get(`${config9.baseUrl}/api/health`, { timeout: 5e3 });
        if (this.failureCounts[region] > 0) {
          logger_default2.info({ region }, "FailoverOrchestrator: Region recovered");
          this.failureCounts[region] = 0;
          availability.setRegionStatus(region, "HEALTHY");
        }
      } catch (error) {
        this.failureCounts[region] = (this.failureCounts[region] || 0) + 1;
        logger_default2.warn({
          region,
          failureCount: this.failureCounts[region],
          error: error.message
        }, "FailoverOrchestrator: Regional health check failed");
        if (this.failureCounts[region] >= this.FAILURE_THRESHOLD) {
          const currentState = status.regions[region];
          if (currentState && currentState.status !== "DOWN") {
            logger_default2.error({ region }, "FailoverOrchestrator: Region threshold exceeded. Marking region as DOWN.");
            availability.setRegionStatus(region, "DOWN");
          }
        }
      }
    }
  }
};
var failoverOrchestrator = FailoverOrchestrator.getInstance();

// src/routes/approvals.ts
init_logger();
init_approvals();
import express55 from "express";
var approvalsLogger2 = logger_default.child({ name: "ApprovalsRouter" });
var resolveUserId = (req) => {
  const user = req.user;
  return user?.sub || user?.id || null;
};
var ensureApprover = (req, res, next) => {
  const role = req.user?.role;
  if (!canApprove(role)) {
    return res.status(403).json({ error: "Approval permissions required" });
  }
  next();
};
function buildApprovalsRouter(maestro) {
  const router105 = express55.Router();
  const singleParam27 = (value) => Array.isArray(value) ? value[0] : typeof value === "string" ? value : void 0;
  router105.use(express55.json());
  router105.post("/", async (req, res, next) => {
    try {
      const requesterId = resolveUserId(req) || req.body.requesterId;
      if (!requesterId) {
        return res.status(400).json({ error: "requesterId is required" });
      }
      const approval = await createApproval({
        requesterId,
        action: req.body.action,
        payload: req.body.payload,
        reason: req.body.reason,
        runId: req.body.runId
      });
      res.status(201).json(approval);
    } catch (error) {
      next(error);
    }
  });
  router105.get("/", async (req, res, next) => {
    try {
      const role = req.user?.role;
      const userId = resolveUserId(req);
      const status = req.query.status || void 0;
      const approvals = await listApprovals({ status });
      const visible = canApprove(role) ? approvals : approvals.filter((item) => item.requester_id === userId);
      res.json(visible);
    } catch (error) {
      next(error);
    }
  });
  router105.get("/:id", async (req, res, next) => {
    try {
      const approval = await getApprovalById(singleParam27(req.params.id) ?? "");
      if (!approval) {
        return res.status(404).json({ error: "Approval not found" });
      }
      res.json(approval);
    } catch (error) {
      next(error);
    }
  });
  router105.post("/:id/approve", ensureApprover, async (req, res, next) => {
    try {
      const approverId = resolveUserId(req);
      if (!approverId) {
        return res.status(400).json({ error: "approverId is required" });
      }
      const approval = await approveApproval(
        singleParam27(req.params.id) ?? "",
        approverId,
        req.body?.reason
      );
      if (!approval) {
        return res.status(409).json({ error: "Approval not pending or not found" });
      }
      let actionResult = null;
      if (maestro && approval.action === "maestro_run" && approval.payload) {
        const payload = approval.payload;
        actionResult = await maestro.runPipeline(
          payload.userId || approverId,
          String(payload.requestText || "")
        );
      } else if (maestro && approval.action === "maestro_task_execution" && approval.payload) {
        const payload = approval.payload;
        const task = await maestro.getTask(payload.taskId);
        if (task) {
          actionResult = await maestro.executeTask(task);
        }
      }
      approvalsLogger2.info(
        {
          approval_id: approval.id,
          action: approval.action,
          approver: approverId,
          run_id: approval.run_id
        },
        "Approval executed"
      );
      res.json({ approval, actionResult });
    } catch (error) {
      next(error);
    }
  });
  router105.post("/:id/reject", ensureApprover, async (req, res, next) => {
    try {
      const approverId = resolveUserId(req);
      if (!approverId) {
        return res.status(400).json({ error: "approverId is required" });
      }
      const approval = await rejectApproval(
        singleParam27(req.params.id) ?? "",
        approverId,
        req.body?.reason
      );
      if (!approval) {
        return res.status(409).json({ error: "Approval not pending or not found" });
      }
      approvalsLogger2.info(
        {
          approval_id: approval.id,
          action: approval.action,
          approver: approverId,
          run_id: approval.run_id
        },
        "Approval rejected"
      );
      res.json({ approval });
    } catch (error) {
      next(error);
    }
  });
  return router105;
}

// src/app.ts
var pinoHttp = pinoHttpModule.default || pinoHttpModule;
var createApp = async () => {
  const tracer11 = initializeTracing();
  if (!tracer11.isInitialized()) {
    await tracer11.initialize();
  }
  const app = express57();
  const logger72 = pino71();
  const isProduction = cfg.NODE_ENV === "production";
  const allowedOrigins = cfg.CORS_ORIGIN.split(",").map((origin) => origin.trim()).filter(Boolean);
  const securityHeadersEnabled = process.env.SECURITY_HEADERS_ENABLED !== "false";
  const cspReportOnly = process.env.SECURITY_HEADERS_CSP_REPORT_ONLY === "true";
  const cspEnabledFlag = process.env.SECURITY_HEADERS_CSP_ENABLED === "true";
  const safetyState = await resolveSafetyState();
  if (safetyState.killSwitch || safetyState.safeMode) {
    logger.warn({ safetyState }, "Safety gates enabled");
  }
  app.use(correlationIdMiddleware);
  app.use(featureFlagContextMiddleware);
  app.use(overloadProtection);
  app.use(compression());
  app.use(hpp());
  app.use(
    securityHeaders({
      enabled: securityHeadersEnabled,
      allowedOrigins,
      enableCsp: cspEnabledFlag || isProduction,
      cspReportOnly
    })
  );
  app.use(
    cors({
      origin: (origin, callback) => {
        if (!origin || cfg.NODE_ENV !== "production") {
          return callback(null, true);
        }
        if (allowedOrigins.includes(origin)) {
          return callback(null, true);
        }
        return callback(new Error(`Origin ${origin} not allowed by Summit CORS policy`));
      },
      credentials: true
    })
  );
  app.use(publicRateLimit);
  app.use(abuseGuard.middleware());
  app.use((req, res, next) => {
    req.log = logger;
    next();
  });
  app.use(requestProfilingMiddleware);
  app.use(
    express57.json({
      limit: "1mb",
      verify: (req, _res, buf) => {
        req.rawBody = buf;
      }
    })
  );
  app.use(sanitizeInput);
  app.use(securityHardening);
  app.use(piiGuardMiddleware);
  app.use(safetyModeMiddleware);
  app.use(circuitBreakerMiddleware);
  app.use(auditLogger);
  app.use(auditFirstMiddleware);
  app.use(httpCacheMiddleware);
  app.use((req, res, next) => {
    const version = req.headers["x-ig-api-version"];
    if (!version) {
      req.headers["x-ig-api-version"] = "1.1";
    }
    req.apiVersion = req.headers["x-ig-api-version"];
    if (req.apiVersion === "1.0") {
    }
    next();
  });
  const {
    productionAuthMiddleware: productionAuthMiddleware2,
    applyProductionSecurity: applyProductionSecurity2
  } = await Promise.resolve().then(() => (init_production_security(), production_security_exports));
  if (cfg.NODE_ENV === "production") {
    applyProductionSecurity2(app);
  }
  const authenticateToken = cfg.NODE_ENV === "production" ? productionAuthMiddleware2 : (req, res, next) => {
    const authHeader = req.headers["authorization"];
    const token = authHeader && authHeader.split(" ")[1];
    if (token) {
      return next();
    }
    if (process.env.ENABLE_INSECURE_DEV_AUTH === "true") {
      console.warn("Development: No token provided, allowing request (ENABLE_INSECURE_DEV_AUTH=true)");
      req.user = {
        sub: "dev-user",
        email: "dev@intelgraph.local",
        role: "admin"
      };
      return next();
    }
    res.status(401).json({ error: "Unauthorized", message: "No token provided" });
  };
  const isPublicWebhook = (req) => {
    return req.path.startsWith("/webhooks/github") || req.path.startsWith("/webhooks/jira") || req.path.startsWith("/webhooks/lifecycle");
  };
  app.use(["/api", "/graphql"], (req, res, next) => {
    if (isPublicWebhook(req)) return next();
    return tenantContext_default()(req, res, next);
  });
  app.use(["/api", "/graphql"], shadowTrafficMiddleware);
  app.use(["/api", "/graphql"], admissionControl);
  app.use(["/api", "/graphql"], (req, res, next) => {
    if (isPublicWebhook(req)) return next();
    return authenticatedRateLimit(req, res, next);
  });
  app.use(["/api", "/graphql"], (req, res, next) => {
    if (isPublicWebhook(req)) return next();
    return residencyEnforcement(req, res, next);
  });
  app.use("/api/residency/exceptions", authenticateToken, routes_default);
  app.use((req, res, next) => {
    snapshotter.trackRequest(req);
    const start = process.hrtime();
    telemetry.incrementActiveConnections();
    telemetry.subsystems.api.requests.add();
    res.on("finish", () => {
      snapshotter.untrackRequest(req);
      const diff = process.hrtime(start);
      const duration = diff[0] * 1e3 + diff[1] * 1e-6;
      telemetry.recordRequest(duration, {
        method: req.method,
        route: req.route?.path ?? req.path,
        status: res.statusCode
      });
      telemetry.decrementActiveConnections();
      if (res.statusCode >= 500) {
        telemetry.subsystems.api.errors.add();
      }
    });
    next();
  });
  const healthRouter = (await Promise.resolve().then(() => (init_health2(), health_exports))).default;
  app.use(healthRouter);
  app.use("/api-docs", ...swaggerUi.serve, swaggerUi.setup(swaggerSpec));
  app.use((req, res, next) => {
    if (req.path === "/graphql") return next();
    return advancedRateLimiter.middleware()(req, res, next);
  });
  app.get("/api/admin/rate-limits/:userId", authenticateToken, ensureRole(["ADMIN", "admin"]), async (req, res) => {
    try {
      const userId = Array.isArray(req.params.userId) ? req.params.userId[0] : req.params.userId;
      const status = await advancedRateLimiter.getStatus(userId);
      res.json(status);
    } catch (err) {
      res.status(500).json({ error: "Failed to fetch rate limit status" });
    }
  });
  app.use("/auth", authRoutes_default);
  app.use("/auth/sso", sso_default);
  app.use("/api/auth", authRoutes_default);
  app.use("/sso", sso_default);
  app.use("/api/policies", policy_management_default);
  app.use("/policies", policy_management_default);
  app.use("/api/receipts", receipts_default);
  app.use("/api/brand-packs", brand_pack_routes_default);
  app.use(["/monitoring", "/api/monitoring"], authenticateToken, monitoring_default);
  app.use("/api", monitoring_backpressure_default);
  app.use("/api/ga-core-metrics", ga_core_metrics_default);
  if (process.env.SKIP_AI_ROUTES !== "true") {
    const { default: aiRouter } = await Promise.resolve().then(() => (init_ai(), ai_exports));
    app.use("/api/ai", aiRouter);
  }
  app.use("/api/ai/nl-graph-query", nl_graph_query_default);
  app.use("/api/narrative-sim", narrative_sim_default);
  app.use("/api/narrative", narrative_routes_default);
  app.use("/api/predictive", predictive_default);
  app.use("/api/export", disclosures_default);
  app.use("/disclosures", disclosures_default);
  app.use("/rbac", rbacRoutes_default);
  app.use("/api/billing", billing_default);
  app.use("/api/er", entity_resolution_default);
  app.use("/api/workspaces", workspaces_default);
  if (process.env.SKIP_WEBHOOKS !== "true") {
    const { default: webhookRouter } = await Promise.resolve().then(() => (init_webhooks(), webhooks_exports));
    app.use("/api/webhooks", webhookRouter);
  }
  app.use("/api/support", support_tickets_default);
  app.use("/api", ticket_links_default2);
  app.use("/api/cases", cases_default);
  app.use("/api/entities", entity_comments_default);
  app.use("/api/aurora", auroraRouter);
  app.use("/api/oracle", oracleRouter);
  app.use("/api/phantom-limb", phantomLimbRouter);
  app.use("/api/echelon2", echelon2Router);
  app.use("/api/mnemosyne", mnemosyneRouter);
  app.use("/api/necromancer", necromancerRouter);
  app.use("/api/zero-day", zeroDayRouter);
  app.use("/api/abyss", abyssRouter);
  app.use("/api/qaf", qaf_default);
  app.use("/api/siem-platform", siem_platform_default);
  app.use("/api/maestro", maestro_default);
  app.use("/api/mcp-apps", mcp_apps_default);
  app.use("/api/tenants", tenants_default);
  app.use("/api/actions", actionsRouter);
  app.use("/api/osint", osint_default);
  app.use("/api/meta-orchestrator", meta_orchestrator_default);
  app.use("/api", admin_smoke_default);
  app.use("/api/scenarios", scenarios_default);
  app.use("/api/costs", resource_costs_default);
  app.use("/api/tenants/:tenantId/billing", billing_default2);
  app.use("/api/tenants/:tenantId/usage", usage_default);
  app.use("/api/internal/command-console", command_console_default);
  app.use("/api/correctness", correctness_program_default);
  app.use("/api", query_preview_stream_default);
  app.use("/api/stream", stream_default);
  app.use("/api/v1/search", search_v1_default);
  app.use("/api/ontology", ontology_default);
  app.use("/search", search_index_default);
  app.use("/api", data_governance_routes_default);
  app.use("/api", sharing_default);
  app.use("/api/gtm", gtmRouter);
  app.use("/airgap", airgapRouter);
  app.use("/analytics", analytics_default);
  app.use("/api", experiments_default);
  app.use("/api", cohorts_default);
  app.use("/api", funnels_default);
  app.use("/api", anomalies_default);
  app.use("/api", exports_default);
  app.use("/api", retention_default);
  app.use("/api/policy-profiles", policy_profiles_default);
  app.use("/api/policy-proposals", authenticateToken, policy_proposals_default);
  app.use("/api/evidence", evidence_default2);
  app.use("/dr", dr_default);
  app.use("/", ops_default);
  app.use("/api/reporting", reporting_default);
  app.use("/api/mastery", mastery_default);
  app.use("/api/crypto-intelligence", crypto_intelligence_default);
  app.use("/api/demo", demo_default);
  app.use("/api/claims", claims_default);
  app.use("/api/feature-flags", feature_flags_default);
  app.use("/api/ml-reviews", ml_review_default);
  app.use("/api/admin/flags", admin_flags_default);
  app.use("/api", audit_events_default);
  app.use("/api", federated_campaign_radar_default);
  app.use("/api/admin", authenticateToken, ensureRole(["ADMIN", "admin"]), gateway_default);
  app.use("/api/plugins", authenticateToken, ensureRole(["ADMIN", "admin"]), plugin_admin_default);
  app.use("/api/integrations", authenticateToken, ensureRole(["ADMIN", "admin"]), integration_admin_default);
  app.use("/api/security", authenticateToken, ensureRole(["ADMIN", "admin"]), security_admin_default);
  app.use("/api/compliance", authenticateToken, ensureRole(["ADMIN", "admin"]), compliance_admin_default);
  app.use("/api/sandbox", authenticateToken, ensureRole(["ADMIN", "admin"]), sandbox_admin_default);
  app.use("/api/v1/onboarding", onboarding_default);
  app.use("/api/v1/support", support_center_default);
  app.use("/api/v1/i18n", i18n_default);
  app.use("/api/v1/experiments", experimentation_default);
  app.use("/api/v1/palettes", palettes_default);
  app.use("/api/v4", router96);
  app.use("/api/vector-store", vector_store_default);
  app.use("/api/intel-graph", intel_graph_default);
  app.use("/api/graphrag", graphrag_default2);
  app.use("/api/intent", intent_default);
  if (cfg.FACTFLOW_ENABLED) {
    app.use("/api/factflow", routes_default2);
  }
  app.get("/metrics", metricsRoute);
  app.use("/api/approvals", authenticateToken, buildApprovalsRouter());
  SummitInvestigate.initialize(app);
  process.stdout.write("[DEBUG] SummitInvestigate initialized\n");
  const { buildMaestroRouter: buildMaestroRouter2 } = await Promise.resolve().then(() => (init_maestro_routes(), maestro_routes_exports));
  const { Maestro: Maestro2 } = await Promise.resolve().then(() => (init_core(), core_exports));
  const { MaestroQueries: MaestroQueries2 } = await Promise.resolve().then(() => (init_queries(), queries_exports));
  const { IntelGraphClientImpl: IntelGraphClientImpl2 } = await Promise.resolve().then(() => (init_client_impl(), client_impl_exports));
  const { CostMeter: CostMeter2 } = await Promise.resolve().then(() => (init_cost_meter(), cost_meter_exports));
  const igClient = new IntelGraphClientImpl2();
  const costMeter = new CostMeter2(igClient, {
    "openai:gpt-4.1": { inputPer1K: 0.01, outputPer1K: 0.03 }
  });
  const llmClient = {
    apiKey: "stub-key",
    costMeter,
    fakeOpenAIChatCompletion: async () => "stub",
    callCompletion: async (prompt, model) => `[Stub LLM Response] for: ${prompt}`
  };
  const maestro = new Maestro2(igClient, costMeter, llmClient, {
    defaultPlannerAgent: "openai:gpt-4.1",
    defaultActionAgent: "openai:gpt-4.1"
  });
  const maestroQueries = new MaestroQueries2(igClient);
  app.use("/api/maestro", buildMaestroRouter2(maestro, maestroQueries));
  app.use("/api/approvals", authenticateToken, buildApprovalsRouter(maestro));
  process.stdout.write("[DEBUG] Maestro router built\n");
  try {
    const { MaestroEngine: MaestroEngine2 } = await Promise.resolve().then(() => (init_engine2(), engine_exports));
    const { MaestroHandlers: MaestroHandlers2 } = await Promise.resolve().then(() => (init_handlers(), handlers_exports));
    const { MaestroAgentService: MaestroAgentService2 } = await Promise.resolve().then(() => (init_agent_service(), agent_service_exports));
    const { DiffusionCoderAdapter: DiffusionCoderAdapter2 } = await Promise.resolve().then(() => (init_diffusion_coder(), diffusion_coder_exports));
    const { getPostgresPool: getPostgresPool3 } = await Promise.resolve().then(() => (init_postgres(), postgres_exports));
    const { getRedisClient: getRedisClient3 } = await Promise.resolve().then(() => (init_redis(), redis_exports));
    const pool4 = getPostgresPool3();
    const redis5 = getRedisClient3();
    const engineV2 = new MaestroEngine2({
      db: pool4,
      redisConnection: redis5
    });
    const agentService = new MaestroAgentService2(pool4);
    const llmServiceV2 = {
      callCompletion: async (runId, taskId, payload) => {
        const result2 = await llmClient.callCompletion(payload.messages[payload.messages.length - 1].content, payload.model);
        return {
          content: typeof result2 === "string" ? result2 : result2.content || JSON.stringify(result2),
          usage: { total_tokens: 0 }
        };
      }
    };
    const diffusionCoder = new DiffusionCoderAdapter2(llmServiceV2);
    const handlersV2 = new MaestroHandlers2(
      engineV2,
      agentService,
      llmServiceV2,
      { executeAlgorithm: async () => ({}) },
      diffusionCoder
    );
    handlersV2.registerAll();
    process.stdout.write("[DEBUG] Maestro V2 Engine & Handlers initialized\n");
  } catch (err) {
    logger.error({ err }, "Failed to initialize Maestro V2 Engine");
  }
  app.get("/search/evidence", authenticateToken, async (req, res) => {
    const { q, skip = 0, limit = 10 } = req.query;
    if (!q) {
      return res.status(400).send({ error: "Query parameter 'q' is required" });
    }
    const driver3 = getNeo4jDriver();
    const session = driver3.session();
    try {
      const searchQuery = `
        CALL db.index.fulltext.queryNodes("evidenceContentSearch", $query) YIELD node, score
        RETURN node, score
        SKIP $skip
        LIMIT $limit
      `;
      const countQuery = `
        CALL db.index.fulltext.queryNodes("evidenceContentSearch", $query) YIELD node
        RETURN count(node) as total
      `;
      const [searchResult, countResult] = await Promise.all([
        session.run(searchQuery, {
          query: q,
          skip: Number(skip),
          limit: Number(limit)
        }),
        session.run(countQuery, { query: q })
      ]);
      const evidence = searchResult.records.map((record2) => ({
        node: record2.get("node").properties,
        score: record2.get("score")
      }));
      const total = countResult.records[0].get("total").toNumber();
      res.send({
        data: evidence,
        metadata: {
          total,
          skip: Number(skip),
          limit: Number(limit),
          pages: Math.ceil(total / Number(limit)),
          currentPage: Math.floor(Number(skip) / Number(limit)) + 1
        }
      });
    } catch (error) {
      logger.error(
        `Error in search/evidence: ${error instanceof Error ? error.message : "Unknown error"}`
      );
      res.status(500).send({ error: "Internal server error" });
    } finally {
      await session.close();
    }
  });
  if (process.env.SKIP_GRAPHQL !== "true") {
    const { typeDefs: typeDefs2 } = await Promise.resolve().then(() => (init_schema2(), schema_exports));
    const { default: resolvers3 } = await Promise.resolve().then(() => (init_resolvers2(), resolvers_exports));
    process.stdout.write("[DEBUG] GraphQL resolvers imported\n");
    const executableSchema = makeExecutableSchema({
      typeDefs: typeDefs2,
      resolvers: resolvers3
    });
    const schema2 = executableSchema;
    const { persistedQueriesPlugin: persistedQueriesPlugin2 } = await Promise.resolve().then(() => (init_persistedQueries(), persistedQueries_exports));
    const { default: pbacPlugin2 } = await Promise.resolve().then(() => (init_pbac(), pbac_exports));
    const { default: resolverMetricsPlugin2 } = await Promise.resolve().then(() => (init_resolverMetrics(), resolverMetrics_exports));
    const { default: auditLoggerPlugin2 } = await Promise.resolve().then(() => (init_auditLogger(), auditLogger_exports));
    const { depthLimit: depthLimit2 } = await Promise.resolve().then(() => (init_depthLimit(), depthLimit_exports));
    const { rateLimitAndCachePlugin: rateLimitAndCachePlugin2 } = await Promise.resolve().then(() => (init_rateLimitAndCache(), rateLimitAndCache_exports));
    const { httpStatusCodePlugin: httpStatusCodePlugin2 } = await Promise.resolve().then(() => (init_httpStatusCodePlugin(), httpStatusCodePlugin_exports));
    const apollo = new ApolloServer({
      schema: schema2,
      // Security plugins - Order matters for execution lifecycle
      plugins: [
        httpStatusCodePlugin2(),
        // Must be first to set HTTP status codes
        persistedQueriesPlugin2,
        resolverMetricsPlugin2,
        auditLoggerPlugin2,
        // rateLimitAndCachePlugin(schema) as any,
        // Enable PBAC in production
        ...cfg.NODE_ENV === "production" ? [pbacPlugin2()] : []
      ],
      // Security configuration based on environment
      introspection: cfg.NODE_ENV !== "production",
      // Enhanced query validation rules
      validationRules: [
        depthLimit2(cfg.NODE_ENV === "production" ? 6 : 8)
        // Stricter in prod
      ],
      // Security context
      formatError: (formattedError, error) => {
        if (formattedError.extensions?.code === "GRAPHQL_VALIDATION_FAILED" || formattedError.extensions?.code === "BAD_USER_INPUT" || formattedError.extensions?.code === "UNAUTHENTICATED" || formattedError.extensions?.code === "FORBIDDEN") {
          return formattedError;
        }
        if (cfg.NODE_ENV === "production") {
          logger.error(
            { err: error, stack: error?.stack },
            `GraphQL Error: ${formattedError.message}`
          );
          return new GraphQLError15("Internal server error", {
            extensions: {
              code: "INTERNAL_SERVER_ERROR",
              http: { status: 500 }
            }
          });
        }
        return formattedError;
      }
    });
    process.stdout.write("[DEBUG] Apollo Server created, starting...\n");
    await apollo.start();
    process.stdout.write("[DEBUG] Apollo Server started\n");
    app.use(
      "/graphql",
      express57.json(),
      authenticateToken,
      ...process.env.SKIP_RATE_LIMITS === "true" ? [] : [advancedRateLimiter.middleware()],
      // Applied AFTER authentication to enable per-user limits
      expressMiddleware(apollo, {
        context: async ({ req }) => getContext({ req })
      })
    );
  } else {
    logger.warn("GraphQL disabled via SKIP_GRAPHQL");
  }
  if (!safetyState.killSwitch && !safetyState.safeMode && process.env.NODE_ENV !== "test") {
    startTrustWorker();
    startRetentionWorker();
    if (cfg.KAFKA_ENABLED) {
      streamIngest.start(["ingest-events"]).catch((err) => {
        logger.error({ err }, "Failed to start streaming ingestion");
      });
    } else {
      logger.info("Streaming ingestion disabled (KAFKA_ENABLED=false)");
    }
  } else {
    logger.warn(
      { safetyState, env: process.env.NODE_ENV },
      "Skipping background workers because safety mode, kill switch or test environment is enabled"
    );
  }
  if (process.env.SKIP_WEBHOOKS !== "true") {
    const { webhookWorker: webhookWorker2 } = await Promise.resolve().then(() => (init_webhook_worker(), webhook_worker_exports));
    if (webhookWorker2) {
    }
  }
  logger.info("Anomaly detector activated.");
  if (process.env.NODE_ENV !== "test") {
    failoverOrchestrator.start();
  }
  app.use(centralizedErrorHandler);
  return app;
};

// src/index.ts
init_metrics2();

// src/services/PartitionMaintenanceService.ts
init_postgres();
init_logger();
import { CronJob } from "cron";
var logger66 = logger.child({ service: "PartitionMaintenanceService" });
var TABLES_TO_MAINTAIN = [
  "provenance_ledger_v2"
];
var PartitionMaintenanceService = class {
  job;
  constructor() {
    this.job = new CronJob("0 2 * * *", () => {
      this.maintainPartitions().catch((err) => {
        logger66.error({ err }, "Partition maintenance job failed");
      });
    });
  }
  start() {
    this.job.start();
    logger66.info("PartitionMaintenanceService started");
    this.maintainPartitions().catch((err) => {
      logger66.error({ err }, "Initial partition maintenance failed");
    });
  }
  stop() {
    this.job.stop();
  }
  async maintainPartitions() {
    logger66.info("Starting partition maintenance");
    const pool4 = getPostgresPool();
    try {
      await pool4.write("SELECT ensure_outbox_partition($1, $2)", [2, 6]);
      logger66.info({ tableName: "outbox_events" }, "Partition maintenance successful");
    } catch (error) {
      logger66.error({ tableName: "outbox_events", error }, "Failed to maintain partitions for table");
    }
    for (const tableName of TABLES_TO_MAINTAIN) {
      try {
        await this.ensureNextMonthPartition(pool4, tableName);
      } catch (error) {
        logger66.error({ tableName, error }, "Failed to maintain partitions for table");
      }
    }
  }
  async ensureNextMonthPartition(pool4, tableName) {
    const now = /* @__PURE__ */ new Date();
    const nextMonth = new Date(now.getFullYear(), now.getMonth() + 1, 1);
    const nextNextMonth = new Date(now.getFullYear(), now.getMonth() + 2, 1);
    const partitionName = `${tableName}_y${nextMonth.getFullYear()}m${String(nextMonth.getMonth() + 1).padStart(2, "0")}`;
    const startStr = nextMonth.toISOString().split("T")[0];
    const endStr = nextNextMonth.toISOString().split("T")[0];
    logger66.debug({ tableName, partitionName }, "Checking partition existence");
    const checkSql = `
      SELECT EXISTS (
        SELECT FROM pg_tables
        WHERE tablename = $1
      );
    `;
    const result2 = await pool4.read(checkSql, [partitionName]);
    const exists = result2.rows[0].exists;
    if (!exists) {
      logger66.info({ tableName, partitionName }, "Creating new partition");
      const createSql = `
        CREATE TABLE IF NOT EXISTS ${partitionName}
        PARTITION OF ${tableName}
        FOR VALUES FROM ('${startStr}') TO ('${endStr}');
      `;
      await pool4.write(createSql);
      logger66.info({ tableName, partitionName }, "Partition created successfully");
    } else {
      logger66.debug({ tableName, partitionName }, "Partition already exists");
    }
  }
};
var partitionMaintenanceService = new PartitionMaintenanceService();

// src/conductor/deployment/ZeroTouchOrchestrator.ts
init_logger();
init_redis();

// src/conductor/deployment/blue-green.ts
init_prometheus();
import { EventEmitter as EventEmitter21 } from "events";
import { spawn as spawn9 } from "child_process";
import Redis13 from "ioredis";

// src/conductor/deployment/AutomatedCanaryService.ts
init_logger();
var AutomatedCanaryService = class _AutomatedCanaryService {
  static instance;
  // Historical baseline (simulated)
  baseline = {
    errorRate: 5e-3,
    // 0.5%
    latencyP95: 150
    // 150ms
  };
  constructor() {
  }
  static getInstance() {
    if (!_AutomatedCanaryService.instance) {
      _AutomatedCanaryService.instance = new _AutomatedCanaryService();
    }
    return _AutomatedCanaryService.instance;
  }
  /**
   * Evaluates canary health using "ML-driven" logic.
   * Compares current metrics against baseline and production.
   */
  async analyze(canaryMetrics, productionMetrics) {
    logger.info({ canaryMetrics, productionMetrics }, "ACA: Starting automated analysis");
    const metrics8 = {
      errorRateAnomalous: this.isAnomalous(canaryMetrics.errorRate, productionMetrics.errorRate, 0.01),
      latencyAnomalous: this.isAnomalous(canaryMetrics.latencyP95, productionMetrics.latencyP95, 50),
      throughputDrop: canaryMetrics.throughput < productionMetrics.throughput * 0.8 && canaryMetrics.activeConnections > 0
    };
    let score = 100;
    const reasons = [];
    if (metrics8.errorRateAnomalous) {
      score -= 40;
      reasons.push("Error rate significantly higher than production");
    }
    if (metrics8.latencyAnomalous) {
      score -= 30;
      reasons.push("P95 latency degradation detected");
    }
    if (metrics8.throughputDrop) {
      score -= 20;
      reasons.push("Throughput drop relative to connection count");
    }
    let decision = "CONTINUE";
    if (score < 60) {
      decision = "ROLLBACK";
    } else if (score >= 90) {
      decision = "PROMOTE";
    }
    const result2 = {
      score,
      decision,
      reason: reasons.length > 0 ? reasons.join("; ") : "All metrics within acceptable bounds",
      metrics: metrics8
    };
    logger.info(result2, "ACA: Analysis complete");
    return result2;
  }
  /**
   * Simulated anomaly detection logic (Z-score like)
   */
  isAnomalous(value, baseline, threshold) {
    return value > baseline + threshold;
  }
};
var automatedCanaryService = AutomatedCanaryService.getInstance();

// src/conductor/deployment/blue-green.ts
var BlueGreenDeploymentEngine = class extends EventEmitter21 {
  redis;
  activeDeployments = /* @__PURE__ */ new Map();
  metricsInterval;
  constructor(redis5) {
    super();
    this.redis = redis5;
    this.startMetricsCollection();
  }
  /**
   * Execute blue-green deployment
   */
  async deploy(config9) {
    const deploymentId = `deploy_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    const execution = {
      id: deploymentId,
      config: config9,
      status: "pending",
      startTime: Date.now(),
      currentPhase: "initialization",
      phases: this.createDeploymentPhases(config9),
      healthMetrics: {
        errorRate: 0,
        latencyP50: 0,
        latencyP95: 0,
        latencyP99: 0,
        throughput: 0,
        cpuUsage: 0,
        memoryUsage: 0,
        activeConnections: 0
      },
      errors: []
    };
    this.activeDeployments.set(deploymentId, execution);
    await this.redis.setex(
      `deployment:${deploymentId}`,
      86400,
      // 24 hours
      JSON.stringify(execution)
    );
    this.executeDeployment(execution);
    this.emit("deployment:started", execution);
    return deploymentId;
  }
  /**
   * Create deployment phases based on strategy
   */
  createDeploymentPhases(config9) {
    const phases = [];
    phases.push({
      name: "pre_deployment_validation",
      status: "pending",
      actions: [
        { name: "validate_configuration", status: "pending" },
        { name: "check_resource_availability", status: "pending" },
        { name: "validate_image_availability", status: "pending" },
        { name: "run_pre_deployment_tests", status: "pending" }
      ],
      logs: []
    });
    if (config9.services.some((s) => s.migrations?.length)) {
      phases.push({
        name: "database_migrations",
        status: "pending",
        actions: [
          { name: "backup_databases", status: "pending" },
          { name: "execute_migrations", status: "pending" },
          { name: "validate_schema_integrity", status: "pending" }
        ],
        logs: []
      });
    }
    phases.push({
      name: "green_environment_preparation",
      status: "pending",
      actions: [
        { name: "create_green_deployment", status: "pending" },
        { name: "configure_green_services", status: "pending" },
        { name: "initialize_green_environment", status: "pending" }
      ],
      logs: []
    });
    phases.push({
      name: "health_validation",
      status: "pending",
      actions: [
        { name: "run_smoke_tests", status: "pending" },
        { name: "validate_service_health", status: "pending" },
        { name: "run_integration_tests", status: "pending" },
        { name: "performance_validation", status: "pending" }
      ],
      logs: []
    });
    if (config9.strategy === "canary") {
      phases.push({
        name: "canary_traffic_management",
        status: "pending",
        actions: [
          { name: "route_canary_traffic", status: "pending" },
          { name: "monitor_canary_metrics", status: "pending" },
          { name: "increment_traffic_split", status: "pending" }
        ],
        logs: []
      });
    } else {
      phases.push({
        name: "traffic_switching",
        status: "pending",
        actions: [
          { name: "switch_load_balancer", status: "pending" },
          { name: "validate_traffic_routing", status: "pending" },
          { name: "drain_blue_environment", status: "pending" }
        ],
        logs: []
      });
    }
    phases.push({
      name: "post_deployment_validation",
      status: "pending",
      actions: [
        { name: "validate_deployment_success", status: "pending" },
        { name: "cleanup_old_environment", status: "pending" },
        { name: "update_deployment_records", status: "pending" }
      ],
      logs: []
    });
    return phases;
  }
  /**
   * Execute deployment phases sequentially
   */
  async executeDeployment(execution) {
    execution.status = "preparing";
    this.emit("deployment:phase_changed", execution);
    try {
      for (const phase of execution.phases) {
        execution.currentPhase = phase.name;
        phase.status = "running";
        phase.startTime = Date.now();
        this.addLog(phase, `Starting phase: ${phase.name}`);
        try {
          await this.executePhase(execution, phase);
          phase.status = "completed";
          phase.endTime = Date.now();
          this.addLog(phase, `Phase completed: ${phase.name}`);
          if (["green_environment_preparation", "traffic_switching"].includes(
            phase.name
          )) {
            const healthOk = await this.validateHealth(execution);
            if (!healthOk && execution.config.environment === "production") {
              throw new Error("Health validation failed - initiating rollback");
            }
          }
        } catch (error) {
          phase.status = "failed";
          phase.endTime = Date.now();
          execution.errors.push(`Phase ${phase.name} failed: ${error.message}`);
          this.addLog(phase, `Phase failed: ${error.message}`);
          throw error;
        }
        await this.persistDeployment(execution);
        this.emit("deployment:phase_completed", { execution, phase });
      }
      execution.status = "completed";
      execution.endTime = Date.now();
      this.addLog(
        execution.phases[execution.phases.length - 1],
        "Deployment completed successfully"
      );
      this.emit("deployment:completed", execution);
      prometheusConductorMetrics.recordOperationalEvent(
        "deployment_success",
        { success: true }
      );
    } catch (error) {
      execution.status = "failed";
      execution.endTime = Date.now();
      execution.errors.push(`Deployment failed: ${error.message}`);
      this.emit("deployment:failed", { execution, error });
      if (execution.config.environment === "production") {
        await this.rollbackDeployment(execution, error.message);
      }
      prometheusConductorMetrics.recordOperationalEvent(
        "deployment_failure",
        { success: false }
      );
    }
    await this.persistDeployment(execution);
  }
  /**
   * Execute individual deployment phase
   */
  async executePhase(execution, phase) {
    for (const action of phase.actions) {
      action.status = "running";
      this.addLog(phase, `Executing action: ${action.name}`);
      const startTime = Date.now();
      try {
        await this.executeAction(execution, action);
        action.status = "completed";
        action.duration = Date.now() - startTime;
        this.addLog(
          phase,
          `Action completed: ${action.name} (${action.duration}ms)`
        );
      } catch (error) {
        action.status = "failed";
        action.error = error.message;
        action.duration = Date.now() - startTime;
        throw error;
      }
    }
  }
  /**
   * Execute individual action
   */
  async executeAction(execution, action) {
    switch (action.name) {
      case "validate_configuration":
        await this.validateConfiguration(execution);
        break;
      case "check_resource_availability":
        await this.checkResourceAvailability(execution);
        break;
      case "validate_image_availability":
        await this.validateImageAvailability(execution);
        break;
      case "run_pre_deployment_tests":
        await this.runPreDeploymentTests(execution);
        break;
      case "backup_databases":
        await this.backupDatabases(execution);
        break;
      case "execute_migrations":
        await this.executeMigrations(execution);
        break;
      case "validate_schema_integrity":
        await this.validateSchemaIntegrity(execution);
        break;
      case "create_green_deployment":
        await this.createGreenDeployment(execution);
        break;
      case "configure_green_services":
        await this.configureGreenServices(execution);
        break;
      case "initialize_green_environment":
        await this.initializeGreenEnvironment(execution);
        break;
      case "run_smoke_tests":
        await this.runSmokeTests(execution);
        break;
      case "validate_service_health":
        await this.validateServiceHealth(execution);
        break;
      case "run_integration_tests":
        await this.runIntegrationTests(execution);
        break;
      case "performance_validation":
        await this.performanceValidation(execution);
        break;
      case "route_canary_traffic":
        await this.routeCanaryTraffic(execution);
        break;
      case "monitor_canary_metrics":
        await this.monitorCanaryMetrics(execution);
        break;
      case "increment_traffic_split":
        await this.incrementTrafficSplit(execution);
        break;
      case "switch_load_balancer":
        await this.switchLoadBalancer(execution);
        break;
      case "validate_traffic_routing":
        await this.validateTrafficRouting(execution);
        break;
      case "drain_blue_environment":
        await this.drainBlueEnvironment(execution);
        break;
      case "validate_deployment_success":
        await this.validateDeploymentSuccess(execution);
        break;
      case "cleanup_old_environment":
        await this.cleanupOldEnvironment(execution);
        break;
      case "update_deployment_records":
        await this.updateDeploymentRecords(execution);
        break;
      default:
        throw new Error(`Unknown action: ${action.name}`);
    }
  }
  /**
   * Action implementations
   */
  async validateConfiguration(execution) {
    const { config: config9 } = execution;
    if (!config9.imageTag || !config9.services.length) {
      throw new Error("Invalid deployment configuration");
    }
    for (const service11 of config9.services) {
      if (!service11.name || !service11.image) {
        throw new Error(`Invalid service configuration: ${service11.name}`);
      }
    }
  }
  async checkResourceAvailability(execution) {
    const output = await this.executeCommand("kubectl get nodes -o json");
    const nodes = JSON.parse(output);
    if (!nodes.items || nodes.items.length === 0) {
      throw new Error("No available nodes in cluster");
    }
  }
  async validateImageAvailability(execution) {
    for (const service11 of execution.config.services) {
      const fullImage = `${service11.image}:${execution.config.imageTag}`;
      await this.executeCommand(`docker manifest inspect ${fullImage}`);
    }
  }
  async runPreDeploymentTests(execution) {
    if (!execution.config.validation.smokeTests) return;
    await this.executeCommand("npm run test:pre-deploy");
  }
  async backupDatabases(execution) {
    const timestamp = (/* @__PURE__ */ new Date()).toISOString().replace(/[:.]/g, "-");
    await this.executeCommand(
      `pg_dump ${process.env.POSTGRES_URL} > backup-${timestamp}.sql`
    );
    await this.executeCommand(
      `neo4j-admin export --database=neo4j backup-neo4j-${timestamp}.dump`
    );
  }
  async executeMigrations(execution) {
    for (const service11 of execution.config.services) {
      if (!service11.migrations?.length) continue;
      for (const migration of service11.migrations) {
        console.log(`Executing migration: ${migration.name}`);
        switch (migration.type) {
          case "postgres":
            await this.executeCommand(
              `psql ${process.env.POSTGRES_URL} -c "${migration.script}"`
            );
            break;
          case "neo4j":
            await this.executeCommand(
              `cypher-shell -u neo4j -p ${process.env.NEO4J_PASSWORD} "${migration.script}"`
            );
            break;
          case "redis":
            await this.executeCommand(`redis-cli EVAL "${migration.script}" 0`);
            break;
        }
      }
    }
  }
  async validateSchemaIntegrity(execution) {
    await this.executeCommand("npm run validate:schema");
  }
  async createGreenDeployment(execution) {
    const { config: config9 } = execution;
    const manifests = this.generateKubernetesManifests(config9, "green");
    for (const manifest of manifests) {
      await this.executeCommand(`echo '${manifest}' | kubectl apply -f -`);
    }
    for (const service11 of config9.services) {
      await this.executeCommand(
        `kubectl wait --for=condition=ready pod -l app=${service11.name}-green --timeout=300s`
      );
    }
  }
  async configureGreenServices(execution) {
    const { config: config9 } = execution;
    for (const service11 of config9.services) {
      const configMap = this.generateConfigMap(service11, "green");
      await this.executeCommand(`echo '${configMap}' | kubectl apply -f -`);
    }
  }
  async initializeGreenEnvironment(execution) {
    await this.executeCommand(
      "kubectl exec deployment/server-green -- npm run initialize"
    );
  }
  async runSmokeTests(execution) {
    if (!execution.config.validation.smokeTests) return;
    const greenUrl = await this.getGreenEnvironmentUrl(execution);
    await this.executeCommand(`API_URL=${greenUrl} npm run test:smoke`);
  }
  async validateServiceHealth(execution) {
    for (const healthCheck of execution.config.healthChecks) {
      await this.executeHealthCheck(healthCheck, "green");
    }
  }
  async runIntegrationTests(execution) {
    if (!execution.config.validation.integrationTests) return;
    const greenUrl = await this.getGreenEnvironmentUrl(execution);
    await this.executeCommand(`API_URL=${greenUrl} npm run test:integration`);
  }
  async performanceValidation(execution) {
    if (!execution.config.validation.performanceTests) return;
    const greenUrl = await this.getGreenEnvironmentUrl(execution);
    const output = await this.executeCommand(
      `k6 run --env API_URL=${greenUrl} performance-test.js`
    );
    const results = this.parseK6Results(output);
    if (results.errorRate > 0.01) {
      throw new Error(
        `Performance validation failed: ${results.errorRate}% error rate`
      );
    }
  }
  async routeCanaryTraffic(execution) {
    const { trafficSplit } = execution.config;
    await this.updateTrafficSplit("green", trafficSplit.canaryPercent);
  }
  async monitorCanaryMetrics(execution) {
    const monitoringDuration = 3e5;
    const startTime = Date.now();
    while (Date.now() - startTime < monitoringDuration) {
      const canaryMetrics = await this.collectHealthMetrics();
      const productionMetrics = await this.collectHealthMetrics();
      execution.healthMetrics = canaryMetrics;
      const acaResult = await automatedCanaryService.analyze(canaryMetrics, productionMetrics);
      this.addLog(
        execution.phases.find((p) => p.name === "canary_traffic_management"),
        `ACA Result: Score=${acaResult.score}, Decision=${acaResult.decision}, Reason=${acaResult.reason}`
      );
      if (acaResult.decision === "ROLLBACK") {
        throw new Error(`Automated Canary Analysis triggered ROLLBACK: ${acaResult.reason}`);
      }
      if (acaResult.decision === "PROMOTE") {
        this.addLog(execution.phases.find((p) => p.name === "canary_traffic_management"), "ACA recommended early promotion");
        return;
      }
      await new Promise((resolve2) => setTimeout(resolve2, 1e4));
    }
  }
  async incrementTrafficSplit(execution) {
    const { trafficSplit } = execution.config;
    let currentPercent = trafficSplit.canaryPercent;
    while (currentPercent < 100) {
      currentPercent = Math.min(
        100,
        currentPercent + trafficSplit.incrementPercent
      );
      await this.updateTrafficSplit("green", currentPercent);
      await new Promise((resolve2) => setTimeout(resolve2, 6e4));
      const metrics8 = await this.collectHealthMetrics();
      if (metrics8.errorRate > execution.config.rollbackThreshold.errorRate) {
        throw new Error(
          `Traffic increment failed: Error rate ${metrics8.errorRate}%`
        );
      }
    }
  }
  async switchLoadBalancer(execution) {
    await this.updateTrafficSplit("green", 100);
    await new Promise((resolve2) => setTimeout(resolve2, 3e4));
  }
  async validateTrafficRouting(execution) {
    const greenUrl = await this.getGreenEnvironmentUrl(execution);
    for (let i = 0; i < 10; i++) {
      const response = await this.executeCommand(
        `curl -s ${greenUrl}/api/health`
      );
      const health = JSON.parse(response);
      if (!health.status === "ok") {
        throw new Error("Traffic routing validation failed");
      }
    }
  }
  async drainBlueEnvironment(execution) {
    await this.executeCommand(
      "kubectl drain deployment/server-blue --grace-period=300"
    );
  }
  async validateDeploymentSuccess(execution) {
    const metrics8 = await this.collectHealthMetrics();
    if (metrics8.errorRate > 1e-3) {
      throw new Error(
        `Deployment validation failed: Error rate ${metrics8.errorRate}%`
      );
    }
  }
  async cleanupOldEnvironment(execution) {
    await this.executeCommand("kubectl delete deployment server-blue");
    await this.executeCommand("kubectl delete service server-blue");
  }
  async updateDeploymentRecords(execution) {
    await this.redis.hset("current_deployment", {
      version: execution.config.imageTag,
      timestamp: Date.now(),
      strategy: execution.config.strategy
    });
  }
  /**
   * Rollback deployment
   */
  async rollbackDeployment(execution, reason) {
    execution.status = "rolled_back";
    execution.rollbackReason = reason;
    console.log(`Rolling back deployment ${execution.id}: ${reason}`);
    try {
      await this.updateTrafficSplit("blue", 100);
      await this.rollbackMigrations(execution);
      await this.cleanupFailedDeployment(execution);
      this.emit("deployment:rolled_back", execution);
    } catch (rollbackError) {
      console.error("Rollback failed:", rollbackError);
      execution.errors.push(`Rollback failed: ${rollbackError.message}`);
    }
  }
  /**
   * Health validation
   */
  async validateHealth(execution) {
    const { rollbackThreshold } = execution.config;
    try {
      const metrics8 = await this.collectHealthMetrics();
      execution.healthMetrics = metrics8;
      return metrics8.errorRate <= rollbackThreshold.errorRate && metrics8.latencyP95 <= rollbackThreshold.latencyP95;
    } catch (error) {
      console.error("Health validation failed:", error);
      return false;
    }
  }
  /**
   * Helper methods
   */
  async executeCommand(command) {
    return new Promise((resolve2, reject) => {
      const child = spawn9("bash", ["-c", command]);
      let stdout = "";
      let stderr = "";
      child.stdout.on("data", (data) => {
        stdout += data.toString();
      });
      child.stderr.on("data", (data) => {
        stderr += data.toString();
      });
      child.on("close", (code) => {
        if (code === 0) {
          resolve2(stdout);
        } else {
          reject(new Error(`Command failed: ${command}
${stderr}`));
        }
      });
    });
  }
  async executeHealthCheck(healthCheck, environment) {
    for (let attempt = 0; attempt < healthCheck.retries; attempt++) {
      try {
        switch (healthCheck.type) {
          case "http":
            const url = healthCheck.target.replace(
              "{{environment}}",
              environment
            );
            await this.executeCommand(
              `curl -f -m ${healthCheck.timeout} ${url}`
            );
            return;
          case "tcp":
            await this.executeCommand(
              `nc -z -w${healthCheck.timeout} ${healthCheck.target}`
            );
            return;
          case "command":
            await this.executeCommand(healthCheck.target);
            return;
        }
      } catch (error) {
        if (attempt === healthCheck.retries - 1) {
          throw error;
        }
        await new Promise(
          (resolve2) => setTimeout(resolve2, healthCheck.interval)
        );
      }
    }
  }
  generateKubernetesManifests(config9, environment) {
    const manifests = [];
    for (const service11 of config9.services) {
      const deployment = {
        apiVersion: "apps/v1",
        kind: "Deployment",
        metadata: {
          name: `${service11.name}-${environment}`,
          labels: {
            app: `${service11.name}-${environment}`,
            version: config9.imageTag,
            environment
          }
        },
        spec: {
          replicas: service11.replicas,
          selector: {
            matchLabels: {
              app: `${service11.name}-${environment}`
            }
          },
          template: {
            metadata: {
              labels: {
                app: `${service11.name}-${environment}`,
                version: config9.imageTag,
                environment
              }
            },
            spec: {
              containers: [
                {
                  name: service11.name,
                  image: `${service11.image}:${config9.imageTag}`,
                  resources: service11.resources,
                  ports: service11.ports.map((port) => ({ containerPort: port })),
                  env: Object.entries(service11.environment).map(
                    ([key, value]) => ({
                      name: key,
                      value
                    })
                  ),
                  readinessProbe: {
                    httpGet: {
                      path: service11.healthEndpoint,
                      port: service11.ports[0]
                    },
                    initialDelaySeconds: 10,
                    periodSeconds: 5
                  },
                  livenessProbe: {
                    httpGet: {
                      path: service11.healthEndpoint,
                      port: service11.ports[0]
                    },
                    initialDelaySeconds: 30,
                    periodSeconds: 10
                  }
                }
              ]
            }
          }
        }
      };
      manifests.push(JSON.stringify(deployment));
    }
    return manifests;
  }
  generateConfigMap(service11, environment) {
    return JSON.stringify({
      apiVersion: "v1",
      kind: "ConfigMap",
      metadata: {
        name: `${service11.name}-${environment}-config`
      },
      data: service11.environment
    });
  }
  async getGreenEnvironmentUrl(execution) {
    const output = await this.executeCommand(
      'kubectl get service server-green -o jsonpath="{.status.loadBalancer.ingress[0].hostname}"'
    );
    return `http://${output.trim()}`;
  }
  async updateTrafficSplit(target, percentage) {
    const ingressConfig = {
      apiVersion: "networking.k8s.io/v1",
      kind: "Ingress",
      metadata: {
        name: "conductor-ingress",
        annotations: {
          "nginx.ingress.kubernetes.io/canary": "true",
          "nginx.ingress.kubernetes.io/canary-weight": percentage.toString()
        }
      }
    };
    await this.executeCommand(
      `echo '${JSON.stringify(ingressConfig)}' | kubectl apply -f -`
    );
  }
  async collectHealthMetrics() {
    return {
      errorRate: Math.random() * 5e-3,
      // 0-0.5% error rate
      latencyP50: 50 + Math.random() * 50,
      latencyP95: 100 + Math.random() * 100,
      latencyP99: 200 + Math.random() * 200,
      throughput: 1e3 + Math.random() * 500,
      cpuUsage: 30 + Math.random() * 40,
      memoryUsage: 40 + Math.random() * 30,
      activeConnections: 50 + Math.random() * 100
    };
  }
  parseK6Results(output) {
    const errorMatch = output.match(/http_req_failed.*?([\d.]+)%/);
    const latencyMatch = output.match(/http_req_duration.*?avg=([\d.]+)ms/);
    return {
      errorRate: errorMatch ? parseFloat(errorMatch[1]) : 0,
      avgLatency: latencyMatch ? parseFloat(latencyMatch[1]) : 0
    };
  }
  async rollbackMigrations(execution) {
    for (const service11 of execution.config.services) {
      if (!service11.migrations?.length) continue;
      for (const migration of service11.migrations.reverse()) {
        if (migration.rollbackScript) {
          console.log(`Rolling back migration: ${migration.name}`);
          switch (migration.type) {
            case "postgres":
              await this.executeCommand(
                `psql ${process.env.POSTGRES_URL} -c "${migration.rollbackScript}"`
              );
              break;
            case "neo4j":
              await this.executeCommand(
                `cypher-shell -u neo4j -p ${process.env.NEO4J_PASSWORD} "${migration.rollbackScript}"`
              );
              break;
            case "redis":
              await this.executeCommand(
                `redis-cli EVAL "${migration.rollbackScript}" 0`
              );
              break;
          }
        }
      }
    }
  }
  async cleanupFailedDeployment(execution) {
    try {
      await this.executeCommand(
        "kubectl delete deployment server-green --ignore-not-found=true"
      );
      await this.executeCommand(
        "kubectl delete service server-green --ignore-not-found=true"
      );
      await this.executeCommand(
        "kubectl delete configmap server-green-config --ignore-not-found=true"
      );
    } catch (error) {
      console.warn("Cleanup warning:", error.message);
    }
  }
  addLog(phase, message) {
    const timestamp = (/* @__PURE__ */ new Date()).toISOString();
    phase.logs.push(`${timestamp}: ${message}`);
  }
  async persistDeployment(execution) {
    await this.redis.setex(
      `deployment:${execution.id}`,
      86400,
      JSON.stringify(execution)
    );
  }
  startMetricsCollection() {
    this.metricsInterval = setInterval(() => {
      const activeCount = this.activeDeployments.size;
      prometheusConductorMetrics.recordOperationalMetric(
        "active_deployments",
        activeCount
      );
    }, 3e4);
  }
  /**
   * Get deployment status
   */
  getDeployment(deploymentId) {
    return this.activeDeployments.get(deploymentId);
  }
  /**
   * List active deployments
   */
  getActiveDeployments() {
    return Array.from(this.activeDeployments.values());
  }
  /**
   * Cancel deployment
   */
  async cancelDeployment(deploymentId, reason) {
    const execution = this.activeDeployments.get(deploymentId);
    if (!execution) {
      throw new Error(`Deployment ${deploymentId} not found`);
    }
    if (["completed", "failed", "rolled_back"].includes(execution.status)) {
      throw new Error(
        `Cannot cancel deployment in status: ${execution.status}`
      );
    }
    await this.rollbackDeployment(execution, `Cancelled: ${reason}`);
  }
  /**
   * Cleanup resources
   */
  destroy() {
    if (this.metricsInterval) {
      clearInterval(this.metricsInterval);
    }
  }
};
var blueGreenDeploymentEngine = new BlueGreenDeploymentEngine(
  new Redis13(process.env.REDIS_URL || "redis://localhost:6379")
);

// src/conductor/deployment/ZeroTouchOrchestrator.ts
var ZeroTouchOrchestrator = class _ZeroTouchOrchestrator {
  static instance;
  redis;
  TRIGGER_CHANNEL = "summit:deployment:triggers";
  constructor() {
    this.redis = getRedisClient2("default");
  }
  static getInstance() {
    if (!_ZeroTouchOrchestrator.instance) {
      _ZeroTouchOrchestrator.instance = new _ZeroTouchOrchestrator();
    }
    return _ZeroTouchOrchestrator.instance;
  }
  async start() {
    logger.info("ZeroTouchOrchestrator: Starting deployment trigger listener");
    const subscriber = this.redis.duplicate();
    await subscriber.subscribe(this.TRIGGER_CHANNEL);
    subscriber.on("message", async (channel, message) => {
      if (channel === this.TRIGGER_CHANNEL) {
        try {
          const trigger = JSON.parse(message);
          await this.handleTrigger(trigger);
        } catch (err) {
          logger.error({ err }, "ZeroTouchOrchestrator: Failed to process deployment trigger");
        }
      }
    });
  }
  async handleTrigger(trigger) {
    logger.info(trigger, "ZeroTouchOrchestrator: Deployment trigger received");
    const config9 = {
      strategy: "canary",
      environment: trigger.environment,
      imageTag: trigger.imageTag,
      services: [
        {
          name: "summit-server",
          image: "summit-server",
          replicas: 3,
          resources: { cpu: "1", memory: "2Gi" },
          ports: [4e3],
          healthEndpoint: "/api/health",
          environment: { NODE_ENV: trigger.environment }
        }
      ],
      healthChecks: [
        { name: "api_health", type: "http", target: "http://{{environment}}.summit.com/api/health", timeout: 5, retries: 3, interval: 5e3 }
      ],
      rollbackThreshold: {
        errorRate: 0.05,
        // 5%
        latencyP95: 200,
        // 200ms
        timeoutSeconds: 300
      },
      trafficSplit: {
        canaryPercent: 10,
        incrementPercent: 20,
        promoteThreshold: 90
      },
      validation: {
        smokeTests: true,
        integrationTests: true,
        performanceTests: false
      }
    };
    try {
      const deployId = await blueGreenDeploymentEngine.deploy(config9);
      logger.info({ deployId }, "ZeroTouchOrchestrator: Automatic deployment initiated");
    } catch (err) {
      logger.error({ err }, "ZeroTouchOrchestrator: Failed to initiate automatic deployment");
    }
  }
};
var zeroTouchOrchestrator = ZeroTouchOrchestrator.getInstance();

// src/services/DriftRemediationService.ts
init_logger();
init_postgres();
init_tenantRouter();
var DriftRemediationService = class _DriftRemediationService {
  static instance;
  intervalId = null;
  constructor() {
  }
  static getInstance() {
    if (!_DriftRemediationService.instance) {
      _DriftRemediationService.instance = new _DriftRemediationService();
    }
    return _DriftRemediationService.instance;
  }
  start() {
    if (this.intervalId) return;
    logger.info("DriftRemediationService: Starting drift correction loop");
    this.intervalId = setInterval(() => this.remediateDrift(), 3e5);
  }
  /**
   * Performs drift detection and remediation.
   */
  async remediateDrift() {
    logger.info("DriftRemediation: Checking for configuration drift...");
    const pool4 = getPostgresPool();
    try {
      const orphanedMappings = await pool4.query(`
        SELECT m.tenant_id, m.partition_key 
        FROM tenant_partition_map m
        LEFT JOIN tenant_partitions p ON m.partition_key = p.partition_key
        WHERE p.partition_key IS NULL
      `);
      if (orphanedMappings.rows.length > 0) {
        logger.warn({ count: orphanedMappings.rows.length }, "DriftRemediation: Found orphaned tenant mappings");
        for (const mapping of orphanedMappings.rows) {
          logger.info({ tenantId: mapping.tenant_id }, "DriftRemediation: Resetting orphaned mapping to primary");
          await pool4.query(
            "UPDATE tenant_partition_map SET partition_key = 'primary', updated_at = NOW() WHERE tenant_id = $1",
            [mapping.tenant_id]
          );
        }
        await tenantRouter.refresh();
      }
      const missingConfigs = await pool4.query(`
        SELECT t.id 
        FROM tenants t
        LEFT JOIN data_residency_configs c ON t.id = c.tenant_id
        WHERE c.tenant_id IS NULL AND t.is_active = true
      `);
      if (missingConfigs.rows.length > 0) {
        logger.warn({ count: missingConfigs.rows.length }, "DriftRemediation: Missing data residency configs");
        for (const tenant of missingConfigs.rows) {
          logger.info({ tenantId: tenant.id }, "DriftRemediation: Creating default residency config");
          await pool4.query(`
            INSERT INTO data_residency_configs (id, tenant_id, region, country, jurisdiction, created_at)
            VALUES ($1, $2, 'us-east-1', 'US', 'US', NOW())
          `, [`residency-${tenant.id}-auto`, tenant.id]);
        }
      }
      logger.info("DriftRemediation: Sync complete");
    } catch (err) {
      logger.error({ err }, "DriftRemediation: Error during drift correction");
    }
  }
};
var driftRemediationService = DriftRemediationService.getInstance();

// src/services/ProjectSunsettingService.ts
init_logger();
init_postgres();
var ProjectSunsettingService = class _ProjectSunsettingService {
  static instance;
  intervalId = null;
  constructor() {
  }
  static getInstance() {
    if (!_ProjectSunsettingService.instance) {
      _ProjectSunsettingService.instance = new _ProjectSunsettingService();
    }
    return _ProjectSunsettingService.instance;
  }
  start() {
    if (this.intervalId) return;
    logger.info("ProjectSunsettingService: Starting decommissioning monitor");
    this.intervalId = setInterval(() => this.sunstetInactiveProjects(), 864e5);
  }
  /**
   * Identifies and decommissions inactive projects (tenants).
   */
  async sunstetInactiveProjects() {
    logger.info("ProjectSunsetting: Running decommissioning scan...");
    const pool4 = getPostgresPool();
    try {
      const stalledResult = await pool4.query(`
        UPDATE tenants 
        SET config = config || '{"lifecycle_status": "STALLED"}'::jsonb,
            updated_at = NOW()
        WHERE is_active = true 
        AND updated_at < NOW() - INTERVAL '30 days'
        AND (config->>'lifecycle_status' IS NULL OR config->>'lifecycle_status' != 'STALLED')
        RETURNING id
      `);
      if (stalledResult.rows.length > 0) {
        logger.warn({ count: stalledResult.rows.length }, "ProjectSunsetting: Marked tenants as STALLED");
      }
      const archivedResult = await pool4.query(`
        UPDATE tenants 
        SET config = config || '{"lifecycle_status": "ARCHIVED"}'::jsonb,
            is_active = false,
            updated_at = NOW()
        WHERE updated_at < NOW() - INTERVAL '60 days'
        AND (config->>'lifecycle_status' = 'STALLED')
        RETURNING id
      `);
      if (archivedResult.rows.length > 0) {
        logger.warn({ count: archivedResult.rows.length }, "ProjectSunsetting: Archived inactive tenants");
      }
      logger.info("ProjectSunsetting: Scan complete");
    } catch (err) {
      logger.error({ err }, "ProjectSunsetting: Error during decommissioning");
    }
  }
};
var projectSunsettingService = ProjectSunsettingService.getInstance();

// src/index.ts
var __filename4 = fileURLToPath5(import.meta.url);
var __dirname4 = path54.dirname(__filename4);
var startServer = async () => {
  const tracer11 = initializeTracing();
  await tracer11.initialize();
  let startKafkaConsumer2 = null;
  let stopKafkaConsumer2 = null;
  if (process.env.AI_ENABLED === "true" || process.env.KAFKA_ENABLED === "true") {
    try {
      const kafkaModule = await Promise.resolve().then(() => (init_kafkaConsumer(), kafkaConsumer_exports));
      startKafkaConsumer2 = kafkaModule.startKafkaConsumer;
      stopKafkaConsumer2 = kafkaModule.stopKafkaConsumer;
    } catch (error) {
      logger.warn("Kafka not available - running in minimal mode");
    }
  }
  await bootstrapSecrets();
  const app = await createApp();
  const schema2 = makeExecutableSchema2({ typeDefs, resolvers: resolvers_default });
  const httpServer = http2.createServer(app);
  if (process.env.REQUIRE_REAL_DBS !== "false") {
    if (!process.env.DISABLE_NEO4J) {
      await initializeNeo4jDriver();
    }
  } else {
    logger.info("REQUIRE_REAL_DBS=false, skipping Neo4j driver initialization");
  }
  const wss = new WebSocketServer2({
    server: httpServer,
    path: "/graphql"
  });
  const voiceWss = new WebSocketServer2({
    server: httpServer,
    path: "/speak"
  });
  new VoiceGateway(voiceWss);
  useServer(
    {
      schema: schema2,
      context: async (ctx) => {
        const request = ctx.extra.request ?? ctx.extra;
        const baseContext = await getContext({ req: request });
        return {
          ...baseContext,
          connectionId: ctx.extra.connectionId,
          pubsub: subscriptionEngine.getPubSub(),
          subscriptionEngine
        };
      },
      onConnect: (ctx) => {
        const connectionId = randomUUID82();
        ctx.extra.connectionId = connectionId;
        subscriptionEngine.registerConnection(
          connectionId,
          ctx.extra.socket
        );
      },
      onSubscribe: (ctx, msg) => {
        const socket = ctx.extra.socket;
        if (!subscriptionEngine.enforceBackpressure(socket)) {
          return [new GraphQLError16("Backpressure threshold exceeded")];
        }
        const connectionId = ctx.extra.connectionId;
        if (connectionId) {
          subscriptionEngine.trackSubscription(connectionId, msg.id);
        }
        ctx.extra.lastFanoutStart = process.hrtime.bigint();
      },
      onNext: (ctx) => {
        const startedAt2 = ctx.extra.lastFanoutStart ?? process.hrtime.bigint();
        subscriptionEngine.recordFanout(startedAt2);
        ctx.extra.lastFanoutStart = process.hrtime.bigint();
      },
      onComplete: (ctx, msg) => {
        const connectionId = ctx.extra.connectionId;
        if (connectionId) {
          subscriptionEngine.completeSubscription(connectionId, msg?.id);
        }
      },
      onError: (ctx, msg, errors) => {
        logger.error(
          { errors, operationId: msg?.id, connectionId: ctx.extra.connectionId },
          "GraphQL WS subscription error"
        );
      },
      onClose: (ctx) => {
        const connectionId = ctx.extra.connectionId;
        if (connectionId) {
          subscriptionEngine.unregisterConnection(connectionId);
        }
      }
      // ...wsMiddleware,
    },
    wss
  );
  if (cfg.NODE_ENV === "production") {
    const clientDistPath = path54.resolve(__dirname4, "../../client/dist");
    app.use(express58.static(clientDistPath));
    app.get("*", (_req, res) => {
      res.sendFile(path54.join(clientDistPath, "index.html"));
    });
  }
  const { initSocket: initSocket2, getIO: getIO2 } = await Promise.resolve().then(() => (init_socket(), socket_exports));
  const port = Number(cfg.PORT || 4e3);
  httpServer.listen(port, async () => {
    logger.info(`Server listening on port ${port}`);
    if (!process.env.DISABLE_NEO4J) {
      const neo4jDriver2 = getNeo4jDriver();
      const dataRetentionService = new DataRetentionService(neo4jDriver2);
      dataRetentionService.startCleanupJob();
    }
    startOSINTWorkers();
    const backupManager = new BackupManager();
    backupManager.startScheduler();
    const { PolicyWatcher: PolicyWatcher2 } = await Promise.resolve().then(() => (init_PolicyWatcher(), PolicyWatcher_exports));
    const policyWatcher = PolicyWatcher2.getInstance();
    policyWatcher.start();
    const { gaCoreMetrics: gaCoreMetrics2 } = await Promise.resolve().then(() => (init_GACoremetricsService(), GACoremetricsService_exports));
    gaCoreMetrics2.start();
    if (!process.env.DISABLE_NEO4J) {
      checkNeo4jIndexes().catch((err) => logger.error("Failed to run initial index check", err));
    }
    partitionMaintenanceService.start();
    zeroTouchOrchestrator.start().catch((err) => logger.error("Failed to start ZeroTouchOrchestrator", err));
    driftRemediationService.start();
    projectSunsettingService.start();
    if (typeof startKafkaConsumer2 === "function") {
      await startKafkaConsumer2();
    }
    if (process.env.NODE_ENV === "development") {
      setTimeout(async () => {
        try {
          const { createSampleData: createSampleData2 } = await Promise.resolve().then(() => (init_sampleData(), sampleData_exports));
          await createSampleData2();
        } catch (error) {
          logger.warn("Failed to create sample data, continuing without it");
        }
      }, 2e3);
    }
  });
  const io = initSocket2(httpServer);
  const { closeNeo4jDriver: closeNeo4jDriver2 } = await Promise.resolve().then(() => (init_neo4j(), neo4j_exports));
  const { closePostgresPool: closePostgresPool2 } = await Promise.resolve().then(() => (init_postgres(), postgres_exports));
  const { closeRedisClient: closeRedisClient2 } = await Promise.resolve().then(() => (init_redis(), redis_exports));
  const shutdown = async (sig) => {
    logger.info(`Shutting down. Signal: ${sig}`);
    const { PolicyWatcher: PolicyWatcher2 } = await Promise.resolve().then(() => (init_PolicyWatcher(), PolicyWatcher_exports));
    PolicyWatcher2.getInstance().stop();
    partitionMaintenanceService.stop();
    wss.close();
    io.close();
    streamingRateLimiter.destroy();
    if (stopKafkaConsumer2) {
      await stopKafkaConsumer2();
    }
    await getTracer().shutdown();
    const shutdownPromises = [
      closePostgresPool2(),
      closeRedisClient2()
    ];
    if (!process.env.DISABLE_NEO4J) {
      shutdownPromises.push(closeNeo4jDriver2());
    }
    await Promise.allSettled(shutdownPromises);
    httpServer.close((err) => {
      if (err) {
        logger.error(
          `Error during shutdown: ${err instanceof Error ? err.message : "Unknown error"}`
        );
        process.exitCode = 1;
      }
      process.exit();
    });
  };
  process.on("SIGINT", shutdown);
  process.on("SIGTERM", shutdown);
};
startServer().catch((err) => {
  logger.error(`Fatal error during startup: ${err}`);
  process.exit(1);
});
//# sourceMappingURL=index.js.map
