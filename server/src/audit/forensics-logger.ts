/**
 * Real-Time Forensics Logger
 *
 * Implements IC-grade forensics logging with Redis Streams:
 * - Real-time event streaming
 * - Tamper-evident logging with hash chains
 * - High-throughput, low-latency design
 * - Consumer group support for distributed processing
 * - Automatic retention and archival
 *
 * @module audit/forensics-logger
 */

import Redis from 'ioredis';
import crypto from 'crypto';
import { EventEmitter } from 'events';
import logger from '../config/logger.js';

// ============================================================================
// Types and Interfaces
// ============================================================================

export interface ForensicsEvent {
  id?: string;               // Auto-generated by Redis Stream
  timestamp: Date;
  eventType: ForensicsEventType;
  severity: EventSeverity;
  category: EventCategory;
  source: EventSource;
  actor: ActorInfo;
  target?: TargetInfo;
  action: string;
  outcome: 'success' | 'failure' | 'pending';
  details: Record<string, unknown>;
  context: EventContext;
  hash?: string;             // Chain hash for tamper detection
  previousHash?: string;     // Previous event hash
}

export type ForensicsEventType =
  | 'authentication'
  | 'authorization'
  | 'access'
  | 'modification'
  | 'deletion'
  | 'export'
  | 'admin_action'
  | 'security_event'
  | 'policy_violation'
  | 'system_event'
  | 'data_access'
  | 'configuration_change';

export type EventSeverity =
  | 'debug'
  | 'info'
  | 'notice'
  | 'warning'
  | 'error'
  | 'critical'
  | 'alert'
  | 'emergency';

export type EventCategory =
  | 'auth'
  | 'authz'
  | 'data'
  | 'admin'
  | 'security'
  | 'system'
  | 'compliance'
  | 'ot';

export interface EventSource {
  service: string;
  instance?: string;
  version?: string;
  hostname?: string;
  ip?: string;
}

export interface ActorInfo {
  id: string;
  type: 'user' | 'service' | 'system' | 'external';
  email?: string;
  name?: string;
  tenantId?: string;
  ip?: string;
  userAgent?: string;
  sessionId?: string;
}

export interface TargetInfo {
  type: string;
  id: string;
  name?: string;
  tenantId?: string;
  classification?: string;
}

export interface EventContext {
  requestId?: string;
  correlationId?: string;
  traceId?: string;
  spanId?: string;
  environment: string;
  region?: string;
  metadata?: Record<string, unknown>;
}

export interface ForensicsConfig {
  streamName: string;
  maxStreamLength: number;      // MAXLEN for stream trimming
  retentionMs: number;          // How long to keep events
  batchSize: number;            // Batch size for reads
  blockTimeMs: number;          // Block time for XREAD
  enableChainHashing: boolean;  // Enable tamper-evident hashing
  consumerGroup: string;
  consumerName: string;
}

export interface StreamInfo {
  length: number;
  firstEntry?: string;
  lastEntry?: string;
  groups: number;
  lastGeneratedId: string;
}

// ============================================================================
// Forensics Logger
// ============================================================================

export class ForensicsLogger extends EventEmitter {
  private redis: Redis;
  private config: ForensicsConfig;
  private lastHash: string = 'GENESIS';
  private isInitialized = false;
  private pendingEvents: ForensicsEvent[] = [];
  private flushTimer: NodeJS.Timeout | null = null;
  private sequenceNumber = 0;

  constructor(redisUrl?: string, config?: Partial<ForensicsConfig>) {
    super();

    this.config = {
      streamName: 'forensics:events',
      maxStreamLength: 1000000,    // Keep last 1M events in stream
      retentionMs: 90 * 24 * 60 * 60 * 1000, // 90 days
      batchSize: 100,
      blockTimeMs: 5000,
      enableChainHashing: true,
      consumerGroup: 'forensics-processors',
      consumerName: `processor-${process.pid}`,
      ...config,
    };

    this.redis = new Redis(redisUrl || process.env.REDIS_URL || 'redis://localhost:6379', {
      keyPrefix: 'audit:',
      retryDelayOnFailover: 100,
      maxRetriesPerRequest: 3,
      lazyConnect: true,
      enableReadyCheck: true,
    });

    this.setupErrorHandlers();
  }

  private setupErrorHandlers(): void {
    this.redis.on('error', (error) => {
      logger.error('Forensics logger Redis error', {
        error: error.message,
      });
      this.emit('error', error);
    });

    this.redis.on('reconnecting', () => {
      logger.warn('Forensics logger Redis reconnecting');
    });
  }

  /**
   * Initialize the forensics logger
   */
  async initialize(): Promise<void> {
    if (this.isInitialized) return;

    try {
      await this.redis.connect();

      // Create consumer group if it doesn't exist
      try {
        await this.redis.xgroup(
          'CREATE',
          this.config.streamName,
          this.config.consumerGroup,
          '$',
          'MKSTREAM'
        );
        logger.info('Created forensics consumer group', {
          group: this.config.consumerGroup,
          stream: this.config.streamName,
        });
      } catch (error: any) {
        // Group already exists, that's fine
        if (!error.message?.includes('BUSYGROUP')) {
          throw error;
        }
      }

      // Load last hash for chain continuity
      await this.loadLastHash();

      // Start flush timer
      this.startFlushTimer();

      this.isInitialized = true;
      logger.info('Forensics logger initialized', {
        stream: this.config.streamName,
        maxLength: this.config.maxStreamLength,
        chainHashing: this.config.enableChainHashing,
      });
    } catch (error) {
      logger.error('Failed to initialize forensics logger', {
        error: error instanceof Error ? error.message : String(error),
      });
      throw error;
    }
  }

  /**
   * Load the last hash from the stream for chain continuity
   */
  private async loadLastHash(): Promise<void> {
    try {
      const lastEntries = await this.redis.xrevrange(
        this.config.streamName,
        '+',
        '-',
        'COUNT',
        1
      );

      if (lastEntries && lastEntries.length > 0) {
        const [, fields] = lastEntries[0];
        const data = this.parseStreamEntry(fields);
        if (data.hash) {
          this.lastHash = data.hash;
          logger.debug('Loaded last hash for chain continuity', {
            hash: this.lastHash.substring(0, 16) + '...',
          });
        }
      }
    } catch (error) {
      logger.warn('Failed to load last hash, starting new chain', {
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  /**
   * Start the flush timer for batching
   */
  private startFlushTimer(): void {
    if (this.flushTimer) {
      clearInterval(this.flushTimer);
    }

    this.flushTimer = setInterval(async () => {
      if (this.pendingEvents.length > 0) {
        await this.flushEvents();
      }
    }, 100); // Flush every 100ms
  }

  /**
   * Log a forensics event
   */
  async log(event: Omit<ForensicsEvent, 'id' | 'timestamp' | 'hash' | 'previousHash'>): Promise<string> {
    const fullEvent: ForensicsEvent = {
      ...event,
      timestamp: new Date(),
    };

    // Add chain hashing if enabled
    if (this.config.enableChainHashing) {
      fullEvent.previousHash = this.lastHash;
      fullEvent.hash = this.computeHash(fullEvent);
      this.lastHash = fullEvent.hash;
    }

    this.pendingEvents.push(fullEvent);

    // Immediate flush if batch is full
    if (this.pendingEvents.length >= this.config.batchSize) {
      await this.flushEvents();
    }

    // Emit event for real-time processing
    this.emit('event', fullEvent);

    return fullEvent.hash || 'no-hash';
  }

  /**
   * Flush pending events to Redis Stream
   */
  private async flushEvents(): Promise<void> {
    if (this.pendingEvents.length === 0) return;

    const events = [...this.pendingEvents];
    this.pendingEvents = [];

    const pipeline = this.redis.pipeline();

    for (const event of events) {
      const streamData = this.eventToStreamData(event);
      pipeline.xadd(
        this.config.streamName,
        'MAXLEN',
        '~',
        this.config.maxStreamLength,
        '*',
        ...streamData
      );
    }

    try {
      await pipeline.exec();
      logger.debug('Flushed forensics events', { count: events.length });
    } catch (error) {
      // Re-queue failed events
      this.pendingEvents.unshift(...events);
      logger.error('Failed to flush forensics events', {
        error: error instanceof Error ? error.message : String(error),
        count: events.length,
      });
    }
  }

  /**
   * Convenience methods for common event types
   */

  async logAuthentication(
    actor: ActorInfo,
    action: string,
    outcome: 'success' | 'failure',
    details: Record<string, unknown> = {}
  ): Promise<string> {
    return this.log({
      eventType: 'authentication',
      severity: outcome === 'failure' ? 'warning' : 'info',
      category: 'auth',
      source: this.getDefaultSource(),
      actor,
      action,
      outcome,
      details,
      context: this.getDefaultContext(),
    });
  }

  async logAuthorization(
    actor: ActorInfo,
    target: TargetInfo,
    action: string,
    outcome: 'success' | 'failure',
    details: Record<string, unknown> = {}
  ): Promise<string> {
    return this.log({
      eventType: 'authorization',
      severity: outcome === 'failure' ? 'warning' : 'info',
      category: 'authz',
      source: this.getDefaultSource(),
      actor,
      target,
      action,
      outcome,
      details,
      context: this.getDefaultContext(),
    });
  }

  async logDataAccess(
    actor: ActorInfo,
    target: TargetInfo,
    action: string,
    details: Record<string, unknown> = {}
  ): Promise<string> {
    return this.log({
      eventType: 'data_access',
      severity: target.classification === 'secret' || target.classification === 'top-secret'
        ? 'notice'
        : 'info',
      category: 'data',
      source: this.getDefaultSource(),
      actor,
      target,
      action,
      outcome: 'success',
      details,
      context: this.getDefaultContext(),
    });
  }

  async logSecurityEvent(
    actor: ActorInfo,
    action: string,
    severity: EventSeverity,
    details: Record<string, unknown> = {}
  ): Promise<string> {
    return this.log({
      eventType: 'security_event',
      severity,
      category: 'security',
      source: this.getDefaultSource(),
      actor,
      action,
      outcome: 'success',
      details,
      context: this.getDefaultContext(),
    });
  }

  async logPolicyViolation(
    actor: ActorInfo,
    target: TargetInfo | undefined,
    policy: string,
    details: Record<string, unknown> = {}
  ): Promise<string> {
    return this.log({
      eventType: 'policy_violation',
      severity: 'warning',
      category: 'compliance',
      source: this.getDefaultSource(),
      actor,
      target,
      action: `policy_violation:${policy}`,
      outcome: 'failure',
      details: { policy, ...details },
      context: this.getDefaultContext(),
    });
  }

  async logAdminAction(
    actor: ActorInfo,
    action: string,
    target: TargetInfo | undefined,
    details: Record<string, unknown> = {}
  ): Promise<string> {
    return this.log({
      eventType: 'admin_action',
      severity: 'notice',
      category: 'admin',
      source: this.getDefaultSource(),
      actor,
      target,
      action,
      outcome: 'success',
      details,
      context: this.getDefaultContext(),
    });
  }

  /**
   * Read events from the stream
   */
  async readEvents(
    startId: string = '-',
    count: number = 100
  ): Promise<ForensicsEvent[]> {
    const entries = await this.redis.xrange(
      this.config.streamName,
      startId,
      '+',
      'COUNT',
      count
    );

    return entries.map(([id, fields]) => ({
      id,
      ...this.parseStreamEntry(fields),
    }));
  }

  /**
   * Read events using consumer group
   */
  async readEventsAsConsumer(count: number = 100): Promise<ForensicsEvent[]> {
    const entries = await this.redis.xreadgroup(
      'GROUP',
      this.config.consumerGroup,
      this.config.consumerName,
      'COUNT',
      count,
      'BLOCK',
      this.config.blockTimeMs,
      'STREAMS',
      this.config.streamName,
      '>'
    );

    if (!entries || entries.length === 0) {
      return [];
    }

    const [, messages] = entries[0];
    return messages.map(([id, fields]: [string, string[]]) => ({
      id,
      ...this.parseStreamEntry(fields),
    }));
  }

  /**
   * Acknowledge processed events
   */
  async acknowledgeEvents(eventIds: string[]): Promise<number> {
    if (eventIds.length === 0) return 0;

    return this.redis.xack(
      this.config.streamName,
      this.config.consumerGroup,
      ...eventIds
    );
  }

  /**
   * Query events by criteria
   */
  async queryEvents(
    criteria: {
      eventType?: ForensicsEventType;
      severity?: EventSeverity;
      actorId?: string;
      targetId?: string;
      startTime?: Date;
      endTime?: Date;
    },
    limit: number = 1000
  ): Promise<ForensicsEvent[]> {
    // Read events within time range
    const startId = criteria.startTime
      ? `${criteria.startTime.getTime()}-0`
      : '-';
    const endId = criteria.endTime
      ? `${criteria.endTime.getTime()}-0`
      : '+';

    const entries = await this.redis.xrange(
      this.config.streamName,
      startId,
      endId,
      'COUNT',
      limit * 2 // Read more to account for filtering
    );

    let events = entries.map(([id, fields]) => ({
      id,
      ...this.parseStreamEntry(fields),
    }));

    // Filter by criteria
    if (criteria.eventType) {
      events = events.filter(e => e.eventType === criteria.eventType);
    }
    if (criteria.severity) {
      events = events.filter(e => e.severity === criteria.severity);
    }
    if (criteria.actorId) {
      events = events.filter(e => e.actor?.id === criteria.actorId);
    }
    if (criteria.targetId) {
      events = events.filter(e => e.target?.id === criteria.targetId);
    }

    return events.slice(0, limit);
  }

  /**
   * Verify chain integrity
   */
  async verifyChainIntegrity(startId?: string, count: number = 1000): Promise<{
    valid: boolean;
    eventsChecked: number;
    firstInvalidId?: string;
    reason?: string;
  }> {
    const events = await this.readEvents(startId || '-', count);

    let previousHash = 'GENESIS';
    let eventsChecked = 0;

    for (const event of events) {
      eventsChecked++;

      if (!event.hash) {
        continue; // Skip events without hashing
      }

      // Verify previous hash link
      if (event.previousHash !== previousHash) {
        return {
          valid: false,
          eventsChecked,
          firstInvalidId: event.id,
          reason: 'Chain link broken: previousHash mismatch',
        };
      }

      // Verify event hash
      const computedHash = this.computeHash(event);
      if (computedHash !== event.hash) {
        return {
          valid: false,
          eventsChecked,
          firstInvalidId: event.id,
          reason: 'Event hash mismatch: possible tampering',
        };
      }

      previousHash = event.hash;
    }

    return {
      valid: true,
      eventsChecked,
    };
  }

  /**
   * Get stream info
   */
  async getStreamInfo(): Promise<StreamInfo> {
    const info = await this.redis.xinfo('STREAM', this.config.streamName);

    const infoMap: Record<string, any> = {};
    for (let i = 0; i < info.length; i += 2) {
      infoMap[info[i]] = info[i + 1];
    }

    return {
      length: infoMap['length'],
      firstEntry: infoMap['first-entry']?.[0],
      lastEntry: infoMap['last-entry']?.[0],
      groups: infoMap['groups'],
      lastGeneratedId: infoMap['last-generated-id'],
    };
  }

  /**
   * Compute hash for chain integrity
   */
  private computeHash(event: ForensicsEvent): string {
    const hashData = JSON.stringify({
      timestamp: event.timestamp,
      eventType: event.eventType,
      severity: event.severity,
      category: event.category,
      actor: event.actor,
      target: event.target,
      action: event.action,
      outcome: event.outcome,
      details: event.details,
      previousHash: event.previousHash,
    });

    return crypto.createHash('sha256').update(hashData).digest('hex');
  }

  /**
   * Convert event to stream data format
   */
  private eventToStreamData(event: ForensicsEvent): string[] {
    return [
      'timestamp', event.timestamp.toISOString(),
      'eventType', event.eventType,
      'severity', event.severity,
      'category', event.category,
      'source', JSON.stringify(event.source),
      'actor', JSON.stringify(event.actor),
      'target', event.target ? JSON.stringify(event.target) : '',
      'action', event.action,
      'outcome', event.outcome,
      'details', JSON.stringify(event.details),
      'context', JSON.stringify(event.context),
      'hash', event.hash || '',
      'previousHash', event.previousHash || '',
    ];
  }

  /**
   * Parse stream entry back to event
   */
  private parseStreamEntry(fields: string[]): ForensicsEvent {
    const data: Record<string, string> = {};
    for (let i = 0; i < fields.length; i += 2) {
      data[fields[i]] = fields[i + 1];
    }

    return {
      timestamp: new Date(data.timestamp),
      eventType: data.eventType as ForensicsEventType,
      severity: data.severity as EventSeverity,
      category: data.category as EventCategory,
      source: JSON.parse(data.source || '{}'),
      actor: JSON.parse(data.actor || '{}'),
      target: data.target ? JSON.parse(data.target) : undefined,
      action: data.action,
      outcome: data.outcome as 'success' | 'failure' | 'pending',
      details: JSON.parse(data.details || '{}'),
      context: JSON.parse(data.context || '{}'),
      hash: data.hash || undefined,
      previousHash: data.previousHash || undefined,
    };
  }

  /**
   * Get default source info
   */
  private getDefaultSource(): EventSource {
    return {
      service: process.env.SERVICE_NAME || 'summit-server',
      instance: process.env.INSTANCE_ID || process.pid.toString(),
      version: process.env.VERSION || '1.0.0',
      hostname: process.env.HOSTNAME,
    };
  }

  /**
   * Get default context
   */
  private getDefaultContext(): EventContext {
    return {
      environment: process.env.NODE_ENV || 'development',
      region: process.env.REGION,
    };
  }

  /**
   * Health check
   */
  async healthCheck(): Promise<{
    status: 'healthy' | 'degraded' | 'unhealthy';
    details: Record<string, unknown>;
  }> {
    try {
      const ping = await this.redis.ping();
      const streamInfo = await this.getStreamInfo();
      const chainVerification = await this.verifyChainIntegrity(undefined, 100);

      return {
        status: ping === 'PONG' && chainVerification.valid ? 'healthy' : 'degraded',
        details: {
          redis: ping === 'PONG' ? 'connected' : 'disconnected',
          streamLength: streamInfo.length,
          chainIntegrity: chainVerification.valid,
          pendingEvents: this.pendingEvents.length,
        },
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        details: {
          error: error instanceof Error ? error.message : String(error),
        },
      };
    }
  }

  /**
   * Shutdown
   */
  async shutdown(): Promise<void> {
    if (this.flushTimer) {
      clearInterval(this.flushTimer);
      this.flushTimer = null;
    }

    // Flush remaining events
    if (this.pendingEvents.length > 0) {
      await this.flushEvents();
    }

    await this.redis.quit();
    this.isInitialized = false;

    logger.info('Forensics logger shutdown complete');
  }
}

// ============================================================================
// Singleton Instance
// ============================================================================

let forensicsLoggerInstance: ForensicsLogger | null = null;

export function getForensicsLogger(
  redisUrl?: string,
  config?: Partial<ForensicsConfig>
): ForensicsLogger {
  if (!forensicsLoggerInstance) {
    forensicsLoggerInstance = new ForensicsLogger(redisUrl, config);
  }
  return forensicsLoggerInstance;
}

export function resetForensicsLogger(): void {
  forensicsLoggerInstance = null;
}
