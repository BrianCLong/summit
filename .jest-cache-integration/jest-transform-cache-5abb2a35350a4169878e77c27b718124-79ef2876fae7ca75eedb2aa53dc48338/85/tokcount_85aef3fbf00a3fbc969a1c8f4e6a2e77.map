{"file":"/Users/brianlong/Developer/summit/server/src/lib/tokcount.ts","mappings":";AAAA;;;GAGG;;AA6BH,8CASC;AAED,oDAGC;AAED,8CAGC;AAED,kCA6CC;AAED,wCAOC;AAED,kDAgBC;AAED,wDAKC;AAtHD,6CAA6C;AAC7C,MAAM,aAAa,GAAsD;IACvE,sBAAsB;IACtB,QAAQ,EAAE,EAAE,KAAK,EAAE,MAAM,EAAE,MAAM,EAAE,IAAI,EAAE;IACzC,aAAa,EAAE,EAAE,KAAK,EAAE,OAAO,EAAE,MAAM,EAAE,MAAM,EAAE;IACjD,aAAa,EAAE,EAAE,KAAK,EAAE,IAAI,EAAE,MAAM,EAAE,IAAI,EAAE;IAC5C,eAAe,EAAE,EAAE,KAAK,EAAE,MAAM,EAAE,MAAM,EAAE,MAAM,EAAE;IAElD,0BAA0B;IAC1B,4BAA4B,EAAE,EAAE,KAAK,EAAE,KAAK,EAAE,MAAM,EAAE,KAAK,EAAE;IAC7D,eAAe,EAAE,EAAE,KAAK,EAAE,KAAK,EAAE,MAAM,EAAE,KAAK,EAAE;IAChD,gBAAgB,EAAE,EAAE,KAAK,EAAE,OAAO,EAAE,MAAM,EAAE,OAAO,EAAE;IAErD,gBAAgB;IAChB,gBAAgB,EAAE,EAAE,KAAK,EAAE,OAAO,EAAE,MAAM,EAAE,KAAK,EAAE;IACnD,kBAAkB,EAAE,EAAE,KAAK,EAAE,QAAQ,EAAE,MAAM,EAAE,MAAM,EAAE;CACxD,CAAC;AAEF,SAAgB,iBAAiB,CAAC,KAAa,EAAE,IAAY;IAC3D,IAAI,CAAC;QACH,+CAA+C;QAC/C,MAAM,EAAE,MAAM,EAAE,GAAG,OAAO,CAAC,eAAe,CAAC,CAAC;QAC5C,OAAO,MAAM,CAAC,IAAI,EAAE,KAAK,CAAC,CAAC,MAAM,CAAC;IACpC,CAAC;IAAC,OAAO,KAAK,EAAE,CAAC;QACf,0CAA0C;QAC1C,OAAO,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;IAClD,CAAC;AACH,CAAC;AAED,SAAgB,oBAAoB,CAAC,IAAY;IAC/C,gFAAgF;IAChF,OAAO,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,MAAM,GAAG,GAAG,CAAC,CAAC,CAAC;AACpD,CAAC;AAED,SAAgB,iBAAiB,CAAC,IAAY;IAC5C,wCAAwC;IACxC,OAAO,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;AAClD,CAAC;AAEM,KAAK,UAAU,WAAW,CAC/B,QAAqB,EACrB,KAAa,EACb,MAAc,EACd,UAAmB;IAEnB,IAAI,YAAY,GAAG,CAAC,CAAC;IACrB,IAAI,gBAAgB,GAAG,CAAC,CAAC;IAEzB,QAAQ,QAAQ,EAAE,CAAC;QACjB,KAAK,QAAQ;YACX,YAAY,GAAG,iBAAiB,CAAC,KAAK,EAAE,MAAM,CAAC,CAAC;YAChD,gBAAgB,GAAG,UAAU,CAAC,CAAC,CAAC,iBAAiB,CAAC,KAAK,EAAE,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YACzE,MAAM;QAER,KAAK,WAAW;YACd,YAAY,GAAG,oBAAoB,CAAC,MAAM,CAAC,CAAC;YAC5C,gBAAgB,GAAG,UAAU,CAAC,CAAC,CAAC,oBAAoB,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YACrE,MAAM;QAER,KAAK,QAAQ;YACX,YAAY,GAAG,iBAAiB,CAAC,MAAM,CAAC,CAAC;YACzC,gBAAgB,GAAG,UAAU,CAAC,CAAC,CAAC,iBAAiB,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YAClE,MAAM;QAER;YACE,MAAM,IAAI,KAAK,CAAC,yBAAyB,QAAQ,EAAE,CAAC,CAAC;IACzD,CAAC;IAED,MAAM,KAAK,GAAG,YAAY,GAAG,gBAAgB,CAAC;IAE9C,2BAA2B;IAC3B,IAAI,gBAAgB,GAAG,CAAC,CAAC;IACzB,MAAM,OAAO,GAAG,aAAa,CAAC,KAAK,CAAC,CAAC;IACrC,IAAI,OAAO,EAAE,CAAC;QACZ,gBAAgB,GAAG,CAAC,YAAY,GAAG,OAAO,CAAC,KAAK,GAAG,IAAI,CAAC,GAAG,CAAC,gBAAgB,GAAG,OAAO,CAAC,MAAM,GAAG,IAAI,CAAC,CAAC;IACxG,CAAC;IAED,OAAO;QACL,KAAK;QACL,MAAM,EAAE,YAAY;QACpB,UAAU,EAAE,gBAAgB;QAC5B,KAAK;QACL,gBAAgB,EAAE,MAAM,CAAC,gBAAgB,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;KACtD,CAAC;AACJ,CAAC;AAED,SAAgB,cAAc,CAAC,KAAa;IAC1C,IAAI,KAAK,CAAC,UAAU,CAAC,MAAM,CAAC,IAAI,KAAK,CAAC,QAAQ,CAAC,QAAQ,CAAC;QAAE,OAAO,QAAQ,CAAC;IAC1E,IAAI,KAAK,CAAC,UAAU,CAAC,SAAS,CAAC,IAAI,KAAK,CAAC,QAAQ,CAAC,WAAW,CAAC;QAAE,OAAO,WAAW,CAAC;IACnF,IAAI,KAAK,CAAC,UAAU,CAAC,SAAS,CAAC,IAAI,KAAK,CAAC,QAAQ,CAAC,QAAQ,CAAC;QAAE,OAAO,QAAQ,CAAC;IAE7E,uCAAuC;IACvC,OAAO,QAAQ,CAAC;AAClB,CAAC;AAED,SAAgB,mBAAmB,CAAC,MAAc,EAAE,cAAsB,MAAM;IAK9E,MAAM,WAAW,GAAG,CAAC,MAAM,GAAG,WAAW,CAAC,GAAG,GAAG,CAAC;IAEjD,IAAI,eAAe,GAAiC,SAAS,CAAC;IAC9D,IAAI,WAAW,IAAI,GAAG;QAAE,eAAe,GAAG,OAAO,CAAC;SAC7C,IAAI,WAAW,IAAI,EAAE;QAAE,eAAe,GAAG,MAAM,CAAC;IAErD,OAAO;QACL,YAAY,EAAE,MAAM,IAAI,WAAW;QACnC,WAAW,EAAE,MAAM,CAAC,WAAW,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC3C,eAAe;KAChB,CAAC;AACJ,CAAC;AAEM,KAAK,UAAU,sBAAsB,CAAC,KAAa,EAAE,IAAY;IACtE,mDAAmD;IACnD,qDAAqD;IACrD,kCAAkC;IAClC,OAAO,iBAAiB,CAAC,IAAI,CAAC,CAAC;AACjC,CAAC","names":[],"sources":["/Users/brianlong/Developer/summit/server/src/lib/tokcount.ts"],"sourcesContent":["/**\n * Token counting utility for multi-provider LLM budgeting\n * Supports OpenAI, Anthropic, and Gemini models with pluggable counting\n */\n\nexport type ModelFamily = \"openai\" | \"anthropic\" | \"gemini\";\nexport type TokCountResult = { \n  model: string; \n  prompt: number; \n  completion?: number; \n  total: number;\n  estimatedCostUSD?: number;\n};\n\n// Model pricing per 1K tokens (input/output)\nconst MODEL_PRICING: Record<string, { input: number; output: number }> = {\n  // OpenAI GPT-4 family\n  \"gpt-4o\": { input: 0.0025, output: 0.01 },\n  \"gpt-4o-mini\": { input: 0.00015, output: 0.0006 },\n  \"gpt-4-turbo\": { input: 0.01, output: 0.03 },\n  \"gpt-3.5-turbo\": { input: 0.0005, output: 0.0015 },\n  \n  // Anthropic Claude family\n  \"claude-3-5-sonnet-20241022\": { input: 0.003, output: 0.015 },\n  \"claude-3-opus\": { input: 0.015, output: 0.075 },\n  \"claude-3-haiku\": { input: 0.00025, output: 0.00125 },\n  \n  // Gemini family\n  \"gemini-1.5-pro\": { input: 0.00125, output: 0.005 },\n  \"gemini-1.5-flash\": { input: 0.000075, output: 0.0003 },\n};\n\nexport function countOpenAITokens(model: string, text: string): number {\n  try {\n    // Use gpt-tokenizer for accurate OpenAI counts\n    const { encode } = require(\"gpt-tokenizer\");\n    return encode(text, model).length;\n  } catch (error) {\n    // Fallback estimation: ~4 chars per token\n    return Math.max(1, Math.round(text.length / 4));\n  }\n}\n\nexport function countAnthropicTokens(text: string): number {\n  // Anthropic estimation: ~3.5 chars per token (slightly more efficient than GPT)\n  return Math.max(1, Math.round(text.length / 3.5));\n}\n\nexport function countGeminiTokens(text: string): number {\n  // Gemini estimation: ~4 chars per token\n  return Math.max(1, Math.round(text.length / 4));\n}\n\nexport async function countTokens(\n  provider: ModelFamily,\n  model: string,\n  prompt: string,\n  completion?: string\n): Promise<TokCountResult> {\n  let promptTokens = 0;\n  let completionTokens = 0;\n\n  switch (provider) {\n    case \"openai\":\n      promptTokens = countOpenAITokens(model, prompt);\n      completionTokens = completion ? countOpenAITokens(model, completion) : 0;\n      break;\n    \n    case \"anthropic\":\n      promptTokens = countAnthropicTokens(prompt);\n      completionTokens = completion ? countAnthropicTokens(completion) : 0;\n      break;\n    \n    case \"gemini\":\n      promptTokens = countGeminiTokens(prompt);\n      completionTokens = completion ? countGeminiTokens(completion) : 0;\n      break;\n    \n    default:\n      throw new Error(`Unsupported provider: ${provider}`);\n  }\n\n  const total = promptTokens + completionTokens;\n  \n  // Calculate estimated cost\n  let estimatedCostUSD = 0;\n  const pricing = MODEL_PRICING[model];\n  if (pricing) {\n    estimatedCostUSD = (promptTokens * pricing.input / 1000) + (completionTokens * pricing.output / 1000);\n  }\n\n  return {\n    model,\n    prompt: promptTokens,\n    completion: completionTokens,\n    total,\n    estimatedCostUSD: Number(estimatedCostUSD.toFixed(6))\n  };\n}\n\nexport function getModelFamily(model: string): ModelFamily {\n  if (model.startsWith(\"gpt-\") || model.includes(\"openai\")) return \"openai\";\n  if (model.startsWith(\"claude-\") || model.includes(\"anthropic\")) return \"anthropic\";\n  if (model.startsWith(\"gemini-\") || model.includes(\"google\")) return \"gemini\";\n  \n  // Default to openai for unknown models\n  return \"openai\";\n}\n\nexport function validateTokenBudget(tokens: number, budgetLimit: number = 120000): {\n  withinBudget: boolean;\n  percentUsed: number;\n  recommendAction: 'proceed' | 'warn' | 'block';\n} {\n  const percentUsed = (tokens / budgetLimit) * 100;\n  \n  let recommendAction: 'proceed' | 'warn' | 'block' = 'proceed';\n  if (percentUsed >= 100) recommendAction = 'block';\n  else if (percentUsed >= 80) recommendAction = 'warn';\n  \n  return {\n    withinBudget: tokens <= budgetLimit,\n    percentUsed: Number(percentUsed.toFixed(2)),\n    recommendAction\n  };\n}\n\nexport async function countVertexTokensExact(model: string, text: string): Promise<number> {\n  // For production: exact token count from Vertex AI\n  // This would call the actual Vertex Count Tokens API\n  // For now, fallback to estimation\n  return countGeminiTokens(text);\n}"],"version":3}