# IntelGraph Maestro Conductor - Production Alert Policies
# Comprehensive monitoring and alerting for production operations

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: intelgraph-maestro-slo-alerts
  namespace: intelgraph-system
  labels:
    app: intelgraph-maestro
    component: monitoring
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
    # ====================================================================
    # SLO BURN RATE ALERTS (Error Budget Management)
    # ====================================================================

    - name: intelgraph.slo.api.burn_rate
      interval: 30s
      rules:
        # Fast burn rate (2x budget in 30 minutes) - CRITICAL
        - alert: IntelGraphAPIFastBurn
          expr: |
            (
              (1 - (rate(http_requests_total{job="intelgraph-api",code!~"5.."}[30m]) / rate(http_requests_total{job="intelgraph-api"}[30m])))
              > (2 * 0.001)  # 2x the 0.1% error budget
            )
          for: 2m
          labels:
            severity: critical
            service: intelgraph-api
            slo: error_rate
            team: platform
            pager: immediate
          annotations:
            summary: 'üö® IntelGraph API burning error budget too fast'
            description: |
              API error rate {{ $value | humanizePercentage }} is burning 2x error budget over 30 minutes.
              Current burn rate will exhaust monthly budget in {{ div 2160 (div $value 0.001) | humanizeDuration }}.
              IMMEDIATE ACTION REQUIRED - Consider emergency rollback.
            runbook_url: 'https://runbooks.intelgraph.ai/slo-burn-rate'
            dashboard_url: 'https://grafana.intelgraph.ai/d/api-overview'

        # Slow burn rate (14x budget in 6 hours) - WARNING
        - alert: IntelGraphAPISlowBurn
          expr: |
            (
              (1 - (rate(http_requests_total{job="intelgraph-api",code!~"5.."}[6h]) / rate(http_requests_total{job="intelgraph-api"}[6h])))
              > (14 * 0.001)  # 14x the 0.1% error budget
            )
          for: 15m
          labels:
            severity: warning
            service: intelgraph-api
            slo: error_rate
            team: platform
            pager: business_hours
          annotations:
            summary: '‚ö†Ô∏è IntelGraph API burning error budget consistently'
            description: |
              API error rate {{ $value | humanizePercentage }} is burning 14x error budget over 6 hours.
              At this rate, monthly budget will be exhausted in {{ div 2160 (div $value 0.001) | humanizeDuration }}.
            runbook_url: 'https://runbooks.intelgraph.ai/slo-burn-rate'

    # ====================================================================
    # API PERFORMANCE ALERTS
    # ====================================================================

    - name: intelgraph.slo.api.latency
      interval: 30s
      rules:
        # API P95 latency SLO breach - CRITICAL
        - alert: IntelGraphAPILatencyBreach
          expr: |
            histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="intelgraph-api"}[5m])) > 0.35
          for: 15m
          labels:
            severity: critical
            service: intelgraph-api
            slo: latency
            team: platform
            pager: immediate
          annotations:
            summary: 'üêå IntelGraph API P95 latency exceeds SLO'
            description: |
              API P95 latency {{ $value | humanizeDuration }} exceeds 350ms SLO for 15 minutes.
              This impacts user experience and may indicate resource constraints or performance regressions.
            runbook_url: 'https://runbooks.intelgraph.ai/api-latency'
            dashboard_url: 'https://grafana.intelgraph.ai/d/api-performance'

        # API P99 latency warning
        - alert: IntelGraphAPILatencyP99High
          expr: |
            histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{job="intelgraph-api"}[5m])) > 0.7
          for: 10m
          labels:
            severity: warning
            service: intelgraph-api
            slo: latency
            team: platform
          annotations:
            summary: 'üêå IntelGraph API P99 latency high'
            description: 'API P99 latency {{ $value | humanizeDuration }} exceeds 700ms threshold'

        # GraphQL query complexity alerts
        - alert: IntelGraphGraphQuerySlow
          expr: |
            histogram_quantile(0.95, rate(graph_query_duration_seconds_bucket{hops="3"}[5m])) > 1.2
          for: 10m
          labels:
            severity: warning
            service: intelgraph-graph
            slo: graph_latency
            team: platform
          annotations:
            summary: 'üï∏Ô∏è Graph 3-hop queries exceeding SLO'
            description: '3-hop graph queries P95 {{ $value | humanizeDuration }} exceeds 1.2s SLO'
            runbook_url: 'https://runbooks.intelgraph.ai/graph-performance'

    # ====================================================================
    # AUTHENTICATION & AUTHORIZATION ALERTS
    # ====================================================================

    - name: intelgraph.security.auth
      interval: 30s
      rules:
        # Authentication failure rate - CRITICAL
        - alert: IntelGraphAuthFailureRate
          expr: |
            rate(http_requests_total{job="intelgraph-api",code="401"}[5m]) > 0.005
          for: 5m
          labels:
            severity: critical
            service: intelgraph-auth
            category: security
            team: security
            pager: immediate
          annotations:
            summary: 'üîê High authentication failure rate detected'
            description: |
              Authentication error rate {{ $value | humanizePercentage }} exceeds 0.5% threshold.
              This may indicate a credential attack or OIDC configuration issue.
            runbook_url: 'https://runbooks.intelgraph.ai/auth-failures'

        # OIDC JWKS rotation failure
        - alert: IntelGraphJWKSRotationFailed
          expr: |
            increase(oidc_jwks_fetch_errors_total[1h]) > 0
          for: 5m
          labels:
            severity: warning
            service: intelgraph-auth
            category: security
            team: security
          annotations:
            summary: 'üîë OIDC JWKS rotation failing'
            description: 'JWKS fetch errors detected, authentication may fail for new tokens'

        # OPA policy decision latency
        - alert: IntelGraphOPADecisionSlow
          expr: |
            histogram_quantile(0.95, rate(opa_decision_duration_seconds_bucket[5m])) > 0.01
          for: 10m
          labels:
            severity: warning
            service: intelgraph-opa
            category: security
            team: security
          annotations:
            summary: 'üõ°Ô∏è OPA policy decisions slow'
            description: 'OPA decision P95 latency {{ $value | humanizeDuration }} exceeds 10ms threshold'

    # ====================================================================
    # DATA PLATFORM ALERTS
    # ====================================================================

    - name: intelgraph.data.postgresql
      interval: 60s
      rules:
        # PostgreSQL connection pool exhaustion
        - alert: IntelGraphPostgreSQLPoolNearFull
          expr: |
            (pg_stat_activity_count / pg_settings_max_connections) > 0.8
          for: 5m
          labels:
            severity: warning
            service: postgresql
            team: dba
          annotations:
            summary: 'üóÑÔ∏è PostgreSQL connection pool near capacity'
            description: 'Connection pool {{ $value | humanizePercentage }} full, may impact performance'

        # PostgreSQL replication lag
        - alert: IntelGraphPostgreSQLReplicationLag
          expr: |
            pg_stat_replication_lag_seconds > 300
          for: 10m
          labels:
            severity: critical
            service: postgresql
            team: dba
            pager: immediate
          annotations:
            summary: 'üóÑÔ∏è PostgreSQL replication lag high'
            description: 'Replication lag {{ $value | humanizeDuration }} exceeds 5 minutes'

    - name: intelgraph.data.neo4j
      interval: 60s
      rules:
        # Neo4j page cache hit rate low
        - alert: IntelGraphNeo4jCacheHitRateLow
          expr: |
            neo4j_page_cache_hit_ratio < 0.85
          for: 15m
          labels:
            severity: warning
            service: neo4j
            team: dba
          annotations:
            summary: 'üï∏Ô∏è Neo4j page cache hit rate low'
            description: |
              Page cache hit rate {{ $value | humanizePercentage }} below 85% threshold.
              Consider increasing cache size or investigating query patterns.

        # Neo4j cluster member down
        - alert: IntelGraphNeo4jMemberDown
          expr: |
            up{job="neo4j"} == 0
          for: 2m
          labels:
            severity: critical
            service: neo4j
            team: dba
            pager: immediate
          annotations:
            summary: 'üï∏Ô∏è Neo4j cluster member down'
            description: 'Neo4j cluster member {{ $labels.instance }} is unreachable'

    - name: intelgraph.data.kafka
      interval: 30s
      rules:
        # Kafka consumer lag high
        - alert: IntelGraphKafkaConsumerLagHigh
          expr: |
            kafka_consumer_lag_sum > 1000
          for: 10m
          labels:
            severity: warning
            service: kafka
            team: platform
          annotations:
            summary: 'üìä Kafka consumer lag high'
            description: 'Consumer lag {{ $value }} messages exceeds 1000 threshold'

        # Dead Letter Queue growth
        - alert: IntelGraphDLQGrowthHigh
          expr: |
            rate(kafka_topic_partition_current_offset{topic=~"dlq-.*"}[5m]) > 0.1
          for: 5m
          labels:
            severity: warning
            service: kafka
            category: data_quality
            team: platform
          annotations:
            summary: '‚ö†Ô∏è Dead Letter Queue growing rapidly'
            description: 'DLQ growth rate {{ $value }} messages/sec indicates processing issues'

    # ====================================================================
    # INFRASTRUCTURE ALERTS
    # ====================================================================

    - name: intelgraph.infrastructure.resources
      interval: 60s
      rules:
        # High CPU usage
        - alert: IntelGraphHighCPUUsage
          expr: |
            100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
          for: 10m
          labels:
            severity: warning
            service: infrastructure
            team: sre
          annotations:
            summary: 'üî• High CPU usage detected'
            description: 'CPU usage {{ $value }}% on {{ $labels.instance }} exceeds 85%'

        # High memory usage
        - alert: IntelGraphHighMemoryUsage
          expr: |
            (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
          for: 10m
          labels:
            severity: warning
            service: infrastructure
            team: sre
          annotations:
            summary: 'üß† High memory usage detected'
            description: 'Memory usage {{ $value }}% on {{ $labels.instance }} exceeds 85%'

        # Pod restart rate high
        - alert: IntelGraphPodRestartRateHigh
          expr: |
            rate(kube_pod_container_status_restarts_total{namespace="intelgraph-system"}[15m]) > 0.01
          for: 5m
          labels:
            severity: warning
            service: kubernetes
            team: sre
          annotations:
            summary: 'üîÑ High pod restart rate'
            description: 'Pod {{ $labels.pod }} restarting frequently'

    # ====================================================================
    # COST & CAPACITY ALERTS
    # ====================================================================

    - name: intelgraph.cost.management
      interval: 300s # 5 minutes
      rules:
        # Cost budget burn rate
        - alert: IntelGraphCostBudgetBurnHigh
          expr: |
            kubecost_cluster_total_cost > 600  # $600/day = $18k/month
          for: 30m
          labels:
            severity: warning
            service: cost-management
            team: finops
          annotations:
            summary: 'üí∞ Daily cost exceeds budget'
            description: 'Daily cost ${{ $value }} exceeds $600 budget (monthly projection: ${{ mul $value 30 }})'

        # Resource quota near limit
        - alert: IntelGraphResourceQuotaNearLimit
          expr: |
            (kube_resourcequota_used / kube_resourcequota_hard) > 0.8
          for: 15m
          labels:
            severity: warning
            service: kubernetes
            team: sre
          annotations:
            summary: 'üìä Resource quota near limit'
            description: '{{ $labels.resource }} quota {{ $value | humanizePercentage }} full in {{ $labels.namespace }}'

    # ====================================================================
    # SECURITY ALERTS
    # ====================================================================

    - name: intelgraph.security.threats
      interval: 60s
      rules:
        # Rate limiting triggered frequently
        - alert: IntelGraphRateLimitHitsHigh
          expr: |
            rate(rate_limit_exceeded_total[5m]) > 10
          for: 5m
          labels:
            severity: warning
            service: intelgraph-api
            category: security
            team: security
          annotations:
            summary: 'üö¶ High rate limit enforcement'
            description: 'Rate limit hits {{ $value }}/sec may indicate abuse or traffic spike'

        # Non-persisted GraphQL queries (security bypass attempt)
        - alert: IntelGraphNonPersistedQueryAttempt
          expr: |
            rate(graphql_requests_total{status="400",reason="non_persisted"}[5m]) > 1
          for: 2m
          labels:
            severity: warning
            service: intelgraph-api
            category: security
            team: security
          annotations:
            summary: 'üîí Non-persisted GraphQL query attempts'
            description: '{{ $value }}/sec attempts to use non-persisted queries (potential bypass attempt)'

        # Certificate expiry warning
        - alert: IntelGraphCertificateExpiringSoon
          expr: |
            (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 < 30
          for: 24h
          labels:
            severity: warning
            service: cert-manager
            category: security
            team: sre
          annotations:
            summary: 'üìú TLS certificate expiring soon'
            description: 'Certificate {{ $labels.name }} expires in {{ div (sub $value (time())) 86400 | humanizeDuration }}'

    # ====================================================================
    # BUSINESS METRICS ALERTS
    # ====================================================================

    - name: intelgraph.business.metrics
      interval: 300s
      rules:
        # Low entity ingestion rate
        - alert: IntelGraphEntityIngestionRateLow
          expr: |
            rate(entities_ingested_total[1h]) < 10
          for: 30m
          labels:
            severity: warning
            service: intelgraph-ingest
            team: product
          annotations:
            summary: 'üì• Entity ingestion rate low'
            description: 'Entity ingestion rate {{ $value }}/hour below expected baseline'

        # GraphQL operation success rate low
        - alert: IntelGraphGraphQLSuccessRateLow
          expr: |
            rate(graphql_operations_total{status="success"}[10m]) / rate(graphql_operations_total[10m]) < 0.95
          for: 10m
          labels:
            severity: warning
            service: intelgraph-api
            team: product
          annotations:
            summary: 'üìä GraphQL success rate low'
            description: 'GraphQL success rate {{ $value | humanizePercentage }} below 95% threshold'

---
# Alert Manager Configuration for routing and escalation

apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: intelgraph-system
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@intelgraph.ai'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

    # Routing tree - determines which alerts go where
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 1h
      receiver: 'default'

      routes:
      # Critical alerts go to PagerDuty immediately
      - match:
          severity: critical
        receiver: 'pagerduty-critical'
        group_wait: 10s
        repeat_interval: 5m
        continue: true

      # Security alerts go to security team
      - match:
          category: security
        receiver: 'security-team'
        continue: true

      # Platform team alerts
      - match:
          team: platform
        receiver: 'platform-team'

      # SRE team alerts
      - match:
          team: sre
        receiver: 'sre-team'

      # DBA team alerts
      - match:
          team: dba
        receiver: 'dba-team'

    # Inhibition rules - suppress less critical alerts when more critical ones fire
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'service']

    receivers:

    # Default receiver
    - name: 'default'
      slack_configs:
      - channel: '#intelgraph-alerts'
        title: 'IntelGraph Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

    # Critical alerts to PagerDuty
    - name: 'pagerduty-critical'
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
        client: 'IntelGraph Maestro'
        client_url: 'https://grafana.intelgraph.ai'
        details:
          alertname: '{{ .GroupLabels.alertname }}'
          service: '{{ .GroupLabels.service }}'
          severity: '{{ .GroupLabels.severity }}'
          description: '{{ .CommonAnnotations.description }}'
          runbook: '{{ .CommonAnnotations.runbook_url }}'

    # Security team notifications
    - name: 'security-team'
      slack_configs:
      - channel: '#security-alerts'
        title: 'üö® Security Alert: {{ .GroupLabels.alertname }}'
        text: |
          *Service*: {{ .GroupLabels.service }}
          *Severity*: {{ .GroupLabels.severity }}
          *Description*: {{ .CommonAnnotations.description }}
          *Runbook*: {{ .CommonAnnotations.runbook_url }}
        color: 'danger'

    # Platform team notifications
    - name: 'platform-team'
      slack_configs:
      - channel: '#platform-alerts'
        title: '‚ö†Ô∏è Platform Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

    # SRE team notifications
    - name: 'sre-team'
      slack_configs:
      - channel: '#sre-alerts'
        title: 'üîß Infrastructure Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

    # DBA team notifications
    - name: 'dba-team'
      slack_configs:
      - channel: '#dba-alerts'
        title: 'üóÑÔ∏è Database Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

    # Alert templates
    templates:
    - '/etc/alertmanager/templates/*.tmpl'
