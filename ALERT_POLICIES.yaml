# IntelGraph Maestro Conductor - Production Alert Policies
# Comprehensive monitoring and alerting for production operations

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: intelgraph-maestro-slo-alerts
  namespace: intelgraph-system
  labels:
    app: intelgraph-maestro
    component: monitoring
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:

  # ====================================================================
  # SLO BURN RATE ALERTS (Error Budget Management)
  # ====================================================================

  - name: intelgraph.slo.api.burn_rate
    interval: 30s
    rules:

    # Fast burn rate (2x budget in 30 minutes) - CRITICAL
    - alert: IntelGraphAPIFastBurn
      expr: |
        (
          (1 - (rate(http_requests_total{job="intelgraph-api",code!~"5.."}[30m]) / rate(http_requests_total{job="intelgraph-api"}[30m])))
          > (2 * 0.001)  # 2x the 0.1% error budget
        )
      for: 2m
      labels:
        severity: critical
        service: intelgraph-api
        slo: error_rate
        team: platform
        pager: immediate
      annotations:
        summary: "üö® IntelGraph API burning error budget too fast"
        description: |
          API error rate {{ $value | humanizePercentage }} is burning 2x error budget over 30 minutes.
          Current burn rate will exhaust monthly budget in {{ div 2160 (div $value 0.001) | humanizeDuration }}.
          IMMEDIATE ACTION REQUIRED - Consider emergency rollback.
        runbook_url: "https://runbooks.intelgraph.ai/slo-burn-rate"
        dashboard_url: "https://grafana.intelgraph.ai/d/api-overview"

    # Slow burn rate (14x budget in 6 hours) - WARNING
    - alert: IntelGraphAPISlowBurn
      expr: |
        (
          (1 - (rate(http_requests_total{job="intelgraph-api",code!~"5.."}[6h]) / rate(http_requests_total{job="intelgraph-api"}[6h])))
          > (14 * 0.001)  # 14x the 0.1% error budget
        )
      for: 15m
      labels:
        severity: warning
        service: intelgraph-api
        slo: error_rate
        team: platform
        pager: business_hours
      annotations:
        summary: "‚ö†Ô∏è IntelGraph API burning error budget consistently"
        description: |
          API error rate {{ $value | humanizePercentage }} is burning 14x error budget over 6 hours.
          At this rate, monthly budget will be exhausted in {{ div 2160 (div $value 0.001) | humanizeDuration }}.
        runbook_url: "https://runbooks.intelgraph.ai/slo-burn-rate"

  # ====================================================================
  # API PERFORMANCE ALERTS
  # ====================================================================

  - name: intelgraph.slo.api.latency
    interval: 30s
    rules:

    # API P95 latency SLO breach - CRITICAL
    - alert: IntelGraphAPILatencyBreach
      expr: |
        histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="intelgraph-api"}[5m])) > 0.35
      for: 15m
      labels:
        severity: critical
        service: intelgraph-api
        slo: latency
        team: platform
        pager: immediate
      annotations:
        summary: "üêå IntelGraph API P95 latency exceeds SLO"
        description: |
          API P95 latency {{ $value | humanizeDuration }} exceeds 350ms SLO for 15 minutes.
          This impacts user experience and may indicate resource constraints or performance regressions.
        runbook_url: "https://runbooks.intelgraph.ai/api-latency"
        dashboard_url: "https://grafana.intelgraph.ai/d/api-performance"

    # API P99 latency warning
    - alert: IntelGraphAPILatencyP99High
      expr: |
        histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{job="intelgraph-api"}[5m])) > 0.7
      for: 10m
      labels:
        severity: warning
        service: intelgraph-api
        slo: latency
        team: platform
      annotations:
        summary: "üêå IntelGraph API P99 latency high"
        description: "API P99 latency {{ $value | humanizeDuration }} exceeds 700ms threshold"

    # GraphQL query complexity alerts
    - alert: IntelGraphGraphQuerySlow
      expr: |
        histogram_quantile(0.95, rate(graph_query_duration_seconds_bucket{hops="3"}[5m])) > 1.2
      for: 10m
      labels:
        severity: warning
        service: intelgraph-graph
        slo: graph_latency
        team: platform
      annotations:
        summary: "üï∏Ô∏è Graph 3-hop queries exceeding SLO"
        description: "3-hop graph queries P95 {{ $value | humanizeDuration }} exceeds 1.2s SLO"
        runbook_url: "https://runbooks.intelgraph.ai/graph-performance"

  # ====================================================================
  # AUTHENTICATION & AUTHORIZATION ALERTS
  # ====================================================================

  - name: intelgraph.security.auth
    interval: 30s
    rules:

    # Authentication failure rate - CRITICAL
    - alert: IntelGraphAuthFailureRate
      expr: |
        rate(http_requests_total{job="intelgraph-api",code="401"}[5m]) > 0.005
      for: 5m
      labels:
        severity: critical
        service: intelgraph-auth
        category: security
        team: security
        pager: immediate
      annotations:
        summary: "üîê High authentication failure rate detected"
        description: |
          Authentication error rate {{ $value | humanizePercentage }} exceeds 0.5% threshold.
          This may indicate a credential attack or OIDC configuration issue.
        runbook_url: "https://runbooks.intelgraph.ai/auth-failures"

    # OIDC JWKS rotation failure
    - alert: IntelGraphJWKSRotationFailed
      expr: |
        increase(oidc_jwks_fetch_errors_total[1h]) > 0
      for: 5m
      labels:
        severity: warning
        service: intelgraph-auth
        category: security
        team: security
      annotations:
        summary: "üîë OIDC JWKS rotation failing"
        description: "JWKS fetch errors detected, authentication may fail for new tokens"

    # OPA policy decision latency
    - alert: IntelGraphOPADecisionSlow
      expr: |
        histogram_quantile(0.95, rate(opa_decision_duration_seconds_bucket[5m])) > 0.01
      for: 10m
      labels:
        severity: warning
        service: intelgraph-opa
        category: security
        team: security
      annotations:
        summary: "üõ°Ô∏è OPA policy decisions slow"
        description: "OPA decision P95 latency {{ $value | humanizeDuration }} exceeds 10ms threshold"

  # ====================================================================
  # DATA PLATFORM ALERTS
  # ====================================================================

  - name: intelgraph.data.postgresql
    interval: 60s
    rules:

    # PostgreSQL connection pool exhaustion
    - alert: IntelGraphPostgreSQLPoolNearFull
      expr: |
        (pg_stat_activity_count / pg_settings_max_connections) > 0.8
      for: 5m
      labels:
        severity: warning
        service: postgresql
        team: dba
      annotations:
        summary: "üóÑÔ∏è PostgreSQL connection pool near capacity"
        description: "Connection pool {{ $value | humanizePercentage }} full, may impact performance"

    # PostgreSQL replication lag
    - alert: IntelGraphPostgreSQLReplicationLag
      expr: |
        pg_stat_replication_lag_seconds > 300
      for: 10m
      labels:
        severity: critical
        service: postgresql
        team: dba
        pager: immediate
      annotations:
        summary: "üóÑÔ∏è PostgreSQL replication lag high"
        description: "Replication lag {{ $value | humanizeDuration }} exceeds 5 minutes"

  - name: intelgraph.data.neo4j
    interval: 60s
    rules:

    # Neo4j page cache hit rate low
    - alert: IntelGraphNeo4jCacheHitRateLow
      expr: |
        neo4j_page_cache_hit_ratio < 0.85
      for: 15m
      labels:
        severity: warning
        service: neo4j
        team: dba
      annotations:
        summary: "üï∏Ô∏è Neo4j page cache hit rate low"
        description: |
          Page cache hit rate {{ $value | humanizePercentage }} below 85% threshold.
          Consider increasing cache size or investigating query patterns.

    # Neo4j cluster member down
    - alert: IntelGraphNeo4jMemberDown
      expr: |
        up{job="neo4j"} == 0
      for: 2m
      labels:
        severity: critical
        service: neo4j
        team: dba
        pager: immediate
      annotations:
        summary: "üï∏Ô∏è Neo4j cluster member down"
        description: "Neo4j cluster member {{ $labels.instance }} is unreachable"

  - name: intelgraph.data.kafka
    interval: 30s
    rules:

    # Kafka consumer lag high
    - alert: IntelGraphKafkaConsumerLagHigh
      expr: |
        kafka_consumer_lag_sum > 1000
      for: 10m
      labels:
        severity: warning
        service: kafka
        team: platform
      annotations:
        summary: "üìä Kafka consumer lag high"
        description: "Consumer lag {{ $value }} messages exceeds 1000 threshold"

    # Dead Letter Queue growth
    - alert: IntelGraphDLQGrowthHigh
      expr: |
        rate(kafka_topic_partition_current_offset{topic=~"dlq-.*"}[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        service: kafka
        category: data_quality
        team: platform
      annotations:
        summary: "‚ö†Ô∏è Dead Letter Queue growing rapidly"
        description: "DLQ growth rate {{ $value }} messages/sec indicates processing issues"

  # ====================================================================
  # INFRASTRUCTURE ALERTS
  # ====================================================================

  - name: intelgraph.infrastructure.resources
    interval: 60s
    rules:

    # High CPU usage
    - alert: IntelGraphHighCPUUsage
      expr: |
        100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
      for: 10m
      labels:
        severity: warning
        service: infrastructure
        team: sre
      annotations:
        summary: "üî• High CPU usage detected"
        description: "CPU usage {{ $value }}% on {{ $labels.instance }} exceeds 85%"

    # High memory usage
    - alert: IntelGraphHighMemoryUsage
      expr: |
        (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 10m
      labels:
        severity: warning
        service: infrastructure
        team: sre
      annotations:
        summary: "üß† High memory usage detected"
        description: "Memory usage {{ $value }}% on {{ $labels.instance }} exceeds 85%"

    # Pod restart rate high
    - alert: IntelGraphPodRestartRateHigh
      expr: |
        rate(kube_pod_container_status_restarts_total{namespace="intelgraph-system"}[15m]) > 0.01
      for: 5m
      labels:
        severity: warning
        service: kubernetes
        team: sre
      annotations:
        summary: "üîÑ High pod restart rate"
        description: "Pod {{ $labels.pod }} restarting frequently"

  # ====================================================================
  # COST & CAPACITY ALERTS
  # ====================================================================

  - name: intelgraph.cost.management
    interval: 300s  # 5 minutes
    rules:

    # Cost budget burn rate
    - alert: IntelGraphCostBudgetBurnHigh
      expr: |
        kubecost_cluster_total_cost > 600  # $600/day = $18k/month
      for: 30m
      labels:
        severity: warning
        service: cost-management
        team: finops
      annotations:
        summary: "üí∞ Daily cost exceeds budget"
        description: "Daily cost ${{ $value }} exceeds $600 budget (monthly projection: ${{ mul $value 30 }})"

    # Resource quota near limit
    - alert: IntelGraphResourceQuotaNearLimit
      expr: |
        (kube_resourcequota_used / kube_resourcequota_hard) > 0.8
      for: 15m
      labels:
        severity: warning
        service: kubernetes
        team: sre
      annotations:
        summary: "üìä Resource quota near limit"
        description: "{{ $labels.resource }} quota {{ $value | humanizePercentage }} full in {{ $labels.namespace }}"

  # ====================================================================
  # SECURITY ALERTS
  # ====================================================================

  - name: intelgraph.security.threats
    interval: 60s
    rules:

    # Rate limiting triggered frequently
    - alert: IntelGraphRateLimitHitsHigh
      expr: |
        rate(rate_limit_exceeded_total[5m]) > 10
      for: 5m
      labels:
        severity: warning
        service: intelgraph-api
        category: security
        team: security
      annotations:
        summary: "üö¶ High rate limit enforcement"
        description: "Rate limit hits {{ $value }}/sec may indicate abuse or traffic spike"

    # Non-persisted GraphQL queries (security bypass attempt)
    - alert: IntelGraphNonPersistedQueryAttempt
      expr: |
        rate(graphql_requests_total{status="400",reason="non_persisted"}[5m]) > 1
      for: 2m
      labels:
        severity: warning
        service: intelgraph-api
        category: security
        team: security
      annotations:
        summary: "üîí Non-persisted GraphQL query attempts"
        description: "{{ $value }}/sec attempts to use non-persisted queries (potential bypass attempt)"

    # Certificate expiry warning
    - alert: IntelGraphCertificateExpiringSoon
      expr: |
        (cert_manager_certificate_expiration_timestamp_seconds - time()) / 86400 < 30
      for: 24h
      labels:
        severity: warning
        service: cert-manager
        category: security
        team: sre
      annotations:
        summary: "üìú TLS certificate expiring soon"
        description: "Certificate {{ $labels.name }} expires in {{ div (sub $value (time())) 86400 | humanizeDuration }}"

  # ====================================================================
  # BUSINESS METRICS ALERTS
  # ====================================================================

  - name: intelgraph.business.metrics
    interval: 300s
    rules:

    # Low entity ingestion rate
    - alert: IntelGraphEntityIngestionRateLow
      expr: |
        rate(entities_ingested_total[1h]) < 10
      for: 30m
      labels:
        severity: warning
        service: intelgraph-ingest
        team: product
      annotations:
        summary: "üì• Entity ingestion rate low"
        description: "Entity ingestion rate {{ $value }}/hour below expected baseline"

    # GraphQL operation success rate low
    - alert: IntelGraphGraphQLSuccessRateLow
      expr: |
        rate(graphql_operations_total{status="success"}[10m]) / rate(graphql_operations_total[10m]) < 0.95
      for: 10m
      labels:
        severity: warning
        service: intelgraph-api
        team: product
      annotations:
        summary: "üìä GraphQL success rate low"
        description: "GraphQL success rate {{ $value | humanizePercentage }} below 95% threshold"

---
# Alert Manager Configuration for routing and escalation

apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: intelgraph-system
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@intelgraph.ai'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

    # Routing tree - determines which alerts go where
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 1h
      receiver: 'default'

      routes:
      # Critical alerts go to PagerDuty immediately
      - match:
          severity: critical
        receiver: 'pagerduty-critical'
        group_wait: 10s
        repeat_interval: 5m
        continue: true

      # Security alerts go to security team
      - match:
          category: security
        receiver: 'security-team'
        continue: true

      # Platform team alerts
      - match:
          team: platform
        receiver: 'platform-team'

      # SRE team alerts
      - match:
          team: sre
        receiver: 'sre-team'

      # DBA team alerts
      - match:
          team: dba
        receiver: 'dba-team'

    # Inhibition rules - suppress less critical alerts when more critical ones fire
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'service']

    receivers:

    # Default receiver
    - name: 'default'
      slack_configs:
      - channel: '#intelgraph-alerts'
        title: 'IntelGraph Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

    # Critical alerts to PagerDuty
    - name: 'pagerduty-critical'
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
        client: 'IntelGraph Maestro'
        client_url: 'https://grafana.intelgraph.ai'
        details:
          alertname: '{{ .GroupLabels.alertname }}'
          service: '{{ .GroupLabels.service }}'
          severity: '{{ .GroupLabels.severity }}'
          description: '{{ .CommonAnnotations.description }}'
          runbook: '{{ .CommonAnnotations.runbook_url }}'

    # Security team notifications
    - name: 'security-team'
      slack_configs:
      - channel: '#security-alerts'
        title: 'üö® Security Alert: {{ .GroupLabels.alertname }}'
        text: |
          *Service*: {{ .GroupLabels.service }}
          *Severity*: {{ .GroupLabels.severity }}
          *Description*: {{ .CommonAnnotations.description }}
          *Runbook*: {{ .CommonAnnotations.runbook_url }}
        color: 'danger'

    # Platform team notifications
    - name: 'platform-team'
      slack_configs:
      - channel: '#platform-alerts'
        title: '‚ö†Ô∏è Platform Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

    # SRE team notifications
    - name: 'sre-team'
      slack_configs:
      - channel: '#sre-alerts'
        title: 'üîß Infrastructure Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

    # DBA team notifications
    - name: 'dba-team'
      slack_configs:
      - channel: '#dba-alerts'
        title: 'üóÑÔ∏è Database Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

    # Alert templates
    templates:
    - '/etc/alertmanager/templates/*.tmpl'