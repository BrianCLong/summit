# Chaos Engineering Experiments
# Sprint 27E: Reliability testing with controlled failure injection

apiVersion: chaos.intelgraph.io/v1
kind: ChaosExperimentSuite
metadata:
  name: intelgraph-reliability-tests
  version: "27.0.0"

spec:
  # Global configuration
  global:
    timeout: "30m"
    rollback_timeout: "5m"
    steady_state_timeout: "2m"
    abort_on_failure: false

  # Baseline requirements
  baseline:
    slo_compliance: 0.95  # Must maintain 95% SLO during experiments
    max_error_rate: 0.05  # 5% error rate threshold
    recovery_time: "2m"   # Max recovery time after experiment

# Service-level experiments
experiments:
  # Database resilience
  - name: "database_connection_loss"
    description: "Test graceful degradation when primary database becomes unavailable"
    type: "network"
    target:
      service: "postgres"
      percentage: 100

    fault_injection:
      type: "network_partition"
      duration: "5m"

    expected_behavior:
      - service_degradation: "read_only_mode"
      - cache_fallback: true
      - user_notification: "Database maintenance mode"
      - recovery_automatic: true

    success_criteria:
      - api_availability: 0.8  # 80% API still functional
      - data_consistency: true
      - no_data_loss: true

  - name: "database_slow_queries"
    description: "Simulate database performance degradation"
    type: "latency"
    target:
      service: "postgres"
      percentage: 100

    fault_injection:
      type: "latency_injection"
      delay: "2s"
      duration: "10m"

    expected_behavior:
      - query_timeout_handling: true
      - connection_pooling_scaling: true
      - circuit_breaker_activation: true

    success_criteria:
      - p95_latency: "<5s"
      - timeout_rate: "<10%"
      - circuit_breaker_trips: ">0"

  - name: "postgres_primary_region_outage"
    description: "Validate promotion when the us-east-1 writer disappears"
    type: "pod_kill"
    target:
      service: "postgres"
      selector:
        replication.summit.sh/role: primary
        topology.kubernetes.io/region: us-east-1

    fault_injection:
      type: "pod_kill"
      duration: "2m"

    expected_behavior:
      - replica_promotion: true
      - lag_budget_respected: "<=30s"
      - service_endpoint_updates: true

    success_criteria:
      - rpo_seconds: "<=5"
      - rto_minutes: "<=3"
      - read_traffic_preserved: ">=90%"

  - name: "neo4j_secondary_region_partition"
    description: "Ensure causal cluster heals after isolating us-west-2"
    type: "network"
    target:
      service: "neo4j"
      selector:
        topology.kubernetes.io/region: us-west-2

    fault_injection:
      type: "network_partition"
      duration: "4m"

    expected_behavior:
      - routing_table_rebuild: true
      - writes_redirected_to_primary: true
      - read_queries_served_from_remaining_regions: true

    success_criteria:
      - bolt_availability: 0.95
      - cluster_core_quorum: true
      - replication_queue_drain: "<120s"

  # API Gateway resilience
  - name: "api_gateway_overload"
    description: "Test rate limiting and backpressure mechanisms"
    type: "stress"
    target:
      service: "gateway"
      endpoint: "/api/v1/query"

    fault_injection:
      type: "traffic_spike"
      multiplier: 10
      duration: "5m"

    expected_behavior:
      - rate_limiting_activation: true
      - queue_backpressure: true
      - graceful_degradation: true

    success_criteria:
      - rate_limit_effective: true
      - queue_overflow: false
      - service_recovery: "<2m"

  # Model service resilience
  - name: "model_service_failure"
    description: "Test fallback when ML models become unavailable"
    type: "service_failure"
    target:
      service: "nlq-service"
      percentage: 100

    fault_injection:
      type: "pod_kill"
      duration: "3m"

    expected_behavior:
      - fallback_model_activation: true
      - cache_response_serving: true
      - user_degraded_experience: true

    success_criteria:
      - fallback_success_rate: 0.9
      - cache_hit_rate: 0.7
      - service_restart: true

  # Network resilience
  - name: "inter_service_network_partition"
    description: "Test service mesh resilience during network partitions"
    type: "network"
    target:
      services: ["api", "gateway", "nlq-service"]

    fault_injection:
      type: "network_partition"
      partition_percentage: 50
      duration: "7m"

    expected_behavior:
      - service_mesh_routing: true
      - circuit_breaker_isolation: true
      - eventual_consistency: true

    success_criteria:
      - partition_tolerance: true
      - data_consistency_eventual: true
      - network_healing: "<3m"

  # Memory pressure
  - name: "memory_pressure_stress"
    description: "Test application behavior under memory constraints"
    type: "resource"
    target:
      service: "api"
      resource: "memory"

    fault_injection:
      type: "memory_stress"
      percentage: 90
      duration: "8m"

    expected_behavior:
      - garbage_collection_aggressive: true
      - cache_eviction_lru: true
      - memory_leak_detection: true

    success_criteria:
      - no_oom_kills: true
      - response_time_degradation: "<2x"
      - memory_recovery: true

  # Disk I/O stress
  - name: "disk_io_saturation"
    description: "Test system behavior under disk I/O pressure"
    type: "resource"
    target:
      service: "postgres"
      resource: "disk"

    fault_injection:
      type: "io_stress"
      read_percentage: 70
      write_percentage: 90
      duration: "6m"

    expected_behavior:
      - query_prioritization: true
      - connection_pooling_adaptive: true
      - disk_queue_management: true

    success_criteria:
      - critical_queries_priority: true
      - disk_recovery: "<4m"
      - no_data_corruption: true

# Compound failure scenarios
compound_experiments:
  - name: "database_and_cache_failure"
    description: "Test behavior when both database and cache fail simultaneously"
    experiments:
      - "database_connection_loss"
      - "redis_service_failure"

    cascade_delay: "30s"
    expected_behavior:
      - read_only_emergency_mode: true
      - static_response_fallback: true
      - user_notification_critical: true

    success_criteria:
      - service_availability: 0.3  # Minimum viable service
      - data_integrity: true
      - recovery_coordination: true

  - name: "peak_load_with_service_degradation"
    description: "Test system under peak load with degraded services"
    experiments:
      - "api_gateway_overload"
      - "model_service_failure"
      - "database_slow_queries"

    cascade_delay: "1m"
    expected_behavior:
      - load_shedding_aggressive: true
      - priority_queue_activation: true
      - emergency_caching: true

    success_criteria:
      - critical_path_preserved: true
      - graceful_user_experience: true
      - system_stability: true

# Monitoring and observability
monitoring:
  dashboards:
    - name: "Chaos Experiment Dashboard"
      url: "/grafana/d/chaos-experiments"
      panels:
        - experiment_status
        - slo_compliance_during_chaos
        - service_health_matrix
        - recovery_time_trends

  alerts:
    - name: "experiment_slo_breach"
      condition: "slo_compliance < 0.90 during chaos"
      severity: "critical"

    - name: "recovery_time_exceeded"
      condition: "recovery_time > baseline.recovery_time"
      severity: "warning"

    - name: "unexpected_cascade_failure"
      condition: "failure_propagation > expected_scope"
      severity: "critical"

# Automated validation
validation:
  pre_experiment:
    - check_system_health: true
    - verify_monitoring_active: true
    - confirm_rollback_procedures: true
    - validate_blast_radius: true

  during_experiment:
    - monitor_slo_compliance: true
    - track_error_propagation: true
    - measure_recovery_metrics: true
    - observe_user_impact: true

  post_experiment:
    - verify_system_recovery: true
    - analyze_failure_patterns: true
    - update_runbooks: true
    - document_lessons_learned: true

# Game day scenarios
game_days:
  - name: "Black Friday Simulation"
    description: "High load + service failures during peak business hours"
    schedule: "quarterly"
    duration: "4h"

    scenarios:
      - peak_traffic_simulation: "10x normal load"
      - random_service_failures: "2-3 services"
      - database_performance_degradation: "50% slower"
      - network_intermittency: "5% packet loss"

    participants:
      - on_call_engineers
      - product_owners
      - customer_support
      - executive_stakeholders

    success_metrics:
      - incident_response_time: "<15m"
      - customer_impact_minimization: true
      - communication_effectiveness: true
      - business_continuity: true

  - name: "Regional Outage Drill"
    description: "Simulate complete regional infrastructure failure"
    schedule: "biannually"
    duration: "2h"

    scenarios:
      - primary_region_failure: "complete outage"
      - failover_to_secondary: "automated"
      - data_replication_validation: true
      - dns_switching: "automatic"

    participants:
      - infrastructure_team
      - database_administrators
      - security_team
      - business_continuity_team

# Failure injection tools
tools:
  kubernetes:
    - chaos_mesh
    - litmus
    - pumba

  network:
    - toxiproxy
    - comcast
    - tc (traffic control)

  application:
    - gremlin
    - chaos_monkey
    - custom_fault_injectors

# Safety mechanisms
safety:
  circuit_breakers:
    enabled: true
    failure_threshold: 5
    recovery_timeout: "30s"

  blast_radius_limits:
    max_affected_services: 3
    max_user_impact: 0.1  # 10% of users
    max_duration: "15m"

  emergency_stops:
    - manual_override: true
    - automated_slo_breach: true
    - cascading_failure_detection: true

  rollback_procedures:
    automatic: true
    manual_override: true
    timeout: "2m"

# Compliance and governance
governance:
  approval_required: true
  reviewers:
    - platform_team_lead
    - sre_manager
    - security_representative

  documentation:
    - experiment_plan: required
    - risk_assessment: required
    - rollback_plan: required
    - lessons_learned: required

  audit_trail:
    - experiment_execution_log: true
    - participant_actions: true
    - system_state_snapshots: true
    - recovery_procedures_used: true
