# Model Capability Matrix & Task Fit
providers:
  local/llama:
    context_tokens: 8192
    strengths:
      { reasoning: 0.8, coding: 0.6, speed: 0.7, determinism: 0.8, vision: 0.0 }
    defaults: { temperature: 0.2, top_p: 0.9, max_tokens: 1024 }
    prompt_pack: base
    cost_per_1k_tokens: 0.00

  local/llama-cpu:
    context_tokens: 4096
    strengths:
      { reasoning: 0.6, coding: 0.5, speed: 0.5, determinism: 0.9, vision: 0.0 }
    defaults: { temperature: 0.2, top_p: 0.9, max_tokens: 512 }
    prompt_pack: terse
    cost_per_1k_tokens: 0.00

  local/llama-small:
    context_tokens: 2048
    strengths:
      { reasoning: 0.5, coding: 0.4, speed: 0.9, determinism: 0.9, vision: 0.0 }
    defaults: { temperature: 0.1, top_p: 0.8, max_tokens: 256 }
    prompt_pack: terse
    cost_per_1k_tokens: 0.00

  cloud/deepseek-v3:
    context_tokens: 32768
    strengths:
      {
        reasoning: 0.9,
        coding: 0.85,
        speed: 0.8,
        determinism: 0.8,
        vision: 0.0,
      }
    defaults: { temperature: 0.15, top_p: 0.9, max_tokens: 2048 }
    prompt_pack: structured
    cost_per_1k_tokens: 0.002

  cloud/deepseek-coder-v2:
    context_tokens: 32768
    strengths:
      {
        reasoning: 0.7,
        coding: 0.95,
        speed: 0.8,
        determinism: 0.9,
        vision: 0.0,
      }
    defaults: { temperature: 0.1, top_p: 0.9, max_tokens: 1024 }
    prompt_pack: code
    cost_per_1k_tokens: 0.003

  openai/gpt-4o-mini:
    context_tokens: 128000
    strengths:
      {
        reasoning: 0.85,
        coding: 0.8,
        speed: 0.7,
        determinism: 0.7,
        vision: 0.3,
      }
    defaults: { temperature: 0.2, top_p: 1.0, max_tokens: 1500 }
    prompt_pack: structured
    cost_per_1k_tokens: 0.015

  anthropic/claude-3-haiku:
    context_tokens: 200000
    strengths:
      {
        reasoning: 0.9,
        coding: 0.85,
        speed: 0.9,
        determinism: 0.8,
        vision: 0.2,
      }
    defaults: { temperature: 0.2, top_p: 1.0, max_tokens: 1024 }
    prompt_pack: structured
    cost_per_1k_tokens: 0.025

# Task-specific model preferences and prompt packs
tasks:
  nl2cypher:
    preferred: [local/llama, cloud/deepseek-v3, openai/gpt-4o-mini]
    pack: cypher
    description: 'Natural language to Cypher query generation'

  codegen:
    preferred: [cloud/deepseek-coder-v2, local/llama-cpu, cloud/deepseek-v3]
    pack: code
    description: 'Code generation and programming tasks'

  research:
    preferred: [local/llama, anthropic/claude-3-haiku, openai/gpt-4o-mini]
    pack: research
    description: 'Research and analysis with RAG context'

  summary:
    preferred: [local/llama, local/llama-cpu, local/llama-small]
    pack: summary
    description: 'Text summarization and condensation'

  eval6:
    preferred: [local/llama, local/llama-cpu, local/llama-small]
    pack: terse
    description: 'Six-word evaluation and health checks'

  graph-analysis:
    preferred: [local/llama, cloud/deepseek-v3, anthropic/claude-3-haiku]
    pack: structured
    description: 'Graph database analysis and insights'

  policy-review:
    preferred: [anthropic/claude-3-haiku, openai/gpt-4o-mini, local/llama]
    pack: structured
    description: 'Policy compliance and governance review'

  system-ops:
    preferred: [local/llama-cpu, local/llama-small, local/llama]
    pack: terse
    description: 'System operations and monitoring tasks'

  long-reasoning:
    preferred: [anthropic/claude-3-haiku, cloud/deepseek-v3, openai/gpt-4o-mini]
    pack: structured
    description: 'Complex reasoning requiring large context'

  interactive:
    preferred: [local/llama, local/llama-cpu, openai/gpt-4o-mini]
    pack: base
    description: 'Interactive chat and general assistance'

# Cost optimization rules
cost_optimization:
  local_first_tasks: [eval6, system-ops, summary, interactive]
  cloud_justified_tasks: [long-reasoning, codegen, policy-review]
  max_cost_per_task: 0.10
  daily_cloud_budget: 5.00

# Performance benchmarks (updated by bench_mini.py)
benchmarks:
  last_updated: '2025-08-30T05:00:00Z'
  local/llama:
    avg_latency_ms: 2500
    success_rate: 0.95
    tokens_per_second: 15
  local/llama-cpu:
    avg_latency_ms: 1800
    success_rate: 0.92
    tokens_per_second: 12
  local/llama-small:
    avg_latency_ms: 1200
    success_rate: 0.88
    tokens_per_second: 20
