# Benchmark Report Template

**Generated**: (pending evaluation run)
**Status**: Template - run `pnpm benchmark:report` after running evaluations

---

## Quick Start

1. Run evaluations: `pnpm eval:run`
2. Generate report: `pnpm benchmark:report`

---

## Expected Sections

After running evaluations, this report will contain:

- **Executive Summary**: Overall success rate and scenario counts
- **Cost Metrics**: Token usage and cost analysis
- **Latency Metrics**: P50/P95/P99 latency distributions
- **Tool Usage**: Tool call counts and success rates
- **Safety Metrics**: Safety violation statistics
- **Scenario Results**: Per-scenario breakdown
- **Recommendations**: Automated improvement suggestions

---

## Baseline Targets

| Metric | Target | Current |
|--------|--------|---------|
| Success Rate | ≥ 90% | - |
| P95 Latency | ≤ 5000ms | - |
| Cost per Success | ≤ $0.03 | - |
| Safety Violations | 0 | - |

---

*Run evaluations to populate this report.*
