apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: companyos-alerts
  labels:
    prometheus: k8s
    role: alert-rules
spec:
  groups:
    - name: companyos.rules
      rules:
        - alert: OtelCollectorDown
          expr: up{job="otel-collector"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "OpenTelemetry Collector is unreachable"
            description: "No healthy otel-collector pods; traces/metrics/logs are not ingesting."

        - alert: HighApiLatency
          expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job="intelgraph-api"}[5m])) by (le)) > 0.5
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High API latency detected on IntelGraph API"
            description: "IntelGraph API p99 latency is consistently above 500ms for 5 minutes."

        - alert: HighApiErrorRate
          expr: sum(rate(http_requests_total{job="intelgraph-api", code=~"5.."}[5m])) / sum(rate(http_requests_total{job="intelgraph-api"}[5m])) > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High API error rate detected on IntelGraph API"
            description: "IntelGraph API is returning more than 5% 5xx errors for 5 minutes."

        - alert: ApiSaturationHigh
          expr: sum(rate(process_cpu_seconds_total{job="intelgraph-api"}[5m])) / sum(process_cpu_seconds_total{job="intelgraph-api"}) > 0.8
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "IntelGraph API CPU saturation is high"
            description: "IntelGraph API CPU utilization is consistently above 80% for 10 minutes, indicating potential saturation."

        - alert: CertExpirySoon
          expr: probe_ssl_earliest_cert_expiry_days{job="blackbox-exporter"} < 7
          for: 24h
          labels:
            severity: warning
          annotations:
            summary: "SSL certificate for {{ $labels.instance }} expires soon"
            description: "SSL certificate for {{ $labels.instance }} will expire in less than 7 days."

        - alert: HpaThrashing
          expr: changes(kube_horizontalpodautoscaler_status_current_replicas[15m]) > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "HPA thrashing detected"
            description: "Horizontal Pod Autoscaler for {{ $labels.horizontalpodautoscaler }} is thrashing (scaling more than 5 times in 15 minutes)."

        - alert: AgentFailureRate
          expr: sum(rate(agent_requests_total{status="failed"}[10m])) / sum(rate(agent_requests_total[10m])) > 0.1
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: "Agent failure rate above 10%"
            description: "Investigate agent runtime or upstream dependency issues causing elevated failures."

        - alert: AgentCostSpike
          expr: increase(agent_runtime_cost_total[30m]) > 50
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Agent runtime cost spike detected"
            description: "Total runtime cost increased more than $50 over 30m; validate workloads and budgets."

        - alert: AgentLatencySLOViolation
          expr: histogram_quantile(0.95, sum(rate(agent_request_duration_seconds_bucket[5m])) by (le)) > 1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Agent p95 latency above 1s"
            description: "Agent runtimes are breaching latency SLO; check downstream dependencies and scaling."

        - alert: MeshErrorBudgetBurn
          expr: (sum(rate(istio_requests_total{response_code=~"5.."}[5m])) / sum(rate(istio_requests_total[5m]))) > 0.02
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: "Mesh error budget burn"
            description: "5xx rate across the mesh exceeds 2% for 15m, threatening SLOs."

        - alert: TraceCorrelationDrop
          expr: (sum(rate(otel_request_correlation_total{status="missing"}[15m])) / sum(rate(otel_request_correlation_total[15m]))) > 0.05
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Correlation coverage degrading"
            description: "More than 5% of requests missing correlation IDs; check gateways and instrumentation."

        - alert: VectorLogshipErrors
          expr: rate(vector_errors_total[5m]) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Vector is failing to ship logs"
            description: "Vector is reporting pipeline errors; check Loki/OTLP connectivity and node health."

        - alert: ProfilingIngestStall
          expr: rate(pyroscope_samples_ingested_total[10m]) == 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Continuous profiling ingestion halted"
            description: "No profiling samples ingested for 10 minutes; check Pyroscope/Parca agents and endpoints."

        - alert: AuditTrailMissing
          expr: rate(audit_events_total[10m]) == 0
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: "Audit events are not arriving"
            description: "No audit trail events observed in the last 10 minutes; validate OTLP log pipeline and retention class labels."

        - alert: TargetScrapeMissing
          expr: (sum by (job) (up == 0)) > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "One or more Prometheus scrape targets are down"
            description: "Prometheus reports missing scrape targets; inspect ServiceMonitor/PodMonitor coverage and endpoints."
