apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: companyos-alerts
  labels:
    prometheus: k8s
    role: alert-rules
spec:
  groups:
    - name: companyos.rules
      rules:
        - alert: HighApiLatency
          expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job="intelgraph-api"}[5m])) by (le)) > 0.5
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: 'High API latency detected on IntelGraph API'
            description: 'IntelGraph API p99 latency is consistently above 500ms for 5 minutes.'

        - alert: HighApiErrorRate
          expr: sum(rate(http_requests_total{job="intelgraph-api", code=~"5.."}[5m])) / sum(rate(http_requests_total{job="intelgraph-api"}[5m])) > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: 'High API error rate detected on IntelGraph API'
            description: 'IntelGraph API is returning more than 5% 5xx errors for 5 minutes.'

        - alert: ApiSaturationHigh
          expr: sum(rate(process_cpu_seconds_total{job="intelgraph-api"}[5m])) / sum(process_cpu_seconds_total{job="intelgraph-api"}) > 0.8
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: 'IntelGraph API CPU saturation is high'
            description: 'IntelGraph API CPU utilization is consistently above 80% for 10 minutes, indicating potential saturation.'

        - alert: CertExpirySoon
          expr: probe_ssl_earliest_cert_expiry_days{job="blackbox-exporter"} < 7
          for: 24h
          labels:
            severity: warning
          annotations:
            summary: 'SSL certificate for {{ $labels.instance }} expires soon'
            description: 'SSL certificate for {{ $labels.instance }} will expire in less than 7 days.'

        - alert: HpaThrashing
          expr: changes(kube_horizontalpodautoscaler_status_current_replicas[15m]) > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: 'HPA thrashing detected'
            description: 'Horizontal Pod Autoscaler for {{ $labels.horizontalpodautoscaler }} is thrashing (scaling more than 5 times in 15 minutes).'

        - alert: AgentFailureRateHigh
          expr: sum(rate(agent_runs_total{status="failed"}[10m])) / sum(rate(agent_runs_total[10m])) > 0.1
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: 'Agent failure rate exceeds 10%'
            description: 'Investigate upstream model errors, timeouts, or dependency failures. Correlate with trace IDs from headers.'

        - alert: AgentCostSpike
          expr: rate(agent_cost_tokens_total[10m]) > (avg_over_time(agent_cost_tokens_total[6h]) * 1.5)
          for: 20m
          labels:
            severity: warning
          annotations:
            summary: 'Agent cost per 10m exceeded 150% of 6h baseline'
            description: 'Costs may spike due to prompt regressions or looping jobs. Validate prompts and rate limits.'

        - alert: MeshSloBreach
          expr: histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{reporter="source"}[5m])) by (le)) > 750
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: 'Mesh p99 latency SLO breach'
            description: 'Service mesh latency above 750ms p99 for 10 minutes; check upstream dependencies.'

        - alert: MeshErrorRate
          expr: sum(rate(istio_requests_total{response_code=~"5.."}[5m])) / sum(rate(istio_requests_total[5m])) > 0.03
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: 'Service mesh 5xx rate above 3%'
            description: 'Sustained 5xx rate across mesh. Correlate with otel trace IDs to identify failing service.'

        - alert: NodeResourcePressure
          expr: (avg(rate(container_cpu_usage_seconds_total{container!="",pod!=""}[5m])) by (node) > 0.8) or (avg(container_memory_working_set_bytes{container!="",pod!=""}) by (node) / avg(machine_memory_bytes) by (node) > 0.85)
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: 'Node resources under pressure'
            description: 'CPU or memory utilization is above thresholds for 15 minutes; consider scaling nodes or workloads.'

        - alert: OtelCollectorStuck
          expr: rate(otelcol_exporter_send_failed_spans{job="otel-collector"}[5m]) > 0
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: 'OTel collector is failing to export spans'
            description: 'Collector reports failed span exports. Check network to Tempo/Grafana Cloud.'

        - alert: ProfileIngestionStalled
          expr: rate(pyroscope_ingester_ingested_profiles_total[10m]) == 0
          for: 20m
          labels:
            severity: warning
          annotations:
            summary: 'Pyroscope ingestion stalled'
            description: 'No profiles ingested in the past 20 minutes; check parca-agent and Pyroscope service health.'
