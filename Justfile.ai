set shell := ["/bin/bash", "-cu"]

ai-models:
    bash tools/ai_models.sh

ai-up:
    bash tools/ollama_user_serve.sh
    bash tools/restart_proxy.sh

ai-restart:
    pkill -f 'litellm' || true
    sleep 0.5
    bash tools/restart_proxy.sh

ai-logs:
    tail -n 200 -f /tmp/litellm.log

ai-ping:
    bash tools/ping_llms.sh

# One quoted prompt
ask q='':
    bash tools/ai_ask.sh local/llama "{{q}}"

# Explicit model + one quoted prompt
ask-model MODEL='local/llama' q='':
    bash tools/ai_ask.sh "{{MODEL}}" "{{q}}"

# Exactly six words (post-processed)
ask6 q='':
    bash tools/ai_ask6.sh local/llama "{{q}}"

ask6-model MODEL='local/llama' q='':
    bash tools/ai_ask6.sh "{{MODEL}}" "{{q}}"

# Optional: pulls base + extras (requires tools/models_extra.sh)
models-refresh:
    @echo "Refreshing Ollama models..."
    @bash tools/models_extra.sh

# List models the proxy sees
models-list:
    curl -s http://127.0.0.1:4000/v1/models | jq .

# Cloud test examples
ask-cloud q='':
    bash tools/ai_ask.sh cloud/deepseek-v3 "{{q}}"

ask-cloud-code q='':
    bash tools/ai_ask.sh cloud/deepseek-coder-v2 "{{q}}"