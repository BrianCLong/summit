# Prometheus Golden Alert Tests
# Sprint 27: Validates alert rules for correctness

rule_files:
  - ../../ops/alerts/slo-burn-rules.yml
  - ../../ops/alerts/infrastructure.yml
  - ../../ops/alerts/application.yml

tests:
  # SLO Burn Rate Tests
  - interval: 1m
    input_series:
      # API request metrics
      - series: 'http_requests_total{job="api-server", status="200"}'
        values: '100+10x10'  # 100, 110, 120, ... for 10 minutes

      - series: 'http_requests_total{job="api-server", status="500"}'
        values: '5+1x10'     # 5, 6, 7, ... for 10 minutes (growing error rate)

      # NL→Cypher latency metrics
      - series: 'nlq_duration_seconds_bucket{job="gateway", le="2.0"}'
        values: '90+5x10'    # 90% under 2s initially, degrading

      - series: 'nlq_duration_seconds_bucket{job="gateway", le="+Inf"}'
        values: '100+10x10'  # Total requests

      # Database connection metrics
      - series: 'neo4j_connections_active{job="neo4j"}'
        values: '50+5x10'    # Growing connection count

      - series: 'neo4j_connections_max{job="neo4j"}'
        values: '100+0x10'   # Static max connections

      # Model orchestration budget
      - series: 'model_budget_spent_dollars{env="prod"}'
        values: '75+5x10'    # Growing towards budget limit

      - series: 'model_budget_limit_dollars{env="prod"}'
        values: '100+0x10'   # Static budget limit

    alert_rule_test:
      # High Error Rate Alert
      - eval_time: 5m
        alertname: HighErrorRate
        exp_alerts:
          - exp_labels:
              severity: warning
              job: api-server
            exp_annotations:
              summary: "High error rate detected"
              description: "Error rate is 10.00% over the last 5 minutes"

      # NL→Cypher SLO Burn
      - eval_time: 8m
        alertname: NLQLatencySLOBurn
        exp_alerts:
          - exp_labels:
              severity: critical
              service: gateway
            exp_annotations:
              summary: "NL→Cypher latency SLO burning fast"
              description: "95th percentile latency is above 2s threshold"

      # Database Connection Pool Exhaustion
      - eval_time: 7m
        alertname: DatabaseConnectionPoolHigh
        exp_alerts:
          - exp_labels:
              severity: warning
              instance: neo4j
            exp_annotations:
              summary: "Database connection pool usage high"
              description: "Connection pool is 95% full"

      # Model Budget Near Limit
      - eval_time: 9m
        alertname: ModelBudgetNearLimit
        exp_alerts:
          - exp_labels:
              severity: warning
              env: prod
            exp_annotations:
              summary: "Model budget approaching limit"
              description: "Model budget is 95% spent for environment prod"

  # Infrastructure Monitoring Tests
  - interval: 30s
    input_series:
      # CPU utilization
      - series: 'cpu_usage_percent{instance="api-1", job="node-exporter"}'
        values: '60+5x8'     # CPU climbing from 60% to 95%

      # Memory utilization
      - series: 'memory_usage_percent{instance="api-1", job="node-exporter"}'
        values: '70+3x8'     # Memory climbing from 70% to 91%

      # Disk space
      - series: 'disk_usage_percent{instance="api-1", mount="/"}'
        values: '80+2x8'     # Disk usage climbing

    alert_rule_test:
      # High CPU Alert
      - eval_time: 3m
        alertname: HighCPUUsage
        exp_alerts:
          - exp_labels:
              severity: warning
              instance: api-1
            exp_annotations:
              summary: "High CPU usage detected"

      # High Memory Alert
      - eval_time: 4m
        alertname: HighMemoryUsage
        exp_alerts:
          - exp_labels:
              severity: warning
              instance: api-1

  # Model Orchestration Safety Tests
  - interval: 1m
    input_series:
      # Model request failures
      - series: 'model_requests_total{provider="openai", model="gpt-4", status="success"}'
        values: '100+8x10'   # Successful requests

      - series: 'model_requests_total{provider="openai", model="gpt-4", status="error"}'
        values: '2+3x10'     # Increasing errors

      # Model response times
      - series: 'model_request_duration_seconds{provider="openai", model="gpt-4", quantile="0.95"}'
        values: '1.5+0.2x10' # Increasing latency

      # Circuit breaker state
      - series: 'circuit_breaker_state{provider="openai"}'
        values: '0+0x5 1+0x5'  # Closed for 5m, then open

    alert_rule_test:
      # Model Provider High Error Rate
      - eval_time: 8m
        alertname: ModelProviderHighErrorRate
        exp_alerts:
          - exp_labels:
              severity: critical
              provider: openai
              model: gpt-4
            exp_annotations:
              summary: "Model provider error rate high"
              description: "Error rate for openai/gpt-4 is above threshold"

      # Model Latency High
      - eval_time: 9m
        alertname: ModelLatencyHigh
        exp_alerts:
          - exp_labels:
              severity: warning
              provider: openai
              model: gpt-4

      # Circuit Breaker Open
      - eval_time: 7m
        alertname: CircuitBreakerOpen
        exp_alerts:
          - exp_labels:
              severity: critical
              provider: openai

  # Security and Policy Tests
  - interval: 1m
    input_series:
      # Failed authentication attempts
      - series: 'auth_failures_total{endpoint="/api/login"}'
        values: '5+10x6'     # Rapid increase in auth failures

      # Policy violations
      - series: 'policy_violations_total{policy="data-export", action="deny"}'
        values: '1+2x5'      # Growing policy violations

      # WebAuthn step-up challenges
      - series: 'webauthn_challenges_total{result="success"}'
        values: '50+5x10'    # Successful challenges

      - series: 'webauthn_challenges_total{result="failure"}'
        values: '2+1x10'     # Some failures

    alert_rule_test:
      # Authentication Failure Spike
      - eval_time: 5m
        alertname: AuthenticationFailureSpike
        exp_alerts:
          - exp_labels:
              severity: critical
              endpoint: /api/login
            exp_annotations:
              summary: "Authentication failure spike detected"

      # Policy Violation Increase
      - eval_time: 4m
        alertname: PolicyViolationIncrease
        exp_alerts:
          - exp_labels:
              severity: warning
              policy: data-export

  # Build and CI/CD Pipeline Tests
  - interval: 5m
    input_series:
      # Build duration
      - series: 'build_duration_seconds{pipeline="main", status="success"}'
        values: '300+30x4'   # Build times increasing

      - series: 'build_duration_seconds{pipeline="main", status="failure"}'
        values: '0+0x3 180+0x1'  # One failure at end

      # Deployment frequency
      - series: 'deployments_total{env="prod", status="success"}'
        values: '10+1x12'    # Regular deployments

      # Test coverage
      - series: 'test_coverage_percent{suite="unit"}'
        values: '85-1x8'     # Coverage dropping

    alert_rule_test:
      # Build Time Regression
      - eval_time: 15m
        alertname: BuildTimeRegression
        exp_alerts:
          - exp_labels:
              severity: warning
              pipeline: main

      # Test Coverage Drop
      - eval_time: 20m
        alertname: TestCoverageDropped
        exp_alerts:
          - exp_labels:
              severity: warning
              suite: unit