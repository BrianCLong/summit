# Alertmanager Configuration for IntelGraph
# Routes alerts to PagerDuty, Slack, and email based on severity

global:
  resolve_timeout: 5m
  slack_api_url: '${SLACK_WEBHOOK_URL}'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

route:
  receiver: 'default-receiver'
  group_by: ['alertname', 'service', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  routes:
    # Critical alerts -> PagerDuty immediately
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 10s
      repeat_interval: 1h
      continue: true

    # Critical alerts also go to Slack #incidents
    - match:
        severity: critical
      receiver: 'slack-incidents'
      continue: true

    # Warning alerts -> Slack #alerts channel
    - match:
        severity: warning
      receiver: 'slack-alerts'
      group_wait: 1m
      repeat_interval: 4h

    # SLO violations get special handling
    - match_re:
        slo: '.+'
      receiver: 'slack-slo'
      group_by: ['slo', 'service']
      continue: true

    # Canary deployment alerts -> immediate action
    - match:
        deployment: canary
        action: abort
      receiver: 'pagerduty-canary'
      group_wait: 0s
      repeat_interval: 5m

receivers:
  - name: 'default-receiver'
    slack_configs:
      - channel: '#monitoring'
        send_resolved: true
        title: '{{ .Status | toUpper }} - {{ .CommonLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        severity: critical
        description: '{{ .CommonAnnotations.summary }}'
        details:
          service: '{{ .CommonLabels.service }}'
          alertname: '{{ .CommonLabels.alertname }}'
          runbook: '{{ .CommonLabels.runbook }}'

  - name: 'pagerduty-canary'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_CANARY_KEY}'
        severity: critical
        description: 'CANARY ABORT: {{ .CommonAnnotations.summary }}'
        client: 'IntelGraph Canary Monitor'

  - name: 'slack-incidents'
    slack_configs:
      - channel: '#incidents'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        title: ':rotating_light: {{ .CommonLabels.alertname }}'
        title_link: '{{ .CommonAnnotations.runbook }}'
        text: |
          *Severity:* {{ .CommonLabels.severity }}
          *Service:* {{ .CommonLabels.service }}
          {{ range .Alerts }}
          {{ .Annotations.description }}
          {{ end }}
        actions:
          - type: button
            text: 'Runbook'
            url: '{{ .CommonAnnotations.runbook }}'
          - type: button
            text: 'Dashboard'
            url: 'https://grafana.intelgraph.ai/d/intelgraph-slo-production'

  - name: 'slack-alerts'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        color: 'warning'
        title: ':warning: {{ .CommonLabels.alertname }}'
        text: '{{ .CommonAnnotations.summary }}'

  - name: 'slack-slo'
    slack_configs:
      - channel: '#slo-alerts'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        title: ':chart_with_downwards_trend: SLO Violation - {{ .CommonLabels.slo }}'
        text: |
          *Service:* {{ .CommonLabels.service }}
          *SLO:* {{ .CommonLabels.slo }}
          {{ range .Alerts }}{{ .Annotations.description }}{{ end }}

inhibit_rules:
  # Don't alert on warnings if critical is firing for same service
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']

  # Don't alert on canary if production is down
  - source_match:
      deployment: 'production'
      severity: 'critical'
    target_match:
      deployment: 'canary'
    equal: ['service']
