# Prometheus alerting rules for IntelGraph
groups:
  - name: intelgraph.rules
    rules:
      # High-level service availability
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: "{{ $labels.job }} service at {{ $labels.instance }} has been down for more than 1 minute."

      # High error rate
      - alert: HighErrorRate
        expr: |
          (
            rate(http_requests_total{status_code=~"5.."}[5m]) /
            rate(http_requests_total[5m])
          ) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }} at {{ $labels.instance }}"

      # High response time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s for {{ $labels.job }}"

      # Memory usage alerts
      - alert: HighMemoryUsage
        expr: |
          (
            process_resident_memory_bytes / 
            (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes + process_resident_memory_bytes)
          ) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }} for {{ $labels.job }}"

      # CPU usage alerts
      - alert: HighCPUUsage
        expr: cpu_usage_percent > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }}% for {{ $labels.job }}"

      # Database connection issues
      - alert: DatabaseConnectionFailure
        expr: db_connections_active == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Database connection failure"
          description: "No active database connections for {{ $labels.database }}"

      # AI/ML job queue buildup
      - alert: AIJobQueueBacklog
        expr: ai_jobs_queued > 100
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "AI job queue backlog"
          description: "{{ $value }} AI jobs queued for {{ $labels.job_type }}"

      # AI/ML job processing time
      - alert: LongRunningAIJob
        expr: |
          histogram_quantile(0.95, rate(ai_job_duration_seconds_bucket[10m])) > 600
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Long-running AI jobs detected"
          description: "95th percentile AI job duration is {{ $value }}s for {{ $labels.job_type }}"

      # GraphQL error rate
      - alert: HighGraphQLErrorRate
        expr: |
          (
            rate(graphql_errors_total[5m]) /
            rate(graphql_requests_total[5m])
          ) > 0.05
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "High GraphQL error rate"
          description: "GraphQL error rate is {{ $value | humanizePercentage }}"

      # WebSocket connection issues
      - alert: WebSocketConnectionDrop
        expr: |
          decrease(websocket_connections_active[5m]) > 50
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Large WebSocket connection drop"
          description: "{{ $value }} WebSocket connections dropped in the last 5 minutes"

      # Disk space alerts
      - alert: LowDiskSpace
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/",fstype!="tmpfs"} /
            node_filesystem_size_bytes{mountpoint="/",fstype!="tmpfs"}
          ) < 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Low disk space"
          description: "Disk space is below 10% on {{ $labels.instance }}"

      # Redis connection issues
      - alert: RedisConnectionFailure
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis connection failure"
          description: "Redis is down at {{ $labels.instance }}"

      # Neo4j connection issues
      - alert: Neo4jConnectionFailure
        expr: neo4j_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Neo4j connection failure"
          description: "Neo4j is down at {{ $labels.instance }}"

      # GraphQL resolver latency
      - alert: HighResolverLatency
        expr: histogram_quantile(0.95, rate(graphql_resolver_duration_seconds_bucket[5m])) > 0.5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High GraphQL resolver latency"
          description: "95th percentile resolver duration is {{ $value }}s"

      # Slow Neo4j queries
      - alert: SlowNeo4jQueries
        expr: histogram_quantile(0.95, rate(neo4j_query_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow Neo4j queries detected"
          description: "95th percentile Neo4j query duration is {{ $value }}s"

      # Low cache hit ratio
      - alert: LowCacheHitRatio
        expr: (rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))) < 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Cache hit ratio below 80%"
          description: "Cache hit ratio is {{ $value | humanizePercentage }}"

      # Slow LLM responses
      - alert: SlowLLMResponses
        expr: histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LLM response time high"
          description: "95th percentile LLM call duration is {{ $value }}s"

      # High HTTP round-trip time
      - alert: HighRTT
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High HTTP round-trip time"
          description: "95th percentile HTTP RTT is {{ $value }}s"

      # Data refresh lag
      - alert: DataRefreshLag
        expr: data_refresh_lag_seconds > 60
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Data refresh lag exceeds 60s"
          description: "Data refresh lag is {{ $value }} seconds"

      # Tenant-specific slow query detection
      - alert: TenantSlowQuery
        expr: histogram_quantile(0.99, rate(neo4j_query_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow queries for tenant {{ $labels.tenant }}"
          description: "99th percentile query duration is {{ $value }}s"

      # Container memory pressure
      - alert: ContainerMemoryPressure
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Memory pressure on {{ $labels.pod }}"
          description: "Container using {{ $value | humanizePercentage }} of its memory limit"

      # GNN prediction failures
      - alert: GNNPredictionFailures
        expr: increase(gnn_prediction_failures_total[5m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "GNN prediction failures detected"
          description: "{{ $value }} failures in the last 5 minutes"

  - name: intelgraph.ml.rules
    rules:
      # ML model loading failures
      - alert: MLModelLoadingFailure
        expr: increase(models_loaded_total{status="error"}[10m]) > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "ML model loading failure"
          description: "{{ $value }} ML model loading failures for {{ $labels.model_type }}"

      # ML prediction accuracy drop
      - alert: MLAccuracyDrop
        expr: ml_model_accuracy < 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "ML model accuracy drop"
          description: "{{ $labels.model_type }} accuracy dropped to {{ $value }}"

      # GPU utilization issues
      - alert: LowGPUUtilization
        expr: gpu_utilization_percent < 10 and on() hour() >= 9 and on() hour() <= 17
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "Low GPU utilization during business hours"
          description: "GPU {{ $labels.gpu_id }} utilization is only {{ $value }}% during business hours"

      # GPU memory issues
      - alert: HighGPUMemoryUsage
        expr: |
          (
            gpu_memory_usage_bytes{type="used"} /
            gpu_memory_usage_bytes{type="total"}
          ) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High GPU memory usage"
          description: "GPU {{ $labels.gpu_id }} memory usage is {{ $value | humanizePercentage }}"
