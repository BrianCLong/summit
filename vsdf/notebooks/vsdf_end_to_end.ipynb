{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7416d6eb",
   "metadata": {},
   "source": [
    "# VSDF End-to-End Walkthrough\n",
    "\n",
    "This notebook demonstrates how to learn constraints from a tabular dataset, generate synthetic samples, and verify fidelity and privacy guarantees using the Verifiable Synthetic Data Forge (VSDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc92b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vsdf import (\n",
    "    SchemaLearner,\n",
    "    ConstraintSpecification,\n",
    "    ConstraintCompiler,\n",
    "    ConstraintDrivenSampler,\n",
    "    ConstraintVerifier,\n",
    ")\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small reference dataset\n",
    "reference = pd.DataFrame({\n",
    "    'age': [25, 32, 40, 28, 36, 52, 47, 30, 45, 38],\n",
    "    'income': [50000, 62000, 58000, 52000, 61000, 75000, 68000, 54000, 72000, 59000],\n",
    "    'segment': ['A', 'B', 'A', 'A', 'B', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'city': ['Denver', 'Denver', 'Boulder', 'Denver', 'Boulder', 'Denver', 'Boulder', 'Denver', 'Boulder', 'Denver'],\n",
    "})\n",
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c08fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn schema metadata and compile constraints\n",
    "schema = SchemaLearner().learn(reference)\n",
    "spec = ConstraintSpecification(\n",
    "    marginal_columns=['segment', 'city'],\n",
    "    correlation_pairs=[('age', 'income')],\n",
    "    marginal_tolerance=0.1,\n",
    "    correlation_tolerance=0.1,\n",
    "    denial_predicates=['age < 21 and income > 60000'],\n",
    "    denial_tolerance=0.0,\n",
    "    dp_epsilon=8.0,\n",
    ")\n",
    "compiler = ConstraintCompiler(schema)\n",
    "constraints = compiler.learn(reference, spec)\n",
    "constraints.dp_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data under the compiled constraints\n",
    "sampler = ConstraintDrivenSampler(schema, constraints, random_state=42)\n",
    "synthetic = sampler.sample(200)\n",
    "synthetic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe616ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify fidelity and privacy metrics\n",
    "verifier = ConstraintVerifier(constraints, privacy_threshold=0.05)\n",
    "report = verifier.verify(synthetic, reference)\n",
    "report.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10e50d",
   "metadata": {},
   "source": [
    "The verifier reports constraint adherence metrics, including marginal distribution distances, correlation deltas, denial-constraint violation rates, and the estimated privacy risk (re-identification propensity). Adjust the tolerances or DP budget to tighten or loosen the acceptance criteria."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
